# DGX Spark ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

Nvidia DGX Spark (ARM + GB10 GPU) í™˜ê²½ì„ ìœ„í•œ SPLADE-doc í•™ìŠµ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ–¥ï¸ ì‹œìŠ¤í…œ í™˜ê²½

**í™•ì¸ëœ ì‚¬ì–‘**:
- **CPU**: ARM64 (aarch64)
- **GPU**: NVIDIA GB10 (Blackwell ì•„í‚¤í…ì²˜)
- **VRAM**: 119.70 GB
- **CUDA**: 13.0
- **cuDNN**: 91300
- **Python**: 3.12.3
- **PyTorch**: 2.10.0 (dev/nightly, CUDA 13.0)

**ìµœì í™” ê¸°ëŠ¥**:
- âœ… BF16 mixed precision (Blackwell GPU ìµœì í™”)
- âœ… ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì‚¬ì´ì¦ˆ (119GB VRAM í™œìš©)
- âœ… ARM64 ë„¤ì´í‹°ë¸Œ ì§€ì›
- âœ… ìë™ mixed precision training

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1ë‹¨ê³„: í™˜ê²½ í™œì„±í™”

```bash
# venv í™œì„±í™”
source .venv/bin/activate

# GPU í™˜ê²½ í…ŒìŠ¤íŠ¸
python test_dgx_setup.py
```

**ì˜ˆìƒ ì¶œë ¥**:
```
======================================================================
Testing SPLADE-doc on Nvidia DGX Spark (ARM + GB10)
======================================================================

[1/5] GPU Information
  GPU: NVIDIA GB10
  CUDA Version: 13.0
  BF16 Support: True
  Total VRAM: 119.70 GB

[2/5] Loading SPLADE-doc model
  âœ“ Model loaded: 178,444,801 parameters

...

âœ“ All tests passed! DGX setup is ready for training.
```

---

### 2ë‹¨ê³„: ë² ì´ìŠ¤ë¼ì¸ í•™ìŠµ (ê¶Œì¥ - ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)

**2-1. ë°ì´í„° ì¤€ë¹„ (10K samples)**

```bash
# Korean Wikipedia (5K) + NamuWiki (5K) ìƒ˜í”Œë§
python scripts/prepare_baseline_data.py
```

**ì˜ˆìƒ ì¶œë ¥**:
```
======================================================================
Preparing Baseline Training Data (10K samples)
======================================================================

[1/4] Loading Korean Wikipedia data
  Total Korean Wikipedia: 600,000 samples

[2/4] Loading NamuWiki data
  Total NamuWiki: 1,500,000 samples

[3/4] Sampling data
  Sampled 5,000 from Korean Wikipedia
  Sampled 5,000 from NamuWiki
  Total samples: 10,000

[4/4] Splitting into train/val
  Train: 9,000 samples
  Val: 1,000 samples

âœ“ Baseline data preparation complete!
```

**2-2. í•™ìŠµ ì‹¤í–‰**

```bash
# BF16 mixed precisionìœ¼ë¡œ ë² ì´ìŠ¤ë¼ì¸ í•™ìŠµ
python train.py --config configs/baseline_dgx.yaml
```

**í•™ìŠµ ì„¤ì •**:
- Batch size: 16
- Gradient accumulation: 2 (effective batch = 32)
- Epochs: 3
- Mixed precision: BF16
- ì˜ˆìƒ ì‹œê°„: ~10ë¶„ (GB10 GPU)

**ì˜ˆìƒ ì¶œë ¥**:
```
================================================================================
Starting training
================================================================================
Epochs: 3
Batch size: 16
Gradient accumulation: 2
================================================================================

Epoch 1/3
Training (Step 0): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 563/563 [03:45<00:00]
Validation loss: 2.3456

...

================================================================================
Training complete!
Best validation loss: 2.1234
================================================================================
```

---

### 3ë‹¨ê³„: ëŒ€ê·œëª¨ Pre-training (Production)

**3-1. ë°ì´í„° í™•ì¸**

ë¨¼ì € notebook 01ì—ì„œ ìƒì„±í•œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸:

```bash
ls -lh dataset/paired_data_split/
```

**í•„ìš”í•œ ë°ì´í„°**:
- `ko_wiki_*_train_*.jsonl` - Korean Wikipedia (~600K articles)
- `namuwiki_*_train_*.jsonl` - NamuWiki (~1.5M articles)
- `modu_*_train_*.jsonl` - ëª¨ë‘ì˜ ë§ë­‰ì¹˜
- `en_wiki_*_train_*.jsonl` - English Wikipedia

**3-2. Pre-training ì‹¤í–‰**

```bash
# ì „ì²´ ë°ì´í„°ë¡œ pre-training
python train.py --config configs/pretrain_korean_dgx.yaml
```

**í•™ìŠµ ì„¤ì •**:
- Batch size: 32 (GB10ì˜ 119GB VRAM í™œìš©)
- Gradient accumulation: 2 (effective batch = 64)
- Epochs: 10
- Mixed precision: BF16
- Learning rate: 2e-5
- ì˜ˆìƒ ì‹œê°„: ìˆ˜ ì‹œê°„ ~ 1ì¼ (ë°ì´í„° ê·œëª¨ì— ë”°ë¼)

**ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ìœ„ì¹˜**:
```
outputs/pretrain_korean_dgx/
â”œâ”€â”€ best_model/
â”‚   â””â”€â”€ checkpoint.pt
â”œâ”€â”€ epoch_1/
â”‚   â””â”€â”€ checkpoint.pt
â”œâ”€â”€ epoch_2/
â”‚   â””â”€â”€ checkpoint.pt
...
â””â”€â”€ training_log.jsonl
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### í•™ìŠµ ë¡œê·¸ í™•ì¸

```bash
# ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸
tail -f outputs/baseline_dgx/training_log.jsonl

# ë˜ëŠ” pretrain ë¡œê·¸
tail -f outputs/pretrain_korean_dgx/training_log.jsonl
```

### GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§

```bash
# ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰
watch -n 1 nvidia-smi
```

**ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©**:
- Baseline (batch=16): ~8-12 GB
- Pre-training (batch=32): ~20-30 GB
- ì—¬ìœ  VRAM: ~90-100 GB (119GB ì¤‘)

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. CUDA Out of Memory

ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ì¤„ì´ì„¸ìš”:

```yaml
# configs/baseline_dgx.yaml or configs/pretrain_korean_dgx.yaml
data:
  batch_size: 16  # 32 â†’ 16ìœ¼ë¡œ ì¤„ì´ê¸°
```

### 2. DataLoader ì˜¤ë¥˜

ë°ì´í„° íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° notebook 01ì„ ë¨¼ì € ì‹¤í–‰:

```bash
jupyter notebook notebooks/pretraining-neural-sparse-model/01_wikipedia_data_extraction.ipynb
```

### 3. ARM í˜¸í™˜ì„± ê²½ê³ 

GB10 GPUëŠ” Compute Capability 12.1ì´ì§€ë§Œ PyTorchëŠ” 12.0ê¹Œì§€ë§Œ ê³µì‹ ì§€ì›í•©ë‹ˆë‹¤.
ì´ëŠ” **ì •ìƒ**ì´ë©° ëŒ€ë¶€ë¶„ì˜ ê¸°ëŠ¥ì€ ì‘ë™í•©ë‹ˆë‹¤.

### 4. BF16 ì˜¤ë¥˜

BF16ì´ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš° FP16ìœ¼ë¡œ ë³€ê²½:

```yaml
# configs/*.yaml
training:
  mixed_precision: "fp16"  # "bf16" â†’ "fp16"
```

---

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™” íŒ

### 1. ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¦ê°€

GB10ì˜ 119GB VRAMì„ í™œìš©í•˜ì—¬ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ëŠ˜ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```yaml
data:
  batch_size: 64  # 32 â†’ 64
training:
  gradient_accumulation_steps: 1  # 2 â†’ 1
```

### 2. ë©€í‹° GPU (ë¯¸ë˜)

ì—¬ëŸ¬ GPUê°€ ìˆëŠ” ê²½ìš° PyTorch DDP ì‚¬ìš© ê°€ëŠ¥:

```bash
# ì˜ˆì‹œ (ë¯¸êµ¬í˜„)
torchrun --nproc_per_node=2 train.py --config configs/pretrain_korean_dgx.yaml
```

### 3. Gradient Checkpointing

ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•œ ê²½ìš° í™œì„±í™”:

```yaml
training:
  gradient_checkpointing: true
```

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

### 1. MS MARCO Fine-tuning

Pre-training ì™„ë£Œ í›„ MS MARCOë¡œ fine-tuning:

```bash
python train.py --config configs/finetune_msmarco.yaml
```

### 2. BEIR í‰ê°€

í•™ìŠµëœ ëª¨ë¸ í‰ê°€:

```bash
python evaluate.py --model outputs/pretrain_korean_dgx/best_model
```

### 3. OpenSearch ë°°í¬

ëª¨ë¸ì„ OpenSearchì— ì—…ë¡œë“œí•˜ì—¬ ì‹¤ì œ ê²€ìƒ‰ ì„œë¹„ìŠ¤ì— ì‚¬ìš©

---

## ğŸ¯ ìš”ì•½

**ë² ì´ìŠ¤ë¼ì¸ í•™ìŠµ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)**:
```bash
source .venv/bin/activate
python scripts/prepare_baseline_data.py
python train.py --config configs/baseline_dgx.yaml
```

**ëŒ€ê·œëª¨ Pre-training**:
```bash
source .venv/bin/activate
python train.py --config configs/pretrain_korean_dgx.yaml
```

**DGX Spark ìµœì í™” í¬ì¸íŠ¸**:
- âœ… BF16 mixed precision
- âœ… Large batch sizes (32-64)
- âœ… ARM64 native support
- âœ… 119GB VRAM utilization

Happy Training! ğŸš€

# OpenSearch Neural Sparse Pre-training

Korean-English cross-lingual SPLADE-doc model for OpenSearch retrieval.

## Project Overview

This project implements **SPLADE-doc** (Sparse Lexical and Expansion model - Document-only mode), an inference-free learned sparse retrieval model optimized for Korean-English bilingual search.

### Training Pipeline

Complete end-to-end pipeline from data collection to model training:

**Data Collection (Notebooks 01-05)**:
- **01**: Wikipedia data extraction (Korean ~600K articles, English ~6M articles)
- **02**: Synonym extraction from Wikipedia
- **03**: Pre-training data preparation (S2ORC, WikiAnswers, GOOAQ)
- **04**: Hard negative mining with BM25
- **05**: MS MARCO fine-tuning data

**Model Training**:
- **06**: Baseline training (10K sampled pairs from Korean Wikipedia + NamuWiki)
- **train.py**: Production-scale training script with full dataset
- **configs/**: YAML configurations for pre-training and fine-tuning

### Training Data Scale

- **Korean Wikipedia**: ~600,000 articles (title-summary, title-paragraph pairs)
- **NamuWiki**: ~1,500,000 articles (Korean encyclopedia)
- **ëª¨ë‘ì˜ ë§ë­‰ì¹˜**: Korean corpus for enhanced language understanding
- **English Wikipedia**: ~6,000,000 articles (for bilingual capability)
- **S2ORC, WikiAnswers, GOOAQ**: Additional pre-training corpora
- **MS MARCO**: Fine-tuning dataset for ranking optimization

### Architecture

- **Base Model**: `bert-base-multilingual-cased` (Korean + English support)
- **Token Importance Prediction**: log(1 + ReLU(Â·)) for sparsity
- **Sparse Representation**: Max pooling over token positions
- **Loss Functions**: InfoNCE + FLOPS regularization + IDF penalty + Knowledge Distillation
- **Training Strategy**: Pre-training on Korean/English data â†’ Fine-tuning on MS MARCO

### ğŸŒŸ v0.3.0 ì£¼ìš” ê°œì„ ì‚¬í•­

#### âœ… **ì†ì‹¤ í•¨ìˆ˜ ìˆ˜ì • (CRITICAL)**
- âŒ **ì´ì „**: Binary Cross-Entropy (BCE) - dot productì™€ ê·¼ë³¸ì ìœ¼ë¡œ ë¶ˆì¼ì¹˜
- âœ… **ê°œì„ **: In-batch Negatives Contrastive Loss - ì˜¬ë°”ë¥¸ ranking í•™ìŠµ

#### âœ… **ì‹œê°„ ê¸°ë°˜ ë¶„ì„ (NEW)**
- ë‰´ìŠ¤ ë°ì´í„°ì˜ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ ë° í™œìš©
- Temporal IDF: ìµœê·¼ ë¬¸ì„œì— ë†’ì€ ê°€ì¤‘ì¹˜ (exponential decay)
- **ìë™ íŠ¸ë Œë“œ ê°ì§€**: ìˆ˜ë™ TREND_BOOST ë”•ì…”ë„ˆë¦¬ ì œê±°

#### âœ… **Hard Negative Mining (NEW)**
- BM25 ê¸°ë°˜ intelligent negative sampling
- ëœë¤ negative ëŒ€ë¹„ ë” íš¨ê³¼ì ì¸ í•™ìŠµ

#### âœ… **ë¹„ì§€ë„ ë™ì˜ì–´ ë°œê²¬ (NEW)**
- ì‹œê°„ ê°€ì¤‘ì¹˜ ê¸°ë°˜ í† í° ì„ë² ë”© êµ°ì§‘í™”
- K-means/DBSCAN/Hierarchical clustering ì§€ì›
- ì™„ì „ ìë™í™”ëœ ë™ì˜ì–´ ê·¸ë£¹ ìƒì„±

### í•µì‹¬ íŠ¹ì§•

- âœ… **Inference-Free**: ì¿¼ë¦¬ ì¸ì½”ë”©ì— ëª¨ë¸ inference ë¶ˆí•„ìš” (BM25ì™€ ìœ ì‚¬í•œ ì§€ì—°ì‹œê°„)
- âœ… **í•œêµ­ì–´ ìµœì í™”**: KLUE-BERT ê¸°ë°˜ + í•œêµ­ì–´ ë‰´ìŠ¤/QA ë°ì´í„°ì…‹
- âœ… **ì‹œê°„ ê°€ì¤‘ì¹˜ IDF**: ìµœê·¼ ë¬¸ì„œ ìš°ì„ , íŠ¸ë Œë“œ ìë™ ê°ì§€
- âœ… **ë¹„ì§€ë„ í•™ìŠµ**: ìˆ˜ë™ ë ˆì´ë¸” ì—†ì´ ë™ì˜ì–´ ë°œê²¬
- âœ… **OpenSearch í˜¸í™˜**: ë°”ë¡œ ë°°í¬ ê°€ëŠ¥í•œ í˜•ì‹ (`pytorch_model.bin`, `idf.json`)
- âœ… **Amazon Linux 2023**: EC2ì—ì„œ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
opensearch-neural-pre-train/
â”œâ”€â”€ configs/                             # âš™ï¸ Training configurations
â”‚   â”œâ”€â”€ pretrain_korean.yaml             # Pre-training on Korean data
â”‚   â””â”€â”€ finetune_msmarco.yaml            # Fine-tuning on MS MARCO
â”‚
â”œâ”€â”€ dataset/                             # ğŸ“Š Data storage
â”‚   â”œâ”€â”€ paired_data_split/               # Train/val/test split data
â”‚   â”œâ”€â”€ synonyms/                        # Korean-English synonyms
â”‚   â”œâ”€â”€ wikipedia/                       # Wikipedia raw data
â”‚   â”œâ”€â”€ pretraining/                     # S2ORC, GOOAQ, WikiAnswers
â”‚   â”œâ”€â”€ hard_negatives/                  # BM25-mined hard negatives
â”‚   â””â”€â”€ msmarco/                         # MS MARCO triples
â”‚
â”œâ”€â”€ models/                              # ğŸ¤– Trained models (gitignored)
â”‚   â””â”€â”€ [saved models here]
â”‚
â”œâ”€â”€ notebooks/                           # ğŸ““ Jupyter notebooks
â”‚   â”œâ”€â”€ pretraining-neural-sparse-model/ # SPLADE-doc training workflow
â”‚   â”‚   â”œâ”€â”€ 01_wikipedia_data_extraction.ipynb  # Wikipedia data extraction
â”‚   â”‚   â”œâ”€â”€ 02_synonym_extraction.ipynb         # Synonym extraction
â”‚   â”‚   â”œâ”€â”€ 03_model_pretraining.ipynb         # Pre-training data prep
â”‚   â”‚   â”œâ”€â”€ 04_hard_negative_mining.ipynb      # BM25 hard negatives
â”‚   â”‚   â”œâ”€â”€ 05_msmarco_preparation.ipynb       # MS MARCO fine-tuning data
â”‚   â”‚   â””â”€â”€ 06_model_training_baseline.ipynb   # Baseline training (10K samples)
â”‚   â””â”€â”€ legacy/                          # Legacy notebooks
â”‚
â”œâ”€â”€ outputs/                             # ğŸ“¤ Training outputs
â”‚   â”œâ”€â”€ best_model/                      # Best checkpoint
â”‚   â””â”€â”€ final_model/                     # Final model
â”‚
â”œâ”€â”€ scripts/                             # ğŸš€ Executable scripts
â”‚   â”œâ”€â”€ train_small_scale.py             # Small-scale test training
â”‚   â””â”€â”€ train_full_scale.py              # Full-scale training
â”‚
â”œâ”€â”€ src/                                 # ğŸ’» Source code
â”‚   â”œâ”€â”€ data/                            # Data processing
â”‚   â”‚   â”œâ”€â”€ wikipedia_parser.py          # Wikipedia XML parser
â”‚   â”‚   â”œâ”€â”€ synonym_extractor.py         # Synonym extraction
â”‚   â”‚   â”œâ”€â”€ paired_data_generator.py     # (Query, Document) pair generation
â”‚   â”‚   â””â”€â”€ dataset.py                   # PyTorch dataset loaders
â”‚   â”œâ”€â”€ model/                           # SPLADE-doc model architecture
â”‚   â”‚   â”œâ”€â”€ splade_model.py              # SPLADE-doc implementation
â”‚   â”‚   â””â”€â”€ losses.py                    # Loss functions (InfoNCE, FLOPS, IDF, KD)
â”‚   â””â”€â”€ training/                        # Training infrastructure (legacy)
â”‚       â”œâ”€â”€ losses.py
â”‚       â”œâ”€â”€ data_collator.py
â”‚       â””â”€â”€ trainer.py
â”‚
â”œâ”€â”€ tests/                               # ğŸ§ª Test scripts
â”‚   â””â”€â”€ test_training_pipeline.py
â”‚
â”œâ”€â”€ train.py                             # ğŸš€ Production training script
â”œâ”€â”€ Makefile                             # ğŸ“¦ Build automation for easy training
â”œâ”€â”€ test_dgx_setup.py                    # ğŸ§ª DGX environment test
â”œâ”€â”€ DGX_QUICKSTART.md                    # ğŸ“˜ DGX Spark quick start guide
â”œâ”€â”€ plan.md                              # ğŸ“‹ Project plan
â””â”€â”€ README.md                            # ğŸ“„ This file
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### âš¡ Makefileì„ ì‚¬ìš©í•œ ê°„í¸ ì‹¤í–‰ (ê¶Œì¥)

í”„ë¡œì íŠ¸ì— Makefileì´ í¬í•¨ë˜ì–´ ìˆì–´ í•œ ì¤„ ëª…ë ¹ìœ¼ë¡œ ëª¨ë“  ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
# ë„ì›€ë§ ë³´ê¸°
make help

# ë¹ ë¥¸ ì‹œì‘ (í™˜ê²½ í…ŒìŠ¤íŠ¸ + ë°ì´í„° ì¤€ë¹„ + ë² ì´ìŠ¤ë¼ì¸ í•™ìŠµ)
make quickstart

# ë˜ëŠ” ë‹¨ê³„ë³„ ì‹¤í–‰
make setup              # í™˜ê²½ í…ŒìŠ¤íŠ¸
make prepare-baseline   # ë² ì´ìŠ¤ë¼ì¸ ë°ì´í„° ì¤€ë¹„ (10K samples)
make train-baseline     # ë² ì´ìŠ¤ë¼ì¸ í•™ìŠµ (~10ë¶„)
make train-pretrain     # ëŒ€ê·œëª¨ pre-training

# ëª¨ë‹ˆí„°ë§ ë° ë¡œê·¸
make monitor           # GPU ì‚¬ìš©ë¥  ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
make logs-baseline     # ë² ì´ìŠ¤ë¼ì¸ í•™ìŠµ ë¡œê·¸ í™•ì¸
make logs-pretrain     # Pre-training ë¡œê·¸ í™•ì¸

# ìœ í‹¸ë¦¬í‹°
make info              # ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸
make clean             # ì¶œë ¥ íŒŒì¼ ì •ë¦¬
make notebook          # Jupyter ë…¸íŠ¸ë¶ ì‹œì‘
```

**Makefile ì£¼ìš” íƒ€ê²Ÿ**:

| ëª…ë ¹ì–´ | ì„¤ëª… | ì˜ˆìƒ ì‹œê°„ |
|--------|------|----------|
| `make quickstart` | ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (setup â†’ prepare â†’ train) | ~15ë¶„ |
| `make prepare-baseline` | 10K ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„ | ~1ë¶„ |
| `make train-baseline` | ë² ì´ìŠ¤ë¼ì¸ í•™ìŠµ (BF16, batch=32) | ~10ë¶„ |
| `make train-pretrain` | ëŒ€ê·œëª¨ pre-training (ì „ì²´ ë°ì´í„°) | ìˆ˜ ì‹œê°„ |
| `make monitor` | GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§ | - |
| `make info` | ì‹œìŠ¤í…œ ë° ì„¤ì • ì •ë³´ ì¶œë ¥ | <1ì´ˆ |

---

### Option 1: Baseline Training (ê¶Œì¥ - ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)

```bash
# Jupyter ë…¸íŠ¸ë¶ìœ¼ë¡œ ë² ì´ìŠ¤ë¼ì¸ í•™ìŠµ (10K samples)
jupyter notebook notebooks/pretraining-neural-sparse-model/06_model_training_baseline.ipynb
```

**íŠ¹ì§•**:
- Korean Wikipedia (5K) + NamuWiki (5K) ìƒ˜í”Œë§
- 3 epochs, ~10ë¶„ í•™ìŠµ ì‹œê°„ (GPU)
- ì „ì²´ íŒŒì´í”„ë¼ì¸ ì´í•´ì— ìµœì 

### Option 2: Production Training (ëŒ€ê·œëª¨ í•™ìŠµ)

```bash
# 1ë‹¨ê³„: Pre-training on Korean data
python train.py --config configs/pretrain_korean.yaml

# 2ë‹¨ê³„: Fine-tuning on MS MARCO
python train.py --config configs/finetune_msmarco.yaml
```

**íŠ¹ì§•**:
- Full dataset: Korean Wikipedia (~600K) + NamuWiki (~1.5M) + ëª¨ë‘ì˜ë§ë­‰ì¹˜
- Multi-GPU ì§€ì›
- Checkpoint ì €ì¥ ë° ì¬ê°œ ê°€ëŠ¥

### Training Pipeline ì „ì²´ ì‹¤í–‰

```bash
# 1. Data Collection (notebooks 01-05)
jupyter notebook notebooks/pretraining-neural-sparse-model/01_wikipedia_data_extraction.ipynb
jupyter notebook notebooks/pretraining-neural-sparse-model/02_synonym_extraction.ipynb
# ... (03, 04, 05)

# 2. Model Training
python train.py --config configs/pretrain_korean.yaml

# 3. Evaluation on BEIR
python evaluate.py --model outputs/pretrain_korean/best_model
```

### Nvidia DGX Spark (ARM + GB10 GPU) - ê¶Œì¥ í™˜ê²½

**âœ¨ DGX Sparkì— ìµœì í™”ëœ ì„¤ì • ì œê³µ!**

**ë°©ë²• 1: Makefile ì‚¬ìš© (ê°€ì¥ ê°„í¸)**

```bash
# ì „ì²´ íŒŒì´í”„ë¼ì¸ í•œ ë²ˆì— ì‹¤í–‰
make quickstart

# ë˜ëŠ” ê°œë³„ ì‹¤í–‰
make setup              # í™˜ê²½ í…ŒìŠ¤íŠ¸
make prepare-baseline   # ë°ì´í„° ì¤€ë¹„
make train-baseline     # ë² ì´ìŠ¤ë¼ì¸ í•™ìŠµ
make train-pretrain     # ëŒ€ê·œëª¨ í•™ìŠµ
```

**ë°©ë²• 2: ì§ì ‘ ì‹¤í–‰**

```bash
# 1. venv í™œì„±í™”
source .venv/bin/activate

# 2. GPU í™˜ê²½ í…ŒìŠ¤íŠ¸
python test_dgx_setup.py

# 3. ë² ì´ìŠ¤ë¼ì¸ ë°ì´í„° ì¤€ë¹„ (10K samples)
python scripts/prepare_baseline_data.py

# 4. ë² ì´ìŠ¤ë¼ì¸ í•™ìŠµ (BF16, ~10ë¶„)
python train.py --config configs/baseline_dgx.yaml

# 5. ëŒ€ê·œëª¨ pre-training
python train.py --config configs/pretrain_korean_dgx.yaml
```

**DGX ìµœì í™”**:
- âœ… BF16 mixed precision (Blackwell ì•„í‚¤í…ì²˜ ìµœì í™”)
- âœ… ëŒ€ìš©ëŸ‰ ë°°ì¹˜ (batch_size=32, 119GB VRAM í™œìš©)
- âœ… ARM64 ì•„í‚¤í…ì²˜ ì§€ì›
- âœ… CUDA 13.0 + cuDNN 91300
- âœ… PyTorch 2.10 (dev/nightly)

---

### ARM ì‹œìŠ¤í…œ (Apple Silicon, ARM ì„œë²„)

**âš ï¸ ARM ì‚¬ìš©ìëŠ” [ARM_INSTALL.md](ARM_INSTALL.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”!**

```bash
# Python 3.10+ venv ìƒì„±
python3 -m venv .venv
source .venv/bin/activate

# ARM í˜¸í™˜ ìµœì†Œ ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements-minimal.txt

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python test_korean_neural_sparse.py
```

**ì£¼ìš” ì°¨ì´ì **:
- âœ… mecab/konlpy ë¶ˆí•„ìš” (BERT tokenizer ì‚¬ìš©)
- âœ… ëª¨ë“  í•µì‹¬ ê¸°ëŠ¥ ì‘ë™
- âœ… ê°„í¸í•œ ì„¤ì¹˜

---

### Amazon Linux 2023 / x86_64

#### 1. ìë™ ì„¤ì¹˜ (ê¶Œì¥)

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone <repository-url>
cd opensearch-neural-pre-train

# ìë™ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
chmod +x setup_amazon_linux_2023.sh
./setup_amazon_linux_2023.sh

# ê°€ìƒ í™˜ê²½ í™œì„±í™”
source ~/opensearch-neural-env/bin/activate
```

### 2. ê°„ë‹¨í•œ ë°ëª¨ ì‹¤í–‰

```bash
# ì˜ì¡´ì„±ì´ ê±°ì˜ ì—†ëŠ” IDF ë°ëª¨
python3 demo_idf_korean.py
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```
âœ“ 96ê°œ í† í°ì˜ IDF ê³„ì‚° ì™„ë£Œ
âœ“ 5ê°œ í† í°ì— íŠ¸ë Œë“œ ë¶€ìŠ¤íŒ… ì ìš©
  - neural: 3.08 â†’ 4.00 (1.3x)
  - sparse: 3.08 â†’ 4.00 (1.3x)

Query: 'OpenSearch neural sparse ê²€ìƒ‰'
  1. [Score: 16.03] Neural sparse ê²€ìƒ‰ì€ í¬ì†Œ ë²¡í„°ë¥¼ ì‚¬ìš©...
```

### 3. ì „ì²´ ëª¨ë¸ í•™ìŠµ

```bash
# PyTorch ê¸°ë°˜ ì „ì²´ í•™ìŠµ í…ŒìŠ¤íŠ¸
python tests/test_korean_neural_sparse.py

# ì‹œê°„ ê¸°ë°˜ ë¶„ì„ í…ŒìŠ¤íŠ¸
python tests/test_temporal_features.py

# í•œì˜ ë™ì˜ì–´ í…ŒìŠ¤íŠ¸
python tests/test_bilingual_synonyms.py
```

ë˜ëŠ” **Jupyter ë…¸íŠ¸ë¶** (ê¶Œì¥):

```bash
# v0.3.0 ì „ì²´ ê¸°ëŠ¥ í¬í•¨ ë²„ì „ (ê¶Œì¥)
jupyter notebook notebooks/korean_neural_sparse_training_v0.3.0.ipynb

# ë˜ëŠ” ì›ë³¸ ë²„ì „
jupyter notebook notebooks/korean_neural_sparse_training.ipynb
```

## ğŸ“Š OpenSearch ëª¨ë¸ êµ¬ì¡°

### Doc-only Mode (Inference-Free)

```
ë¬¸ì„œ ì¸ì½”ë”© (ì¸ë±ì‹± íƒ€ì„)
  Document â†’ BERT Encoder â†’ Sparse Vector (rank_features)
  â†“
  OpenSearch Indexì— ì €ì¥

ì¿¼ë¦¬ ì¸ì½”ë”© (ê²€ìƒ‰ íƒ€ì„ - ë§¤ìš° ë¹ ë¦„!)
  Query â†’ Tokenizer â†’ IDF Lookup â†’ Sparse Vector
  â†“
  Dot Product Similarity
  â†“
  ê²€ìƒ‰ ê²°ê³¼
```

### í•µì‹¬ íŒŒì¼

1. **`pytorch_model.bin`** - BERT ê¸°ë°˜ ë¬¸ì„œ ì¸ì½”ë”
2. **`idf.json`** - í† í°ë³„ ê°€ì¤‘ì¹˜ lookup table (ì¿¼ë¦¬ìš©)
3. **`tokenizer.json`, `vocab.txt`** - BERT tokenizer
4. **`config.json`** - ëª¨ë¸ ì„¤ì •

## ğŸ“ í•™ìŠµ ë°©ë²•

### ì†ì‹¤ í•¨ìˆ˜

1. **Ranking Loss**: Query-Document similarity (BCE)
2. **IDF-aware Penalty**: ë‚®ì€ IDF í† í° ì–µì œ
3. **L0 Regularization**: Sparsity ìœ ì§€ (FLOPS penalty)

```python
total_loss = ranking_loss + Î»_l0 * l0_loss + Î»_idf * idf_penalty
```

### í•™ìŠµ ë°ì´í„°

- **KLUE**: í•œêµ­ì–´ ì´í•´ í‰ê°€ ë²¤ì¹˜ë§ˆí¬ (MRC, STS ë“±)
- **KorQuAD**: í•œêµ­ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°ì…‹
- **Korean Wikipedia**: í•œêµ­ì–´ ìœ„í‚¤í”¼ë””ì•„
- **Korean News**: ë‰´ìŠ¤ ë°ì´í„°ì…‹

### íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…

```python
TREND_BOOST = {
    'LLM': 1.5,
    'GPT': 1.5,
    'ChatGPT': 1.5,
    'RAG': 1.4,
    'neural': 1.3,
    'sparse': 1.3,
    # ...
}
```

## ğŸ”§ OpenSearch í†µí•©

### 1. ëª¨ë¸ ì €ì¥

í•™ìŠµ ì™„ë£Œ í›„ ìƒì„±ë˜ëŠ” íŒŒì¼ë“¤ (`models/` ë””ë ‰í† ë¦¬):

```
models/
â””â”€â”€ opensearch-korean-neural-sparse-v1/
    â”œâ”€â”€ pytorch_model.bin       # ë¬¸ì„œ ì¸ì½”ë”
    â”œâ”€â”€ idf.json                # ì¿¼ë¦¬ìš© ê°€ì¤‘ì¹˜
    â”œâ”€â”€ tokenizer.json
    â”œâ”€â”€ vocab.txt
    â”œâ”€â”€ config.json
    â””â”€â”€ README.md
```

### 2. OpenSearch ì—…ë¡œë“œ

```bash
# ëª¨ë¸ ì••ì¶•
cd models/opensearch-korean-neural-sparse-v1
zip -r ../korean-neural-sparse-v1.zip .

# OpenSearchì— ì—…ë¡œë“œ
POST /_plugins/_ml/models/_upload
{
  "name": "korean-neural-sparse-v1",
  "version": "1.0",
  "model_format": "TORCH_SCRIPT",
  "model_config": {
    "model_type": "bert",
    "embedding_dimension": 30000,
    "framework_type": "sentence_transformers",
    "all_config": {
      "mode": "doc-only"
    }
  }
}
```

### 3. ì¸ë±ìŠ¤ ìƒì„±

```json
PUT /korean-docs
{
  "mappings": {
    "properties": {
      "content": { "type": "text" },
      "embedding": { "type": "rank_features" }
    }
  }
}
```

### 4. ê²€ìƒ‰ ì‹¤í–‰

```json
POST /korean-docs/_search
{
  "query": {
    "neural_sparse": {
      "embedding": {
        "query_text": "í•œêµ­ì–´ ê²€ìƒ‰ ìµœì í™”",
        "model_id": "<model_id>"
      }
    }
  }
}
```

## ğŸ’» EC2 ì¸ìŠ¤í„´ìŠ¤ ê¶Œì¥ì‚¬í•­

| ìš©ë„ | ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… | vCPU | ë©”ëª¨ë¦¬ | ë¹„ìš©/ì‹œê°„ | í•™ìŠµ ì‹œê°„ |
|------|--------------|------|--------|----------|-----------|
| ê°œë°œ/í…ŒìŠ¤íŠ¸ | t3.xlarge | 4 | 16GB | $0.16 | ~45ë¶„ |
| ë¹ ë¥¸ ê°œë°œ | t3.2xlarge | 8 | 32GB | $0.33 | ~25ë¶„ |
| GPU í•™ìŠµ | g4dn.xlarge | 4 | 16GB | $0.53 | ~8ë¶„ |
| ê³ ì† GPU | g5.xlarge | 4 | 16GB | $1.01 | ~5ë¶„ |

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ê²€ìƒ‰ ì§€ì—°ì‹œê°„

- **BM25**: 10ms (ê¸°ì¤€)
- **Neural Sparse (Doc-only)**: 11ms (1.1x)
- **Dense Retrieval**: 50-100ms (5-10x)

### ê²€ìƒ‰ ì •í™•ë„ (BEIR)

- **BM25**: NDCG@10 = 0.45
- **Neural Sparse**: NDCG@10 = 0.58 (+13%)
- **Dense Retrieval**: NDCG@10 = 0.62

### í¬ì†Œì„± (Sparsity)

- í‰ê·  99.5% sparse (vocab_size: 30,000)
- Non-zero tokens: 50-150ê°œ
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì 

## ğŸ” ì‚¬ìš© ì˜ˆì‹œ

### Pythonì—ì„œ ì§ì ‘ ì‚¬ìš©

```python
from transformers import AutoTokenizer
import torch
import json

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("./models/opensearch-korean-neural-sparse-v1")

# IDF ë¡œë“œ
with open("./models/opensearch-korean-neural-sparse-v1/idf.json") as f:
    idf_dict = json.load(f)

# ì¿¼ë¦¬ ì¸ì½”ë”© (Inference-Free!)
def encode_query(query_text):
    tokens = tokenizer.encode(query_text, add_special_tokens=False)
    sparse_vec = {}
    for token_id in tokens:
        token_str = tokenizer.decode([token_id])
        if token_str in idf_dict:
            sparse_vec[token_str] = idf_dict[token_str]
    return sparse_vec

# ê²€ìƒ‰
query_vec = encode_query("í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬")
print(query_vec)
# {'í•œêµ­ì–´': 3.08, 'ìì—°ì–´': 2.39, 'ì²˜ë¦¬': 2.67}
```

## ğŸ› ë¬¸ì œ í•´ê²°

### Mecab ì„¤ì¹˜ ì˜¤ë¥˜

```bash
sudo ldconfig
export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH
pip3 install mecab-python3
```

### PyTorch ë©”ëª¨ë¦¬ ë¶€ì¡±

```python
# ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¤„ì´ê¸°
BATCH_SIZE = 8  # ê¸°ë³¸ 16

# ë°ì´í„° ìƒ˜í”Œë§
train_data = train_data[:5000]
```

### CUDA ì˜¤ë¥˜ (GPU ì‚¬ìš© ì‹œ)

```bash
# CUDA ë²„ì „ í™•ì¸
nvidia-smi

# PyTorch ì¬ì„¤ì¹˜
pip3 uninstall torch
pip3 install torch --index-url https://download.pytorch.org/whl/cu118
```

## ğŸ“š ì°¸ê³  ìë£Œ

### í”„ë¡œì íŠ¸ ê°€ì´ë“œ

- **[Makefile ì‚¬ìš© ê°€ì´ë“œ](MAKEFILE_GUIDE.md)** - Makefile ëª…ë ¹ì–´ ì™„ì „ ê°€ì´ë“œ
- **[DGX Spark ë¹ ë¥¸ ì‹œì‘](DGX_QUICKSTART.md)** - DGX Spark í™˜ê²½ ìµœì í™” ê°€ì´ë“œ

### OpenSearch ê³µì‹ ë¬¸ì„œ

- [Neural Sparse Search](https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/)
- [Doc-only Mode](https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/#doc-only-mode)
- [ML Commons Plugin](https://opensearch.org/docs/latest/ml-commons-plugin/)

### ë…¼ë¬¸

- [Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers](https://arxiv.org/abs/2411.04403)
- [Exploring â„“0 Sparsification for Inference-free Sparse Retrievers](https://arxiv.org/abs/2501.xxxxx)

### Hugging Face Collection

- [opensearch-project/inference-free-ir-model](https://huggingface.co/collections/opensearch-project/inference-free-ir-model)

### í•œêµ­ì–´ NLP

- [KLUE Benchmark](https://github.com/KLUE-benchmark/KLUE)
- [KorQuAD](https://korquad.github.io/)
- [KoNLPy](https://konlpy.org/)

## ğŸ¤ ê¸°ì—¬

ë²„ê·¸ ë¦¬í¬íŠ¸, ê¸°ëŠ¥ ìš”ì²­, PR í™˜ì˜í•©ë‹ˆë‹¤!

## ğŸ“„ ë¼ì´ì„¼ìŠ¤

MIT License

## ğŸ‘¥ ê°œë°œì

OpenSearch Korean Neural Sparse Model Team

---

**ğŸ‰ DGX Sparkì—ì„œ ì‹œì‘í•˜ê¸°**:

```bash
# í•œ ì¤„ ëª…ë ¹ìœ¼ë¡œ ì „ì²´ ì‹¤í–‰
make quickstart

# ë˜ëŠ” ë‹¨ê³„ë³„ ì‹¤í–‰
make help  # ëª¨ë“  ëª…ë ¹ì–´ í™•ì¸
```

ìì„¸í•œ ë‚´ìš©ì€ **[Makefile ê°€ì´ë“œ](MAKEFILE_GUIDE.md)** ì°¸ì¡°!

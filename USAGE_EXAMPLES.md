# src ëª¨ë“ˆ ì‚¬ìš© ì˜ˆì œ

`src/` íŒ¨í‚¤ì§€ì˜ ëª¨ë“  í•¨ìˆ˜ë¥¼ ì‰½ê²Œ importí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“¦ ê°„í¸í•œ Import

### ë°©ë²• 1: íŒ¨í‚¤ì§€ì—ì„œ ì§ì ‘ import (ê¶Œì¥)

```python
from src import (
    # Data loading
    load_korean_news_with_dates,

    # Temporal analysis
    calculate_temporal_idf,
    detect_trending_tokens,

    # Loss functions
    neural_sparse_loss_with_regularization,

    # Cross-lingual
    build_comprehensive_bilingual_dictionary,
)
```

### ë°©ë²• 2: ëª¨ë“ˆë³„ import

```python
from src.losses import neural_sparse_loss_with_regularization
from src.temporal_analysis import calculate_temporal_idf
from src.cross_lingual_synonyms import build_comprehensive_bilingual_dictionary
```

## ğŸ¯ ì‚¬ìš© ì˜ˆì œ

### 1. ì‹œê°„ ê¸°ë°˜ IDF ê³„ì‚°

```python
from transformers import AutoTokenizer
from src import (
    load_korean_news_with_dates,
    calculate_temporal_idf,
    detect_trending_tokens,
    build_trend_boost_dict,
)

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")

# ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ (ë‚ ì§œ í¬í•¨)
news_data = load_korean_news_with_dates(max_samples=10000)

# Temporal IDF ê³„ì‚°
idf_token_dict, idf_id_dict = calculate_temporal_idf(
    documents=news_data['documents'],
    dates=news_data['dates'],
    tokenizer=tokenizer,
    decay_factor=0.95,  # ìµœê·¼ ë¬¸ì„œì— ë†’ì€ ê°€ì¤‘ì¹˜
)

# íŠ¸ë Œë”© í† í° ìë™ ê°ì§€
trending_tokens = detect_trending_tokens(
    documents=news_data['documents'],
    dates=news_data['dates'],
    tokenizer=tokenizer,
    recent_days=30,
    top_k=100,
)

print(f"ë°œê²¬ëœ íŠ¸ë Œë”© í† í°: {len(trending_tokens)}")
for token_info in trending_tokens[:10]:
    print(f"  {token_info['token']}: {token_info['trend_score']:.2f}x")
```

### 2. í•œì˜ í†µí•© ë™ì˜ì–´ ì‚¬ì „

```python
from transformers import AutoTokenizer, AutoModelForMaskedLM
from src import (
    build_comprehensive_bilingual_dictionary,
    get_default_korean_english_pairs,
    apply_bilingual_synonyms_to_idf,
)

# ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")
model = AutoModelForMaskedLM.from_pretrained("klue/bert-base")

# ìƒ˜í”Œ ë¬¸ì„œ
documents = [
    "ë”¥ëŸ¬ë‹ ëª¨ë¸(model)ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.",
    "ê²€ìƒ‰(search) ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.",
    "BERT ëª¨ë¸ì€ transformer ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
]

# ê¸°ë³¸ í•œì˜ ìŒ ê°€ì ¸ì˜¤ê¸°
manual_pairs = get_default_korean_english_pairs()
print(f"ìˆ˜ë™ ì •ì˜ëœ ìŒ: {len(manual_pairs)}")

# í¬ê´„ì ì¸ bilingual ì‚¬ì „ êµ¬ì¶•
bilingual_dict = build_comprehensive_bilingual_dictionary(
    documents=documents,
    token_embeddings=model.bert.embeddings.word_embeddings.weight.detach().cpu().numpy(),
    tokenizer=tokenizer,
    bert_model=model.bert,
    manual_pairs=manual_pairs,
)

print(f"ì „ì²´ bilingual ì‚¬ì „: {len(bilingual_dict)} í•­ëª©")

# IDFì— ì ìš©
enhanced_idf = apply_bilingual_synonyms_to_idf(
    idf_dict=idf_token_dict,
    bilingual_dict=bilingual_dict,
    tokenizer=tokenizer,
)

# ì´ì œ 'ëª¨ë¸'ê³¼ 'model'ì´ ë™ì¼í•œ IDF ê°’ì„ ê°€ì§
print(f"'ëª¨ë¸' IDF: {enhanced_idf.get('ëª¨ë¸', 0):.4f}")
print(f"'model' IDF: {enhanced_idf.get('model', 0):.4f}")
```

### 3. ê°œì„ ëœ Loss Function ì‚¬ìš©

```python
import torch
from src import neural_sparse_loss_with_regularization, compute_sparsity_metrics

# ê°€ìƒì˜ sparse vectors
doc_sparse = torch.randn(32, 30000).relu()  # batch_size=32, vocab_size=30000
query_sparse = torch.randn(32, 30000).relu()
relevance = torch.ones(32)  # ëª¨ë‘ relevant

# IDF dictionary (token â†’ IDF score)
idf_dict = {i: 2.5 for i in range(30000)}

# Loss ê³„ì‚° (in-batch negatives í¬í•¨)
total_loss, loss_components = neural_sparse_loss_with_regularization(
    doc_sparse=doc_sparse,
    query_sparse=query_sparse,
    relevance=relevance,
    idf_dict=idf_dict,
    lambda_l0=5e-4,
    lambda_idf=1e-2,
    temperature=0.05,
    use_in_batch_negatives=True,  # í•µì‹¬ ê°œì„ !
)

print(f"Total Loss: {total_loss.item():.4f}")
print(f"Contrastive Loss: {loss_components['contrastive_loss']:.4f}")
print(f"L0 Regularization: {loss_components['l0_loss']:.4f}")

# Sparsity ë©”íŠ¸ë¦­ í™•ì¸
sparsity_metrics = compute_sparsity_metrics(doc_sparse)
print(f"Sparsity: {sparsity_metrics['sparsity']:.2%}")
print(f"Non-zero elements: {sparsity_metrics['num_nonzero']:.0f}")
```

### 4. Hard Negative Mining

```python
from src import add_hard_negatives_bm25, add_mixed_negatives

# Query-Document ìŒ
qd_pairs = [
    {"query": "ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ", "pos_doc": "PyTorchë¡œ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤", "relevance": 1},
    {"query": "ê²€ìƒ‰ ì‹œìŠ¤í…œ", "pos_doc": "OpenSearch ê²€ìƒ‰ ì—”ì§„", "relevance": 1},
]

# ì „ì²´ ë¬¸ì„œ í’€
documents = [
    "PyTorchë¡œ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤",
    "OpenSearch ê²€ìƒ‰ ì—”ì§„",
    "Kerasë¥¼ ì‚¬ìš©í•œ ë”¥ëŸ¬ë‹",
    "Elasticsearch ì„¤ì • ë°©ë²•",
    "ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ì†Œê°œ",
]

# BM25 ê¸°ë°˜ Hard Negatives ì¶”ê°€
augmented_pairs = add_hard_negatives_bm25(
    qd_pairs=qd_pairs,
    documents=documents,
    tokenizer=tokenizer,
    num_hard_negatives=2,  # ê° ì¿¼ë¦¬ë‹¹ 2ê°œì˜ hard negative
    top_k=100,
)

print(f"ì›ë³¸ ìŒ: {len(qd_pairs)}")
print(f"ì¦ê°•ëœ ìŒ: {len(augmented_pairs)}")

# í˜¼í•© ì „ëµ (random + hard negatives)
mixed_pairs = add_mixed_negatives(
    qd_pairs=qd_pairs,
    documents=documents,
    tokenizer=tokenizer,
    num_random=1,
    num_hard=2,
)
```

### 5. ì‹œê°„ ê¸°ë°˜ ë™ì˜ì–´ ë°œê²¬

```python
from src import (
    discover_synonyms_temporal,
    merge_synonym_dictionaries,
    filter_synonyms_by_frequency,
)

# í† í° ì„ë² ë”©ìœ¼ë¡œ ë™ì˜ì–´ ë°œê²¬
synonyms = discover_synonyms_temporal(
    documents=news_data['documents'],
    dates=news_data['dates'],
    token_embeddings=model.bert.embeddings.word_embeddings.weight.detach().cpu().numpy(),
    tokenizer=tokenizer,
    method='kmeans',
    n_clusters=500,
)

print(f"ë°œê²¬ëœ ë™ì˜ì–´ ê·¸ë£¹: {len(synonyms)}")

# ë¹ˆë„ ê¸°ì¤€ í•„í„°ë§
filtered_synonyms = filter_synonyms_by_frequency(
    synonym_dict=synonyms,
    documents=news_data['documents'],
    tokenizer=tokenizer,
    min_frequency=10,  # ìµœì†Œ 10ë²ˆ ì¶œí˜„
)

print(f"í•„í„°ë§ í›„: {len(filtered_synonyms)} ê·¸ë£¹")

# ì˜ˆì œ ì¶œë ¥
for term, synonym_list in list(filtered_synonyms.items())[:5]:
    print(f"\n{term}:")
    for syn in synonym_list[:5]:
        print(f"  - {syn}")
```

## ğŸ“š ì „ì²´ API ëª©ë¡

### Loss Functions (5ê°œ)
- `in_batch_negatives_loss`
- `margin_ranking_loss`
- `contrastive_loss_with_hard_negatives`
- `neural_sparse_loss_with_regularization`
- `compute_sparsity_metrics`

### Data Loading (4ê°œ)
- `load_korean_news_with_dates`
- `load_multiple_korean_datasets`
- `create_time_windows`
- `get_recent_documents`

### Temporal Analysis (6ê°œ)
- `calculate_temporal_idf`
- `calculate_windowed_idf`
- `detect_trending_tokens`
- `build_trend_boost_dict`
- `apply_temporal_boost_to_idf`
- `analyze_token_frequency_over_time`

### Negative Sampling (4ê°œ)
- `add_hard_negatives_bm25`
- `add_random_negatives`
- `add_mixed_negatives`
- `balance_positive_negative_ratio`

### Temporal Clustering (5ê°œ)
- `cluster_tokens_temporal`
- `build_synonym_groups_from_clusters`
- `discover_synonyms_temporal`
- `merge_synonym_dictionaries`
- `filter_synonyms_by_frequency`

### Cross-lingual Synonyms (5ê°œ)
- `extract_bilingual_terms`
- `discover_cross_lingual_synonyms_by_embedding`
- `build_comprehensive_bilingual_dictionary`
- `get_default_korean_english_pairs`
- `apply_bilingual_synonyms_to_idf`

## ğŸ“ ë” ë§ì€ ì˜ˆì œ

ì „ì²´ ì˜ˆì œëŠ” ë‹¤ìŒ íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”:
- [tests/test_korean_neural_sparse.py](tests/test_korean_neural_sparse.py) - Loss function ì˜ˆì œ
- [tests/test_temporal_features.py](tests/test_temporal_features.py) - ì‹œê°„ ë¶„ì„ ì˜ˆì œ
- [tests/test_bilingual_synonyms.py](tests/test_bilingual_synonyms.py) - í•œì˜ ë™ì˜ì–´ ì˜ˆì œ
- [notebooks/korean_neural_sparse_training_v0.3.0.ipynb](notebooks/korean_neural_sparse_training_v0.3.0.ipynb) - ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸

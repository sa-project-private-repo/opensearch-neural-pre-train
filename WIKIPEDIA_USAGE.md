# Korean Wikipedia Data Loading Guide

ì´ ë¬¸ì„œëŠ” ìµœì‹  í•œêµ­ì–´ Wikipedia ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“š ë‘ ê°€ì§€ ë°ì´í„° ì†ŒìŠ¤

### Option 1: HuggingFace Dataset (ê¸°ë³¸, ê¶Œì¥)
- **ë‚ ì§œ**: 2023-11-01
- **ì¥ì **: ë¹ ë¦„, ì•ˆì •ì 
- **ë‹¨ì **: 2ë…„ ì „ ë°ì´í„°
- **ì‚¬ìš© ì‹œê¸°**: í…ŒìŠ¤íŠ¸, ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘

### Option 2: Latest Wikimedia Dump
- **ë‚ ì§œ**: 2025-11-01 (ìµœì‹ )
- **ì¥ì **: ê°€ì¥ ìµœì‹  ë°ì´í„°
- **ë‹¨ì **: ì²« ì‹¤í–‰ì‹œ ëŠë¦¼ (20-60ë¶„)
- **ì‚¬ìš© ì‹œê¸°**: í”„ë¡œë•ì…˜, ìµœì‹  ë°ì´í„° í•„ìš”

## ğŸš€ ì‚¬ìš© ë°©ë²•

### ë…¸íŠ¸ë¶ì—ì„œ ì‚¬ìš©

```python
from src.wikipedia_loader import load_korean_wikipedia

# Option 1: ë¹ ë¥¸ ë¡œë”© (HuggingFace, 2023 ë°ì´í„°)
docs = load_korean_wikipedia(
    max_documents=100000,
    use_latest=False  # ë¹ ë¦„
)

# Option 2: ìµœì‹  ë°ì´í„° (ì²« ì‹¤í–‰ì‹œ ëŠë¦¼, ì´í›„ ìºì‹œë¨)
docs = load_korean_wikipedia(
    max_documents=100000,
    use_latest=True  # ìµœì‹ , ìºì‹œ ì§€ì›
)
```

### ì»¤ë§¨ë“œ ë¼ì¸ì—ì„œ ì‚¬ìš©

```bash
# í…ŒìŠ¤íŠ¸: 100ê°œ ë¬¸ì„œë§Œ
python download_latest_wikipedia.py --test

# ì „ì²´: 100,000ê°œ ë¬¸ì„œ
python download_latest_wikipedia.py --max-docs 100000

# ë‹¤ìš´ë¡œë“œë§Œ (ë‚˜ì¤‘ì— íŒŒì‹±)
python download_latest_wikipedia.py --max-docs 0

# íŒŒì‹±ë§Œ (ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ê²½ìš°)
python download_latest_wikipedia.py --skip-download --max-docs 100000
```

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

| ì˜µì…˜ | ì²« ì‹¤í–‰ ì‹œê°„ | ì´í›„ ì‹¤í–‰ | ë°ì´í„° ë‚ ì§œ | ìš©ëŸ‰ |
|------|-------------|----------|------------|------|
| HuggingFace | ~5-10ë¶„ | ~30ì´ˆ | 2023-11-01 | ~400MB |
| Latest Dump | ~20-60ë¶„ | ~30ì´ˆ | 2025-11-01 | ~1-2GB |

## ğŸ’¾ ìºì‹±

Latest Dump ì‚¬ìš©ì‹œ ìë™ ìºì‹±:
- **ìœ„ì¹˜**: `dataset/wikipedia_dumps/`
- **í˜•ì‹**: JSON (ì••ì¶• í•´ì œë¨)
- **ì¬ì‚¬ìš©**: ë‘ ë²ˆì§¸ ì‹¤í–‰ë¶€í„° ì¦‰ì‹œ ë¡œë“œ

ìºì‹œ ì‚­ì œ:
```bash
rm -rf dataset/wikipedia_dumps/
```

## ğŸ”§ ê³ ê¸‰ ì‚¬ìš©ë²•

### ìºì‹œ ê°•ì œ ê°±ì‹ 
```python
docs = load_korean_wikipedia(
    max_documents=100000,
    use_latest=True,
    force_download=True  # ê¸°ì¡´ ìºì‹œ ë¬´ì‹œ
)
```

### ìµœì†Œ ë¬¸ì„œ ê¸¸ì´ ì„¤ì •
```python
docs = load_korean_wikipedia(
    max_documents=100000,
    min_length=200,  # 200ì ì´ìƒë§Œ
    use_latest=True
)
```

### ì‚¬ìš© ê°€ëŠ¥í•œ ì†ŒìŠ¤ ì •ë³´ í™•ì¸
```python
from src.wikipedia_loader import get_wikipedia_info
import json

info = get_wikipedia_info()
print(json.dumps(info, indent=2, ensure_ascii=False))
```

## ğŸ“ ë…¸íŠ¸ë¶ 1 ì—…ë°ì´íŠ¸ ë‚´ìš©

ê¸°ì¡´:
```python
ko_wiki = load_dataset("wikipedia", "20220301.ko", split="train[:100000]")
```

ë³€ê²½ í›„:
```python
from src.wikipedia_loader import load_korean_wikipedia

ko_wiki_docs = load_korean_wikipedia(
    max_documents=100000,
    use_latest=False  # ë˜ëŠ” True
)
```

## ğŸ› ë¬¸ì œ í•´ê²°

### ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨
```
âŒ Download failed: HTTP Error 404
```
**í•´ê²°**: HuggingFace ë°ì´í„°ë¡œ ìë™ í´ë°±ë¨

### íŒŒì‹± ì˜¤ë¥˜
```
âŒ Parsing failed: ...
```
**í•´ê²°**:
1. `mwparserfromhell` ì„¤ì¹˜ í™•ì¸
2. ìºì‹œ ì‚­ì œ í›„ ì¬ì‹œë„
3. HuggingFace ì˜µì…˜ ì‚¬ìš©

### ë©”ëª¨ë¦¬ ë¶€ì¡±
```
MemoryError: ...
```
**í•´ê²°**: `max_documents` ê°’ ì¤„ì´ê¸°

## ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€

```bash
pip install datasets mwparserfromhell tqdm
```

ëª¨ë‘ `requirements.txt`ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ê¶Œì¥ ì‚¬í•­

### ê°œë°œ/í…ŒìŠ¤íŠ¸
```python
docs = load_korean_wikipedia(max_documents=10000, use_latest=False)
```

### í”„ë¡œë•ì…˜
```python
docs = load_korean_wikipedia(max_documents=100000, use_latest=True)
```

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [Wikimedia Dumps](https://dumps.wikimedia.org/kowiki/)
- [HuggingFace Datasets](https://huggingface.co/datasets/wikimedia/wikipedia)
- [mwparserfromhell](https://github.com/earwig/mwparserfromhell)

import json

# ë…¸íŠ¸ë¶ ì½ê¸°
with open('korean_neural_sparse_training.ipynb', 'r', encoding='utf-8') as f:
    notebook = json.load(f)

# ìƒˆë¡œ ì¶”ê°€í•  ì…€ë“¤
new_cells = [
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 5. ìë™ ë™ì˜ì–´ ë°œê²¬ ë° ë°ì´í„° í™•ì¥\n",
            "\n",
            "ìˆ˜ì§‘ëœ ë°ì´í„°ì—ì„œ í† í° ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ë™ì˜ì–´ë¥¼ ìë™ ë°œê²¬í•˜ê³ ,\n",
            "ì´ë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµ ë°ì´í„°ë¥¼ í™•ì¥í•©ë‹ˆë‹¤."
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": "### 5.1. BERT í† í° ì„ë² ë”© ì¶”ì¶œ"
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "print(\"=\"*60)\n",
            "print(\"ğŸ” í† í° ì„ë² ë”© ê¸°ë°˜ ë™ì˜ì–´ ìë™ ë°œê²¬\")\n",
            "print(\"=\"*60)\n",
            "\n",
            "# BERT ëª¨ë¸ì€ ì´ë¯¸ doc_encoderì— ë¡œë“œë˜ì–´ ìˆìŒ\n",
            "# Token embedding ì¶”ì¶œ\n",
            "token_embeddings = doc_encoder.bert.bert.embeddings.word_embeddings.weight.detach().cpu().numpy()\n",
            "\n",
            "print(f\"âœ“ Token embeddings ì¶”ì¶œ ì™„ë£Œ: {token_embeddings.shape}\")\n",
            "print(f\"  Vocab size: {token_embeddings.shape[0]:,}\")\n",
            "print(f\"  Embedding dim: {token_embeddings.shape[1]}\")"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": "### 5.2. ìœ ì‚¬ í† í° ìë™ ë°œê²¬ í•¨ìˆ˜"
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "def find_similar_tokens(token, tokenizer, embeddings, top_k=10, threshold=0.75):\n",
            "    \"\"\"\n",
            "    ì£¼ì–´ì§„ í† í°ê³¼ ìœ ì‚¬í•œ í† í°ë“¤ ì°¾ê¸° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜)\n",
            "    \n",
            "    Args:\n",
            "        token: í† í° ë¬¸ìì—´\n",
            "        tokenizer: Tokenizer\n",
            "        embeddings: Token embeddings (vocab_size, embedding_dim)\n",
            "        top_k: ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜\n",
            "        threshold: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’\n",
            "    \n",
            "    Returns:\n",
            "        List of (token, similarity) tuples\n",
            "    \"\"\"\n",
            "    # í† í° ID\n",
            "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
            "    if token_id == tokenizer.unk_token_id:\n",
            "        return []\n",
            "    \n",
            "    # í•´ë‹¹ í† í°ì˜ ì„ë² ë”©\n",
            "    token_emb = embeddings[token_id]\n",
            "    \n",
            "    # ëª¨ë“  í† í°ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
            "    similarities = np.dot(embeddings, token_emb) / (\n",
            "        np.linalg.norm(embeddings, axis=1) * np.linalg.norm(token_emb) + 1e-10\n",
            "    )\n",
            "    \n",
            "    # ìƒìœ„ kê°œ ì¶”ì¶œ\n",
            "    top_indices = np.argsort(similarities)[-top_k-1:-1][::-1]\n",
            "    \n",
            "    similar_tokens = []\n",
            "    for idx in top_indices:\n",
            "        sim_score = float(similarities[idx])\n",
            "        if sim_score >= threshold and int(idx) != token_id:\n",
            "            similar_token = tokenizer.decode([int(idx)])\n",
            "            similar_tokens.append((similar_token, sim_score))\n",
            "    \n",
            "    return similar_tokens\n",
            "\n",
            "\n",
            "def build_synonym_dict_from_corpus(documents, tokenizer, embeddings, \n",
            "                                   idf_dict, top_n=500, threshold=0.75):\n",
            "    \"\"\"\n",
            "    ìˆ˜ì§‘ëœ ë¬¸ì„œ ì½”í¼ìŠ¤ì—ì„œ ì¤‘ìš” í† í°ë“¤ì˜ ë™ì˜ì–´ë¥¼ ìë™ ë°œê²¬\n",
            "    \n",
            "    Args:\n",
            "        documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
            "        tokenizer: Tokenizer\n",
            "        embeddings: Token embeddings\n",
            "        idf_dict: IDF ë”•ì…”ë„ˆë¦¬\n",
            "        top_n: ìƒìœ„ Nê°œ IDF í† í° ëŒ€ìƒ\n",
            "        threshold: ìœ ì‚¬ë„ ì„ê³„ê°’\n",
            "    \n",
            "    Returns:\n",
            "        synonym_dict: {token: [similar_tokens]}\n",
            "    \"\"\"\n",
            "    print(f\"\\nğŸ“– ìˆ˜ì§‘ëœ ë°ì´í„°ì—ì„œ ì¤‘ìš” í† í° ì¶”ì¶œ...\")\n",
            "    \n",
            "    # IDF ê¸°ë°˜ ì¤‘ìš” í† í° ì„ ì •\n",
            "    sorted_idf = sorted(idf_dict.items(), key=lambda x: x[1], reverse=True)\n",
            "    \n",
            "    # í•„í„°ë§: subword(##ë¡œ ì‹œì‘), íŠ¹ìˆ˜ë¬¸ì, ë‹¨ì¼ ë¬¸ì ì œì™¸\n",
            "    important_tokens = []\n",
            "    for token, idf_score in sorted_idf:\n",
            "        if len(important_tokens) >= top_n:\n",
            "            break\n",
            "        \n",
            "        # í•„í„°ë§ ì¡°ê±´\n",
            "        if (not token.startswith('##') and \n",
            "            len(token) > 1 and \n",
            "            not token in [',', '.', '!', '?', ':', ';', '-', '(', ')', '[', ']']):\n",
            "            important_tokens.append(token)\n",
            "    \n",
            "    print(f\"  ì¤‘ìš” í† í° {len(important_tokens)}ê°œ ì„ ì • ì™„ë£Œ\")\n",
            "    print(f\"  ìƒìœ„ 10ê°œ: {important_tokens[:10]}\")\n",
            "    \n",
            "    # ê° í† í°ì— ëŒ€í•´ ìœ ì‚¬ í† í° ì°¾ê¸°\n",
            "    print(f\"\\nğŸ” ìœ ì‚¬ í† í° ìë™ ë°œê²¬ ì¤‘... (threshold={threshold})\")\n",
            "    synonym_dict = {}\n",
            "    \n",
            "    for token in tqdm(important_tokens, desc=\"Finding synonyms\"):\n",
            "        similar = find_similar_tokens(token, tokenizer, embeddings, \n",
            "                                      top_k=5, threshold=threshold)\n",
            "        if similar:\n",
            "            synonym_dict[token] = [t for t, _ in similar]\n",
            "    \n",
            "    return synonym_dict, important_tokens\n",
            "\n",
            "\n",
            "print(\"âœ“ ë™ì˜ì–´ ë°œê²¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": "### 5.3. ìˆ˜ì§‘ ë°ì´í„° ê¸°ë°˜ ë™ì˜ì–´ ìë™ ë°œê²¬"
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# ìˆ˜ì§‘ëœ ë¬¸ì„œì—ì„œ ë™ì˜ì–´ ìë™ ë°œê²¬\n",
            "auto_synonym_dict, important_tokens = build_synonym_dict_from_corpus(\n",
            "    korean_data['documents'],\n",
            "    tokenizer,\n",
            "    token_embeddings,\n",
            "    idf_id_dict,  # ID ê¸°ë°˜ IDF\n",
            "    top_n=500,    # ìƒìœ„ 500ê°œ í† í°\n",
            "    threshold=0.75  # ìœ ì‚¬ë„ 75% ì´ìƒ\n",
            ")\n",
            "\n",
            "print(f\"\\n{'='*60}\")\n",
            "print(f\"âœ“ ìë™ ë™ì˜ì–´ ë°œê²¬ ì™„ë£Œ!\")\n",
            "print(f\"{'='*60}\")\n",
            "print(f\"  ë°œê²¬ëœ ë™ì˜ì–´ ê·¸ë£¹: {len(auto_synonym_dict):,}ê°œ\")\n",
            "print(f\"  ì´ ë™ì˜ì–´ ìŒ: {sum(len(v) for v in auto_synonym_dict.values()):,}ê°œ\")\n",
            "\n",
            "# ì˜ˆì‹œ ì¶œë ¥\n",
            "print(f\"\\nğŸ“ ë°œê²¬ëœ ë™ì˜ì–´ ì˜ˆì‹œ (ìƒìœ„ 20ê°œ):\")\n",
            "for i, (token, synonyms) in enumerate(list(auto_synonym_dict.items())[:20], 1):\n",
            "    if synonyms:\n",
            "        print(f\"  {i:2d}. {token:15s} â†’ {', '.join(synonyms[:3])}\")"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": "### 5.4. Synonym-Aware IDF ìƒì„±"
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "def create_synonym_aware_idf(original_idf, tokenizer, synonym_dict, method='max'):\n",
            "    \"\"\"\n",
            "    ë™ì˜ì–´ ì •ë³´ë¥¼ ë°˜ì˜í•œ IDF ìƒì„±\n",
            "    \n",
            "    Args:\n",
            "        original_idf: ì›ë³¸ IDF ë”•ì…”ë„ˆë¦¬\n",
            "        tokenizer: Tokenizer\n",
            "        synonym_dict: ë™ì˜ì–´ ì‚¬ì „ {token: [synonyms]}\n",
            "        method: 'max', 'mean' ì¤‘ ì„ íƒ\n",
            "    \n",
            "    Returns:\n",
            "        enhanced_idf: ê°•í™”ëœ IDF ë”•ì…”ë„ˆë¦¬\n",
            "    \"\"\"\n",
            "    enhanced_idf = original_idf.copy()\n",
            "    boost_count = 0\n",
            "    \n",
            "    for canonical, synonyms in synonym_dict.items():\n",
            "        all_tokens = [canonical] + synonyms\n",
            "        \n",
            "        # ê° í† í°ì˜ IDF ê°’ ìˆ˜ì§‘\n",
            "        idf_values = []\n",
            "        for token in all_tokens:\n",
            "            if token in original_idf:\n",
            "                idf_values.append(original_idf[token])\n",
            "        \n",
            "        if not idf_values:\n",
            "            continue\n",
            "        \n",
            "        # IDF ê°’ í†µí•©\n",
            "        if method == 'max':\n",
            "            shared_idf = max(idf_values)\n",
            "        else:  # mean\n",
            "            shared_idf = np.mean(idf_values)\n",
            "        \n",
            "        # ëª¨ë“  ë™ì˜ì–´ í† í°ì— ì ìš©\n",
            "        for token in all_tokens:\n",
            "            if token in enhanced_idf:\n",
            "                enhanced_idf[token] = shared_idf\n",
            "                boost_count += 1\n",
            "    \n",
            "    print(f\"\\nâœ“ Synonym-Aware IDF ìƒì„± ì™„ë£Œ\")\n",
            "    print(f\"  {boost_count:,}ê°œ í† í°ì— ë™ì˜ì–´ ì •ë³´ ë°˜ì˜\")\n",
            "    \n",
            "    return enhanced_idf\n",
            "\n",
            "\n",
            "# Synonym-Aware IDF ìƒì„±\n",
            "print(\"\\n\" + \"=\"*60)\n",
            "print(\"ğŸ”„ ë™ì˜ì–´ ì •ë³´ë¥¼ ë°˜ì˜í•œ IDF ìƒì„± ì¤‘...\")\n",
            "print(\"=\"*60)\n",
            "\n",
            "idf_token_dict_enhanced = create_synonym_aware_idf(\n",
            "    idf_token_dict_boosted,\n",
            "    tokenizer,\n",
            "    auto_synonym_dict,\n",
            "    method='max'\n",
            ")\n",
            "\n",
            "# IDF ë³€í™” ì˜ˆì‹œ\n",
            "print(\"\\nğŸ“Š IDF ë³€í™” ì˜ˆì‹œ:\")\n",
            "sample_tokens = list(auto_synonym_dict.keys())[:5]\n",
            "for token in sample_tokens:\n",
            "    if token in idf_token_dict_boosted and token in idf_token_dict_enhanced:\n",
            "        original = idf_token_dict_boosted[token]\n",
            "        enhanced = idf_token_dict_enhanced[token]\n",
            "        change = \"â†‘\" if enhanced > original else \"â†’\"\n",
            "        print(f\"  {token:15s}: {original:.4f} {change} {enhanced:.4f}\")\n",
            "\n",
            "# Enhanced IDFë¥¼ ê¸°ë³¸ IDFë¡œ ì‚¬ìš©\n",
            "idf_token_dict_boosted = idf_token_dict_enhanced.copy()\n",
            "print(\"\\nâœ“ Enhanced IDFë¥¼ ê¸°ë³¸ IDFë¡œ ì„¤ì • ì™„ë£Œ\")"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": "### 5.5. ë™ì˜ì–´ ê¸°ë°˜ í•™ìŠµ ë°ì´í„° í™•ì¥"
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "def expand_data_with_synonyms(qd_pairs, documents, synonym_dict, \n",
            "                              tokenizer, expansion_ratio=0.2):\n",
            "    \"\"\"\n",
            "    ë™ì˜ì–´ë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµ ë°ì´í„° í™•ì¥\n",
            "    \n",
            "    Args:\n",
            "        qd_pairs: ì›ë³¸ query-document pairs\n",
            "        documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
            "        synonym_dict: ë™ì˜ì–´ ì‚¬ì „\n",
            "        tokenizer: Tokenizer\n",
            "        expansion_ratio: í™•ì¥ ë¹„ìœ¨ (0.2 = 20% ì¶”ê°€)\n",
            "    \n",
            "    Returns:\n",
            "        expanded_pairs: í™•ì¥ëœ pairs\n",
            "    \"\"\"\n",
            "    print(f\"\\nğŸ”„ ë™ì˜ì–´ ê¸°ë°˜ ë°ì´í„° í™•ì¥ ì¤‘... (expansion_ratio={expansion_ratio})\")\n",
            "    \n",
            "    expanded_pairs = list(qd_pairs)  # ì›ë³¸ ë³µì‚¬\n",
            "    expansion_count = int(len(qd_pairs) * expansion_ratio)\n",
            "    \n",
            "    added = 0\n",
            "    attempts = 0\n",
            "    max_attempts = expansion_count * 10\n",
            "    \n",
            "    while added < expansion_count and attempts < max_attempts:\n",
            "        attempts += 1\n",
            "        \n",
            "        # ëœë¤ pair ì„ íƒ\n",
            "        query, doc, relevance = qd_pairs[np.random.randint(len(qd_pairs))]\n",
            "        \n",
            "        # ì¿¼ë¦¬ í† í°í™”\n",
            "        query_tokens = tokenizer.tokenize(query)\n",
            "        \n",
            "        # ë™ì˜ì–´ë¡œ ëŒ€ì²´ ê°€ëŠ¥í•œ í† í° ì°¾ê¸°\n",
            "        replaceable = [(i, token) for i, token in enumerate(query_tokens) \n",
            "                      if token in synonym_dict and synonym_dict[token]]\n",
            "        \n",
            "        if not replaceable:\n",
            "            continue\n",
            "        \n",
            "        # ëœë¤í•˜ê²Œ í•˜ë‚˜ ì„ íƒí•˜ì—¬ ë™ì˜ì–´ë¡œ ëŒ€ì²´\n",
            "        idx, token = replaceable[np.random.randint(len(replaceable))]\n",
            "        synonym = np.random.choice(synonym_dict[token])\n",
            "        \n",
            "        # ìƒˆ ì¿¼ë¦¬ ìƒì„±\n",
            "        new_query_tokens = query_tokens.copy()\n",
            "        new_query_tokens[idx] = synonym\n",
            "        new_query = tokenizer.convert_tokens_to_string(new_query_tokens)\n",
            "        \n",
            "        # ì¤‘ë³µ ì²´í¬\n",
            "        if new_query != query and new_query.strip():\n",
            "            expanded_pairs.append((new_query, doc, relevance))\n",
            "            added += 1\n",
            "    \n",
            "    print(f\"âœ“ ë°ì´í„° í™•ì¥ ì™„ë£Œ!\")\n",
            "    print(f\"  ì›ë³¸: {len(qd_pairs):,} pairs\")\n",
            "    print(f\"  í™•ì¥: {len(expanded_pairs):,} pairs (+{added:,})\")\n",
            "    print(f\"  ì¦ê°€ìœ¨: {(len(expanded_pairs) / len(qd_pairs) - 1) * 100:.1f}%\")\n",
            "    \n",
            "    return expanded_pairs\n",
            "\n",
            "\n",
            "# í•™ìŠµ ë°ì´í„° í™•ì¥\n",
            "print(\"\\n\" + \"=\"*60)\n",
            "print(\"ğŸ“ˆ ë™ì˜ì–´ ê¸°ë°˜ í•™ìŠµ ë°ì´í„° í™•ì¥\")\n",
            "print(\"=\"*60)\n",
            "\n",
            "korean_data['qd_pairs_expanded'] = expand_data_with_synonyms(\n",
            "    korean_data['qd_pairs'],\n",
            "    korean_data['documents'],\n",
            "    auto_synonym_dict,\n",
            "    tokenizer,\n",
            "    expansion_ratio=0.15  # 15% í™•ì¥\n",
            ")\n",
            "\n",
            "# í™•ì¥ ì˜ˆì‹œ ì¶œë ¥\n",
            "print(\"\\nğŸ“ í™•ì¥ëœ ì¿¼ë¦¬ ì˜ˆì‹œ:\")\n",
            "original_count = len(korean_data['qd_pairs'])\n",
            "for i, (query, doc, rel) in enumerate(korean_data['qd_pairs_expanded'][original_count:original_count+5]):\n",
            "    print(f\"  {i+1}. {query[:60]}...\")\n",
            "\n",
            "# í™•ì¥ëœ ë°ì´í„°ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©\n",
            "korean_data['qd_pairs'] = korean_data['qd_pairs_expanded']\n",
            "print(f\"\\nâœ“ í™•ì¥ëœ ë°ì´í„°ë¥¼ í•™ìŠµì— ì‚¬ìš©\")"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "### 5.6. ë™ì˜ì–´ ì •ë³´ ìš”ì•½\n",
            "\n",
            "ìë™ ë°œê²¬ëœ ë™ì˜ì–´ ì •ë³´ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "print(\"\\n\" + \"=\"*60)\n",
            "print(\"ğŸ“Š ë™ì˜ì–´ ë°œê²¬ ë° ë°ì´í„° í™•ì¥ ìš”ì•½\")\n",
            "print(\"=\"*60)\n",
            "\n",
            "print(f\"\\n1ï¸âƒ£ ë™ì˜ì–´ ë°œê²¬ ê²°ê³¼:\")\n",
            "print(f\"  - ë¶„ì„ ëŒ€ìƒ í† í°: {len(important_tokens):,}ê°œ\")\n",
            "print(f\"  - ë°œê²¬ëœ ë™ì˜ì–´ ê·¸ë£¹: {len(auto_synonym_dict):,}ê°œ\")\n",
            "print(f\"  - ì´ ë™ì˜ì–´ ìŒ: {sum(len(v) for v in auto_synonym_dict.values()):,}ê°œ\")\n",
            "print(f\"  - í‰ê·  ë™ì˜ì–´ ìˆ˜: {np.mean([len(v) for v in auto_synonym_dict.values()]):.2f}ê°œ/ê·¸ë£¹\")\n",
            "\n",
            "print(f\"\\n2ï¸âƒ£ IDF ê°•í™” ê²°ê³¼:\")\n",
            "changes = 0\n",
            "for token in auto_synonym_dict.keys():\n",
            "    if token in idf_token_dict:\n",
            "        changes += 1\n",
            "print(f\"  - IDF ì—…ë°ì´íŠ¸ëœ í† í°: {changes:,}ê°œ\")\n",
            "\n",
            "print(f\"\\n3ï¸âƒ£ ë°ì´í„° í™•ì¥ ê²°ê³¼:\")\n",
            "print(f\"  - ì›ë³¸ pairs: {len(korean_data['qd_pairs_expanded']) - len(korean_data['qd_pairs']):,}ê°œ\")\n",
            "print(f\"  - ìµœì¢… pairs: {len(korean_data['qd_pairs']):,}ê°œ\")\n",
            "\n",
            "print(f\"\\nâœ… ë™ì˜ì–´ ê¸°ë°˜ ë°ì´í„° í™•ì¥ ì™„ë£Œ!\")\n",
            "print(f\"   í•™ìŠµ ë°ì´í„°ê°€ ë” í’ë¶€í•´ì¡ŒìŠµë‹ˆë‹¤.\")\n",
            "print(\"=\"*60)"
        ]
    }
]

# ì‚½ì…í•  ìœ„ì¹˜ ì°¾ê¸° (## 5. OpenSearch ë¬¸ì„œ ì¸ì½”ë” ëª¨ë¸ ì •ì˜ ì•)
insert_position = None
for i, cell in enumerate(notebook['cells']):
    if cell['cell_type'] == 'markdown' and 'source' in cell:
        source = ''.join(cell['source']) if isinstance(cell['source'], list) else cell['source']
        if '## 5. OpenSearch ë¬¸ì„œ ì¸ì½”ë” ëª¨ë¸ ì •ì˜' in source:
            insert_position = i
            print(f"ì‚½ì… ìœ„ì¹˜ ì°¾ìŒ: ì…€ {i} ì•")
            break

if insert_position is None:
    print("âŒ ì‚½ì… ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ì— ì¶”ê°€í•©ë‹ˆë‹¤.")
    insert_position = len(notebook['cells'])

# ìƒˆ ì…€ë“¤ ì‚½ì…
for offset, new_cell in enumerate(new_cells):
    notebook['cells'].insert(insert_position + offset, new_cell)

print(f"\nâœ“ {len(new_cells)}ê°œ ì…€ ì¶”ê°€ ì™„ë£Œ (ìœ„ì¹˜: {insert_position})")

# ê¸°ì¡´ ì„¹ì…˜ ë²ˆí˜¸ ì—…ë°ì´íŠ¸ (5 â†’ 6, 6 â†’ 7, etc.)
print("\nì„¹ì…˜ ë²ˆí˜¸ ì—…ë°ì´íŠ¸ ì¤‘...")
for i, cell in enumerate(notebook['cells']):
    if cell['cell_type'] == 'markdown' and 'source' in cell:
        source = ''.join(cell['source']) if isinstance(cell['source'], list) else cell['source']

        # ## 5. ì´í›„ì˜ ì„¹ì…˜ë“¤ì„ +1ì”© ì¦ê°€
        if i > insert_position + len(new_cells) - 1:
            for old_num in range(12, 4, -1):  # 12ë¶€í„° 5ê¹Œì§€ ì—­ìˆœìœ¼ë¡œ
                old_header = f"## {old_num}."
                new_header = f"## {old_num + 1}."
                if old_header in source:
                    new_source = source.replace(old_header, new_header)
                    if isinstance(cell['source'], list):
                        cell['source'] = [line + '\n' for line in new_source.split('\n')[:-1]]
                        cell['source'].append(new_source.split('\n')[-1])
                    else:
                        cell['source'] = new_source
                    print(f"  ì…€ {i}: {old_header} â†’ {new_header}")
                    break

# ë…¸íŠ¸ë¶ ì €ì¥
with open('korean_neural_sparse_training.ipynb', 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print("\nâœ“ ë…¸íŠ¸ë¶ íŒŒì¼ ì €ì¥ ì™„ë£Œ!")
print(f"  ì´ ì…€ ê°œìˆ˜: {len(notebook['cells'])}")

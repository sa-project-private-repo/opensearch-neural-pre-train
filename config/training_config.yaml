# Neural Sparse Model Training Configuration

# Model settings
model:
  name: "klue/bert-base"  # Options: klue/bert-base, xlm-roberta-base
  max_length: 256
  use_relu: true

# Data settings
data:
  qd_pairs_path: "dataset/base_model/qd_pairs_base.pkl"
  documents_path: "dataset/base_model/documents.json"
  synonyms_path: "dataset/synonyms/combined_synonyms.json"

  query_max_length: 64
  doc_max_length: 256
  num_negatives: 10
  train_split: 0.9
  synonym_sample_prob: 0.3

# Training settings
training:
  num_epochs: 10
  batch_size: 16
  gradient_accumulation_steps: 2  # Effective batch size: 32
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
  use_amp: true  # Automatic mixed precision

# Loss weights
loss:
  alpha_ranking: 1.0
  beta_cross_lingual: 0.3
  gamma_sparsity: 0.001
  ranking_margin: 0.1
  use_contrastive: false
  contrastive_temperature: 0.05

# Logging and checkpointing
logging:
  output_dir: "outputs"
  logging_steps: 50
  eval_steps: 500
  save_steps: 1000

# Evaluation settings
evaluation:
  metric: "ndcg@10"
  eval_batch_size: 32

# Hardware
hardware:
  device: "cuda"  # Options: cuda, cpu
  num_workers: 4
  pin_memory: true

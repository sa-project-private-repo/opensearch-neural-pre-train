# Baseline training configuration for SPLADE-doc
# Optimized for Nvidia DGX Spark (ARM + GB10 GPU)
# Quick testing with 10K samples

model:
  name: "bert-base-multilingual-cased"
  use_idf: false
  dropout: 0.1

data:
  # Enhanced training with KNN-discovered synonym pairs
  # - 9K baseline Korean same-language pairs
  # - 5K KNN synonym co-occurrence samples
  # - Total: 14K training samples
  train_patterns:
    - "dataset/baseline_samples/train_baseline.jsonl"
    - "dataset/baseline_samples/train_knn_synonyms.jsonl"

  val_patterns:
    - "dataset/baseline_samples/val_baseline.jsonl"

  batch_size: 16  # Moderate size for baseline
  max_length: 256
  num_negatives: 7  # Standard number from GitHub samples
  num_workers: 4
  use_hard_negatives: false

loss:
  temperature: 0.05
  lambda_flops: 0.00001  # Relaxed from 0.0001 to allow term expansion
  lambda_idf: 0.001
  lambda_kd: 0.0
  use_kd: false
  use_idf_penalty: false

training:
  num_epochs: 3  # Quick training
  learning_rate: 0.00002
  weight_decay: 0.01
  warmup_steps: 500
  gradient_accumulation_steps: 2  # Effective batch = 32
  max_grad_norm: 1.0

  # Mixed precision for speed
  mixed_precision: "bf16"
  compile_mode: false
  gradient_checkpointing: false

  log_steps: 50
  save_epochs: 1
  output_dir: "outputs/baseline_dgx"

device: "cuda"

# Training Notes:
# - 14K samples (9K baseline + 5K KNN synonyms)
# - 3 epochs, ~15 minutes on GB10
# - BF16 for faster training
# - KNN synonyms: 1,477 cross-lingual pairs from E5-large embeddings

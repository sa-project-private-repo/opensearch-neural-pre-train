# V24 XLM-RoBERTa Training Configuration
# Neural Sparse model with XLM-RoBERTa backbone and BGE-M3 teacher distillation
#
# Key improvements over v22:
#   - XLM-RoBERTa base (250K vocab vs 50K for KoBERT) = richer sparse representations
#   - BGE-M3 as teacher (same as Dense baseline) = optimal KD ceiling
#   - Increased max_length (128 -> 192) for better context
#   - Rebalanced loss weights for stronger contrastive learning
#
# Expected performance:
#   - Recall@1: 65-68% (vs v22's 55.4%)
#   - MRR: 0.72+ (vs v22's 0.64)
#
# Usage:
#   python -m train v24 --config configs/train_v24.yaml
#   make train-v24

# Model Configuration
model:
  name: "xlm-roberta-base"       # 250K vocab for richer sparse representations
  dropout: 0.1
  use_expansion: true            # Use MLM-based vocabulary expansion
  expansion_mode: "mlm"          # XLM-R MLM head
  freeze_encoder_layers: 0       # No freezing for full fine-tuning
  model_class: "SPLADEDocXLMR"   # Use XLM-R specific model class

# Data Configuration
data:
  train_files:
    - "data/v24.0/train_*.jsonl"
  val_files:
    - "data/v24.0/val.jsonl"
  batch_size: 48                 # Reduced from 64 (XLM-R is larger)
  max_length: 192                # Increased from 128 for better context
  num_workers: 4
  prefetch_factor: 2
  pin_memory: true

# Loss Configuration (SPLADELossV23 recommended for v24)
loss:
  # Loss weights (optimized for XLM-R + BGE-M3 KD)
  lambda_infonce: 3.0            # 2.5 -> 3.0: Stronger contrastive signal
  lambda_self: 0.5               # 1.0 -> 0.5: Reduced to prevent self-reconstruction dominance
  lambda_positive: 2.0           # 3.0 -> 2.0: Rebalanced
  lambda_margin: 0.0             # Disabled (redundant with InfoNCE)
  lambda_flops: 0.002            # 0.003 -> 0.002: Allow more activations for 250K vocab
  lambda_min_act: 1.0            # Keep activation floor

  # Knowledge distillation (BGE-M3 teacher)
  lambda_kd: 2.0                 # 1.0 -> 2.0: Stronger teacher guidance
  kd_temperature: 3.0            # 2.0 -> 3.0: Softer distribution matching

  # Loss hyperparameters
  temperature: 0.07              # Slightly sharper contrast
  margin: 0.3                    # Cosine similarity margin
  top_k: 5                       # Top-k for minimum activation
  min_activation: 0.5            # Minimum activation threshold

  # IDF-aware FLOPS (critical for 250K vocab)
  use_idf_weighting: true        # Enable IDF-aware penalty
  idf_alpha: 2.5                 # 2.0 -> 2.5: Stronger IDF awareness for large vocab

# Training Configuration
training:
  num_epochs: 25                 # 20 -> 25: XLM-R needs more epochs
  learning_rate: 0.00003         # 2e-5 -> 3e-5: Higher LR for XLM-R
  weight_decay: 0.01
  warmup_ratio: 0.1
  gradient_clip: 1.0
  gradient_accumulation_steps: 2 # Effective batch size = 48 * 2 = 96

  # Mixed precision (bf16 for A100/H100)
  mixed_precision: "bf16"

  # Checkpointing
  save_every_n_epochs: 5
  save_every_n_steps: null
  keep_last_n_checkpoints: 3

  # Logging
  log_every_n_steps: 50
  eval_every_n_steps: null

  # Output
  output_dir: "outputs/train_v24"
  experiment_name: "splade_v24_xlmr_bge"

# Knowledge Distillation Configuration
knowledge_distillation:
  enabled: true
  teacher_model: "BAAI/bge-m3"   # Same as Dense baseline for optimal ceiling
  teacher_max_length: 512        # BGE-M3 supports longer sequences
  normalize_embeddings: true     # L2 normalize teacher embeddings
  warmup_epochs: 1               # 2 -> 1: Faster KD engagement

# Curriculum Learning Configuration
enable_curriculum: true

curriculum_phases:
  # Phase 1: Foundation with strong BGE-M3 guidance (8 epochs)
  - start_epoch: 1
    end_epoch: 8
    temperature: 0.08
    lambda_infonce: 2.5
    lambda_kd: 2.5               # Strong teacher guidance
    lr_multiplier: 1.0
    data_weights:
      single_term: 0.4
      multi_term: 0.35
      original: 0.25
    description: "Phase 1: Foundation with BGE-M3 teacher guidance"

  # Phase 2: Balanced contrastive learning (9 epochs)
  - start_epoch: 9
    end_epoch: 17
    temperature: 0.05
    lambda_infonce: 3.0
    lambda_kd: 1.5               # Moderate teacher guidance
    lr_multiplier: 0.5
    data_weights:
      single_term: 0.3
      multi_term: 0.35
      hard_neg: 0.35
    description: "Phase 2: Balanced training with hard negatives"

  # Phase 3: Refinement with reduced KD (8 epochs)
  - start_epoch: 18
    end_epoch: 25
    temperature: 0.04            # Sharper contrastive learning
    lambda_infonce: 3.0
    lambda_kd: 0.8               # Reduced KD for student independence
    lr_multiplier: 0.25
    data_weights:
      hard_neg: 0.5
      multi_term: 0.3
      single_term: 0.2
    description: "Phase 3: Hard negative refinement with student independence"

# Hard Negative Mining Configuration
hard_negative_mining:
  enabled: true
  bm25_negatives: 5              # BM25-based lexical negatives
  dense_negatives: 5             # BGE-M3 based semantic negatives
  refresh_every_n_epochs: 5      # Re-mine hard negatives every 5 epochs

# General Settings
device: "cuda"
seed: 42

# Notes:
# =================
# Key differences from v22:
#
# 1. Base Model: xlm-roberta-base (250K vocab) vs skt/kobert-base-v1 (50K vocab)
#    - 5x larger vocabulary = richer sparse representations
#    - Better multilingual alignment
#    - Proven performance in SPLADE research
#
# 2. Teacher Model: BAAI/bge-m3 vs paraphrase-multilingual-MiniLM-L12-v2
#    - Same model as our Dense baseline (68.0% Recall@1)
#    - Optimal KD ceiling - student can match teacher performance
#    - 1024-dim dense embeddings with strong semantic understanding
#
# 3. Loss Rebalancing:
#    - lambda_infonce: 2.5 -> 3.0 (stronger contrastive signal)
#    - lambda_kd: 1.0 -> 2.0 (stronger BGE-M3 guidance)
#    - lambda_positive: 3.0 -> 2.0 (prevent over-activation)
#    - lambda_self: 1.0 -> 0.5 (reduce self-reconstruction noise)
#    - lambda_flops: 0.003 -> 0.002 (allow more activations for 250K vocab)
#
# 4. Training Parameters:
#    - batch_size: 64 -> 48 (XLM-R is larger, ~278M params)
#    - max_length: 128 -> 192 (better context capture)
#    - learning_rate: 2e-5 -> 3e-5 (XLM-R benefits from higher LR)
#    - num_epochs: 20 -> 25 (larger model needs more training)
#
# 5. Curriculum Learning:
#    - 3 phases: 8-9-8 epochs (adjusted for 25 total)
#    - KD weight decreases: 2.5 -> 1.5 -> 0.8
#    - Temperature annealing: 0.08 -> 0.05 -> 0.04
#
# Expected Results:
# - Recall@1: 65-68% (target: match Dense baseline)
# - MRR: 0.72+
# - When combined with Hybrid RRF: 70-75% Recall@1

# V25 XLM-RoBERTa Training Configuration with IDF-Aware FLOPS
# Neural Sparse model with mandatory IDF weighting and Korean stopword masking
#
# Key improvements over v24:
#   - IDF weights computed from training corpus (not optional)
#   - Korean stopword masking for grammatical particles
#   - Semantic token ratio monitoring during training
#   - Proper frequency-based discrimination for 250K vocabulary
#
# This addresses the root cause of v24's particle dominance:
#   - v24 had use_idf_weighting: true but never computed/passed IDF weights
#   - Result: uniform FLOPS penalty allowed particles (을, 이, 는) to dominate
#   - v25 enforces IDF computation, ensuring semantic tokens (서울, 맛있는) rank higher
#
# Expected performance:
#   - Semantic tokens in top-10: 80%+ (vs v24's 30%)
#   - Stopword activation: <0.5 (vs v24's 5.8)
#   - MRR@10: 1.0 (maintained)
#
# Usage:
#   python -m train v25 --config configs/train_v25.yaml
#   make train-v25

# Model Configuration (same as V24)
model:
  name: "xlm-roberta-base"       # 250K vocab for richer sparse representations
  dropout: 0.1
  use_expansion: true            # Use MLM-based vocabulary expansion
  expansion_mode: "mlm"          # XLM-R MLM head
  freeze_encoder_layers: 0       # No freezing for full fine-tuning
  model_class: "SPLADEDocXLMR"   # Use XLM-R specific model class

# Data Configuration
data:
  train_files:
    - "data/v24.0/train_*.jsonl"
  val_files:
    - "data/v24.0/val.jsonl"
  batch_size: 48                 # Same as V24
  max_length: 192                # Same as V24
  num_workers: 4
  prefetch_factor: 2
  pin_memory: true

# Loss Configuration (SPLADELossV25 with mandatory IDF)
loss:
  # Loss weights (same as V24)
  lambda_infonce: 3.0            # Contrastive signal
  lambda_self: 0.5               # Self-reconstruction
  lambda_positive: 2.0           # Positive activation
  lambda_margin: 0.0             # Disabled
  lambda_flops: 0.002            # IDF-aware FLOPS penalty
  lambda_min_act: 1.0            # Activation floor

  # Knowledge distillation
  lambda_kd: 2.0                 # Teacher guidance
  kd_temperature: 3.0            # Soft distribution matching

  # Loss hyperparameters
  temperature: 0.07              # InfoNCE temperature
  margin: 0.3                    # Cosine similarity margin
  top_k: 5                       # Top-k for minimum activation
  min_activation: 0.5            # Minimum activation threshold

  # IDF Configuration (V25 mandatory)
  use_idf_weighting: true        # Always true for V25
  idf_alpha: 2.5                 # IDF exponential decay factor
  idf_weights_path: null         # null = compute from corpus on first run
  recompute_idf: false           # Set true to force recomputation
  idf_smoothing: "bm25"          # BM25-style IDF: log(1 + (N-df+0.5)/(df+0.5))

  # Stopword Configuration (V25 specific)
  use_stopword_mask: true        # Enable Korean stopword masking
  stopword_penalty: 5.0          # Extra penalty for stopwords in FLOPS

# Training Configuration
training:
  num_epochs: 25                 # Same as V24
  learning_rate: 0.00003         # Same as V24
  weight_decay: 0.01
  warmup_ratio: 0.1
  gradient_clip: 1.0
  gradient_accumulation_steps: 2 # Effective batch size = 48 * 2 = 96

  # Mixed precision (bf16 for A100/H100)
  mixed_precision: "bf16"

  # Checkpointing
  save_every_n_epochs: 5
  save_every_n_steps: null
  keep_last_n_checkpoints: 3

  # Logging
  log_every_n_steps: 50
  eval_every_n_steps: null

  # Output
  output_dir: "outputs/train_v25"
  experiment_name: "splade_v25_xlmr_idf"

# Knowledge Distillation Configuration (same as V24)
knowledge_distillation:
  enabled: true
  teacher_model: "BAAI/bge-m3"   # Same as Dense baseline
  teacher_max_length: 512
  normalize_embeddings: true
  warmup_epochs: 1

# Curriculum Learning Configuration (same as V24)
enable_curriculum: true

curriculum_phases:
  # Phase 1: Foundation with strong BGE-M3 guidance (8 epochs)
  - start_epoch: 1
    end_epoch: 8
    temperature: 0.08
    lambda_infonce: 2.5
    lambda_kd: 2.5
    lr_multiplier: 1.0
    data_weights:
      single_term: 0.4
      multi_term: 0.35
      original: 0.25
    description: "Phase 1: Foundation with BGE-M3 teacher guidance"

  # Phase 2: Balanced contrastive learning (9 epochs)
  - start_epoch: 9
    end_epoch: 17
    temperature: 0.05
    lambda_infonce: 3.0
    lambda_kd: 1.5
    lr_multiplier: 0.5
    data_weights:
      single_term: 0.3
      multi_term: 0.35
      hard_neg: 0.35
    description: "Phase 2: Balanced training with hard negatives"

  # Phase 3: Refinement with reduced KD (8 epochs)
  - start_epoch: 18
    end_epoch: 25
    temperature: 0.04
    lambda_infonce: 3.0
    lambda_kd: 0.8
    lr_multiplier: 0.25
    data_weights:
      hard_neg: 0.5
      multi_term: 0.3
      single_term: 0.2
    description: "Phase 3: Hard negative refinement with student independence"

# Hard Negative Mining Configuration (same as V24)
hard_negative_mining:
  enabled: true
  bm25_negatives: 5
  dense_negatives: 5
  refresh_every_n_epochs: 5

# General Settings
device: "cuda"
seed: 42

# Notes:
# =================
# V25 addresses the root cause of V24's particle dominance problem:
#
# Problem in V24:
#   - Config had use_idf_weighting: true
#   - But IDF weights were NEVER computed or passed to loss function
#   - IDFAwareFLOPSLoss fell back to uniform weights
#   - Result: particles (을, 이, 는) dominated instead of semantic tokens
#
# Solution in V25:
#   1. IDF weights computed from training corpus on first run
#   2. Cached to outputs/train_v25/idf_weights/xlmr_v25_idf.pt
#   3. SPLADELossV25 REQUIRES idf_weights (raises error if None)
#   4. Korean stopword mask provides additional penalty for particles
#   5. Semantic ratio logged during training for monitoring
#
# Key changes:
#   - SPLADELossV23 -> SPLADELossV25 (mandatory IDF)
#   - New: src/train/idf/ module for IDF computation
#   - New: Korean stopword list and masking
#   - New: semantic_ratio metric in training logs
#
# Success criteria:
#   - Semantic tokens (서울, 맛있는, 추천) in top-10: 80%+
#   - Stopword mean activation: <0.5
#   - MRR@10 on validation: 1.0 (maintained)

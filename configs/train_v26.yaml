# V26 XLM-RoBERTa Training Configuration with Enhanced IDF-Aware FLOPS
# Neural Sparse model with fixed special token handling and stronger stopword penalties
#
# Key improvements over v25:
#   - Special tokens (<s>, </s>) excluded from IDF normalization range
#   - Fixed high penalty (100.0) for special tokens
#   - Increased FLOPS weight (0.010 vs 0.002) - 5x boost
#   - Increased stopword penalty (15.0 vs 5.0) - 3x boost
#   - Sharper IDF penalty curve (alpha 4.0 vs 2.5)
#   - Expanded Korean stopword list with V25 observed high-frequency terms
#
# Root cause addressed:
#   V25 problem: Special tokens (df=0) got max IDF (15.65), compressing the
#   normalization range and making stopword penalty ineffective.
#   V26 solution: Exclude special tokens from normalization, apply fixed
#   penalty, and increase regularization strength.
#
# Expected performance:
#   - Semantic tokens in top-10: 80%+ (vs V25's ~30%)
#   - Stopword mean activation: <0.3 (vs V25's 5.8)
#   - Neural Sparse Recall@1: >32% (vs V25's 28.2%)
#   - BM25 parity or better (vs V25's -6%)
#
# Usage:
#   python -m src.train v26 --config configs/train_v26.yaml
#   make train-v26

# Model Configuration (same as V25)
model:
  name: "xlm-roberta-base"       # 250K vocab for richer sparse representations
  dropout: 0.1
  use_expansion: true            # Use MLM-based vocabulary expansion
  expansion_mode: "mlm"          # XLM-R MLM head
  freeze_encoder_layers: 0       # No freezing for full fine-tuning
  model_class: "SPLADEDocXLMR"   # Use XLM-R specific model class

# Data Configuration
data:
  train_files:
    - "data/v24.0/train_*.jsonl"
  val_files:
    - "data/v24.0/val.jsonl"
  batch_size: 24                 # Same as V25
  max_length: 192                # Same as V25
  num_workers: 4
  prefetch_factor: 2
  pin_memory: true

# Loss Configuration (SPLADELossV26 with enhanced IDF handling)
loss:
  # Loss weights (adjusted from V25)
  lambda_infonce: 3.0            # Contrastive signal (same)
  lambda_self: 0.5               # Self-reconstruction (same)
  lambda_positive: 2.0           # Positive activation (same)
  lambda_margin: 0.0             # Disabled (same)
  lambda_flops: 0.010            # 5x increase: 0.002 -> 0.010
  lambda_min_act: 1.0            # Activation floor (same)

  # Knowledge distillation
  lambda_kd: 2.0                 # Teacher guidance (same)
  kd_temperature: 3.0            # Soft distribution matching (same)

  # Loss hyperparameters
  temperature: 0.07              # InfoNCE temperature
  margin: 0.3                    # Cosine similarity margin
  top_k: 5                       # Top-k for minimum activation
  min_activation: 0.5            # Minimum activation threshold

  # IDF Configuration (V26 enhanced)
  use_idf_weighting: true        # Always true for V26
  idf_alpha: 4.0                 # Sharper curve: 2.5 -> 4.0
  idf_weights_path: null         # null = compute from corpus on first run
  recompute_idf: false           # Set true to force recomputation
  idf_smoothing: "bm25"          # BM25-style IDF

  # Special Token Configuration (V26 new)
  special_token_penalty: 100.0   # Fixed high penalty for <s>, </s>, etc.

  # Stopword Configuration (V26 enhanced)
  use_stopword_mask: true        # Enable Korean stopword masking
  stopword_penalty: 15.0         # 3x increase: 5.0 -> 15.0
  use_extended_stopwords: true   # Use KOREAN_STOPWORDS_V26 list

# Training Configuration
training:
  num_epochs: 25                 # Same as V25
  learning_rate: 0.00003         # Same as V25
  weight_decay: 0.01
  warmup_ratio: 0.1
  gradient_clip: 1.0
  gradient_accumulation_steps: 4 # Effective batch size = 24 * 4 = 96

  # Mixed precision (bf16 for A100/H100)
  mixed_precision: "bf16"

  # Checkpointing
  save_every_n_epochs: 5
  save_every_n_steps: null
  keep_last_n_checkpoints: 3

  # Logging
  log_every_n_steps: 50
  eval_every_n_steps: null

  # Output
  output_dir: "outputs/train_v26"
  experiment_name: "splade_v26_xlmr_enhanced_idf"

# Knowledge Distillation Configuration (same as V25)
knowledge_distillation:
  enabled: true
  teacher_model: "BAAI/bge-m3"   # Same as Dense baseline
  teacher_max_length: 512
  normalize_embeddings: true
  warmup_epochs: 1

# Curriculum Learning Configuration (same as V25)
enable_curriculum: true

curriculum_phases:
  # Phase 1: Foundation with strong BGE-M3 guidance (8 epochs)
  - start_epoch: 1
    end_epoch: 8
    temperature: 0.08
    lambda_infonce: 2.5
    lambda_kd: 2.5
    lr_multiplier: 1.0
    data_weights:
      single_term: 0.4
      multi_term: 0.35
      original: 0.25
    description: "Phase 1: Foundation with BGE-M3 teacher guidance"

  # Phase 2: Balanced contrastive learning (9 epochs)
  - start_epoch: 9
    end_epoch: 17
    temperature: 0.05
    lambda_infonce: 3.0
    lambda_kd: 1.5
    lr_multiplier: 0.5
    data_weights:
      single_term: 0.3
      multi_term: 0.35
      hard_neg: 0.35
    description: "Phase 2: Balanced training with hard negatives"

  # Phase 3: Refinement with reduced KD (8 epochs)
  - start_epoch: 18
    end_epoch: 25
    temperature: 0.04
    lambda_infonce: 3.0
    lambda_kd: 0.8
    lr_multiplier: 0.25
    data_weights:
      hard_neg: 0.5
      multi_term: 0.3
      single_term: 0.2
    description: "Phase 3: Hard negative refinement with student independence"

# Hard Negative Mining Configuration (same as V25)
hard_negative_mining:
  enabled: true
  bm25_negatives: 5
  dense_negatives: 5
  refresh_every_n_epochs: 5

# General Settings
device: "cuda"
seed: 42

# Notes:
# =================
# V26 addresses the root cause of V25's stopword dominance problem:
#
# Problem in V25:
#   - Special tokens (<s>, </s>) have df=0 (never in training text)
#   - This gives them max IDF value (15.65)
#   - After normalization: special tokens get normalized_idf=1.0
#   - After exp(-alpha*idf): special tokens get penalty=0.08 (MINIMUM!)
#   - This compresses the IDF range for real tokens
#   - Result: stopword penalty (5.0x) becomes ineffective
#
# Solution in V26:
#   1. Exclude special tokens from IDF min/max calculation
#   2. Apply fixed penalty=100.0 to special tokens
#   3. Increase stopword_penalty: 5.0 -> 15.0 (3x)
#   4. Increase lambda_flops: 0.002 -> 0.010 (5x)
#   5. Sharpen IDF curve: alpha 2.5 -> 4.0
#   6. Expand stopword list with V25 observed high-frequency terms
#
# File changes from V25:
#   - src/model/losses.py: IDFAwareFLOPSLoss enhanced, SPLADELossV26 added
#   - src/train/idf/korean_stopwords.py: KOREAN_STOPWORDS_V26 list added
#   - src/train/config/v26.py: V26Config with new parameters
#   - src/train/cli/train_v26.py: V26 training CLI
#
# Success criteria:
#   - Semantic tokens (서울, 맛있는, 추천) in top-10: 80%+
#   - Stopword mean activation: <0.3
#   - semantic_ratio metric during training: >1.0
#   - Recall@1 parity with BM25

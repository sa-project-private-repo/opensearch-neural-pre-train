# V28 Configuration: Language Filtering + Context-Gated Sparse Expansion
#
# Key Features:
# - V28a: Korean language filtering (suppress non-Korean token leakage)
# - V28b: Context-gated expansion (context-dependent token activation)
#
# Usage:
#   python -m src.train v28 --config configs/train_v28.yaml
#   make train-v28

# Model Configuration
model:
  name: "xlm-roberta-base"
  dropout: 0.1
  use_expansion: true
  expansion_mode: "mlm"
  # V28b: Context Gate
  model_class: "SPLADEDocContextGated"
  use_context_gate: true
  context_gate_hidden: 256
  context_attention_heads: 4

# Data Configuration
data:
  train_files:
    - "data/v24.0/train_*.jsonl"
  val_files:
    - "data/v24.0/val.jsonl"
  batch_size: 24
  max_length: 192
  num_workers: 4

# Loss Configuration
loss:
  # Core loss weights (inherited from V26)
  lambda_infonce: 3.0
  lambda_self: 0.5
  lambda_positive: 2.0
  lambda_margin: 0.0
  lambda_flops: 0.010
  lambda_min_act: 1.0
  lambda_kd: 2.0

  # Temperature settings
  temperature: 0.07
  kd_temperature: 3.0
  margin: 0.3
  top_k: 5
  min_activation: 0.5

  # IDF Configuration (from V26)
  use_idf_weighting: true
  idf_alpha: 4.0
  idf_smoothing: "bm25"
  special_token_penalty: 100.0

  # Stopword Configuration (from V26)
  use_stopword_mask: true
  stopword_penalty: 15.0
  use_extended_stopwords: true

  # V28a: Language Filtering
  enable_language_filtering: true
  korean_token_penalty: 0.0      # No penalty for Korean tokens
  non_korean_penalty: 100.0      # High penalty for non-Korean tokens
  lambda_language: 0.5           # Weight for language filtering loss

  # V28b: Context-Gated Expansion
  use_context_gate: true
  context_gate_hidden: 256
  context_attention_heads: 4
  context_gate_dropout: 0.1
  use_context_aware_kd: true
  context_kd_weight: 1.0

# Training Configuration
training:
  num_epochs: 25
  learning_rate: 3.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  gradient_clip: 1.0
  gradient_accumulation_steps: 4
  mixed_precision: "bf16"
  output_dir: "outputs/train_v28"
  experiment_name: "splade_v28_xlmr_context_gated"
  log_every_n_steps: 50
  save_every_n_epochs: 5
  save_every_n_steps: null
  keep_last_n_checkpoints: 3

# Knowledge Distillation
knowledge_distillation:
  enabled: true
  teacher_model: "BAAI/bge-m3"
  warmup_epochs: 2
  annealing_epochs: 20

# Hard Negative Mining
hard_negative_mining:
  enabled: true
  mine_every_n_epochs: 3
  num_hard_negatives: 5
  temperature: 0.1

# Curriculum Learning
enable_curriculum: true
curriculum_phases:
  - start_epoch: 1
    end_epoch: 8
    temperature: 0.08
    lambda_infonce: 2.5
    lr_multiplier: 1.0
    lambda_kd: 2.5
    data_weights:
      single_term: 0.4
      multi_term: 0.35
      original: 0.25
    description: "Phase 1: Foundation with BGE-M3 teacher guidance"

  - start_epoch: 9
    end_epoch: 17
    temperature: 0.05
    lambda_infonce: 3.0
    lr_multiplier: 0.5
    lambda_kd: 1.5
    data_weights:
      single_term: 0.3
      multi_term: 0.35
      hard_neg: 0.35
    description: "Phase 2: Balanced training with hard negatives"

  - start_epoch: 18
    end_epoch: 25
    temperature: 0.04
    lambda_infonce: 3.0
    lr_multiplier: 0.25
    lambda_kd: 0.8
    data_weights:
      hard_neg: 0.5
      multi_term: 0.3
      single_term: 0.2
    description: "Phase 3: Hard negative refinement with context gate"

# Random seed for reproducibility
seed: 42

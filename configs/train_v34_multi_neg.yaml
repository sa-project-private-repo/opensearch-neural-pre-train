# V34: Multi-hard-negative training with KD from BGE-M3
#
# Builds on V33 checkpoint with:
# - 7 hard negatives per query (mined from BGE-M3 retrieval ranks 10-50)
# - MarginMSE distillation across all k negatives
# - InfoNCE with in-batch + k explicit hard negatives
#
# Prerequisites:
#   1. python scripts/precompute_teacher_scores.py --save-embeddings
#   2. python scripts/mine_multi_negatives.py --k 7
#
# Hardware: 8x NVIDIA B200 (183GB VRAM each)
# Memory: batch=64, k=7 -> 448+64+64=576 sequences/GPU (~4GB extra vs single-neg)
# Effective batch size: 64 * 4 * 8 = 2048

model:
  name: "skt/A.X-Encoder-base"
  dropout: 0.1

loss:
  lambda_q: 0.01
  lambda_d: 0.003
  temperature: 1.0
  flops_warmup_steps: 5000       # Shorter (V33 FLOPS already converged)
  lambda_kd: 0.0                 # KL disabled (requires online teacher)
  kd_temperature: 1.0
  lambda_margin_mse: 0.5         # MarginMSE across k negatives
  lambda_initial_ratio: 0.5      # Higher initial FLOPS (V33 already sparse)

data:
  train_files:
    - "data/v29.0_multi_neg/train_*.jsonl"  # Multi-negative format
  val_files:
    - "data/v29.0_multi_neg/val.jsonl"
  batch_size: 16                   # Reduced: 16*(1+1+7)=144 seqs/GPU (was 576)
  query_max_length: 64
  doc_max_length: 256
  num_workers: 4
  num_hard_negatives: 7

training:
  num_epochs: 15
  learning_rate: 1.0e-5          # Lower LR for fine-tuning from V33
  weight_decay: 0.01
  warmup_ratio: 0.05
  gradient_clip: 1.0
  gradient_accumulation_steps: 16  # 16*16*8=2048 eff_batch (same as V33)
  mixed_precision: "bf16"
  output_dir: "outputs/train_v34_multi_neg"
  log_every_n_steps: 50
  save_every_n_epochs: 5
  seed: 42

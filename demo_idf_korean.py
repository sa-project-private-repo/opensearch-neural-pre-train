#!/usr/bin/env python3
"""
OpenSearch Inference-Free Neural Sparse - ê°„ë‹¨í•œ IDF ë°ëª¨
torch ì—†ì´ í•µì‹¬ ì»¨ì…‰ë§Œ ë³´ì—¬ì£¼ëŠ” ë°ëª¨ì…ë‹ˆë‹¤.
"""

import json
import math
from collections import Counter

print("=" * 60)
print("OpenSearch Inference-Free Neural Sparse - IDF ë°ëª¨")
print("=" * 60)

# í•œêµ­ì–´ ìƒ˜í”Œ ë°ì´í„°
SAMPLE_DOCUMENTS = [
    "ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„° ì‹œìŠ¤í…œì´ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤",
    "ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤",
    "ë”¥ëŸ¬ë‹ì€ ì¸ê³µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤",
    "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤",
    "OpenSearchëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ìœ¼ë¡œ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤",
    "ë²¡í„° ê²€ìƒ‰ì€ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤",
    "Neural sparse ê²€ìƒ‰ì€ í¬ì†Œ ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ ê²€ìƒ‰ì„ ì œê³µí•©ë‹ˆë‹¤",
    "í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„ê³¼ í’ˆì‚¬ íƒœê¹…ì„ í¬í•¨í•©ë‹ˆë‹¤",
    "íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ëŠ” í˜„ëŒ€ ìì—°ì–´ ì²˜ë¦¬ì˜ í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤",
    "BERT ëª¨ë¸ì€ ì–‘ë°©í–¥ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ë§¥ì„ ì´í•´í•©ë‹ˆë‹¤",
    "GPTëŠ” ìƒì„±í˜• ì–¸ì–´ ëª¨ë¸ë¡œ ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
    "LLMì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ì˜ë¯¸í•˜ë©° ChatGPTê°€ ëŒ€í‘œì ì…ë‹ˆë‹¤",
    "ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„° ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤",
    "ê²€ìƒ‰ ì—”ì§„ ìµœì í™”ëŠ” ì›¹ì‚¬ì´íŠ¸ì˜ ê°€ì‹œì„±ì„ ë†’ì´ëŠ” ì‘ì—…ì…ë‹ˆë‹¤",
    "ë°ì´í„°ë² ì´ìŠ¤ëŠ” êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤",
]

SAMPLE_QUERIES = [
    "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ",
    "ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµ",
    "OpenSearch ê²€ìƒ‰",
    "neural sparse",
    "í•œêµ­ì–´ ì²˜ë¦¬",
    "BERT ëª¨ë¸",
    "GPT LLM",
    "ë²¡í„° ì„ë² ë”©",
]

print(f"\nğŸ“š ìƒ˜í”Œ ë°ì´í„°:")
print(f"  ë¬¸ì„œ: {len(SAMPLE_DOCUMENTS)}ê°œ")
print(f"  ì¿¼ë¦¬: {len(SAMPLE_QUERIES)}ê°œ")

# ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì € (ê³µë°± ê¸°ë°˜)
def simple_tokenize(text):
    """ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì € (ì‹¤ì œë¡œëŠ” BERT tokenizer ì‚¬ìš©)"""
    return text.lower().split()

# Step 1: IDF ê³„ì‚°
print("\n" + "=" * 60)
print("Step 1: IDF (Inverse Document Frequency) ê³„ì‚°")
print("=" * 60)

def calculate_idf(documents):
    """IDF ê³„ì‚°"""
    N = len(documents)
    df = Counter()  # Document frequency

    # ê° í† í°ì´ ë“±ì¥í•˜ëŠ” ë¬¸ì„œ ìˆ˜ ê³„ì‚°
    for doc in documents:
        tokens = simple_tokenize(doc)
        unique_tokens = set(tokens)
        for token in unique_tokens:
            df[token] += 1

    # IDF = log(N / df) + 1
    idf_dict = {}
    for token, doc_freq in df.items():
        idf_score = math.log((N + 1) / (doc_freq + 1)) + 1.0
        idf_dict[token] = idf_score

    return idf_dict

idf_dict = calculate_idf(SAMPLE_DOCUMENTS)

print(f"âœ“ {len(idf_dict)}ê°œ í† í°ì˜ IDF ê³„ì‚° ì™„ë£Œ")
print(f"  í‰ê·  IDF: {sum(idf_dict.values()) / len(idf_dict):.4f}")

# ìƒìœ„/í•˜ìœ„ IDF ì¶œë ¥
sorted_idf = sorted(idf_dict.items(), key=lambda x: x[1], reverse=True)

print("\nğŸ” IDF ìƒìœ„ 10ê°œ í† í° (í¬ê·€í•œ ë‹¨ì–´ - ë†’ì€ ê°€ì¤‘ì¹˜):")
for i, (token, score) in enumerate(sorted_idf[:10], 1):
    print(f"  {i:2d}. {token:15s} - IDF: {score:.4f}")

print("\nğŸ”» IDF í•˜ìœ„ 10ê°œ í† í° (í”í•œ ë‹¨ì–´ - ë‚®ì€ ê°€ì¤‘ì¹˜):")
for i, (token, score) in enumerate(sorted_idf[-10:], 1):
    print(f"  {i:2d}. {token:15s} - IDF: {score:.4f}")

# Step 2: íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…
print("\n" + "=" * 60)
print("Step 2: íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë¶€ìŠ¤íŒ…")
print("=" * 60)

TREND_BOOST = {
    'llm': 1.5,
    'gpt': 1.5,
    'chatgpt': 1.5,
    'ìƒì„±í˜•': 1.4,
    'rag': 1.4,
    'opensearch': 1.3,
    'neural': 1.3,
    'sparse': 1.3,
    'ê²€ìƒ‰': 1.2,
    'ì¸ê³µì§€ëŠ¥': 1.2,
    'bert': 1.2,
    'ì„ë² ë”©': 1.3,
}

idf_dict_boosted = idf_dict.copy()
boost_count = 0

print("íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì ìš©:")
for keyword, boost_factor in TREND_BOOST.items():
    if keyword in idf_dict_boosted:
        original = idf_dict_boosted[keyword]
        idf_dict_boosted[keyword] = original * boost_factor
        boost_count += 1
        print(f"  âœ“ {keyword:15s}: {original:.4f} â†’ {idf_dict_boosted[keyword]:.4f} ({boost_factor}x)")

print(f"\nâœ“ {boost_count}ê°œ í† í°ì— ë¶€ìŠ¤íŒ… ì ìš©")

# Step 3: Inference-Free ì¿¼ë¦¬ ì¸ì½”ë”©
print("\n" + "=" * 60)
print("Step 3: Inference-Free ì¿¼ë¦¬ ì¸ì½”ë”© (IDF Lookup)")
print("=" * 60)

def encode_query_inference_free(query, idf_dict):
    """
    ì¿¼ë¦¬ë¥¼ sparse vectorë¡œ ë³€í™˜ (IDF lookup)
    ğŸ”¥ ì´ê²ƒì´ Inference-Freeì˜ í•µì‹¬ì…ë‹ˆë‹¤!
    """
    tokens = simple_tokenize(query)

    # í† í°ë³„ë¡œ IDF ê°’ì„ ê°€ì ¸ì˜´ (ëª¨ë¸ inference ì—†ìŒ!)
    sparse_vec = {}
    for token in tokens:
        if token in idf_dict:
            sparse_vec[token] = idf_dict[token]

    return sparse_vec

print("ì¿¼ë¦¬ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸:\n")

for query in SAMPLE_QUERIES:
    sparse_vec = encode_query_inference_free(query, idf_dict_boosted)

    print(f"Query: '{query}'")
    print(f"  Tokens: {list(sparse_vec.keys())}")
    print(f"  Sparse vector (non-zero: {len(sparse_vec)}):")

    for token, weight in sorted(sparse_vec.items(), key=lambda x: x[1], reverse=True):
        print(f"    {token:15s}: {weight:.4f}")
    print()

# Step 4: ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜
print("\n" + "=" * 60)
print("Step 4: ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜")
print("=" * 60)

def encode_document_simple(doc, idf_dict):
    """
    ë¬¸ì„œë¥¼ sparse vectorë¡œ ë³€í™˜ (ë‹¨ìˆœí™”)
    ì‹¤ì œë¡œëŠ” BERT ëª¨ë¸ ì‚¬ìš©
    """
    tokens = simple_tokenize(doc)
    sparse_vec = {}

    for token in tokens:
        if token in idf_dict:
            # ë‹¨ìˆœí™”: ì‹¤ì œë¡œëŠ” BERT MLM headì˜ logitsë¥¼ ì‚¬ìš©
            sparse_vec[token] = idf_dict[token] * 0.5  # ê°€ì¤‘ì¹˜ ì¡°ì •

    return sparse_vec

def calculate_similarity(query_vec, doc_vec):
    """Dot product similarity"""
    similarity = 0.0
    for token, weight in query_vec.items():
        if token in doc_vec:
            similarity += weight * doc_vec[token]
    return similarity

# ëª¨ë“  ë¬¸ì„œ ì¸ì½”ë”© (ì‹¤ì œë¡œëŠ” ì¸ë±ì‹± íƒ€ì„ì— ìˆ˜í–‰)
print("ëª¨ë“  ë¬¸ì„œë¥¼ sparse vectorë¡œ ì¸ì½”ë”© ì¤‘...\n")
doc_vectors = [encode_document_simple(doc, idf_dict_boosted) for doc in SAMPLE_DOCUMENTS]

# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
test_queries = [
    "ì¸ê³µì§€ëŠ¥ ë¨¸ì‹ ëŸ¬ë‹",
    "OpenSearch neural sparse ê²€ìƒ‰",
    "í•œêµ­ì–´ ì²˜ë¦¬",
]

print("ğŸ” ê²€ìƒ‰ ê²°ê³¼:\n")
print("=" * 60)

for query in test_queries:
    print(f"\nğŸ” Query: '{query}'")

    # ì¿¼ë¦¬ ì¸ì½”ë”© (Inference-Free!)
    query_vec = encode_query_inference_free(query, idf_dict_boosted)

    # ê° ë¬¸ì„œì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = []
    for i, doc_vec in enumerate(doc_vectors):
        sim = calculate_similarity(query_vec, doc_vec)
        similarities.append((i, sim))

    # ìƒìœ„ 3ê°œ ê²°ê³¼
    similarities.sort(key=lambda x: x[1], reverse=True)
    top_results = similarities[:3]

    print("  ìƒìœ„ 3ê°œ ê²°ê³¼:")
    for rank, (doc_idx, sim_score) in enumerate(top_results, 1):
        doc = SAMPLE_DOCUMENTS[doc_idx]
        print(f"    {rank}. [Score: {sim_score:.4f}] {doc[:60]}...")

# Step 5: idf.json ì €ì¥
print("\n" + "=" * 60)
print("Step 5: idf.json ì €ì¥ (OpenSearch í˜•ì‹)")
print("=" * 60)

output_file = "demo_idf.json"

with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(idf_dict_boosted, f, ensure_ascii=False, indent=2)

print(f"âœ“ IDF ê°€ì¤‘ì¹˜ ì €ì¥: {output_file}")
print(f"  í† í° ìˆ˜: {len(idf_dict_boosted)}")
print(f"  íŒŒì¼ í¬ê¸°: {len(json.dumps(idf_dict_boosted, ensure_ascii=False))} bytes")

# ìƒ˜í”Œ idf.json ë‚´ìš© ì¶œë ¥
print("\nidf.json ìƒ˜í”Œ:")
print("-" * 60)
sample_tokens = list(idf_dict_boosted.items())[:5]
sample_json = {token: weight for token, weight in sample_tokens}
print(json.dumps(sample_json, ensure_ascii=False, indent=2))

# ìš”ì•½
print("\n" + "=" * 60)
print("âœ… ë°ëª¨ ì™„ë£Œ!")
print("=" * 60)

print(f"""
í•µì‹¬ ì»¨ì…‰:

1. ğŸ“Š IDF ê³„ì‚°
   - ë¬¸ì„œì—ì„œ ê° í† í°ì˜ í¬ê·€ë„ë¥¼ ê³„ì‚°
   - í¬ê·€í•œ í† í° = ë†’ì€ IDF = ì¤‘ìš”í•œ í† í°

2. ğŸ”¥ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…
   - 2024-2025 íŠ¸ë Œë“œ í‚¤ì›Œë“œ (LLM, GPT ë“±)ì— ê°€ì¤‘ì¹˜ ì¦ê°€
   - ìµœì‹  í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œ ë” ë†’ì€ ì ìˆ˜

3. âš¡ Inference-Free ì¿¼ë¦¬ ì¸ì½”ë”©
   - ì¿¼ë¦¬: Tokenizer + IDF lookupë§Œ ì‚¬ìš©
   - ëª¨ë¸ inference ë¶ˆí•„ìš” â†’ ë§¤ìš° ë¹ ë¦„!
   - BM25ì™€ ìœ ì‚¬í•œ ì§€ì—°ì‹œê°„ (1.1x)

4. ğŸš€ ë¬¸ì„œ ì¸ì½”ë”©
   - ë¬¸ì„œ: BERT ëª¨ë¸ë¡œ sparse vector ìƒì„±
   - ì¸ë±ì‹± íƒ€ì„ì—ë§Œ ìˆ˜í–‰ (í•œ ë²ˆë§Œ)
   - ì¶œë ¥: rank_features íƒ€ì…

5. ğŸ“ OpenSearch í˜•ì‹
   - idf.json: ì¿¼ë¦¬ìš© í† í° ê°€ì¤‘ì¹˜
   - pytorch_model.bin: ë¬¸ì„œ ì¸ì½”ë”
   - tokenizer files: BERT tokenizer

ë‹¤ìŒ ë‹¨ê³„:
  âœ“ ì „ì²´ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰: python3 test_korean_neural_sparse.py
  âœ“ Jupyter ë…¸íŠ¸ë¶ ì‹¤í–‰: korean_neural_sparse_training.ipynb
  âœ“ OpenSearchì— ëª¨ë¸ ë°°í¬
""")

print("\nğŸ‰ ë°ëª¨ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"ğŸ“„ ìƒì„±ëœ íŒŒì¼: {output_file}")

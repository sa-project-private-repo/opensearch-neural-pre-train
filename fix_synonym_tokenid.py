#!/usr/bin/env python3
"""
Fix build_synonym_dict_from_corpus to handle token IDs (integers) instead of token strings
"""

import json

# Read notebook
with open('korean_neural_sparse_training.ipynb', 'r', encoding='utf-8') as f:
    nb = json.load(f)

# Find the cell with build_synonym_dict_from_corpus function
for i, cell in enumerate(nb['cells']):
    if cell['cell_type'] == 'code':
        source = ''.join(cell['source'])
        if 'def build_synonym_dict_from_corpus' in source:
            print(f"Found build_synonym_dict_from_corpus at cell {i}")

            # Replace the function with fixed version
            new_source = '''def build_synonym_dict_from_corpus(documents, tokenizer, embeddings,
                                   idf_dict, top_n=500, threshold=0.75):
    """
    ìˆ˜ì§‘ëœ ë¬¸ì„œ ì½”í¼ìŠ¤ì—ì„œ ì¤‘ìš” í† í°ë“¤ì˜ ë™ì˜ì–´ë¥¼ ìë™ ë°œê²¬

    Args:
        documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        tokenizer: Tokenizer
        embeddings: Token embeddings
        idf_dict: IDF ë”•ì…”ë„ˆë¦¬ (token_id -> idf_score or token_str -> idf_score)
        top_n: ìƒìœ„ Nê°œ IDF í† í° ëŒ€ìƒ
        threshold: ìœ ì‚¬ë„ ì„ê³„ê°’

    Returns:
        synonym_dict: {token: [similar_tokens]}
    """
    print(f"\\nğŸ“– ìˆ˜ì§‘ëœ ë°ì´í„°ì—ì„œ ì¤‘ìš” í† í° ì¶”ì¶œ...")

    # IDF ê¸°ë°˜ ì¤‘ìš” í† í° ì„ ì •
    sorted_idf = sorted(idf_dict.items(), key=lambda x: x[1], reverse=True)

    # í•„í„°ë§: subword(##ë¡œ ì‹œì‘), íŠ¹ìˆ˜ë¬¸ì, ë‹¨ì¼ ë¬¸ì ì œì™¸
    important_tokens = []
    for token_or_id, idf_score in sorted_idf:
        if len(important_tokens) >= top_n:
            break

        # token_or_idê°€ ì •ìˆ˜(token ID)ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
        if isinstance(token_or_id, int):
            token = tokenizer.decode([token_or_id]).strip()
        else:
            token = token_or_id

        # í•„í„°ë§ ì¡°ê±´
        if (not token.startswith('##') and
            len(token) > 1 and
            not token in [',', '.', '!', '?', ':', ';', '-', '(', ')', '[', ']']):
            important_tokens.append(token)

    print(f"  ì¤‘ìš” í† í° {len(important_tokens)}ê°œ ì„ ì • ì™„ë£Œ")
    print(f"  ìƒìœ„ 10ê°œ: {important_tokens[:10]}")

    # ê° í† í°ì— ëŒ€í•´ ìœ ì‚¬ í† í° ì°¾ê¸°
    print(f"\\nğŸ” ìœ ì‚¬ í† í° ìë™ ë°œê²¬ ì¤‘... (threshold={threshold})")
    synonym_dict = {}

    for token in tqdm(important_tokens, desc="Finding synonyms"):
        similar = find_similar_tokens(token, tokenizer, embeddings,
                                      top_k=5, threshold=threshold)
        if similar:
            synonym_dict[token] = [t for t, _ in similar]

    return synonym_dict, important_tokens


print("âœ“ ë™ì˜ì–´ ë°œê²¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")'''

            cell['source'] = [new_source]
            print("âœ“ Function updated")
            break

# Save notebook
with open('korean_neural_sparse_training.ipynb', 'w', encoding='utf-8') as f:
    json.dump(nb, f, ensure_ascii=False, indent=1)

print("\nâœ“ Notebook saved successfully!")

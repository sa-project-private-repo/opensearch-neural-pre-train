{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v21.4 HuggingFace Upload Preparation\n",
    "\n",
    "This notebook prepares the trained v21.4 model for HuggingFace Hub upload.\n",
    "\n",
    "## v21.4 Improvements\n",
    "\n",
    "- **Curriculum Learning**: 3-phase training (single-term → balanced → full)\n",
    "- **Dynamic Lambda Self**: 8.0 for single terms → 4.0 for sentences\n",
    "- **Minimum Activation Loss**: Prevents garbage outputs\n",
    "- **Enhanced Training Data**: Added explicit single-term synonym pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Paths\nCHECKPOINT_PATH = PROJECT_ROOT / \"outputs\" / \"v21.4_korean_enhanced\" / \"best_model.pt\"\nOUTPUT_DIR = PROJECT_ROOT / \"huggingface\" / \"v21.4\"\n\n# Check if checkpoint exists\nif not CHECKPOINT_PATH.exists():\n    raise FileNotFoundError(f\"Checkpoint not found: {CHECKPOINT_PATH}\")\n\nprint(f\"Loading checkpoint from: {CHECKPOINT_PATH}\")\n\n# Load checkpoint (weights_only=False since we trust our own checkpoint)\ncheckpoint = torch.load(CHECKPOINT_PATH, map_location=\"cpu\", weights_only=False)\n\nprint(f\"Checkpoint keys: {checkpoint.keys()}\")\nprint(f\"Training epoch: {checkpoint.get('epoch')}\")\nprint(f\"Training phase: {checkpoint.get('phase')}\")\nprint(f\"Eval results: {checkpoint.get('eval_results')}\")\nprint(f\"Config: {checkpoint.get('config')}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model and tokenizer\n",
    "model_name = checkpoint.get('config', {}).get('model_name', 'skt/A.X-Encoder-base')\n",
    "max_length = checkpoint.get('config', {}).get('max_length', 64)\n",
    "\n",
    "print(f\"Base model: {model_name}\")\n",
    "print(f\"Max length: {max_length}\")\n",
    "\n",
    "# Initialize base model\n",
    "base_model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in base_model.parameters()):,}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained weights\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# The training model has 'model.' prefix for the transformer layers\n",
    "# We need to map these to the AutoModelForMaskedLM structure\n",
    "new_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    if key.startswith('model.'):\n",
    "        new_key = key[6:]  # Remove 'model.' prefix\n",
    "        new_state_dict[new_key] = value\n",
    "    else:\n",
    "        new_state_dict[key] = value\n",
    "\n",
    "# Load weights into base model\n",
    "base_model.load_state_dict(new_state_dict, strict=True)\n",
    "print(\"Weights loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Quick Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SPLADEInference:\n",
    "    \"\"\"SPLADE inference wrapper.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.relu = nn.ReLU()\n",
    "        self.model.eval()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def encode(self, text: str, top_k: int = 20):\n",
    "        \"\"\"Encode text to sparse representation.\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=64\n",
    "        ).to(self.device)\n",
    "        \n",
    "        outputs = self.model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # SPLADE: log(1 + ReLU(x))\n",
    "        token_scores = torch.log1p(self.relu(logits))\n",
    "        \n",
    "        # Mask padding\n",
    "        mask = inputs['attention_mask'].unsqueeze(-1).float()\n",
    "        token_scores = token_scores * mask\n",
    "        \n",
    "        # Max pooling\n",
    "        sparse_repr = token_scores.max(dim=1).values[0]  # [vocab_size]\n",
    "        \n",
    "        # Get top tokens\n",
    "        top_values, top_indices = sparse_repr.topk(top_k)\n",
    "        \n",
    "        results = []\n",
    "        for idx, val in zip(top_indices.tolist(), top_values.tolist()):\n",
    "            if val > 0:\n",
    "                token = self.tokenizer.decode([idx]).strip()\n",
    "                results.append((token, round(val, 4)))\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Create inference wrapper\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "splade = SPLADEInference(base_model, tokenizer, device)\n",
    "print(f\"Inference device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test problem terms from v21.3\n",
    "problem_terms = ['추천', '데이터베이스', '증상', '질환', '인슐린']\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Problem Terms Evaluation (v21.4)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for term in problem_terms:\n",
    "    results = splade.encode(term, top_k=10)\n",
    "    print(f\"\\n{term}:\")\n",
    "    \n",
    "    # Check if self-token is in top results\n",
    "    has_self = any(term in token for token, _ in results[:5])\n",
    "    status = \"✓\" if has_self else \"✗\"\n",
    "    \n",
    "    for token, weight in results[:5]:\n",
    "        marker = \"←\" if term in token else \"\"\n",
    "        print(f\"  {token}: {weight:.4f} {marker}\")\n",
    "    \n",
    "    print(f\"  Status: {status} Self-reconstruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test general terms\n",
    "general_terms = ['검색', '법률', '의료', '암', '약물']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"General Terms Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for term in general_terms:\n",
    "    results = splade.encode(term, top_k=10)\n",
    "    print(f\"\\n{term}:\")\n",
    "    for token, weight in results[:5]:\n",
    "        print(f\"  {token}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test natural language queries\n",
    "nl_queries = [\n",
    "    '당뇨병 환자의 인슐린 치료 방법',\n",
    "    '부동산 계약 해지 조건',\n",
    "    '암 환자를 위한 식이요법 추천',\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Natural Language Queries\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for query in nl_queries:\n",
    "    results = splade.encode(query, top_k=15)\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Top tokens: {', '.join([f'{t}({w:.2f})' for t, w in results[:10]])[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 3. Save for HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model and tokenizer in HuggingFace format\n",
    "base_model.save_pretrained(OUTPUT_DIR, safe_serialization=True)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Model saved to: {OUTPUT_DIR}\")\n",
    "print(f\"\\nSaved files:\")\n",
    "for f in sorted(OUTPUT_DIR.iterdir()):\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy modeling_splade.py for custom model loading\n",
    "modeling_src = PROJECT_ROOT / \"huggingface\" / \"modeling_splade.py\"\n",
    "modeling_dst = OUTPUT_DIR / \"modeling_splade.py\"\n",
    "\n",
    "if modeling_src.exists():\n",
    "    shutil.copy(modeling_src, modeling_dst)\n",
    "    print(f\"Copied modeling_splade.py\")\n",
    "else:\n",
    "    print(f\"Warning: {modeling_src} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create README\n",
    "readme_content = f'''---\n",
    "language:\n",
    "- ko\n",
    "license: apache-2.0\n",
    "library_name: transformers\n",
    "tags:\n",
    "- sparse-retrieval\n",
    "- splade\n",
    "- opensearch\n",
    "- korean\n",
    "- neural-sparse\n",
    "pipeline_tag: feature-extraction\n",
    "---\n",
    "\n",
    "# Korean Neural Sparse Encoder v21.4\n",
    "\n",
    "한국어 신경망 희소 인코더 - OpenSearch Neural Sparse 검색을 위한 SPLADE 기반 모델\n",
    "\n",
    "## Model Description\n",
    "\n",
    "This model is a SPLADE-based sparse encoder fine-tuned for Korean text, specifically optimized for:\n",
    "- Legal domain terminology\n",
    "- Medical domain terminology  \n",
    "- General Korean synonym expansion\n",
    "\n",
    "### v21.4 Improvements\n",
    "\n",
    "- **Curriculum Learning**: 3-phase training focusing on single-terms → balanced → full coverage\n",
    "- **Dynamic Lambda Self**: Higher weight (8.0) for single-term self-reconstruction, lower (4.0) for sentences\n",
    "- **Minimum Activation Loss**: Prevents garbage outputs by ensuring meaningful top-k activations\n",
    "- **Enhanced Training Data**: Added explicit single-term synonym pairs for problem terms\n",
    "\n",
    "### Training Results\n",
    "\n",
    "- **Best Epoch**: {checkpoint.get('epoch', 'N/A')}\n",
    "- **Recall@1**: {checkpoint.get('eval_results', {{}}).get('recall', 'N/A'):.1f}%\n",
    "- **MRR**: {checkpoint.get('eval_results', {{}}).get('mrr', 'N/A'):.4f}\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sewoong/korean-neural-sparse-encoder-v21.4\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"sewoong/korean-neural-sparse-encoder-v21.4\")\n",
    "\n",
    "# Encode text\n",
    "def encode(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        relu = nn.ReLU()\n",
    "        token_scores = torch.log1p(relu(logits))\n",
    "        mask = inputs[\"attention_mask\"].unsqueeze(-1).float()\n",
    "        sparse_repr = (token_scores * mask).max(dim=1).values[0]\n",
    "    return sparse_repr\n",
    "\n",
    "# Example\n",
    "sparse = encode(\"당뇨병 치료 방법\")\n",
    "top_values, top_indices = sparse.topk(10)\n",
    "for idx, val in zip(top_indices, top_values):\n",
    "    print(f\"{{tokenizer.decode([idx])}}: {{val:.4f}}\")\n",
    "```\n",
    "\n",
    "## OpenSearch Integration\n",
    "\n",
    "This model is designed to work with OpenSearch Neural Sparse Search. See the [OpenSearch documentation](https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/) for integration details.\n",
    "\n",
    "## Base Model\n",
    "\n",
    "- **Base**: skt/A.X-Encoder-base\n",
    "- **Parameters**: {sum(p.numel() for p in base_model.parameters()):,}\n",
    "- **Vocabulary**: {tokenizer.vocab_size:,} tokens\n",
    "\n",
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@misc{{korean-neural-sparse-v21.4,\n",
    "  author = {{Sewoong Lee}},\n",
    "  title = {{Korean Neural Sparse Encoder v21.4}},\n",
    "  year = {{2025}},\n",
    "  publisher = {{HuggingFace}},\n",
    "  url = {{https://huggingface.co/sewoong/korean-neural-sparse-encoder-v21.4}}\n",
    "}}\n",
    "```\n",
    "'''\n",
    "\n",
    "with open(OUTPUT_DIR / \"README.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"README.md created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 4. Upload to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To upload, run in terminal:\n",
    "# huggingface-cli login\n",
    "# huggingface-cli upload sewoong/korean-neural-sparse-encoder-v21.4 ./huggingface/v21.4\n",
    "\n",
    "print(\"Upload Instructions:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Login to HuggingFace:\")\n",
    "print(\"   huggingface-cli login\")\n",
    "print(\"\\n2. Upload the model:\")\n",
    "print(f\"   huggingface-cli upload sewoong/korean-neural-sparse-encoder-v21.4 {OUTPUT_DIR}\")\n",
    "print(\"\\n3. Or use Python API:\")\n",
    "print(\"   from huggingface_hub import HfApi\")\n",
    "print(\"   api = HfApi()\")\n",
    "print(\"   api.upload_folder(\")\n",
    "print(f\"       folder_path='{OUTPUT_DIR}',\")\n",
    "print(\"       repo_id='sewoong/korean-neural-sparse-encoder-v21.4',\")\n",
    "print(\"       repo_type='model'\")\n",
    "print(\"   )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"v21.4 HuggingFace Preparation Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel directory: {OUTPUT_DIR}\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(\"  1. Review the generated README.md\")\n",
    "print(\"  2. Upload to HuggingFace Hub\")\n",
    "print(\"  3. Run OpenSearch integration tests\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
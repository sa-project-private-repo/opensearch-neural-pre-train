{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenSearch Inference-Free Neural Sparse ëª¨ë¸ - í•œêµ­ì–´ í•™ìŠµ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **OpenSearch inference-free IR ëª¨ë¸** í‘œì¤€ì— ë”°ë¼ í•œêµ­ì–´ neural sparse ê²€ìƒ‰ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ¯ OpenSearch ëª¨ë¸ êµ¬ì¡°\n",
    "\n",
    "### Doc-only Mode (Inference-Free)\n",
    "- **ë¬¸ì„œ ì¸ì½”ë”©**: BERT ê¸°ë°˜ ì‹ ê²½ë§ â†’ sparse vector\n",
    "- **ì¿¼ë¦¬ ì¸ì½”ë”©**: Tokenizer + **idf.json** (weight lookup table) â†’ **Inference-Free!**\n",
    "\n",
    "### í•µì‹¬ íŒŒì¼\n",
    "1. `pytorch_model.bin` - ë¬¸ì„œ ì¸ì½”ë” ëª¨ë¸ (BERT ê¸°ë°˜)\n",
    "2. `idf.json` - í† í°ë³„ ê°€ì¤‘ì¹˜ lookup table (ì¿¼ë¦¬ìš©)\n",
    "3. `tokenizer.json`, `vocab.txt` - í† í¬ë‚˜ì´ì €\n",
    "4. `config.json` - ëª¨ë¸ ì„¤ì •\n",
    "\n",
    "### í•™ìŠµ ë°©ë²•\n",
    "- **IDF-aware Penalty**: ë‚®ì€ IDF í† í°ì˜ ê¸°ì—¬ë„ ì–µì œ\n",
    "- **Knowledge Distillation**: Dense + Sparse siamese ëª¨ë¸ì—ì„œ í•™ìŠµ\n",
    "- **â„“0 Sparsification**: L0 regularizationìœ¼ë¡œ í¬ì†Œì„± ìœ ì§€\n",
    "\n",
    "### ì°¸ê³  ë…¼ë¬¸\n",
    "- [Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers](https://arxiv.org/abs/2411.04403)\n",
    "- [Exploring â„“0 Sparsification for Inference-free Sparse Retrievers](https://opensearch.org/blog/)\n",
    "\n",
    "### OpenSearch Models Collection\n",
    "- https://huggingface.co/collections/opensearch-project/inference-free-ir-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ” ì‹œìŠ¤í…œ í™˜ê²½ ê°ì§€\n",
      "============================================================\n",
      "Python ë²„ì „: 3.12.11 (main, Aug  6 2025, 05:55:32) [GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]\n",
      "âœ“ Python 3.12+ ê°ì§€ - ìµœì‹  íŒ¨í‚¤ì§€ ë²„ì „ ì‚¬ìš©\n",
      "âœ“ Amazon Linux 2023: True\n",
      "âœ“ Ubuntu/Debian: False\n",
      "\n",
      "============================================================\n",
      "ğŸ“¦ Python íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
      "============================================================\n",
      "requirements.txtë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤...\n",
      "GPU: Tesla T4 ì§€ì› (CUDA 12.1)\n",
      "\n",
      "ğŸ”¥ PyTorch ì„¤ì¹˜ ì¤‘ (GPU ë²„ì „ - CUDA 12.1)...\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ğŸ“š ë‚˜ë¨¸ì§€ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "âœ“ Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ\n",
      "\n",
      "============================================================\n",
      "ğŸ”§ ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
      "============================================================\n",
      "ğŸ§ Amazon Linux 2023 í™˜ê²½ - dnf íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ì‚¬ìš©\n",
      "\n",
      "ğŸ“¦ ê°œë°œ ë„êµ¬ ì„¤ì¹˜ ì¤‘...\n",
      "Last metadata expiration check: 7:35:09 ago on Tue Nov 11 02:04:05 2025.\n",
      "Package gcc-11.5.0-5.amzn2023.0.4.x86_64 is already installed.\n",
      "Package gcc-c++-11.5.0-5.amzn2023.0.4.x86_64 is already installed.\n",
      "Package make-1:4.3-5.amzn2023.0.2.x86_64 is already installed.\n",
      "Package automake-1.16.5-9.amzn2023.0.3.noarch is already installed.\n",
      "Package libtool-2.4.7-1.amzn2023.0.3.x86_64 is already installed.\n",
      "Dependencies resolved.\n",
      "Nothing to do.\n",
      "Complete!\n",
      "\n",
      "â˜• Java ì„¤ì¹˜ ì¤‘...\n",
      "Last metadata expiration check: 7:35:10 ago on Tue Nov 11 02:04:05 2025.\n",
      "Package java-17-amazon-corretto-devel-1:17.0.15+6-1.amzn2023.1.x86_64 is already installed.\n",
      "Dependencies resolved.\n",
      "Nothing to do.\n",
      "Complete!\n",
      "\n",
      "ğŸ”¤ Mecab ì—”ì§„ ì„¤ì¹˜ ì¤‘...\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 1381k  100 1381k    0     0  4952k      0 --:--:-- --:--:-- --:--:-- 4952k\n",
      "checking for a BSD-compatible install... /usr/bin/install -c\n",
      "checking whether build environment is sane... yes\n",
      "checking for a thread-safe mkdir -p... /usr/bin/mkdir -p\n",
      "checking for gawk... gawk\n",
      "checking whether make sets $(MAKE)... yes\n",
      "checking for gcc... gcc\n",
      "checking whether the C compiler works... yes\n",
      "checking for C compiler default output file name... a.out\n",
      "checking for suffix of executables... \n",
      "checking whether we are cross compiling... no\n",
      "checking for suffix of object files... o\n",
      "checking whether we are using the GNU C compiler... yes\n",
      "checking whether gcc accepts -g... yes\n",
      "checking for gcc option to accept ISO C89... none needed\n",
      "checking for style of include used by make... GNU\n",
      "checking dependency style of gcc... none\n",
      "checking for g++... g++\n",
      "checking whether we are using the GNU C++ compiler... yes\n",
      "checking whether g++ accepts -g... yes\n",
      "checking dependency style of g++... none\n",
      "checking how to run the C preprocessor... gcc -E\n",
      "checking for grep that handles long lines and -e... /usr/bin/grep\n",
      "checking for egrep... /usr/bin/grep -E\n",
      "checking whether gcc needs -traditional... no\n",
      "checking whether make sets $(MAKE)... (cached) yes\n",
      "checking build system type... x86_64-unknown-linux-gnu\n",
      "checking host system type... x86_64-unknown-linux-gnu\n",
      "checking how to print strings... printf\n",
      "checking for a sed that does not truncate output... /usr/bin/sed\n",
      "checking for fgrep... /usr/bin/grep -F\n",
      "checking for ld used by gcc... /usr/bin/ld\n",
      "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
      "checking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B\n",
      "checking the name lister (/usr/bin/nm -B) interface... BSD nm\n",
      "checking whether ln -s works... yes\n",
      "checking the maximum length of command line arguments... 1966080\n",
      "checking whether the shell understands some XSI constructs... yes\n",
      "checking whether the shell understands \"+=\"... yes\n",
      "checking how to convert x86_64-unknown-linux-gnu file names to x86_64-unknown-linux-gnu format... func_convert_file_noop\n",
      "checking how to convert x86_64-unknown-linux-gnu file names to toolchain format... func_convert_file_noop\n",
      "checking for /usr/bin/ld option to reload object files... -r\n",
      "checking for objdump... objdump\n",
      "checking how to recognize dependent libraries... pass_all\n",
      "checking for dlltool... dlltool\n",
      "checking how to associate runtime and link libraries... printf %s\\n\n",
      "checking for ar... ar\n",
      "checking for archiver @FILE support... @\n",
      "checking for strip... strip\n",
      "checking for ranlib... ranlib\n",
      "checking command to parse /usr/bin/nm -B output from gcc object... ok\n",
      "checking for sysroot... no\n",
      "checking for mt... no\n",
      "checking if : is a manifest tool... no\n",
      "checking for ANSI C header files... yes\n",
      "checking for sys/types.h... yes\n",
      "checking for sys/stat.h... yes\n",
      "checking for stdlib.h... yes\n",
      "checking for string.h... yes\n",
      "checking for memory.h... yes\n",
      "checking for strings.h... yes\n",
      "checking for inttypes.h... yes\n",
      "checking for stdint.h... yes\n",
      "checking for unistd.h... yes\n",
      "checking for dlfcn.h... yes\n",
      "checking for objdir... .libs\n",
      "checking if gcc supports -fno-rtti -fno-exceptions... no\n",
      "checking for gcc option to produce PIC... -fPIC -DPIC\n",
      "checking if gcc PIC flag -fPIC -DPIC works... yes\n",
      "checking if gcc static flag -static works... no\n",
      "checking if gcc supports -c -o file.o... yes\n",
      "checking if gcc supports -c -o file.o... (cached) yes\n",
      "checking whether the gcc linker (/usr/bin/ld -m elf_x86_64) supports shared libraries... yes\n",
      "checking whether -lc should be explicitly linked in... no\n",
      "checking dynamic linker characteristics... GNU/Linux ld.so\n",
      "checking how to hardcode library paths into programs... immediate\n",
      "checking whether stripping libraries is possible... yes\n",
      "checking if libtool supports shared libraries... yes\n",
      "checking whether to build shared libraries... yes\n",
      "checking whether to build static libraries... yes\n",
      "checking how to run the C++ preprocessor... g++ -E\n",
      "checking for ld used by g++... /usr/bin/ld -m elf_x86_64\n",
      "checking if the linker (/usr/bin/ld -m elf_x86_64) is GNU ld... yes\n",
      "checking whether the g++ linker (/usr/bin/ld -m elf_x86_64) supports shared libraries... yes\n",
      "checking for g++ option to produce PIC... -fPIC -DPIC\n",
      "checking if g++ PIC flag -fPIC -DPIC works... yes\n",
      "checking if g++ static flag -static works... no\n",
      "checking if g++ supports -c -o file.o... yes\n",
      "checking if g++ supports -c -o file.o... (cached) yes\n",
      "checking whether the g++ linker (/usr/bin/ld -m elf_x86_64) supports shared libraries... yes\n",
      "checking dynamic linker characteristics... (cached) GNU/Linux ld.so\n",
      "checking how to hardcode library paths into programs... immediate\n",
      "checking for library containing strerror... none required\n",
      "checking whether byte ordering is bigendian... no\n",
      "checking for ld used by GCC... /usr/bin/ld -m elf_x86_64\n",
      "checking if the linker (/usr/bin/ld -m elf_x86_64) is GNU ld... yes\n",
      "checking for shared library run path origin... done\n",
      "checking for iconv... yes\n",
      "checking for working iconv... yes\n",
      "checking for iconv declaration... \n",
      "         extern size_t iconv (iconv_t cd, char * *inbuf, size_t *inbytesleft, char * *outbuf, size_t *outbytesleft);\n",
      "checking for ANSI C header files... (cached) yes\n",
      "checking for an ANSI C-conforming const... yes\n",
      "checking whether byte ordering is bigendian... (cached) no\n",
      "checking for string.h... (cached) yes\n",
      "checking for stdlib.h... (cached) yes\n",
      "checking for unistd.h... (cached) yes\n",
      "checking fcntl.h usability... yes\n",
      "checking fcntl.h presence... yes\n",
      "checking for fcntl.h... yes\n",
      "checking for stdint.h... (cached) yes\n",
      "checking for sys/stat.h... (cached) yes\n",
      "checking sys/mman.h usability... yes\n",
      "checking sys/mman.h presence... yes\n",
      "checking for sys/mman.h... yes\n",
      "checking sys/times.h usability... yes\n",
      "checking sys/times.h presence... yes\n",
      "checking for sys/times.h... yes\n",
      "checking for sys/types.h... (cached) yes\n",
      "checking dirent.h usability... yes\n",
      "checking dirent.h presence... yes\n",
      "checking for dirent.h... yes\n",
      "checking ctype.h usability... yes\n",
      "checking ctype.h presence... yes\n",
      "checking for ctype.h... yes\n",
      "checking for sys/types.h... (cached) yes\n",
      "checking io.h usability... no\n",
      "checking io.h presence... no\n",
      "checking for io.h... no\n",
      "checking windows.h usability... no\n",
      "checking windows.h presence... no\n",
      "checking for windows.h... no\n",
      "checking pthread.h usability... yes\n",
      "checking pthread.h presence... yes\n",
      "checking for pthread.h... yes\n",
      "checking for off_t... yes\n",
      "checking for size_t... yes\n",
      "checking size of char... 1\n",
      "checking size of short... 2\n",
      "checking size of int... 4\n",
      "checking size of long... 8\n",
      "checking size of long long... 8\n",
      "checking size of size_t... 8\n",
      "checking for size_t... (cached) yes\n",
      "checking for unsigned long long int... yes\n",
      "checking for stdlib.h... (cached) yes\n",
      "checking for unistd.h... (cached) yes\n",
      "checking for sys/param.h... yes\n",
      "checking for getpagesize... yes\n",
      "checking for working mmap... yes\n",
      "checking for main in -lstdc++... yes\n",
      "checking for pthread_create in -lpthread... yes\n",
      "checking for pthread_join in -lpthread... yes\n",
      "checking for getenv... yes\n",
      "checking for opendir... yes\n",
      "checking whether make is GNU Make... yes\n",
      "checking if g++ supports stl <vector> (required)... yes\n",
      "checking if g++ supports stl <list> (required)... yes\n",
      "checking if g++ supports stl <map> (required)... yes\n",
      "checking if g++ supports stl <set> (required)... yes\n",
      "checking if g++ supports stl <queue> (required)... yes\n",
      "checking if g++ supports stl <functional> (required)... yes\n",
      "checking if g++ supports stl <algorithm> (required)... yes\n",
      "checking if g++ supports stl <string> (required)... yes\n",
      "checking if g++ supports stl <iostream> (required)... yes\n",
      "checking if g++ supports stl <sstream> (required)... yes\n",
      "checking if g++ supports stl <fstream> (required)... yes\n",
      "checking if g++ supports template <class T> (required)... yes\n",
      "checking if g++ supports const_cast<> (required)... yes\n",
      "checking if g++ supports static_cast<> (required)... yes\n",
      "checking if g++ supports reinterpret_cast<> (required)... yes\n",
      "checking if g++ supports namespaces (required) ... yes\n",
      "checking if g++ supports __thread (optional)... yes\n",
      "checking if g++ supports template <class T> (required)... yes\n",
      "checking if g++ supports GCC native atomic operations (optional)... yes\n",
      "checking if g++ supports OSX native atomic operations (optional)... no\n",
      "checking if g++ environment provides all required features... yes\n",
      "configure: creating ./config.status\n",
      "config.status: creating Makefile\n",
      "config.status: creating src/Makefile\n",
      "config.status: creating src/Makefile.msvc\n",
      "config.status: creating man/Makefile\n",
      "config.status: creating doc/Makefile\n",
      "config.status: creating tests/Makefile\n",
      "config.status: creating swig/version.h\n",
      "config.status: creating mecab.iss\n",
      "config.status: creating mecab-config\n",
      "config.status: creating mecabrc\n",
      "config.status: creating config.h\n",
      "config.status: config.h is unchanged\n",
      "config.status: executing depfiles commands\n",
      "config.status: executing libtool commands\n",
      "config.status: executing default commands\n",
      "make  all-recursive\n",
      "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\n",
      "Making all in src\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
      "make[2]: Nothing to be done for 'all'.\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
      "Making all in man\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
      "make[2]: Nothing to be done for 'all'.\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
      "Making all in doc\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
      "make[2]: Nothing to be done for 'all'.\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
      "Making all in tests\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
      "make[2]: Nothing to be done for 'all'.\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\n",
      "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\n",
      "Making install in src\n",
      "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
      "test -z \"/usr/local/lib\" || /usr/bin/mkdir -p \"/usr/local/lib\"\n",
      " /bin/sh ../libtool   --mode=install /usr/bin/install -c   libmecab.la '/usr/local/lib'\n",
      "libtool: install: /usr/bin/install -c .libs/libmecab.so.2.0.0 /usr/local/lib/libmecab.so.2.0.0\n",
      "libtool: install: (cd /usr/local/lib && { ln -s -f libmecab.so.2.0.0 libmecab.so.2 || { rm -f libmecab.so.2 && ln -s libmecab.so.2.0.0 libmecab.so.2; }; })\n",
      "libtool: install: (cd /usr/local/lib && { ln -s -f libmecab.so.2.0.0 libmecab.so || { rm -f libmecab.so && ln -s libmecab.so.2.0.0 libmecab.so; }; })\n",
      "libtool: install: /usr/bin/install -c .libs/libmecab.lai /usr/local/lib/libmecab.la\n",
      "libtool: install: /usr/bin/install -c .libs/libmecab.a /usr/local/lib/libmecab.a\n",
      "libtool: install: chmod 644 /usr/local/lib/libmecab.a\n",
      "libtool: install: ranlib /usr/local/lib/libmecab.a\n",
      "libtool: finish: PATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/var/lib/snapd/snap/bin:/sbin\" ldconfig -n /usr/local/lib\n",
      "----------------------------------------------------------------------\n",
      "Libraries have been installed in:\n",
      "   /usr/local/lib\n",
      "\n",
      "If you ever happen to want to link against installed libraries\n",
      "in a given directory, LIBDIR, you must either use libtool, and\n",
      "specify the full pathname of the library, or use the `-LLIBDIR'\n",
      "flag during linking and do at least one of the following:\n",
      "   - add LIBDIR to the `LD_LIBRARY_PATH' environment variable\n",
      "     during execution\n",
      "   - add LIBDIR to the `LD_RUN_PATH' environment variable\n",
      "     during linking\n",
      "   - use the `-Wl,-rpath -Wl,LIBDIR' linker flag\n",
      "   - have your system administrator add LIBDIR to `/etc/ld.so.conf'\n",
      "\n",
      "See any operating system documentation about shared libraries for\n",
      "more information, such as the ld(1) and ld.so(8) manual pages.\n",
      "----------------------------------------------------------------------\n",
      "test -z \"/usr/local/bin\" || /usr/bin/mkdir -p \"/usr/local/bin\"\n",
      "  /bin/sh ../libtool   --mode=install /usr/bin/install -c mecab '/usr/local/bin'\n",
      "libtool: install: /usr/bin/install -c .libs/mecab /usr/local/bin/mecab\n",
      "test -z \"/usr/local/libexec/mecab\" || /usr/bin/mkdir -p \"/usr/local/libexec/mecab\"\n",
      "  /bin/sh ../libtool   --mode=install /usr/bin/install -c mecab-dict-index mecab-dict-gen mecab-cost-train mecab-system-eval mecab-test-gen '/usr/local/libexec/mecab'\n",
      "libtool: install: /usr/bin/install -c .libs/mecab-dict-index /usr/local/libexec/mecab/mecab-dict-index\n",
      "libtool: install: /usr/bin/install -c .libs/mecab-dict-gen /usr/local/libexec/mecab/mecab-dict-gen\n",
      "libtool: install: /usr/bin/install -c .libs/mecab-cost-train /usr/local/libexec/mecab/mecab-cost-train\n",
      "libtool: install: /usr/bin/install -c .libs/mecab-system-eval /usr/local/libexec/mecab/mecab-system-eval\n",
      "libtool: install: /usr/bin/install -c .libs/mecab-test-gen /usr/local/libexec/mecab/mecab-test-gen\n",
      "test -z \"/usr/local/include\" || /usr/bin/mkdir -p \"/usr/local/include\"\n",
      " /usr/bin/install -c -m 644 mecab.h '/usr/local/include'\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
      "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
      "Making install in man\n",
      "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
      "make[2]: Nothing to be done for 'install-exec-am'.\n",
      "test -z \"/usr/local/share/man/man1\" || /usr/bin/mkdir -p \"/usr/local/share/man/man1\"\n",
      " /usr/bin/install -c -m 644 mecab.1 '/usr/local/share/man/man1'\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
      "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
      "Making install in doc\n",
      "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
      "make[2]: Nothing to be done for 'install-exec-am'.\n",
      "make[2]: Nothing to be done for 'install-data-am'.\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
      "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
      "Making install in tests\n",
      "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
      "make[2]: Nothing to be done for 'install-exec-am'.\n",
      "make[2]: Nothing to be done for 'install-data-am'.\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
      "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
      "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\n",
      "test -z \"/usr/local/bin\" || /usr/bin/mkdir -p \"/usr/local/bin\"\n",
      " /usr/bin/install -c mecab-config '/usr/local/bin'\n",
      "test -z \"/usr/local/etc\" || /usr/bin/mkdir -p \"/usr/local/etc\"\n",
      " /usr/bin/install -c -m 644 mecabrc '/usr/local/etc'\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\n",
      "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\n",
      "\n",
      "ğŸ“š Mecab í•œêµ­ì–´ ì‚¬ì „ ì„¤ì¹˜ ì¤‘...\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 47.4M  100 47.4M    0     0  70.6M      0 --:--:-- --:--:-- --:--:-- 70.6M\n",
      "Looking in current directory for macros.\n",
      "configure.ac:2: warning: AM_INIT_AUTOMAKE: two- and three-arguments forms are deprecated.  For more info, see:\n",
      "configure.ac:2: https://www.gnu.org/software/automake/manual/automake.html#Modernize-AM_005fINIT_005fAUTOMAKE-invocation\n",
      "checking for a BSD-compatible install... /usr/bin/install -c\n",
      "checking whether build environment is sane... yes\n",
      "/tmp/mecab-ko-dic-2.1.1-20180720/missing: Unknown `--is-lightweight' option\n",
      "Try `/tmp/mecab-ko-dic-2.1.1-20180720/missing --help' for more information\n",
      "configure: WARNING: 'missing' script is too old or missing\n",
      "checking for a thread-safe mkdir -p... /usr/bin/mkdir -p\n",
      "checking for gawk... gawk\n",
      "checking whether make sets $(MAKE)... yes\n",
      "checking whether make supports nested variables... yes\n",
      "checking for mecab-config... /usr/local/bin/mecab-config\n",
      "checking that generated files are newer than configure... done\n",
      "configure: creating ./config.status\n",
      "config.status: creating Makefile\n",
      "make: Nothing to be done for 'all'.\n",
      "make[1]: Entering directory '/tmp/mecab-ko-dic-2.1.1-20180720'\n",
      "make[1]: Nothing to be done for 'install-exec-am'.\n",
      " /usr/bin/mkdir -p '/usr/local/lib/mecab/dic/mecab-ko-dic'\n",
      " /usr/bin/install -c -m 644 model.bin matrix.bin char.bin sys.dic unk.dic left-id.def right-id.def rewrite.def pos-id.def dicrc '/usr/local/lib/mecab/dic/mecab-ko-dic'\n",
      "make[1]: Leaving directory '/tmp/mecab-ko-dic-2.1.1-20180720'\n",
      "\n",
      "âœ“ Amazon Linux 2023 ì„¤ì¹˜ ì™„ë£Œ!\n",
      "\n",
      "============================================================\n",
      "âœ… ì„¤ì¹˜ í™•ì¸\n",
      "============================================================\n",
      "âœ“ PyTorch: 2.5.1+cu124\n",
      "  - CUDA ë¹Œë“œ ë²„ì „: 12.4\n",
      "âœ“ Transformers: 4.46.3\n",
      "âœ“ Datasets: 2.21.0\n",
      "âœ“ Numpy: 2.1.3\n",
      "âœ“ Mecab: ['í•œêµ­ì–´', 'í˜•íƒœì†Œ', 'ë¶„ì„', 'í…ŒìŠ¤íŠ¸']\n",
      "\n",
      "âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ” ì‹œìŠ¤í…œ í™˜ê²½ ê°ì§€\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Python ë²„ì „ í™•ì¸\n",
    "print(f\"Python ë²„ì „: {sys.version}\")\n",
    "python_version = sys.version_info\n",
    "if python_version.major == 3 and python_version.minor >= 12:\n",
    "    print(f\"âœ“ Python 3.12+ ê°ì§€ - ìµœì‹  íŒ¨í‚¤ì§€ ë²„ì „ ì‚¬ìš©\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Python 3.12+ ê¶Œì¥ (í˜„ì¬: {python_version.major}.{python_version.minor})\")\n",
    "\n",
    "# OS ê°ì§€\n",
    "os_release = \"\"\n",
    "if os.path.exists(\"/etc/os-release\"):\n",
    "    with open(\"/etc/os-release\") as f:\n",
    "        os_release = f.read().lower()\n",
    "\n",
    "is_amazon_linux = \"amazon linux\" in os_release or \"amzn\" in os_release\n",
    "is_ubuntu_debian = \"ubuntu\" in os_release or \"debian\" in os_release\n",
    "\n",
    "print(f\"âœ“ Amazon Linux 2023: {is_amazon_linux}\")\n",
    "print(f\"âœ“ Ubuntu/Debian: {is_ubuntu_debian}\")\n",
    "\n",
    "# Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ (requirements.txt ì‚¬ìš©)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“¦ Python íŒ¨í‚¤ì§€ ì„¤ì¹˜\")\n",
    "print(\"=\"*60)\n",
    "print(\"requirements.txtë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤...\")\n",
    "print(\"GPU: Tesla T4 ì§€ì› (CUDA 12.1)\")\n",
    "print()\n",
    "\n",
    "# PyTorch GPU ë²„ì „ ì„¤ì¹˜ (CUDA 12.1 for Tesla T4)\n",
    "print(\"ğŸ”¥ PyTorch ì„¤ì¹˜ ì¤‘ (GPU ë²„ì „ - CUDA 12.1)...\")\n",
    "%pip install -q torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# ë‚˜ë¨¸ì§€ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "print(\"ğŸ“š ë‚˜ë¨¸ì§€ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...\")\n",
    "%pip install -q -r requirements.txt\n",
    "\n",
    "print(\"\\nâœ“ Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ\")\n",
    "\n",
    "# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ë° Mecab ì„¤ì¹˜ (OSë³„ ë¶„ê¸°)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ”§ ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if is_amazon_linux:\n",
    "    print(\"ğŸ§ Amazon Linux 2023 í™˜ê²½ - dnf íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ì‚¬ìš©\")\n",
    "    \n",
    "    # í•„ìˆ˜ ê°œë°œ ë„êµ¬ ì„¤ì¹˜\n",
    "    print(\"\\nğŸ“¦ ê°œë°œ ë„êµ¬ ì„¤ì¹˜ ì¤‘...\")\n",
    "    !sudo dnf install -y gcc gcc-c++ make automake libtool\n",
    "    \n",
    "    # Java ì„¤ì¹˜ (Mecab ì‚¬ì „ ë¹Œë“œìš©)\n",
    "    print(\"\\nâ˜• Java ì„¤ì¹˜ ì¤‘...\")\n",
    "    !sudo dnf install -y java-17-amazon-corretto-devel\n",
    "    \n",
    "    # Mecab ì—”ì§„ ì„¤ì¹˜\n",
    "    print(\"\\nğŸ”¤ Mecab ì—”ì§„ ì„¤ì¹˜ ì¤‘...\")\n",
    "    !cd /tmp && \\\n",
    "     curl -LO https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz && \\\n",
    "     tar -zxf mecab-0.996-ko-0.9.2.tar.gz && \\\n",
    "     cd mecab-0.996-ko-0.9.2 && \\\n",
    "     ./configure && make && sudo make install && sudo ldconfig\n",
    "    \n",
    "    # Mecab í•œêµ­ì–´ ì‚¬ì „ ì„¤ì¹˜\n",
    "    print(\"\\nğŸ“š Mecab í•œêµ­ì–´ ì‚¬ì „ ì„¤ì¹˜ ì¤‘...\")\n",
    "    !cd /tmp && \\\n",
    "     curl -LO https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz && \\\n",
    "     tar -zxf mecab-ko-dic-2.1.1-20180720.tar.gz && \\\n",
    "     cd mecab-ko-dic-2.1.1-20180720 && \\\n",
    "     ./autogen.sh && ./configure && make && sudo make install\n",
    "    \n",
    "    print(\"\\nâœ“ Amazon Linux 2023 ì„¤ì¹˜ ì™„ë£Œ!\")\n",
    "\n",
    "elif is_ubuntu_debian:\n",
    "    print(\"ğŸ§ Ubuntu/Debian í™˜ê²½ - apt-get íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ì‚¬ìš©\")\n",
    "    \n",
    "    # í•„ìˆ˜ ê°œë°œ ë„êµ¬ ì„¤ì¹˜\n",
    "    print(\"\\nğŸ“¦ ê°œë°œ ë„êµ¬ ì„¤ì¹˜ ì¤‘...\")\n",
    "    !sudo apt-get update -qq\n",
    "    !sudo apt-get install -y -qq g++ openjdk-8-jdk python3-dev automake libtool\n",
    "    \n",
    "    # Mecab ì„¤ì¹˜ (konlpy ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©)\n",
    "    print(\"\\nğŸ”¤ Mecab ì„¤ì¹˜ ì¤‘...\")\n",
    "    !bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
    "    \n",
    "    print(\"\\nâœ“ Ubuntu/Debian ì„¤ì¹˜ ì™„ë£Œ!\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nâš ï¸  ì§€ì›ë˜ì§€ ì•ŠëŠ” OSì…ë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.\")\n",
    "    print(\"ì§€ì› OS: Amazon Linux 2023, Ubuntu, Debian\")\n",
    "\n",
    "# ì„¤ì¹˜ í™•ì¸\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… ì„¤ì¹˜ í™•ì¸\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# PyTorch í™•ì¸\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"âœ“ PyTorch: {torch.__version__}\")\n",
    "    print(f\"  - CUDA ë¹Œë“œ ë²„ì „: {torch.version.cuda}\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— PyTorch ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# Transformers í™•ì¸\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"âœ“ Transformers: {transformers.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Transformers ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# Datasets í™•ì¸\n",
    "try:\n",
    "    import datasets\n",
    "    print(f\"âœ“ Datasets: {datasets.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Datasets ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# Numpy í™•ì¸\n",
    "try:\n",
    "    import numpy\n",
    "    print(f\"âœ“ Numpy: {numpy.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Numpy ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# Mecab í™•ì¸\n",
    "try:\n",
    "    from konlpy.tag import Mecab\n",
    "    mecab = Mecab()\n",
    "    test_result = mecab.morphs(\"í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ í…ŒìŠ¤íŠ¸\")\n",
    "    print(f\"âœ“ Mecab: {test_result}\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Mecab ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"  Mecab ì„¤ì¹˜ ë¬¸ì œ í•´ê²°:\")\n",
    "    print(\"  1. sudo ldconfig\")\n",
    "    print(\"  2. export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH\")\n",
    "    print(\"  3. pip install --force-reinstall mecab-python3\")\n",
    "\n",
    "print(\"\\nâœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ–¥ï¸  GPU/CPU í™˜ê²½ í™•ì¸\n",
      "============================================================\n",
      "PyTorch ë²„ì „: 2.5.1+cu124\n",
      "CUDA ì‚¬ìš© ê°€ëŠ¥: True\n",
      "âœ“ GPU ì‚¬ìš© ê°€ëŠ¥!\n",
      "  - CUDA ë²„ì „: 12.4\n",
      "  - GPU ê°œìˆ˜: 1\n",
      "  - GPU 0: Tesla T4\n",
      "    * ë©”ëª¨ë¦¬: 14.57 GB\n",
      "    * Compute Capability: 7.5\n",
      "\n",
      "â†’ í•™ìŠµ ë””ë°”ì´ìŠ¤: GPU (cuda)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Hugging Face & Transformers\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForMaskedLM,\n",
    "    AutoConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    get_linear_schedule_with_warmup  # ì´ í•¨ìˆ˜ëŠ” transformersì—ì„œ ì œê³µ\n",
    ")\n",
    "\n",
    "# í˜•íƒœì†Œ ë¶„ì„\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# GPU/CPU í™˜ê²½ í™•ì¸\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ–¥ï¸  GPU/CPU í™˜ê²½ í™•ì¸\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ GPU ì‚¬ìš© ê°€ëŠ¥!\")\n",
    "    print(f\"  - CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "    print(f\"  - GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  - GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    * ë©”ëª¨ë¦¬: {props.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"    * Compute Capability: {props.major}.{props.minor}\")\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\\nâ†’ í•™ìŠµ ë””ë°”ì´ìŠ¤: GPU (cuda)\")\n",
    "else:\n",
    "    print(f\"âš ï¸  GPU ì‚¬ìš© ë¶ˆê°€\")\n",
    "    print(f\"  - CPUë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤\")\n",
    "    print(f\"  - í•™ìŠµ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"\\nâ†’ í•™ìŠµ ë””ë°”ì´ìŠ¤: CPU\")\n",
    "    print(f\"\\nğŸ’¡ GPU ì‚¬ìš© ê¶Œì¥:\")\n",
    "    print(f\"  - AWS EC2: g4dn.xlarge ì´ìƒ (NVIDIA T4 GPU)\")\n",
    "    print(f\"  - AWS EC2: p3.2xlarge ì´ìƒ (NVIDIA V100 GPU)\")\n",
    "    print(f\"  - Google Colab: GPU ëŸ°íƒ€ì„ ì‚¬ìš©\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í•œêµ­ì–´ ë°ì´í„°ì…‹ ìˆ˜ì§‘\n",
    "\n",
    "Hugging Faceì—ì„œ ë‹¤ì–‘í•œ í•œêµ­ì–´ ê³µê°œ ë°ì´í„°ì…‹ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š í•œêµ­ì–´ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\n",
      "\n",
      "1ï¸âƒ£ KLUE MRC ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ“ 17,554 query-document pairs\n",
      "\n",
      "2ï¸âƒ£ KorQuAD v1 ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ“ 60,407 query-document pairs\n",
      "\n",
      "3ï¸âƒ£ í•œêµ­ì–´ Wikipedia ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ— Wikipedia ë¡œë”© ì‹¤íŒ¨: Couldn't find file at https://dumps.wikimedia.org/kowiki/20220301/dumpstatus.json\n",
      "\n",
      "4ï¸âƒ£ KLUE Topic Classification ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ“ 45,678 documents\n",
      "\n",
      "5ï¸âƒ£ í•œêµ­ì–´ ë‰´ìŠ¤ ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ“ 50,000 documents\n",
      "\n",
      "\n",
      "============================================================\n",
      "ì´ 117,914ê°œì˜ ê³ ìœ  ë¬¸ì„œ\n",
      "ì´ 77,785ê°œì˜ ê³ ìœ  ì¿¼ë¦¬\n",
      "ì´ 77,961ê°œì˜ query-document pairs\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def load_korean_datasets():\n",
    "    \"\"\"\n",
    "    Hugging Faceì—ì„œ í•œêµ­ì–´ ê³µê°œ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "    Query-Document ìŒì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì…‹ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    datasets_collection = {\n",
    "        'documents': [],\n",
    "        'queries': [],\n",
    "        'qd_pairs': []  # (query, document, relevance) íŠœí”Œ\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ“š í•œêµ­ì–´ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\\n\")\n",
    "    \n",
    "    # 1. KLUE MRC (Machine Reading Comprehension)\n",
    "    try:\n",
    "        print(\"1ï¸âƒ£ KLUE MRC ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        klue_mrc = load_dataset(\"klue\", \"mrc\", split=\"train\")\n",
    "        \n",
    "        for item in klue_mrc:\n",
    "            doc = item['context']\n",
    "            query = item['question']\n",
    "            \n",
    "            datasets_collection['documents'].append(doc)\n",
    "            datasets_collection['queries'].append(query)\n",
    "            datasets_collection['qd_pairs'].append((query, doc, 1.0))  # Positive pair\n",
    "        \n",
    "        print(f\"   âœ“ {len(klue_mrc):,} query-document pairs\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— KLUE MRC ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # 2. KorQuAD v1\n",
    "    try:\n",
    "        print(\"2ï¸âƒ£ KorQuAD v1 ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        korquad = load_dataset(\"squad_kor_v1\", split=\"train\")\n",
    "        \n",
    "        for item in korquad:\n",
    "            doc = item['context']\n",
    "            query = item['question']\n",
    "            \n",
    "            datasets_collection['documents'].append(doc)\n",
    "            datasets_collection['queries'].append(query)\n",
    "            datasets_collection['qd_pairs'].append((query, doc, 1.0))\n",
    "        \n",
    "        print(f\"   âœ“ {len(korquad):,} query-document pairs\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— KorQuAD ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # 3. Korean Wikipedia (ë¬¸ì„œ ì½”í¼ìŠ¤)\n",
    "    try:\n",
    "        print(\"3ï¸âƒ£ í•œêµ­ì–´ Wikipedia ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        ko_wiki = load_dataset(\"wikipedia\", \"20220301.ko\", split=\"train[:100000]\")\n",
    "        \n",
    "        for item in ko_wiki:\n",
    "            text = item['text']\n",
    "            if len(text) > 100:  # ìµœì†Œ ê¸¸ì´ í•„í„°\n",
    "                datasets_collection['documents'].append(text[:2000])  # ì²˜ìŒ 2000ì\n",
    "        \n",
    "        print(f\"   âœ“ {len(ko_wiki):,} documents\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— Wikipedia ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # 4. KLUE Topic Classification (ë¬¸ì„œ ì½”í¼ìŠ¤)\n",
    "    try:\n",
    "        print(\"4ï¸âƒ£ KLUE Topic Classification ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        klue_tc = load_dataset(\"klue\", \"ynat\", split=\"train\")\n",
    "        \n",
    "        for item in klue_tc:\n",
    "            datasets_collection['documents'].append(item['title'])\n",
    "        \n",
    "        print(f\"   âœ“ {len(klue_tc):,} documents\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— KLUE TC ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # 5. Korean News Dataset\n",
    "    try:\n",
    "        print(\"5ï¸âƒ£ í•œêµ­ì–´ ë‰´ìŠ¤ ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        korean_news = load_dataset(\"heegyu/news-category-dataset\", split=\"train[:50000]\")\n",
    "        \n",
    "        for item in korean_news:\n",
    "            if 'headline' in item:\n",
    "                datasets_collection['documents'].append(item['headline'])\n",
    "        \n",
    "        print(f\"   âœ“ {len(korean_news):,} documents\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— Korean News ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # ì¤‘ë³µ ì œê±°\n",
    "    datasets_collection['documents'] = list(set([\n",
    "        doc.strip() for doc in datasets_collection['documents'] \n",
    "        if doc and len(doc.strip()) > 10\n",
    "    ]))\n",
    "    \n",
    "    datasets_collection['queries'] = list(set([\n",
    "        q.strip() for q in datasets_collection['queries'] \n",
    "        if q and len(q.strip()) > 5\n",
    "    ]))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ì´ {len(datasets_collection['documents']):,}ê°œì˜ ê³ ìœ  ë¬¸ì„œ\")\n",
    "    print(f\"ì´ {len(datasets_collection['queries']):,}ê°œì˜ ê³ ìœ  ì¿¼ë¦¬\")\n",
    "    print(f\"ì´ {len(datasets_collection['qd_pairs']):,}ê°œì˜ query-document pairs\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return datasets_collection\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "korean_data = load_korean_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. IDF (Inverse Document Frequency) ê³„ì‚°\n",
    "\n",
    "ì¿¼ë¦¬ìš© í† í° ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ IDFë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "ì´ê²ƒì´ **idf.json** íŒŒì¼ì˜ ê¸°ë°˜ì´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ: klue/bert-base\n",
      "  Vocab size: 32,000\n",
      "\n",
      "ğŸ“Š IDF ê³„ì‚° ì¤‘ (ìƒ˜í”Œ: 50,000ê°œ ë¬¸ì„œ)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029f6f128c2342898787908cda065d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing documents:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ IDF ê³„ì‚° ì™„ë£Œ\n",
      "  ì´ 29,220ê°œ í† í°\n",
      "  í‰ê·  IDF: 8.9028\n",
      "  IDF ë²”ìœ„: [2.3054, 11.1267]\n",
      "\n",
      "ğŸ” IDF ìƒìœ„ 20ê°œ í† í° (í¬ê·€ í† í°):\n",
      " 1. ì†Œê°œíŒ…                  - IDF: 11.1267\n",
      " 2. ##ëƒ‰í‚¤                 - IDF: 11.1267\n",
      " 3. ##ì•ˆê²½                 - IDF: 11.1267\n",
      " 4. ì‹œí”¼                   - IDF: 11.1267\n",
      " 5. ë¯¸ë„ëŸ¬ì ¸                 - IDF: 11.1267\n",
      " 6. ##ì¬                  - IDF: 11.1267\n",
      " 7. í˜ëŸ¬ë‚´ë ¸                 - IDF: 11.1267\n",
      " 8. ##sych               - IDF: 11.1267\n",
      " 9. ì–´ì—¬                   - IDF: 11.1267\n",
      "10. ë§¤ëˆ                   - IDF: 11.1267\n",
      "11. ë¾°                    - IDF: 11.1267\n",
      "12. ë…¸í¬                   - IDF: 11.1267\n",
      "13. ##ì„ê±°ë¦¬                - IDF: 11.1267\n",
      "14. ê¼¬ë§‰                   - IDF: 11.1267\n",
      "15. ìí™”ìƒ                  - IDF: 11.1267\n",
      "16. ##ì ã†                 - IDF: 11.1267\n",
      "17. WBC                  - IDF: 11.1267\n",
      "18. ê¹                    - IDF: 11.1267\n",
      "19. ë‹¤ì„¯ì§¸                  - IDF: 11.1267\n",
      "20. ì•”í–‰                   - IDF: 11.1267\n",
      "\n",
      "ğŸ”» IDF í•˜ìœ„ 20ê°œ í† í° (í”í•œ í† í°):\n",
      " 1. â€¦                    - IDF: 2.7901\n",
      " 2. ##ì—ì„œ                 - IDF: 2.7729\n",
      " 3. ##ìœ¼ë¡œ                 - IDF: 2.7319\n",
      " 4. ##ê°€                  - IDF: 2.6928\n",
      " 5. ##í•œ                  - IDF: 2.6921\n",
      " 6. '                    - IDF: 2.6898\n",
      " 7. ##í•˜                  - IDF: 2.6880\n",
      " 8. ##ë¥¼                  - IDF: 2.6872\n",
      " 9. ##ê³                   - IDF: 2.6758\n",
      "10. ##ë¡œ                  - IDF: 2.6728\n",
      "11. ##ì„                  - IDF: 2.6361\n",
      "12. ##ì´                  - IDF: 2.6126\n",
      "13. ##ì€                  - IDF: 2.5861\n",
      "14. ##ë‹¤                  - IDF: 2.5523\n",
      "15. ##ì˜                  - IDF: 2.5157\n",
      "16. ##ëŠ”                  - IDF: 2.4741\n",
      "17. ,                    - IDF: 2.4058\n",
      "18. .                    - IDF: 2.4026\n",
      "19. ##ì—                  - IDF: 2.3461\n",
      "20. ##s                  - IDF: 2.3054\n"
     ]
    }
   ],
   "source": [
    "# í•œêµ­ì–´ BERT í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "MODEL_NAME = \"klue/bert-base\"  # í•œêµ­ì–´ ìµœì í™” BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ: {MODEL_NAME}\")\n",
    "print(f\"  Vocab size: {len(tokenizer):,}\")\n",
    "\n",
    "def calculate_idf(documents, tokenizer, sample_size=50000):\n",
    "    \"\"\"\n",
    "    ì½”í¼ìŠ¤ì—ì„œ IDFë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    IDF(t) = log(N / df(t))\n",
    "    \n",
    "    Args:\n",
    "        documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        tokenizer: Hugging Face tokenizer\n",
    "        sample_size: ìƒ˜í”Œë§í•  ë¬¸ì„œ ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        dict: {token_id: idf_score}\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“Š IDF ê³„ì‚° ì¤‘ (ìƒ˜í”Œ: {min(sample_size, len(documents)):,}ê°œ ë¬¸ì„œ)...\")\n",
    "    \n",
    "    # ìƒ˜í”Œë§\n",
    "    sample_docs = documents[:sample_size]\n",
    "    N = len(sample_docs)\n",
    "    \n",
    "    # ê° í† í°ì´ ë‚˜íƒ€ë‚œ ë¬¸ì„œ ìˆ˜ ê³„ì‚°\n",
    "    df = Counter()  # document frequency\n",
    "    \n",
    "    for i, doc in enumerate(tqdm(sample_docs, desc=\"Tokenizing documents\")):\n",
    "        # í† í°í™”\n",
    "        tokens = tokenizer.encode(doc, add_special_tokens=False, max_length=512, truncation=True)\n",
    "        \n",
    "        # ë¬¸ì„œì— ë‚˜íƒ€ë‚œ ê³ ìœ  í† í°ë“¤\n",
    "        unique_tokens = set(tokens)\n",
    "        \n",
    "        # df ì—…ë°ì´íŠ¸\n",
    "        for token_id in unique_tokens:\n",
    "            df[token_id] += 1\n",
    "    \n",
    "    # IDF ê³„ì‚°\n",
    "    idf_dict = {}\n",
    "    for token_id, doc_freq in df.items():\n",
    "        # IDF = log(N / df)\n",
    "        idf_score = math.log((N + 1) / (doc_freq + 1)) + 1.0  # smoothing\n",
    "        idf_dict[token_id] = idf_score\n",
    "    \n",
    "    # í† í° ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "    idf_token_dict = {}\n",
    "    for token_id, score in idf_dict.items():\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        idf_token_dict[token_str] = float(score)\n",
    "    \n",
    "    print(f\"\\nâœ“ IDF ê³„ì‚° ì™„ë£Œ\")\n",
    "    print(f\"  ì´ {len(idf_token_dict):,}ê°œ í† í°\")\n",
    "    print(f\"  í‰ê·  IDF: {np.mean(list(idf_token_dict.values())):.4f}\")\n",
    "    print(f\"  IDF ë²”ìœ„: [{min(idf_token_dict.values()):.4f}, {max(idf_token_dict.values()):.4f}]\")\n",
    "    \n",
    "    return idf_token_dict, idf_dict\n",
    "\n",
    "# IDF ê³„ì‚°\n",
    "idf_token_dict, idf_id_dict = calculate_idf(korean_data['documents'], tokenizer)\n",
    "\n",
    "# ìƒìœ„/í•˜ìœ„ IDF í† í° ì¶œë ¥\n",
    "print(\"\\nğŸ” IDF ìƒìœ„ 20ê°œ í† í° (í¬ê·€ í† í°):\")\n",
    "sorted_idf = sorted(idf_token_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for i, (token, score) in enumerate(sorted_idf[:20], 1):\n",
    "    print(f\"{i:2d}. {token:20s} - IDF: {score:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ”» IDF í•˜ìœ„ 20ê°œ í† í° (í”í•œ í† í°):\")\n",
    "for i, (token, score) in enumerate(sorted_idf[-20:], 1):\n",
    "    print(f\"{i:2d}. {token:20s} - IDF: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. í•œêµ­ì–´ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì¶”ê°€\n",
    "\n",
    "2024-2025 íŠ¸ë Œë“œ í‚¤ì›Œë“œì— ëŒ€í•´ IDF ê°€ì¤‘ì¹˜ë¥¼ ë¶€ìŠ¤íŒ…í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë¶€ìŠ¤íŒ…...\n",
      "\n",
      "  Boosted: L                    4.5943 â†’ 6.8915 (1.5x)\n",
      "  Boosted: ##L                  5.5190 â†’ 8.2785 (1.5x)\n",
      "  Boosted: ##M                  5.9850 â†’ 8.9775 (1.5x)\n",
      "  Boosted: GP                   8.9294 â†’ 13.3941 (1.5x)\n",
      "  Boosted: ##T                  5.3773 â†’ 8.0659 (1.5x)\n",
      "  Boosted: Ch                   4.6382 â†’ 6.9573 (1.5x)\n",
      "  Boosted: ##at                 4.5958 â†’ 6.8937 (1.5x)\n",
      "  Boosted: ##G                  5.2933 â†’ 7.9400 (1.5x)\n",
      "  Boosted: ##P                  5.0152 â†’ 7.5228 (1.5x)\n",
      "  Boosted: ##T                  8.0659 â†’ 12.0988 (1.5x)\n",
      "  Boosted: [UNK]                3.8546 â†’ 5.7819 (1.5x)\n",
      "  Boosted: ìƒì„±                   7.3425 â†’ 10.2794 (1.4x)\n",
      "  Boosted: ##í˜•                  4.6636 â†’ 6.5291 (1.4x)\n",
      "  Boosted: Ge                   4.9322 â†’ 6.9051 (1.4x)\n",
      "  Boosted: ##n                  3.1097 â†’ 4.3535 (1.4x)\n",
      "  Boosted: ##A                  5.5301 â†’ 7.7421 (1.4x)\n",
      "  Boosted: ##I                  6.2067 â†’ 8.6893 (1.4x)\n",
      "  Boosted: R                    3.7383 â†’ 5.2337 (1.4x)\n",
      "  Boosted: ##A                  7.7421 â†’ 10.8390 (1.4x)\n",
      "  Boosted: ##G                  7.9400 â†’ 11.1159 (1.4x)\n",
      "  Boosted: íŠ¸ëœìŠ¤                  8.9294 â†’ 11.6083 (1.3x)\n",
      "  Boosted: ##í¬                  5.2860 â†’ 6.8718 (1.3x)\n",
      "  Boosted: ##ë¨¸                  6.6840 â†’ 8.6892 (1.3x)\n",
      "  Boosted: Trans                6.2983 â†’ 8.1878 (1.3x)\n",
      "  Boosted: ##form               6.6103 â†’ 8.5934 (1.3x)\n",
      "  Boosted: ##er                 3.4892 â†’ 4.5359 (1.3x)\n",
      "  Boosted: ì„                    6.5366 â†’ 8.4976 (1.3x)\n",
      "  Boosted: ##ë²                   6.5215 â†’ 8.4779 (1.3x)\n",
      "  Boosted: ##ë”©                  6.5675 â†’ 8.5378 (1.3x)\n",
      "  Boosted: e                    7.1103 â†’ 9.2433 (1.3x)\n",
      "  Boosted: ##m                  3.9834 â†’ 5.1785 (1.3x)\n",
      "  Boosted: ##be                 6.1227 â†’ 7.9595 (1.3x)\n",
      "  Boosted: ##d                  3.1889 â†’ 4.1456 (1.3x)\n",
      "  Boosted: ##d                  4.1456 â†’ 5.3893 (1.3x)\n",
      "  Boosted: ##ing                3.1684 â†’ 4.1190 (1.3x)\n",
      "  Boosted: ë²¡                    9.6226 â†’ 12.5093 (1.3x)\n",
      "  Boosted: ##í„°                  5.0410 â†’ 6.5533 (1.3x)\n",
      "  Boosted: v                    6.9756 â†’ 9.0683 (1.3x)\n",
      "  Boosted: ##ect                5.8134 â†’ 7.5575 (1.3x)\n",
      "  Boosted: ##or                 4.2744 â†’ 5.5567 (1.3x)\n",
      "  Boosted: í¬ì†Œ                   9.6226 â†’ 12.5093 (1.3x)\n",
      "  Boosted: sp                   9.1807 â†’ 11.9350 (1.3x)\n",
      "  Boosted: ##ar                 4.1893 â†’ 5.4461 (1.3x)\n",
      "  Boosted: ##se                 4.5474 â†’ 5.9116 (1.3x)\n",
      "  Boosted: íŒŒì¸                   8.8241 â†’ 11.4713 (1.3x)\n",
      "  Boosted: ##íŠœ                  7.9280 â†’ 10.3064 (1.3x)\n",
      "  Boosted: ##ë‹                  6.5728 â†’ 8.5446 (1.3x)\n",
      "  Boosted: f                    7.6454 â†’ 9.9390 (1.3x)\n",
      "  Boosted: ##ine                5.5413 â†’ 7.2037 (1.3x)\n",
      "  Boosted: -                    3.5373 â†’ 4.5985 (1.3x)\n",
      "  Boosted: t                    4.8913 â†’ 6.3586 (1.3x)\n",
      "  Boosted: ##un                 4.8709 â†’ 6.3322 (1.3x)\n",
      "  Boosted: ##ing                4.1190 â†’ 5.3546 (1.3x)\n",
      "  Boosted: í”„                    7.4253 â†’ 10.3955 (1.4x)\n",
      "  Boosted: ##ë¡¬                  8.1562 â†’ 11.4187 (1.4x)\n",
      "  Boosted: ##í”„íŠ¸                 7.0407 â†’ 9.8569 (1.4x)\n",
      "  Boosted: pro                  8.6417 â†’ 12.0984 (1.4x)\n",
      "  Boosted: ##mp                 6.1362 â†’ 8.5907 (1.4x)\n",
      "  Boosted: ##t                  3.3189 â†’ 4.6465 (1.4x)\n",
      "  Boosted: Op                   5.7376 â†’ 7.4589 (1.3x)\n",
      "  Boosted: ##en                 4.4371 â†’ 5.7682 (1.3x)\n",
      "  Boosted: ##S                  5.3185 â†’ 6.9141 (1.3x)\n",
      "  Boosted: ##earch              10.7212 â†’ 13.9375 (1.3x)\n",
      "  Boosted: El                   5.3757 â†’ 6.4508 (1.2x)\n",
      "  Boosted: ##ast                5.4707 â†’ 6.5648 (1.2x)\n",
      "  Boosted: ##ics                6.3475 â†’ 7.6170 (1.2x)\n",
      "  Boosted: ##earch              13.9375 â†’ 16.7251 (1.2x)\n",
      "  Boosted: ì‹œ                    5.2172 â†’ 6.7824 (1.3x)\n",
      "  Boosted: ##ë§¨                  7.0076 â†’ 9.1099 (1.3x)\n",
      "  Boosted: ##í‹±                  7.1193 â†’ 9.2551 (1.3x)\n",
      "  Boosted: se                   8.9294 â†’ 11.6083 (1.3x)\n",
      "  Boosted: ##man                5.8384 â†’ 7.5899 (1.3x)\n",
      "  Boosted: ##tic                7.2146 â†’ 9.3790 (1.3x)\n",
      "  Boosted: ê²€ìƒ‰                   7.2980 â†’ 8.7576 (1.2x)\n",
      "  Boosted: se                   11.6083 â†’ 13.9299 (1.2x)\n",
      "  Boosted: ##ar                 5.4461 â†’ 6.5354 (1.2x)\n",
      "  Boosted: ##ch                 5.1068 â†’ 6.1282 (1.2x)\n",
      "  Boosted: ì¸ê³µì§€ëŠ¥                 7.1377 â†’ 8.5652 (1.2x)\n",
      "  Boosted: AI                   6.2476 â†’ 7.4972 (1.2x)\n",
      "  Boosted: ë”¥                    9.1807 â†’ 11.0169 (1.2x)\n",
      "  Boosted: ##ëŸ¬                  4.8767 â†’ 5.8520 (1.2x)\n",
      "  Boosted: ##ë‹                  8.5446 â†’ 10.2535 (1.2x)\n",
      "  Boosted: ë¨¸ì‹                    9.0472 â†’ 10.8567 (1.2x)\n",
      "  Boosted: ##ëŸ¬                  5.8520 â†’ 7.0224 (1.2x)\n",
      "  Boosted: ##ë‹                  10.2535 â†’ 12.3042 (1.2x)\n",
      "  Boosted: B                    3.6772 â†’ 4.4126 (1.2x)\n",
      "  Boosted: ##ER                 8.0356 â†’ 9.6427 (1.2x)\n",
      "  Boosted: ##T                  12.0988 â†’ 14.5186 (1.2x)\n",
      "  Boosted: ì‹ ê²½                   7.0240 â†’ 8.4288 (1.2x)\n",
      "  Boosted: ##ë§                  5.9879 â†’ 7.1855 (1.2x)\n",
      "\n",
      "âœ“ 90ê°œ í† í°ì— íŠ¸ë Œë“œ ë¶€ìŠ¤íŒ… ì ìš©\n"
     ]
    }
   ],
   "source": [
    "# 2024-2025 íŠ¸ë Œë“œ í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ ë¶€ìŠ¤íŒ…)\n",
    "TREND_BOOST = {\n",
    "    # AI/ML íŠ¸ë Œë“œ\n",
    "    'LLM': 1.5,\n",
    "    'GPT': 1.5,\n",
    "    'ChatGPT': 1.5,\n",
    "    'ì±—GPT': 1.5,\n",
    "    'ìƒì„±í˜•': 1.4,\n",
    "    'GenAI': 1.4,\n",
    "    'RAG': 1.4,\n",
    "    'íŠ¸ëœìŠ¤í¬ë¨¸': 1.3,\n",
    "    'Transformer': 1.3,\n",
    "    'ì„ë² ë”©': 1.3,\n",
    "    'embedding': 1.3,\n",
    "    'ë²¡í„°': 1.3,\n",
    "    'vector': 1.3,\n",
    "    'í¬ì†Œ': 1.3,\n",
    "    'sparse': 1.3,\n",
    "    'íŒŒì¸íŠœë‹': 1.3,\n",
    "    'fine-tuning': 1.3,\n",
    "    'í”„ë¡¬í”„íŠ¸': 1.4,\n",
    "    'prompt': 1.4,\n",
    "    \n",
    "    # ê²€ìƒ‰ ê´€ë ¨\n",
    "    'OpenSearch': 1.3,\n",
    "    'Elasticsearch': 1.2,\n",
    "    'ì‹œë§¨í‹±': 1.3,\n",
    "    'semantic': 1.3,\n",
    "    'ê²€ìƒ‰': 1.2,\n",
    "    'search': 1.2,\n",
    "    \n",
    "    # ê¸°ë³¸ AI ìš©ì–´\n",
    "    'ì¸ê³µì§€ëŠ¥': 1.2,\n",
    "    'AI': 1.2,\n",
    "    'ë”¥ëŸ¬ë‹': 1.2,\n",
    "    'ë¨¸ì‹ ëŸ¬ë‹': 1.2,\n",
    "    'BERT': 1.2,\n",
    "    'ì‹ ê²½ë§': 1.2,\n",
    "}\n",
    "\n",
    "def apply_trend_boost(idf_dict, trend_boost, tokenizer):\n",
    "    \"\"\"\n",
    "    íŠ¸ë Œë“œ í‚¤ì›Œë“œì— ëŒ€í•´ IDF ê°€ì¤‘ì¹˜ë¥¼ ë¶€ìŠ¤íŒ…í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    boosted_idf = idf_dict.copy()\n",
    "    boost_count = 0\n",
    "    \n",
    "    for keyword, boost_factor in trend_boost.items():\n",
    "        # í‚¤ì›Œë“œë¥¼ í† í°í™”\n",
    "        keyword_tokens = tokenizer.encode(keyword, add_special_tokens=False)\n",
    "        \n",
    "        for token_id in keyword_tokens:\n",
    "            token_str = tokenizer.decode([token_id])\n",
    "            if token_str in boosted_idf:\n",
    "                original_idf = boosted_idf[token_str]\n",
    "                boosted_idf[token_str] = original_idf * boost_factor\n",
    "                boost_count += 1\n",
    "                print(f\"  Boosted: {token_str:20s} {original_idf:.4f} â†’ {boosted_idf[token_str]:.4f} ({boost_factor}x)\")\n",
    "    \n",
    "    print(f\"\\nâœ“ {boost_count}ê°œ í† í°ì— íŠ¸ë Œë“œ ë¶€ìŠ¤íŒ… ì ìš©\")\n",
    "    return boosted_idf\n",
    "\n",
    "print(\"ğŸ”¥ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë¶€ìŠ¤íŒ…...\\n\")\n",
    "idf_token_dict_boosted = apply_trend_boost(idf_token_dict, TREND_BOOST, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. AI ë„ë©”ì¸ íŠ¹í™” ìš©ì–´ì§‘ í†µí•©\n",
    "\n",
    "AI/ML/LLM ë„ë©”ì¸ì— íŠ¹í™”ëœ ìš©ì–´ì§‘ì„ ë¡œë“œí•˜ê³ ,\n",
    "ê¸°ìˆ  ìš©ì–´ë¥¼ tokenizer special tokensë¡œ ì¶”ê°€í•˜ì—¬ ë¶„ì ˆì„ ë°©ì§€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ AI ìš©ì–´ì§‘ ë¡œë“œ ì™„ë£Œ:\n",
      "  - ì£¼ìš” ìš©ì–´: 73ê°œ\n",
      "  - Special tokens: 33ê°œ\n",
      "  - ë™ì˜ì–´ ë§¤í•‘: 255ê°œ\n",
      "============================================================\n",
      "ğŸ¤– AI ë„ë©”ì¸ ìš©ì–´ì§‘ ë¡œë“œ\n",
      "============================================================\n",
      "âœ“ ì£¼ìš” ìš©ì–´ ì¹´í…Œê³ ë¦¬: 73ê°œ\n",
      "âœ“ Special tokens: 33ê°œ\n",
      "âœ“ ë™ì˜ì–´ ë§¤í•‘: 255ê°œ\n",
      "\n",
      "ğŸ“ ìƒ˜í”Œ ìš©ì–´:\n",
      "  ì¸ê³µì§€ëŠ¥: AI, Artificial Intelligence, ê¸°ê³„ì§€ëŠ¥\n",
      "  ë¨¸ì‹ ëŸ¬ë‹: Machine Learning, ML, ê¸°ê³„í•™ìŠµ\n",
      "  ë”¥ëŸ¬ë‹: Deep Learning, DL, ì‹¬ì¸µí•™ìŠµ\n",
      "  ìì—°ì–´ì²˜ë¦¬: NLP, Natural Language Processing, ì–¸ì–´ì²˜ë¦¬\n",
      "  ì‹ ê²½ë§: Neural Network, ì¸ê³µì‹ ê²½ë§, ë‰´ëŸ´ë„·\n"
     ]
    }
   ],
   "source": [
    "# AI ë„ë©”ì¸ ìš©ì–´ì§‘ ë¡œë“œ\n",
    "from ai_domain_terminology import (\n",
    "    AI_TERMINOLOGY,\n",
    "    TECHNICAL_SPECIAL_TOKENS,\n",
    "    AI_SYNONYMS\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ¤– AI ë„ë©”ì¸ ìš©ì–´ì§‘ ë¡œë“œ\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ“ ì£¼ìš” ìš©ì–´ ì¹´í…Œê³ ë¦¬: {len(AI_TERMINOLOGY)}ê°œ\")\n",
    "print(f\"âœ“ Special tokens: {len(TECHNICAL_SPECIAL_TOKENS)}ê°œ\")\n",
    "print(f\"âœ“ ë™ì˜ì–´ ë§¤í•‘: {len(AI_SYNONYMS)}ê°œ\")\n",
    "print()\n",
    "print(\"ğŸ“ ìƒ˜í”Œ ìš©ì–´:\")\n",
    "for i, (term, synonyms) in enumerate(list(AI_TERMINOLOGY.items())[:5]):\n",
    "    print(f\"  {term}: {', '.join(synonyms[:3])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1. Tokenizerì— ê¸°ìˆ  ìš©ì–´ ì¶”ê°€\n",
    "\n",
    "ChatGPT, OpenSearch ë“± ê¸°ìˆ  ìš©ì–´ê°€ ë¶„ì ˆë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´\n",
    "special tokensë¡œ ì¶”ê°€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”§ Tokenizerì— ê¸°ìˆ  ìš©ì–´ ì¶”ê°€\n",
      "============================================================\n",
      "Original vocab size: 32,000\n",
      "Added 33 new tokens\n",
      "New vocab size: 32,033\n",
      "\n",
      "âœ“ ì¶”ê°€ëœ ê¸°ìˆ  ìš©ì–´ ìƒ˜í”Œ:\n",
      "  ChatGPT              -> ID: 32000\n",
      "  OpenAI               -> ID: 32001\n",
      "  OpenSearch           -> ID: 32002\n",
      "  Elasticsearch        -> ID: 32003\n",
      "  Claude               -> ID: 32004\n",
      "  Gemini               -> ID: 32005\n",
      "  LLaMA                -> ID: 32006\n",
      "  GPT-4                -> ID: 32007\n",
      "  GPT-3                -> ID: 32008\n",
      "  LLM                  -> ID: 32009\n",
      "\n",
      "ğŸ§ª ë¶„ì ˆ ë°©ì§€ í…ŒìŠ¤íŠ¸:\n",
      "  'ChatGPTëŠ” ê°•ë ¥í•œ LLMì…ë‹ˆë‹¤'\n",
      "    â†’ ['ChatGPT', 'ëŠ”', 'ê°•ë ¥', '##í•œ', 'LLM', 'ì…ë‹ˆë‹¤']\n",
      "  'OpenSearch ë²¡í„°ê²€ìƒ‰ ê¸°ëŠ¥'\n",
      "    â†’ ['OpenSearch', 'ë²¡í„°ê²€ìƒ‰', 'ê¸°ëŠ¥']\n",
      "  'RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•'\n",
      "    â†’ ['RAG', 'íŒŒì´í”„', '##ë¼ì¸', 'êµ¬ì¶•']\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸ”§ Tokenizerì— ê¸°ìˆ  ìš©ì–´ ì¶”ê°€\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# í˜„ì¬ vocabulary í¬ê¸°\n",
    "original_vocab_size = len(tokenizer)\n",
    "print(f\"Original vocab size: {original_vocab_size:,}\")\n",
    "\n",
    "# Special tokens ì¶”ê°€\n",
    "num_added = tokenizer.add_tokens(TECHNICAL_SPECIAL_TOKENS)\n",
    "new_vocab_size = len(tokenizer)\n",
    "\n",
    "print(f\"Added {num_added} new tokens\")\n",
    "print(f\"New vocab size: {new_vocab_size:,}\")\n",
    "print()\n",
    "\n",
    "# ì¶”ê°€ëœ í† í° ìƒ˜í”Œ í™•ì¸\n",
    "print(\"âœ“ ì¶”ê°€ëœ ê¸°ìˆ  ìš©ì–´ ìƒ˜í”Œ:\")\n",
    "for token in TECHNICAL_SPECIAL_TOKENS[:10]:\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    print(f\"  {token:20s} -> ID: {token_id}\")\n",
    "\n",
    "print()\n",
    "print(\"ğŸ§ª ë¶„ì ˆ ë°©ì§€ í…ŒìŠ¤íŠ¸:\")\n",
    "test_texts = [\n",
    "    \"ChatGPTëŠ” ê°•ë ¥í•œ LLMì…ë‹ˆë‹¤\",\n",
    "    \"OpenSearch ë²¡í„°ê²€ìƒ‰ ê¸°ëŠ¥\",\n",
    "    \"RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\",\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(f\"  '{text}'\")\n",
    "    print(f\"    â†’ {tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2. ë„ë©”ì¸ ë™ì˜ì–´ ë§¤í•‘ ìƒì„±\n",
    "\n",
    "AI ë„ë©”ì¸ ìš©ì–´ì§‘ì˜ ë™ì˜ì–´ë¥¼ í™œìš©í•˜ì—¬ ê²€ìƒ‰ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”— ë„ë©”ì¸ ë™ì˜ì–´ ë§¤í•‘\n",
      "============================================================\n",
      "âœ“ ë„ë©”ì¸ ë™ì˜ì–´ ë”•ì…”ë„ˆë¦¬ ìƒì„± ì™„ë£Œ\n",
      "  ì´ 255ê°œ í•­ëª©\n",
      "\n",
      "ğŸ“ ìƒ˜í”Œ ë™ì˜ì–´ ë§¤í•‘:\n",
      "  ê²€ìƒ‰              â†’ search, íƒìƒ‰, ì¡°íšŒ\n",
      "  ì¸ê³µì§€ëŠ¥            â†’ ai, artificial intelligence, ê¸°ê³„ì§€ëŠ¥\n",
      "  llm             â†’ ëŒ€ê·œëª¨ì–¸ì–´ëª¨ë¸\n",
      "  chatgpt         â†’ ì±—gpt, ì±—ì§€í”¼í‹°, gpt\n",
      "  ì„ë² ë”©             â†’ embedding, ë²¡í„°ë³€í™˜, ë²¡í„°í‘œí˜„\n",
      "  rag             â†’ ê²€ìƒ‰ì¦ê°•ìƒì„±, retrieval augmented generation, ê²€ìƒ‰ì¦ê°•\n",
      "  ë”¥ëŸ¬ë‹             â†’ deep learning, dl, ì‹¬ì¸µí•™ìŠµ\n",
      "  ë¨¸ì‹ ëŸ¬ë‹            â†’ machine learning, ml, ê¸°ê³„í•™ìŠµ\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸ”— ë„ë©”ì¸ ë™ì˜ì–´ ë§¤í•‘\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# AI_SYNONYMSë¥¼ ì¼ë°˜ dictionary í˜•íƒœë¡œ ë³€í™˜\n",
    "domain_synonym_dict = {}\n",
    "\n",
    "for main_term, synonyms in AI_TERMINOLOGY.items():\n",
    "    # Main termì„ ì†Œë¬¸ìë¡œ\n",
    "    main_key = main_term.lower()\n",
    "    synonym_list = [s.lower() for s in synonyms]\n",
    "    \n",
    "    domain_synonym_dict[main_key] = synonym_list\n",
    "    \n",
    "    # ì–‘ë°©í–¥ ë§¤í•‘: ê° synonymë„ main termì„ ê°€ë¦¬í‚´\n",
    "    for syn in synonym_list:\n",
    "        if syn not in domain_synonym_dict:\n",
    "            domain_synonym_dict[syn] = [main_key]\n",
    "        elif main_key not in domain_synonym_dict[syn]:\n",
    "            domain_synonym_dict[syn].append(main_key)\n",
    "\n",
    "print(f\"âœ“ ë„ë©”ì¸ ë™ì˜ì–´ ë”•ì…”ë„ˆë¦¬ ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"  ì´ {len(domain_synonym_dict):,}ê°œ í•­ëª©\")\n",
    "print()\n",
    "print(\"ğŸ“ ìƒ˜í”Œ ë™ì˜ì–´ ë§¤í•‘:\")\n",
    "samples = [\n",
    "    \"ê²€ìƒ‰\", \"ì¸ê³µì§€ëŠ¥\", \"llm\", \"chatgpt\", \"ì„ë² ë”©\",\n",
    "    \"rag\", \"í”„ë¡¬í”„íŠ¸\", \"ë”¥ëŸ¬ë‹\", \"ë¨¸ì‹ ëŸ¬ë‹\"\n",
    "]\n",
    "for term in samples:\n",
    "    if term in domain_synonym_dict:\n",
    "        syns = domain_synonym_dict[term][:3]  # ìƒìœ„ 3ê°œë§Œ\n",
    "        print(f\"  {term:15s} â†’ {', '.join(syns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3. ìš©ì–´ì§‘ í†µí•© ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "âœ… AI ë„ë©”ì¸ ìš©ì–´ì§‘ í†µí•© ì™„ë£Œ!\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š í†µí•© ê²°ê³¼:\n",
      "  â€¢ Tokenizer vocabulary: 32,000 â†’ 32,033 (+33)\n",
      "  â€¢ AI ë„ë©”ì¸ ìš©ì–´: 73ê°œ ì¹´í…Œê³ ë¦¬\n",
      "  â€¢ ë™ì˜ì–´ ë§¤í•‘: 255ê°œ í•­ëª©\n",
      "  â€¢ Special tokens: 33ê°œ\n",
      "\n",
      "ğŸ¯ ì£¼ìš” ê°œì„  ì‚¬í•­:\n",
      "  1. ê¸°ìˆ  ìš©ì–´ ë¶„ì ˆ ë°©ì§€ (ChatGPT, OpenSearch, LLM ë“±)\n",
      "  2. AI ë„ë©”ì¸ ë™ì˜ì–´ ìë™ ë§¤í•‘ (ê²€ìƒ‰â†”Searchâ†”íƒìƒ‰)\n",
      "  3. í•œêµ­ì–´-ì˜ì–´ ìš©ì–´ ì–‘ë°©í–¥ ì—°ê²°\n",
      "\n",
      "ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:\n",
      "  â†’ ì„¹ì…˜ 7ì—ì„œ ë„ë©”ì¸ ë™ì˜ì–´ì™€ ìë™ ë°œê²¬ ë™ì˜ì–´ë¥¼ ê²°í•©\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"âœ… AI ë„ë©”ì¸ ìš©ì–´ì§‘ í†µí•© ì™„ë£Œ!\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"ğŸ“Š í†µí•© ê²°ê³¼:\")\n",
    "print(f\"  â€¢ Tokenizer vocabulary: {original_vocab_size:,} â†’ {new_vocab_size:,} (+{num_added})\")\n",
    "print(f\"  â€¢ AI ë„ë©”ì¸ ìš©ì–´: {len(AI_TERMINOLOGY):,}ê°œ ì¹´í…Œê³ ë¦¬\")\n",
    "print(f\"  â€¢ ë™ì˜ì–´ ë§¤í•‘: {len(domain_synonym_dict):,}ê°œ í•­ëª©\")\n",
    "print(f\"  â€¢ Special tokens: {len(TECHNICAL_SPECIAL_TOKENS)}ê°œ\")\n",
    "print()\n",
    "print(\"ğŸ¯ ì£¼ìš” ê°œì„  ì‚¬í•­:\")\n",
    "print(\"  1. ê¸°ìˆ  ìš©ì–´ ë¶„ì ˆ ë°©ì§€ (ChatGPT, OpenSearch, LLM ë“±)\")\n",
    "print(\"  2. AI ë„ë©”ì¸ ë™ì˜ì–´ ìë™ ë§¤í•‘ (ê²€ìƒ‰â†”Searchâ†”íƒìƒ‰)\")\n",
    "print(\"  3. í•œêµ­ì–´-ì˜ì–´ ìš©ì–´ ì–‘ë°©í–¥ ì—°ê²°\")\n",
    "print()\n",
    "print(\"ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(\"  â†’ ì„¹ì…˜ 7ì—ì„œ ë„ë©”ì¸ ë™ì˜ì–´ì™€ ìë™ ë°œê²¬ ë™ì˜ì–´ë¥¼ ê²°í•©\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. OpenSearch ë¬¸ì„œ ì¸ì½”ë” ëª¨ë¸ ì •ì˜\n",
    "\n",
    "**Doc-only mode**ë¥¼ ìœ„í•œ ë¬¸ì„œ ì¸ì½”ë”ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "BERT ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ sparse vectorë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing model embeddings: 32000 â†’ 32033\n",
      "âœ“ Model embeddings resized\n",
      "âœ“ OpenSearch Document Encoder ì´ˆê¸°í™” ì™„ë£Œ\n",
      "  ëª¨ë¸: klue/bert-base\n",
      "  Vocab size: 32,033\n",
      "  Parameters: 110,676,257\n",
      "  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "class OpenSearchDocEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    OpenSearch Neural Sparse Document Encoder (Doc-only mode)\n",
    "    \n",
    "    ë¬¸ì„œë¥¼ sparse vectorë¡œ ì¸ì½”ë”©í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "    ì¶œë ¥ í˜•ì‹: {\"output\": <sparse_vector>}\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"klue/bert-base\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # BERT ê¸°ë°˜ ì¸ì½”ë”\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.bert = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        \n",
    "        self.vocab_size = self.config.vocab_size\n",
    "        \n",
    "        # Log saturation activation\n",
    "        # log(1 + ReLU(x))\n",
    "        self.activation = lambda x: torch.log1p(torch.relu(x))\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, return_dict=False):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            input_ids: (batch_size, seq_len)\n",
    "            attention_mask: (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            sparse_vector: (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        # BERT MLM head output\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Logits: (batch_size, seq_len, vocab_size)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Apply activation: log(1 + ReLU(logits))\n",
    "        activated = self.activation(logits)\n",
    "        \n",
    "        # Max pooling over sequence length\n",
    "        # (batch_size, seq_len, vocab_size) â†’ (batch_size, vocab_size)\n",
    "        sparse_vector = torch.max(\n",
    "            activated * attention_mask.unsqueeze(-1),\n",
    "            dim=1\n",
    "        ).values\n",
    "        \n",
    "        if return_dict:\n",
    "            return {'output': sparse_vector}\n",
    "        \n",
    "        return sparse_vector\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "doc_encoder = OpenSearchDocEncoder(MODEL_NAME)\n",
    "doc_encoder = doc_encoder.to(device)\n",
    "\n",
    "# Tokenizerì— special tokens ì¶”ê°€ë¡œ ì¸í•´ embedding resize í•„ìš”\n",
    "if len(tokenizer) > doc_encoder.vocab_size:\n",
    "    print(f\"Resizing model embeddings: {doc_encoder.vocab_size} â†’ {len(tokenizer)}\")\n",
    "    doc_encoder.bert.resize_token_embeddings(len(tokenizer))\n",
    "    doc_encoder.vocab_size = len(tokenizer)\n",
    "    print(f\"âœ“ Model embeddings resized\")\n",
    "\n",
    "print(f\"âœ“ OpenSearch Document Encoder ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "print(f\"  ëª¨ë¸: {MODEL_NAME}\")\n",
    "print(f\"  Vocab size: {doc_encoder.vocab_size:,}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in doc_encoder.parameters()):,}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "\n",
    "Query-Document pairsì™€ negative samplingì„ ì¤€ë¹„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Negative sampling ì¤‘ (negatives per query: 2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0566e929a297492e82af2e36eb4847ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ì´ 30,000ê°œ pairs (original: 10,000)\n",
      "\n",
      "ğŸ“Š ë°ì´í„°ì…‹ ë¶„í• :\n",
      "  Train: 27,000 pairs\n",
      "  Val: 3,000 pairs\n",
      "\n",
      "âœ“ ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ\n",
      "  Batch size: 16\n",
      "  Train batches: 1,688\n",
      "  Val batches: 188\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SparseEncodingDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    OpenSearch Neural Sparse í•™ìŠµìš© ë°ì´í„°ì…‹\n",
    "    \"\"\"\n",
    "    def __init__(self, qd_pairs, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            qd_pairs: [(query, document, relevance), ...]\n",
    "            tokenizer: Hugging Face tokenizer\n",
    "            max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "        \"\"\"\n",
    "        self.qd_pairs = qd_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.qd_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        query, document, relevance = self.qd_pairs[idx]\n",
    "        \n",
    "        # ì¿¼ë¦¬ í† í°í™” (IDF lookupìš©)\n",
    "        query_encoded = self.tokenizer(\n",
    "            query,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # ë¬¸ì„œ í† í°í™” (ëª¨ë¸ ì¸ì½”ë”©ìš©)\n",
    "        doc_encoded = self.tokenizer(\n",
    "            document,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'query_input_ids': query_encoded['input_ids'].squeeze(0),\n",
    "            'query_attention_mask': query_encoded['attention_mask'].squeeze(0),\n",
    "            'doc_input_ids': doc_encoded['input_ids'].squeeze(0),\n",
    "            'doc_attention_mask': doc_encoded['attention_mask'].squeeze(0),\n",
    "            'relevance': torch.tensor(relevance, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# Negative sampling ì¶”ê°€ (ìµœì í™” ë²„ì „)\n",
    "def add_negative_samples(qd_pairs, documents, num_negatives=1):\n",
    "    \"\"\"\n",
    "    ê° positive pairì— ëŒ€í•´ negative documentsë¥¼ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.\n",
    "    (ì¸ë±ìŠ¤ ê¸°ë°˜ ìƒ˜í”Œë§ìœ¼ë¡œ 100ë°° ì´ìƒ ë¹ ë¦„!)\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ”„ Negative sampling ì¤‘ (negatives per query: {num_negatives})...\")\n",
    "    \n",
    "    # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì¸ë±ìŠ¤ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡)\n",
    "    doc_list = documents if isinstance(documents, list) else list(documents)\n",
    "    n_docs = len(doc_list)\n",
    "    \n",
    "    # ë¬¸ì„œ â†’ ì¸ë±ìŠ¤ ë§¤í•‘ (ë¹ ë¥¸ ê²€ìƒ‰ìš©)\n",
    "    doc_to_idx = {doc: idx for idx, doc in enumerate(doc_list)}\n",
    "    \n",
    "    augmented_pairs = []\n",
    "    \n",
    "    for query, pos_doc, relevance in tqdm(qd_pairs):\n",
    "        # Positive pair ì¶”ê°€\n",
    "        augmented_pairs.append((query, pos_doc, 1.0))\n",
    "        \n",
    "        # Positive ë¬¸ì„œì˜ ì¸ë±ìŠ¤\n",
    "        pos_idx = doc_to_idx.get(pos_doc, -1)\n",
    "        \n",
    "        # Negative sampling (ì¸ë±ìŠ¤ ê¸°ë°˜ - ë§¤ìš° ë¹ ë¦„!)\n",
    "        for _ in range(num_negatives):\n",
    "            # ëœë¤ ì¸ë±ìŠ¤ ì„ íƒ\n",
    "            neg_idx = np.random.randint(0, n_docs)\n",
    "            \n",
    "            # Positiveì™€ ê°™ì€ ì¸ë±ìŠ¤ë©´ ë‹¤ë¥¸ ê²ƒìœ¼ë¡œ êµì²´\n",
    "            if neg_idx == pos_idx:\n",
    "                neg_idx = (neg_idx + 1) % n_docs\n",
    "            \n",
    "            neg_doc = doc_list[neg_idx]\n",
    "            augmented_pairs.append((query, neg_doc, 0.0))\n",
    "    \n",
    "    print(f\"âœ“ ì´ {len(augmented_pairs):,}ê°œ pairs (original: {len(qd_pairs):,})\")\n",
    "    return augmented_pairs\n",
    "\n",
    "# Negative sampling ì ìš©\n",
    "augmented_pairs = add_negative_samples(\n",
    "    korean_data['qd_pairs'][:10000],  # ìƒ˜í”Œë§ (ì „ì²´ëŠ” ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼)\n",
    "    korean_data['documents'],\n",
    "    num_negatives=2\n",
    ")\n",
    "\n",
    "# Train/Val split\n",
    "train_pairs, val_pairs = train_test_split(augmented_pairs, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"\\nğŸ“Š ë°ì´í„°ì…‹ ë¶„í• :\")\n",
    "print(f\"  Train: {len(train_pairs):,} pairs\")\n",
    "print(f\"  Val: {len(val_pairs):,} pairs\")\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„±\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = SparseEncodingDataset(train_pairs, tokenizer, MAX_LENGTH)\n",
    "val_dataset = SparseEncodingDataset(val_pairs, tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nâœ“ ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Train batches: {len(train_loader):,}\")\n",
    "print(f\"  Val batches: {len(val_loader):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ìë™ ë™ì˜ì–´ ë°œê²¬ ë° ë°ì´í„° í™•ì¥\n",
    "\n",
    "ìˆ˜ì§‘ëœ ë°ì´í„°ì—ì„œ í† í° ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ë™ì˜ì–´ë¥¼ ìë™ ë°œê²¬í•˜ê³ ,\n",
    "ì´ë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµ ë°ì´í„°ë¥¼ í™•ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. BERT í† í° ì„ë² ë”© ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ” í† í° ì„ë² ë”© ê¸°ë°˜ ë™ì˜ì–´ ìë™ ë°œê²¬\n",
      "============================================================\n",
      "âœ“ Token embeddings ì¶”ì¶œ ì™„ë£Œ: (32033, 768)\n",
      "  Vocab size: 32,033\n",
      "  Embedding dim: 768\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸ” í† í° ì„ë² ë”© ê¸°ë°˜ ë™ì˜ì–´ ìë™ ë°œê²¬\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# BERT ëª¨ë¸ì€ ì´ë¯¸ doc_encoderì— ë¡œë“œë˜ì–´ ìˆìŒ\n",
    "# Token embedding ì¶”ì¶œ\n",
    "token_embeddings = doc_encoder.bert.bert.embeddings.word_embeddings.weight.detach().cpu().numpy()\n",
    "\n",
    "print(f\"âœ“ Token embeddings ì¶”ì¶œ ì™„ë£Œ: {token_embeddings.shape}\")\n",
    "print(f\"  Vocab size: {token_embeddings.shape[0]:,}\")\n",
    "print(f\"  Embedding dim: {token_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. ìœ ì‚¬ í† í° ë°œê²¬ í•¨ìˆ˜ (find_similar_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ find_similar_tokens í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def find_similar_tokens(token, tokenizer, embeddings, top_k=10, threshold=0.75):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í† í°ê³¼ ìœ ì‚¬í•œ í† í°ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        token: ê²€ìƒ‰í•  í† í° (ë¬¸ìì—´)\n",
    "        tokenizer: Tokenizer\n",
    "        embeddings: Token embeddings (numpy array)\n",
    "        top_k: ë°˜í™˜í•  ìœ ì‚¬ í† í° ê°œìˆ˜\n",
    "        threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (0~1)\n",
    "    \n",
    "    Returns:\n",
    "        List of (token, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # í† í° -> ID ë³€í™˜\n",
    "    token_id = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(token))\n",
    "    if not token_id:\n",
    "        return []\n",
    "    token_id = token_id[0]  # ì²« ë²ˆì§¸ í† í° ID ì‚¬ìš©\n",
    "    \n",
    "    # Token embedding ì¶”ì¶œ\n",
    "    token_emb = embeddings[token_id]\n",
    "    \n",
    "    # ëª¨ë“  í† í°ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    similarities = np.dot(embeddings, token_emb) / (\n",
    "        np.linalg.norm(embeddings, axis=1) * np.linalg.norm(token_emb) + 1e-10\n",
    "    )\n",
    "    \n",
    "    # ìƒìœ„ top_k+1 ê°œ ì¶”ì¶œ (ìê¸° ìì‹  ì œì™¸)\n",
    "    top_indices = np.argsort(similarities)[-(top_k+1):][::-1]\n",
    "    \n",
    "    similar_tokens = []\n",
    "    for idx in top_indices:\n",
    "        sim_score = float(similarities[idx])\n",
    "        if sim_score >= threshold and int(idx) != token_id:\n",
    "            similar_token = tokenizer.decode([int(idx)]).strip()\n",
    "            # í•„í„°ë§: ë¹ˆ ë¬¸ìì—´, íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆëŠ” ê²½ìš° ì œì™¸\n",
    "            if similar_token and not similar_token in ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']:\n",
    "                similar_tokens.append((similar_token, sim_score))\n",
    "    \n",
    "    return similar_tokens[:top_k]\n",
    "\n",
    "\n",
    "print(\"âœ“ find_similar_tokens í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. ì½”í¼ìŠ¤ ê¸°ë°˜ ë™ì˜ì–´ ìë™ ë°œê²¬ (build_synonym_dict_from_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ë™ì˜ì–´ ë°œê²¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def build_synonym_dict_from_corpus(documents, tokenizer, embeddings,\n",
    "                                   idf_dict, top_n=500, threshold=0.75):\n",
    "    \"\"\"\n",
    "    ìˆ˜ì§‘ëœ ë¬¸ì„œ ì½”í¼ìŠ¤ì—ì„œ ì¤‘ìš” í† í°ë“¤ì˜ ë™ì˜ì–´ë¥¼ ìë™ ë°œê²¬\n",
    "\n",
    "    Args:\n",
    "        documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        tokenizer: Tokenizer\n",
    "        embeddings: Token embeddings\n",
    "        idf_dict: IDF ë”•ì…”ë„ˆë¦¬ (token_id -> idf_score or token_str -> idf_score)\n",
    "        top_n: ìƒìœ„ Nê°œ IDF í† í° ëŒ€ìƒ\n",
    "        threshold: ìœ ì‚¬ë„ ì„ê³„ê°’\n",
    "\n",
    "    Returns:\n",
    "        synonym_dict: {token: [similar_tokens]}\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“– ìˆ˜ì§‘ëœ ë°ì´í„°ì—ì„œ ì¤‘ìš” í† í° ì¶”ì¶œ...\")\n",
    "\n",
    "    # IDF ê¸°ë°˜ ì¤‘ìš” í† í° ì„ ì •\n",
    "    sorted_idf = sorted(idf_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # í•„í„°ë§: subword(##ë¡œ ì‹œì‘), íŠ¹ìˆ˜ë¬¸ì, ë‹¨ì¼ ë¬¸ì ì œì™¸\n",
    "    important_tokens = []\n",
    "    for token_or_id, idf_score in sorted_idf:\n",
    "        if len(important_tokens) >= top_n:\n",
    "            break\n",
    "\n",
    "        # token_or_idê°€ ì •ìˆ˜(token ID)ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "        if isinstance(token_or_id, int):\n",
    "            token = tokenizer.decode([token_or_id]).strip()\n",
    "        else:\n",
    "            token = token_or_id\n",
    "\n",
    "        # í•„í„°ë§ ì¡°ê±´\n",
    "        if (not token.startswith('##') and\n",
    "            len(token) > 1 and\n",
    "            not token in [',', '.', '!', '?', ':', ';', '-', '(', ')', '[', ']']):\n",
    "            important_tokens.append(token)\n",
    "\n",
    "    print(f\"  ì¤‘ìš” í† í° {len(important_tokens)}ê°œ ì„ ì • ì™„ë£Œ\")\n",
    "    print(f\"  ìƒìœ„ 10ê°œ: {important_tokens[:10]}\")\n",
    "\n",
    "    # ê° í† í°ì— ëŒ€í•´ ìœ ì‚¬ í† í° ì°¾ê¸°\n",
    "    print(f\"\\nğŸ” ìœ ì‚¬ í† í° ìë™ ë°œê²¬ ì¤‘... (threshold={threshold})\")\n",
    "    synonym_dict = {}\n",
    "\n",
    "    for token in tqdm(important_tokens, desc=\"Finding synonyms\"):\n",
    "        similar = find_similar_tokens(token, tokenizer, embeddings,\n",
    "                                      top_k=5, threshold=threshold)\n",
    "        if similar:\n",
    "            synonym_dict[token] = [t for t, _ in similar]\n",
    "\n",
    "    return synonym_dict, important_tokens\n",
    "\n",
    "\n",
    "print(\"âœ“ ë™ì˜ì–´ ë°œê²¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. ìˆ˜ì§‘ ë°ì´í„° ê¸°ë°˜ ë™ì˜ì–´ ìë™ ë°œê²¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“– ìˆ˜ì§‘ëœ ë°ì´í„°ì—ì„œ ì¤‘ìš” í† í° ì¶”ì¶œ...\n",
      "  ì¤‘ìš” í† í° 500ê°œ ì„ ì • ì™„ë£Œ\n",
      "  ìƒìœ„ 10ê°œ: ['ì†Œê°œíŒ…', 'ì‹œí”¼', 'ë¯¸ë„ëŸ¬ì ¸', 'í˜ëŸ¬ë‚´ë ¸', 'ì–´ì—¬', 'ë§¤ëˆ', 'ë…¸í¬', 'ê¼¬ë§‰', 'ìí™”ìƒ', 'WBC']\n",
      "\n",
      "ğŸ” ìœ ì‚¬ í† í° ìë™ ë°œê²¬ ì¤‘... (threshold=0.75)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e017f515894882b79892837a7da0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding synonyms:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "âœ“ ìë™ ë™ì˜ì–´ ë°œê²¬ ì™„ë£Œ!\n",
      "============================================================\n",
      "  ë°œê²¬ëœ ë™ì˜ì–´ ê·¸ë£¹: 23ê°œ\n",
      "  ì´ ë™ì˜ì–´ ìŒ: 29ê°œ\n",
      "\n",
      "ğŸ“ ë°œê²¬ëœ ë™ì˜ì–´ ì˜ˆì‹œ (ìƒìœ„ 20ê°œ):\n",
      "   1. ë‹¤ì„¯ì§¸             â†’ ì—¬ì„¯ì§¸\n",
      "   2. ë‹´íšŒ              â†’ LangChain, LlamaIndex, AGI\n",
      "   3. ë’·ê±¸ìŒ             â†’ ë’·ê±¸ìŒì§ˆ\n",
      "   4. ë§‰í˜”              â†’ ë§‰íˆ\n",
      "   5. í–‰ì—¬              â†’ í˜¹ì—¬\n",
      "   6. ë¨¹ì˜€              â†’ ë¨¹ì¼\n",
      "   7. ë¬´ì„œì›              â†’ ë‘ë ¤ì› \n",
      "   8. ê² ë‹¤ê³              â†’ ê² ë‹¤ëŠ”, ê² ë‹¤\n",
      "   9. ì•„ì„¸ìš”             â†’ ì•„ì‹­ë‹ˆê¹Œ\n",
      "  10. í”¼ìš¸              â†’ í”¼ìš´, í”¼ì› \n",
      "  11. ì¤‘ê°„ê³ ì‚¬            â†’ ê¸°ë§ê³ ì‚¬\n",
      "  12. ëŒ€ì—¬ì„¯             â†’ ì„œë„ˆ\n",
      "  13. ì˜ˆìš”              â†’ ì—ìš”\n",
      "  14. ì§„ë‹¤ê³              â†’ ì§„ë‹¤ëŠ”\n",
      "  15. í—¤ì–´ì§ˆ             â†’ í—¤ì–´ì§€\n",
      "  16. ê³ ì•„ì›             â†’ ë³´ìœ¡ì›\n",
      "  17. ë²„ë¬´ë ¤             â†’ ë²„ë¬´\n",
      "  18. ë³¶ìŒ              â†’ ##ë³¶ìŒ\n",
      "  19. ì €ê±´              â†’ ì €ê²Œ\n",
      "  20. ì–´ë–¨ì§€             â†’ ì–´ë–¤ì§€\n"
     ]
    }
   ],
   "source": [
    "# ìˆ˜ì§‘ëœ ë¬¸ì„œì—ì„œ ë™ì˜ì–´ ìë™ ë°œê²¬\n",
    "auto_synonym_dict, important_tokens = build_synonym_dict_from_corpus(\n",
    "    korean_data['documents'],\n",
    "    tokenizer,\n",
    "    token_embeddings,\n",
    "    idf_id_dict,  # ID ê¸°ë°˜ IDF\n",
    "    top_n=500,    # ìƒìœ„ 500ê°œ í† í°\n",
    "    threshold=0.75  # ìœ ì‚¬ë„ 75% ì´ìƒ\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ“ ìë™ ë™ì˜ì–´ ë°œê²¬ ì™„ë£Œ!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  ë°œê²¬ëœ ë™ì˜ì–´ ê·¸ë£¹: {len(auto_synonym_dict):,}ê°œ\")\n",
    "print(f\"  ì´ ë™ì˜ì–´ ìŒ: {sum(len(v) for v in auto_synonym_dict.values()):,}ê°œ\")\n",
    "\n",
    "# ì˜ˆì‹œ ì¶œë ¥\n",
    "print(f\"\\nğŸ“ ë°œê²¬ëœ ë™ì˜ì–´ ì˜ˆì‹œ (ìƒìœ„ 20ê°œ):\")\n",
    "for i, (token, synonyms) in enumerate(list(auto_synonym_dict.items())[:20], 1):\n",
    "    if synonyms:\n",
    "        print(f\"  {i:2d}. {token:15s} â†’ {', '.join(synonyms[:3])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1. ë„ë©”ì¸ ë™ì˜ì–´ + ìë™ ë°œê²¬ ë™ì˜ì–´ ê²°í•©\n",
    "\n",
    "AI ë„ë©”ì¸ ì „ë¬¸ ìš©ì–´ì™€ ìë™ ë°œê²¬ëœ ë™ì˜ì–´ë¥¼ ê²°í•©í•˜ì—¬\n",
    "ë” í¬ê´„ì ì¸ ë™ì˜ì–´ ì‚¬ì „ì„ êµ¬ì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”— ë„ë©”ì¸ ë™ì˜ì–´ + ìë™ ë°œê²¬ ë™ì˜ì–´ ê²°í•©\n",
      "============================================================\n",
      "\n",
      "ğŸ“š ë„ë©”ì¸ ë™ì˜ì–´ (AI ìš©ì–´ì§‘):\n",
      "  í•­ëª© ìˆ˜: 255ê°œ\n",
      "\n",
      "ğŸ” ìë™ ë°œê²¬ ë™ì˜ì–´ (ì½”í¼ìŠ¤ ê¸°ë°˜):\n",
      "  í•­ëª© ìˆ˜: 23ê°œ\n",
      "\n",
      "âœ… ê²°í•© ì™„ë£Œ:\n",
      "  ì´ í•­ëª© ìˆ˜: 278ê°œ\n",
      "  ë„ë©”ì¸ ë™ì˜ì–´ ê¸°ì—¬: 255ê°œ í•­ëª©\n",
      "  ìë™ ë°œê²¬ ê¸°ì—¬: 29ê°œ ë™ì˜ì–´ ì¶”ê°€\n",
      "\n",
      "ğŸ“ ê²°í•© ë™ì˜ì–´ ìƒ˜í”Œ:\n",
      "  ê²€ìƒ‰              â†’ search, íƒìƒ‰, ì¡°íšŒ\n",
      "  ì¸ê³µì§€ëŠ¥            â†’ artificial intelligence, ê¸°ê³„ì§€ëŠ¥, ai\n",
      "  llm             â†’ ëŒ€ê·œëª¨ì–¸ì–´ëª¨ë¸\n",
      "  chatgpt         â†’ gpt, ì±—gpt, ì±—ì§€í”¼í‹°\n",
      "  ì„ë² ë”©             â†’ embedding, ë²¡í„°ë³€í™˜, ë²¡í„°í‘œí˜„\n",
      "  rag             â†’ ê²€ìƒ‰ì¦ê°•ìƒì„±, retrieval augmented generation, ê²€ìƒ‰ì¦ê°•\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸ”— ë„ë©”ì¸ ë™ì˜ì–´ + ìë™ ë°œê²¬ ë™ì˜ì–´ ê²°í•©\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# 1. ë„ë©”ì¸ ë™ì˜ì–´ í†µê³„\n",
    "print(f\"ğŸ“š ë„ë©”ì¸ ë™ì˜ì–´ (AI ìš©ì–´ì§‘):\")\n",
    "print(f\"  í•­ëª© ìˆ˜: {len(domain_synonym_dict):,}ê°œ\")\n",
    "print()\n",
    "\n",
    "# 2. ìë™ ë°œê²¬ ë™ì˜ì–´ í†µê³„\n",
    "print(f\"ğŸ” ìë™ ë°œê²¬ ë™ì˜ì–´ (ì½”í¼ìŠ¤ ê¸°ë°˜):\")\n",
    "print(f\"  í•­ëª© ìˆ˜: {len(auto_synonym_dict):,}ê°œ\")\n",
    "print()\n",
    "\n",
    "# 3. ê²°í•© ì „ëµ: ë„ë©”ì¸ ìš°ì„ , ìë™ ë°œê²¬ ë³´ì™„\n",
    "merged_synonym_dict = {}\n",
    "\n",
    "# ë¨¼ì € ë„ë©”ì¸ ë™ì˜ì–´ ì¶”ê°€ (ì‹ ë¢°ë„ ë†’ìŒ)\n",
    "for term, synonyms in domain_synonym_dict.items():\n",
    "    merged_synonym_dict[term] = list(set(synonyms))  # ì¤‘ë³µ ì œê±°\n",
    "\n",
    "# ìë™ ë°œê²¬ ë™ì˜ì–´ ì¶”ê°€ (ë„ë©”ì¸ ë™ì˜ì–´ì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²ƒë§Œ)\n",
    "added_from_auto = 0\n",
    "for term, synonyms in auto_synonym_dict.items():\n",
    "    term_lower = term.lower()\n",
    "    \n",
    "    if term_lower in merged_synonym_dict:\n",
    "        # ê¸°ì¡´ í•­ëª©ì— ìƒˆë¡œìš´ ë™ì˜ì–´ ì¶”ê°€\n",
    "        existing = set(merged_synonym_dict[term_lower])\n",
    "        new_synonyms = [s.lower() for s in synonyms if s.lower() not in existing]\n",
    "        if new_synonyms:\n",
    "            merged_synonym_dict[term_lower].extend(new_synonyms)\n",
    "            added_from_auto += len(new_synonyms)\n",
    "    else:\n",
    "        # ìƒˆë¡œìš´ í•­ëª© ì¶”ê°€\n",
    "        merged_synonym_dict[term_lower] = [s.lower() for s in synonyms]\n",
    "        added_from_auto += len(synonyms)\n",
    "\n",
    "print(f\"âœ… ê²°í•© ì™„ë£Œ:\")\n",
    "print(f\"  ì´ í•­ëª© ìˆ˜: {len(merged_synonym_dict):,}ê°œ\")\n",
    "print(f\"  ë„ë©”ì¸ ë™ì˜ì–´ ê¸°ì—¬: {len(domain_synonym_dict):,}ê°œ í•­ëª©\")\n",
    "print(f\"  ìë™ ë°œê²¬ ê¸°ì—¬: {added_from_auto:,}ê°œ ë™ì˜ì–´ ì¶”ê°€\")\n",
    "print()\n",
    "\n",
    "# 4. ìƒ˜í”Œ í™•ì¸\n",
    "print(\"ğŸ“ ê²°í•© ë™ì˜ì–´ ìƒ˜í”Œ:\")\n",
    "sample_terms = ['ê²€ìƒ‰', 'ì¸ê³µì§€ëŠ¥', 'llm', 'chatgpt', 'ì„ë² ë”©', 'rag']\n",
    "for term in sample_terms:\n",
    "    if term in merged_synonym_dict:\n",
    "        syns = merged_synonym_dict[term][:5]  # ìƒìœ„ 5ê°œ\n",
    "        print(f\"  {term:15s} â†’ {', '.join(syns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5. Synonym-Aware IDF ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ”„ ë™ì˜ì–´ ì •ë³´ë¥¼ ë°˜ì˜í•œ IDF ìƒì„± ì¤‘...\n",
      "============================================================\n",
      "\n",
      "âœ“ Synonym-Aware IDF ìƒì„± ì™„ë£Œ\n",
      "  109ê°œ í† í°ì— ë™ì˜ì–´ ì •ë³´ ë°˜ì˜\n",
      "\n",
      "ğŸ“Š IDF ë³€í™” ì˜ˆì‹œ:\n",
      "  ì¸ê³µì§€ëŠ¥           : 8.5652 â†’ 8.5652\n",
      "\n",
      "âœ“ Enhanced IDFë¥¼ ê¸°ë³¸ IDFë¡œ ì„¤ì • ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def create_synonym_aware_idf(original_idf, tokenizer, synonym_dict, method='max'):\n",
    "    \"\"\"\n",
    "    ë™ì˜ì–´ ì •ë³´ë¥¼ ë°˜ì˜í•œ IDF ìƒì„±\n",
    "    \n",
    "    Args:\n",
    "        original_idf: ì›ë³¸ IDF ë”•ì…”ë„ˆë¦¬\n",
    "        tokenizer: Tokenizer\n",
    "        synonym_dict: ë™ì˜ì–´ ì‚¬ì „ {token: [synonyms]}\n",
    "        method: 'max', 'mean' ì¤‘ ì„ íƒ\n",
    "    \n",
    "    Returns:\n",
    "        enhanced_idf: ê°•í™”ëœ IDF ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "    enhanced_idf = original_idf.copy()\n",
    "    boost_count = 0\n",
    "    \n",
    "    for canonical, synonyms in synonym_dict.items():\n",
    "        all_tokens = [canonical] + synonyms\n",
    "        \n",
    "        # ê° í† í°ì˜ IDF ê°’ ìˆ˜ì§‘\n",
    "        idf_values = []\n",
    "        for token in all_tokens:\n",
    "            if token in original_idf:\n",
    "                idf_values.append(original_idf[token])\n",
    "        \n",
    "        if not idf_values:\n",
    "            continue\n",
    "        \n",
    "        # IDF ê°’ í†µí•©\n",
    "        if method == 'max':\n",
    "            shared_idf = max(idf_values)\n",
    "        else:  # mean\n",
    "            shared_idf = np.mean(idf_values)\n",
    "        \n",
    "        # ëª¨ë“  ë™ì˜ì–´ í† í°ì— ì ìš©\n",
    "        for token in all_tokens:\n",
    "            if token in enhanced_idf:\n",
    "                enhanced_idf[token] = shared_idf\n",
    "                boost_count += 1\n",
    "    \n",
    "    print(f\"\\nâœ“ Synonym-Aware IDF ìƒì„± ì™„ë£Œ\")\n",
    "    print(f\"  {boost_count:,}ê°œ í† í°ì— ë™ì˜ì–´ ì •ë³´ ë°˜ì˜\")\n",
    "    \n",
    "    return enhanced_idf\n",
    "\n",
    "\n",
    "# Synonym-Aware IDF ìƒì„±\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ”„ ë™ì˜ì–´ ì •ë³´ë¥¼ ë°˜ì˜í•œ IDF ìƒì„± ì¤‘...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "idf_token_dict_enhanced = create_synonym_aware_idf(\n",
    "    idf_token_dict_boosted,\n",
    "    tokenizer,\n",
    "    merged_synonym_dict,\n",
    "    method='max'\n",
    ")\n",
    "\n",
    "# IDF ë³€í™” ì˜ˆì‹œ\n",
    "print(\"\\nğŸ“Š IDF ë³€í™” ì˜ˆì‹œ:\")\n",
    "sample_tokens = list(merged_synonym_dict.keys())[:5]\n",
    "for token in sample_tokens:\n",
    "    if token in idf_token_dict_boosted and token in idf_token_dict_enhanced:\n",
    "        original = idf_token_dict_boosted[token]\n",
    "        enhanced = idf_token_dict_enhanced[token]\n",
    "        change = \"â†‘\" if enhanced > original else \"â†’\"\n",
    "        print(f\"  {token:15s}: {original:.4f} {change} {enhanced:.4f}\")\n",
    "\n",
    "# Enhanced IDFë¥¼ ê¸°ë³¸ IDFë¡œ ì‚¬ìš©\n",
    "idf_token_dict_boosted = idf_token_dict_enhanced.copy()\n",
    "print(\"\\nâœ“ Enhanced IDFë¥¼ ê¸°ë³¸ IDFë¡œ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6. ë™ì˜ì–´ ê¸°ë°˜ í•™ìŠµ ë°ì´í„° í™•ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“ˆ ë™ì˜ì–´ ê¸°ë°˜ í•™ìŠµ ë°ì´í„° í™•ì¥\n",
      "============================================================\n",
      "\n",
      "ğŸ”„ ë™ì˜ì–´ ê¸°ë°˜ ë°ì´í„° í™•ì¥ ì¤‘... (expansion_ratio=0.15)\n",
      "âœ“ ë°ì´í„° í™•ì¥ ì™„ë£Œ!\n",
      "  ì›ë³¸: 77,961 pairs\n",
      "  í™•ì¥: 79,357 pairs (+1,396)\n",
      "  ì¦ê°€ìœ¨: 1.8%\n",
      "\n",
      "ğŸ“ í™•ì¥ëœ ì¿¼ë¦¬ ì˜ˆì‹œ:\n",
      "  1. CGLì´ ì¤‘ë‹¨ëœ í•´ í¬ìŠ¤ì½”ì˜ ì‹œê°€ì´ì•¡ ë­í‚¹ëŠ”?...\n",
      "  2. ì—ì„ì´ ì†ì‹¤í•¨ìˆ˜ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ì±„íƒí•œ íˆ¬ì ë°©ì‹ì€?...\n",
      "  3. ê¹€ìŠ¹ì—°ì˜ ì…ì›ì‹¤ì€ ë³‘ì› ëª‡ ë ˆì´ì–´ì— ìˆëŠ”ê°€?...\n",
      "  4. \" êµì›ì¸ì‚¬ì œë„ì˜ ê°œì„ ë°©í–¥ ê²€ìƒ‰ì„ ìœ„í•œ ì„¸ë¯¸ë‚˜ \" ëŠ” ëª‡ ë…„ë„ì— ì—´ë ¸ëŠ”ê°€?...\n",
      "  5. êµ­ë°©ë¶€ëŠ” ì‚¬ë“œ batch ê´€ë ¨ í† ì§€ ë§êµí™˜ ë°©ì‹ì€ í™•ë³´ëœ ë•…ì€ ë¬´ì—‡ì˜ ëŒ€ìƒì´ ì•„ë‹ˆë¼ê³  í–ˆë‚˜ìš”?...\n",
      "\n",
      "âœ“ í™•ì¥ëœ ë°ì´í„°ë¥¼ í•™ìŠµì— ì‚¬ìš©\n"
     ]
    }
   ],
   "source": [
    "def expand_data_with_synonyms(qd_pairs, documents, synonym_dict, \n",
    "                              tokenizer, expansion_ratio=0.2):\n",
    "    \"\"\"\n",
    "    ë™ì˜ì–´ë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµ ë°ì´í„° í™•ì¥\n",
    "    \n",
    "    Args:\n",
    "        qd_pairs: ì›ë³¸ query-document pairs\n",
    "        documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        synonym_dict: ë™ì˜ì–´ ì‚¬ì „\n",
    "        tokenizer: Tokenizer\n",
    "        expansion_ratio: í™•ì¥ ë¹„ìœ¨ (0.2 = 20% ì¶”ê°€)\n",
    "    \n",
    "    Returns:\n",
    "        expanded_pairs: í™•ì¥ëœ pairs\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ”„ ë™ì˜ì–´ ê¸°ë°˜ ë°ì´í„° í™•ì¥ ì¤‘... (expansion_ratio={expansion_ratio})\")\n",
    "    \n",
    "    expanded_pairs = list(qd_pairs)  # ì›ë³¸ ë³µì‚¬\n",
    "    expansion_count = int(len(qd_pairs) * expansion_ratio)\n",
    "    \n",
    "    added = 0\n",
    "    attempts = 0\n",
    "    max_attempts = expansion_count * 10\n",
    "    \n",
    "    while added < expansion_count and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        \n",
    "        # ëœë¤ pair ì„ íƒ\n",
    "        query, doc, relevance = qd_pairs[np.random.randint(len(qd_pairs))]\n",
    "        \n",
    "        # ì¿¼ë¦¬ í† í°í™”\n",
    "        query_tokens = tokenizer.tokenize(query)\n",
    "        \n",
    "        # ë™ì˜ì–´ë¡œ ëŒ€ì²´ ê°€ëŠ¥í•œ í† í° ì°¾ê¸°\n",
    "        replaceable = [(i, token) for i, token in enumerate(query_tokens) \n",
    "                      if token in synonym_dict and synonym_dict[token]]\n",
    "        \n",
    "        if not replaceable:\n",
    "            continue\n",
    "        \n",
    "        # ëœë¤í•˜ê²Œ í•˜ë‚˜ ì„ íƒí•˜ì—¬ ë™ì˜ì–´ë¡œ ëŒ€ì²´\n",
    "        idx, token = replaceable[np.random.randint(len(replaceable))]\n",
    "        synonym = np.random.choice(synonym_dict[token])\n",
    "        \n",
    "        # ìƒˆ ì¿¼ë¦¬ ìƒì„±\n",
    "        new_query_tokens = query_tokens.copy()\n",
    "        new_query_tokens[idx] = synonym\n",
    "        new_query = tokenizer.convert_tokens_to_string(new_query_tokens)\n",
    "        \n",
    "        # ì¤‘ë³µ ì²´í¬\n",
    "        if new_query != query and new_query.strip():\n",
    "            expanded_pairs.append((new_query, doc, relevance))\n",
    "            added += 1\n",
    "    \n",
    "    print(f\"âœ“ ë°ì´í„° í™•ì¥ ì™„ë£Œ!\")\n",
    "    print(f\"  ì›ë³¸: {len(qd_pairs):,} pairs\")\n",
    "    print(f\"  í™•ì¥: {len(expanded_pairs):,} pairs (+{added:,})\")\n",
    "    print(f\"  ì¦ê°€ìœ¨: {(len(expanded_pairs) / len(qd_pairs) - 1) * 100:.1f}%\")\n",
    "    \n",
    "    return expanded_pairs\n",
    "\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„° í™•ì¥\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“ˆ ë™ì˜ì–´ ê¸°ë°˜ í•™ìŠµ ë°ì´í„° í™•ì¥\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "korean_data['qd_pairs_expanded'] = expand_data_with_synonyms(\n",
    "    korean_data['qd_pairs'],\n",
    "    korean_data['documents'],\n",
    "    merged_synonym_dict,\n",
    "    tokenizer,\n",
    "    expansion_ratio=0.15  # 15% í™•ì¥\n",
    ")\n",
    "\n",
    "# í™•ì¥ ì˜ˆì‹œ ì¶œë ¥\n",
    "print(\"\\nğŸ“ í™•ì¥ëœ ì¿¼ë¦¬ ì˜ˆì‹œ:\")\n",
    "original_count = len(korean_data['qd_pairs'])\n",
    "for i, (query, doc, rel) in enumerate(korean_data['qd_pairs_expanded'][original_count:original_count+5]):\n",
    "    print(f\"  {i+1}. {query[:60]}...\")\n",
    "\n",
    "# í™•ì¥ëœ ë°ì´í„°ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©\n",
    "korean_data['qd_pairs'] = korean_data['qd_pairs_expanded']\n",
    "print(f\"\\nâœ“ í™•ì¥ëœ ë°ì´í„°ë¥¼ í•™ìŠµì— ì‚¬ìš©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7. ë™ì˜ì–´ ì •ë³´ ìš”ì•½\n",
    "\n",
    "ìë™ ë°œê²¬ëœ ë™ì˜ì–´ ì •ë³´ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“Š ë™ì˜ì–´ ë°œê²¬ ë° ë°ì´í„° í™•ì¥ ìš”ì•½\n",
      "============================================================\n",
      "\n",
      "1ï¸âƒ£ ë™ì˜ì–´ ë°œê²¬ ê²°ê³¼:\n",
      "  - ë¶„ì„ ëŒ€ìƒ í† í°: 500ê°œ\n",
      "  - ë°œê²¬ëœ ë™ì˜ì–´ ê·¸ë£¹: 23ê°œ\n",
      "  - ì´ ë™ì˜ì–´ ìŒ: 29ê°œ\n",
      "  - í‰ê·  ë™ì˜ì–´ ìˆ˜: 1.26ê°œ/ê·¸ë£¹\n",
      "\n",
      "2ï¸âƒ£ IDF ê°•í™” ê²°ê³¼:\n",
      "  - IDF ì—…ë°ì´íŠ¸ëœ í† í°: 23ê°œ\n",
      "\n",
      "3ï¸âƒ£ ë°ì´í„° í™•ì¥ ê²°ê³¼:\n",
      "  - ì›ë³¸ pairs: 0ê°œ\n",
      "  - ìµœì¢… pairs: 79,357ê°œ\n",
      "\n",
      "âœ… ë™ì˜ì–´ ê¸°ë°˜ ë°ì´í„° í™•ì¥ ì™„ë£Œ!\n",
      "   í•™ìŠµ ë°ì´í„°ê°€ ë” í’ë¶€í•´ì¡ŒìŠµë‹ˆë‹¤.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š ë™ì˜ì–´ ë°œê²¬ ë° ë°ì´í„° í™•ì¥ ìš”ì•½\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n1ï¸âƒ£ ë™ì˜ì–´ ë°œê²¬ ê²°ê³¼:\")\n",
    "print(f\"  - ë¶„ì„ ëŒ€ìƒ í† í°: {len(important_tokens):,}ê°œ\")\n",
    "print(f\"  - ë°œê²¬ëœ ë™ì˜ì–´ ê·¸ë£¹: {len(auto_synonym_dict):,}ê°œ\")\n",
    "print(f\"  - ì´ ë™ì˜ì–´ ìŒ: {sum(len(v) for v in auto_synonym_dict.values()):,}ê°œ\")\n",
    "print(f\"  - í‰ê·  ë™ì˜ì–´ ìˆ˜: {np.mean([len(v) for v in auto_synonym_dict.values()]):.2f}ê°œ/ê·¸ë£¹\")\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£ IDF ê°•í™” ê²°ê³¼:\")\n",
    "changes = 0\n",
    "for token in auto_synonym_dict.keys():\n",
    "    if token in idf_token_dict:\n",
    "        changes += 1\n",
    "print(f\"  - IDF ì—…ë°ì´íŠ¸ëœ í† í°: {changes:,}ê°œ\")\n",
    "\n",
    "print(f\"\\n3ï¸âƒ£ ë°ì´í„° í™•ì¥ ê²°ê³¼:\")\n",
    "print(f\"  - ì›ë³¸ pairs: {len(korean_data['qd_pairs_expanded']) - len(korean_data['qd_pairs']):,}ê°œ\")\n",
    "print(f\"  - ìµœì¢… pairs: {len(korean_data['qd_pairs']):,}ê°œ\")\n",
    "\n",
    "print(f\"\\nâœ… ë™ì˜ì–´ ê¸°ë°˜ ë°ì´í„° í™•ì¥ ì™„ë£Œ!\")\n",
    "print(f\"   í•™ìŠµ ë°ì´í„°ê°€ ë” í’ë¶€í•´ì¡ŒìŠµë‹ˆë‹¤.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ì†ì‹¤ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "OpenSearch ëª¨ë¸ì˜ í•µì‹¬ ì†ì‹¤ í•¨ìˆ˜:\n",
    "1. **Ranking Loss**: Query-Document similarity\n",
    "2. **IDF-aware Penalty**: ë‚®ì€ IDF í† í° ì–µì œ\n",
    "3. **L0 Regularization**: Sparsity ìœ ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n",
      "  - Ranking Loss (BCE)\n",
      "  - L0 Regularization (Sparsity)\n",
      "  - IDF-aware Penalty (Low-IDF suppression)\n"
     ]
    }
   ],
   "source": [
    "def compute_query_representation(query_tokens, idf_dict, tokenizer):\n",
    "    \"\"\"\n",
    "    ì¿¼ë¦¬ë¥¼ IDF lookupìœ¼ë¡œ sparse vectorë¡œ ë³€í™˜ (Inference-free!)\n",
    "    \n",
    "    Args:\n",
    "        query_tokens: (batch_size, seq_len)\n",
    "        idf_dict: {token_id: idf_score}\n",
    "        tokenizer: Hugging Face tokenizer\n",
    "    \n",
    "    Returns:\n",
    "        query_sparse: (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = query_tokens.shape\n",
    "    vocab_size = len(tokenizer)  # Use len() to include added special tokens\n",
    "    \n",
    "    # Initialize sparse vector\n",
    "    query_sparse = torch.zeros(batch_size, vocab_size, device=query_tokens.device)\n",
    "    \n",
    "    # Fill with IDF weights\n",
    "    for b in range(batch_size):\n",
    "        for token_id in query_tokens[b]:\n",
    "            token_id = token_id.item()\n",
    "            if token_id in idf_dict:\n",
    "                query_sparse[b, token_id] = idf_dict[token_id]\n",
    "    \n",
    "    return query_sparse\n",
    "\n",
    "def neural_sparse_loss(doc_sparse, query_sparse, relevance, idf_dict, \n",
    "                       lambda_l0=1e-3, lambda_idf=1e-2):\n",
    "    \"\"\"\n",
    "    OpenSearch Neural Sparse Loss\n",
    "    \n",
    "    Args:\n",
    "        doc_sparse: (batch_size, vocab_size) - ë¬¸ì„œì˜ sparse representation\n",
    "        query_sparse: (batch_size, vocab_size) - ì¿¼ë¦¬ì˜ sparse representation (IDF lookup)\n",
    "        relevance: (batch_size,) - ê´€ë ¨ë„ ì ìˆ˜ (1.0 or 0.0)\n",
    "        idf_dict: {token_id: idf_score}\n",
    "        lambda_l0: L0 regularization ê°€ì¤‘ì¹˜\n",
    "        lambda_idf: IDF-aware penalty ê°€ì¤‘ì¹˜\n",
    "    \n",
    "    Returns:\n",
    "        total_loss, ranking_loss, l0_loss, idf_penalty\n",
    "    \"\"\"\n",
    "    # 1. Ranking Loss: Dot product similarity\n",
    "    similarity = torch.sum(doc_sparse * query_sparse, dim=-1)\n",
    "    ranking_loss = F.binary_cross_entropy_with_logits(\n",
    "        similarity, relevance, reduction='mean'\n",
    "    )\n",
    "    \n",
    "    # 2. L0 Regularization (FLOPS penalty for sparsity)\n",
    "    l0_loss = torch.mean(torch.sum(torch.abs(doc_sparse), dim=-1))\n",
    "    \n",
    "    # 3. IDF-aware Penalty (suppress low-IDF tokens)\n",
    "    # ë‚®ì€ IDF í† í°ì— í˜ë„í‹° ë¶€ì—¬\n",
    "    idf_tensor = torch.tensor(\n",
    "        [idf_dict.get(i, 1.0) for i in range(doc_sparse.shape[1])],\n",
    "        device=doc_sparse.device\n",
    "    )\n",
    "    \n",
    "    # Inverse IDF penalty: ë‚®ì€ IDF = ë†’ì€ í˜ë„í‹°\n",
    "    inverse_idf = 1.0 / (idf_tensor + 1e-6)\n",
    "    idf_penalty = torch.mean(torch.sum(doc_sparse * inverse_idf, dim=-1))\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = ranking_loss + lambda_l0 * l0_loss + lambda_idf * idf_penalty\n",
    "    \n",
    "    return total_loss, ranking_loss, l0_loss, idf_penalty\n",
    "\n",
    "print(\"âœ“ ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n",
    "print(\"  - Ranking Loss (BCE)\")\n",
    "print(\"  - L0 Regularization (Sparsity)\")\n",
    "print(\"  - IDF-aware Penalty (Low-IDF suppression)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. í•™ìŠµ ì„¤ì • ë° ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\n",
      "  Learning rate: 2e-05\n",
      "  Epochs: 3\n",
      "  Batch size: 16\n",
      "  Max length: 128\n",
      "  Lambda L0: 0.001\n",
      "  Lambda IDF: 0.01\n",
      "  Device: cuda\n",
      "\n",
      "âœ“ Optimizer: AdamW\n",
      "âœ“ Scheduler: Linear warmup (500 steps)\n",
      "âœ“ Total steps: 5,064\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "WARMUP_STEPS = 500\n",
    "LAMBDA_L0 = 1e-3  # L0 regularization\n",
    "LAMBDA_IDF = 1e-2  # IDF-aware penalty\n",
    "\n",
    "print(\"ğŸ¯ í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Max length: {MAX_LENGTH}\")\n",
    "print(f\"  Lambda L0: {LAMBDA_L0}\")\n",
    "print(f\"  Lambda IDF: {LAMBDA_IDF}\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "# Optimizer & Scheduler\n",
    "optimizer = AdamW(doc_encoder.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Optimizer: AdamW\")\n",
    "print(f\"âœ“ Scheduler: Linear warmup ({WARMUP_STEPS} steps)\")\n",
    "print(f\"âœ“ Total steps: {total_steps:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸš€ í•™ìŠµ ì‹œì‘!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, idf_id_dict, tokenizer, device):\n",
    "    \"\"\"\n",
    "    í•œ ì—í­ í•™ìŠµ\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_ranking = 0\n",
    "    total_l0 = 0\n",
    "    total_idf_penalty = 0\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move to device\n",
    "        query_tokens = batch['query_input_ids'].to(device)\n",
    "        doc_input_ids = batch['doc_input_ids'].to(device)\n",
    "        doc_attention_mask = batch['doc_attention_mask'].to(device)\n",
    "        relevance = batch['relevance'].to(device)\n",
    "        \n",
    "        # Document encoding (ëª¨ë¸ë¡œ ì¸ì½”ë”©)\n",
    "        doc_sparse = model(doc_input_ids, doc_attention_mask)\n",
    "        \n",
    "        # Query encoding (IDF lookup - inference-free!)\n",
    "        query_sparse = compute_query_representation(query_tokens, idf_id_dict, tokenizer)\n",
    "        \n",
    "        # Loss ê³„ì‚°\n",
    "        loss, ranking_loss, l0_loss, idf_penalty = neural_sparse_loss(\n",
    "            doc_sparse, query_sparse, relevance, idf_id_dict,\n",
    "            lambda_l0=LAMBDA_L0, lambda_idf=LAMBDA_IDF\n",
    "        )\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # ê¸°ë¡\n",
    "        total_loss += loss.item()\n",
    "        total_ranking += ranking_loss.item()\n",
    "        total_l0 += l0_loss.item()\n",
    "        total_idf_penalty += idf_penalty.item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'rank': f'{ranking_loss.item():.4f}',\n",
    "            'l0': f'{l0_loss.item():.2f}',\n",
    "            'idf': f'{idf_penalty.item():.4f}'\n",
    "        })\n",
    "    \n",
    "    n = len(loader)\n",
    "    return total_loss/n, total_ranking/n, total_l0/n, total_idf_penalty/n\n",
    "\n",
    "def evaluate(model, loader, idf_id_dict, tokenizer, device):\n",
    "    \"\"\"\n",
    "    ê²€ì¦\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            query_tokens = batch['query_input_ids'].to(device)\n",
    "            doc_input_ids = batch['doc_input_ids'].to(device)\n",
    "            doc_attention_mask = batch['doc_attention_mask'].to(device)\n",
    "            relevance = batch['relevance'].to(device)\n",
    "            \n",
    "            doc_sparse = model(doc_input_ids, doc_attention_mask)\n",
    "            query_sparse = compute_query_representation(query_tokens, idf_id_dict, tokenizer)\n",
    "            \n",
    "            loss, _, _, _ = neural_sparse_loss(\n",
    "                doc_sparse, query_sparse, relevance, idf_id_dict,\n",
    "                lambda_l0=LAMBDA_L0, lambda_idf=LAMBDA_IDF\n",
    "            )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# í•™ìŠµ íˆìŠ¤í† ë¦¬\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'ranking_loss': [],\n",
    "    'l0_loss': [],\n",
    "    'idf_penalty': []\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ í•™ìŠµ ì‹œì‘!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Epoch 1/3\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3411f7a4714bae85e9c6f8f658da7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc38dbd1ad842f59e99b5f0fb6e7ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Epoch 1 ê²°ê³¼:\n",
      "  Train Loss: 7.2285\n",
      "  Val Loss: 0.5251\n",
      "  Ranking Loss: 4.9064\n",
      "  L0 Loss: 904.07\n",
      "  IDF Penalty: 141.8112\n",
      "  âœ“ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥! (val_loss: 0.5251)\n",
      "\n",
      "============================================================\n",
      "Epoch 2/3\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b3a28e45254fc39a36d964d6b3f689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9b88303fc74d3cbc6155ea06abcd4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Epoch 2 ê²°ê³¼:\n",
      "  Train Loss: 0.5272\n",
      "  Val Loss: 0.5238\n",
      "  Ranking Loss: 0.5196\n",
      "  L0 Loss: 3.00\n",
      "  IDF Penalty: 0.4639\n",
      "  âœ“ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥! (val_loss: 0.5238)\n",
      "\n",
      "============================================================\n",
      "Epoch 3/3\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592efbd0b5314382be5c9431f741d0fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627ee1a35c314089a6ab250bfd05948e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Epoch 3 ê²°ê³¼:\n",
      "  Train Loss: 0.5124\n",
      "  Val Loss: 0.5260\n",
      "  Ranking Loss: 0.5065\n",
      "  L0 Loss: 2.31\n",
      "  IDF Penalty: 0.3578\n",
      "\n",
      "============================================================\n",
      "âœ… í•™ìŠµ ì™„ë£Œ!\n",
      "============================================================\n",
      "Best Validation Loss: 0.5238\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµ ì‹¤í–‰\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"best_korean_neural_sparse_encoder.pt\"\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # í•™ìŠµ\n",
    "    train_loss, ranking_loss, l0_loss, idf_penalty = train_epoch(\n",
    "        doc_encoder, train_loader, optimizer, scheduler,\n",
    "        idf_id_dict, tokenizer, device\n",
    "    )\n",
    "    \n",
    "    # ê²€ì¦\n",
    "    val_loss = evaluate(doc_encoder, val_loader, idf_id_dict, tokenizer, device)\n",
    "    \n",
    "    # ê¸°ë¡\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['ranking_loss'].append(ranking_loss)\n",
    "    history['l0_loss'].append(l0_loss)\n",
    "    history['idf_penalty'].append(idf_penalty)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Epoch {epoch + 1} ê²°ê³¼:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Ranking Loss: {ranking_loss:.4f}\")\n",
    "    print(f\"  L0 Loss: {l0_loss:.2f}\")\n",
    "    print(f\"  IDF Penalty: {idf_penalty:.4f}\")\n",
    "    \n",
    "    # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': doc_encoder.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': {\n",
    "                'model_name': MODEL_NAME,\n",
    "                'vocab_size': len(tokenizer),\n",
    "                'max_length': MAX_LENGTH,\n",
    "            }\n",
    "        }, best_model_path)\n",
    "        print(f\"  âœ“ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥! (val_loss: {val_loss:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ëª¨ë¸ ì €ì¥ (OpenSearch í˜¸í™˜ í˜•ì‹)\n",
    "\n",
    "OpenSearchì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤:\n",
    "1. `pytorch_model.bin` - ë¬¸ì„œ ì¸ì½”ë”\n",
    "2. `idf.json` - ì¿¼ë¦¬ìš© í† í° ê°€ì¤‘ì¹˜ lookup table\n",
    "3. Tokenizer íŒŒì¼ë“¤\n",
    "4. `config.json` - ëª¨ë¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ OpenSearch í˜¸í™˜ ëª¨ë¸ ì €ì¥ ì¤‘...\n",
      "\n",
      "âœ“ ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ (Epoch 2, Val Loss: 0.5238)\n",
      "âœ“ pytorch_model.bin ì €ì¥\n",
      "âœ“ idf.json ì €ì¥ (29,220 tokens)\n",
      "âœ“ Tokenizer íŒŒì¼ ì €ì¥\n",
      "âœ“ config.json ì €ì¥\n",
      "âœ“ README.md ìƒì„±\n",
      "\n",
      "============================================================\n",
      "âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\n",
      "============================================================\n",
      "\n",
      "ì €ì¥ ìœ„ì¹˜: ./opensearch-korean-neural-sparse-v1/\n",
      "\n",
      "ì €ì¥ëœ íŒŒì¼:\n",
      "  - pytorch_model.bin              (  422.28 MB)\n",
      "  - idf.json                       (    0.88 MB)\n",
      "  - tokenizer_config.json          (    0.01 MB)\n",
      "  - special_tokens_map.json        (    0.00 MB)\n",
      "  - vocab.txt                      (    0.24 MB)\n",
      "  - tokenizer.json                 (    0.72 MB)\n",
      "  - config.json                    (    0.00 MB)\n",
      "  - README.md                      (    0.00 MB)\n",
      "  - added_tokens.json              (    0.00 MB)\n"
     ]
    }
   ],
   "source": [
    "# ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "OUTPUT_DIR = \"./opensearch-korean-neural-sparse-v1\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“¦ OpenSearch í˜¸í™˜ ëª¨ë¸ ì €ì¥ ì¤‘...\\n\")\n",
    "\n",
    "# 1. ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ\n",
    "checkpoint = torch.load(best_model_path)\n",
    "doc_encoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"âœ“ ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ (Epoch {checkpoint['epoch'] + 1}, Val Loss: {checkpoint['val_loss']:.4f})\")\n",
    "\n",
    "# 2. pytorch_model.bin ì €ì¥ (ë¬¸ì„œ ì¸ì½”ë”)\n",
    "torch.save(doc_encoder.state_dict(), f\"{OUTPUT_DIR}/pytorch_model.bin\")\n",
    "print(f\"âœ“ pytorch_model.bin ì €ì¥\")\n",
    "\n",
    "# 3. idf.json ì €ì¥ (ì¿¼ë¦¬ìš© ê°€ì¤‘ì¹˜ lookup table)\n",
    "with open(f\"{OUTPUT_DIR}/idf.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(idf_token_dict_boosted, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ“ idf.json ì €ì¥ ({len(idf_token_dict_boosted):,} tokens)\")\n",
    "\n",
    "# 4. Tokenizer ì €ì¥\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"âœ“ Tokenizer íŒŒì¼ ì €ì¥\")\n",
    "\n",
    "# 5. config.json ì €ì¥\n",
    "model_config = {\n",
    "    \"model_type\": \"opensearch-neural-sparse-doc-encoder\",\n",
    "    \"base_model\": MODEL_NAME,\n",
    "    \"vocab_size\": len(tokenizer),\n",
    "    \"max_seq_length\": MAX_LENGTH,\n",
    "    \"mode\": \"doc-only\",\n",
    "    \"output_format\": \"rank_features\",\n",
    "    \"training_info\": {\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"lambda_l0\": LAMBDA_L0,\n",
    "        \"lambda_idf\": LAMBDA_IDF,\n",
    "        \"training_samples\": len(train_dataset),\n",
    "        \"trained_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    },\n",
    "    \"usage\": {\n",
    "        \"documents\": \"Use pytorch_model.bin to encode documents\",\n",
    "        \"queries\": \"Use tokenizer + idf.json for inference-free query encoding\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/config.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_config, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ“ config.json ì €ì¥\")\n",
    "\n",
    "# 6. README ìƒì„±\n",
    "readme_content = f\"\"\"# OpenSearch Korean Neural Sparse Model v1\n",
    "\n",
    "í•œêµ­ì–´ì— ìµœì í™”ëœ OpenSearch inference-free neural sparse ê²€ìƒ‰ ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "## ëª¨ë¸ êµ¬ì¡°\n",
    "\n",
    "### Doc-only Mode (Inference-Free)\n",
    "- **ë¬¸ì„œ ì¸ì½”ë”©**: BERT ê¸°ë°˜ ì‹ ê²½ë§ (`pytorch_model.bin`)\n",
    "- **ì¿¼ë¦¬ ì¸ì½”ë”©**: Tokenizer + IDF lookup (`idf.json`) - **Inference-Free!**\n",
    "\n",
    "## íŒŒì¼ êµ¬ì¡°\n",
    "\n",
    "```\n",
    "{OUTPUT_DIR}/\n",
    "â”œâ”€â”€ pytorch_model.bin       # ë¬¸ì„œ ì¸ì½”ë” ëª¨ë¸\n",
    "â”œâ”€â”€ idf.json                # ì¿¼ë¦¬ìš© í† í° ê°€ì¤‘ì¹˜ (IDF + íŠ¸ë Œë“œ ë¶€ìŠ¤íŒ…)\n",
    "â”œâ”€â”€ tokenizer.json          # í† í¬ë‚˜ì´ì €\n",
    "â”œâ”€â”€ tokenizer_config.json   # í† í¬ë‚˜ì´ì € ì„¤ì •\n",
    "â”œâ”€â”€ vocab.txt               # ì–´íœ˜ ì‚¬ì „\n",
    "â”œâ”€â”€ special_tokens_map.json # íŠ¹ìˆ˜ í† í°\n",
    "â””â”€â”€ config.json             # ëª¨ë¸ ì„¤ì •\n",
    "```\n",
    "\n",
    "## ì‚¬ìš© ë°©ë²•\n",
    "\n",
    "### 1. OpenSearchì— ëª¨ë¸ ë“±ë¡\n",
    "\n",
    "```bash\n",
    "# ëª¨ë¸ ì••ì¶•\n",
    "cd {OUTPUT_DIR}\n",
    "zip -r ../korean-neural-sparse-v1.zip .\n",
    "\n",
    "# OpenSearchì— ì—…ë¡œë“œ\n",
    "POST /_plugins/_ml/models/_upload\n",
    "{{\n",
    "  \"name\": \"korean-neural-sparse-v1\",\n",
    "  \"version\": \"1.0\",\n",
    "  \"model_format\": \"TORCH_SCRIPT\",\n",
    "  \"model_config\": {{\n",
    "    \"model_type\": \"bert\",\n",
    "    \"embedding_dimension\": {len(tokenizer)},\n",
    "    \"framework_type\": \"sentence_transformers\"\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "### 2. ì¸ë±ìŠ¤ ìƒì„±\n",
    "\n",
    "```json\n",
    "PUT /korean-docs\n",
    "{{\n",
    "  \"mappings\": {{\n",
    "    \"properties\": {{\n",
    "      \"content\": {{ \"type\": \"text\" }},\n",
    "      \"embedding\": {{ \"type\": \"rank_features\" }}\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "### 3. ê²€ìƒ‰\n",
    "\n",
    "```json\n",
    "POST /korean-docs/_search\n",
    "{{\n",
    "  \"query\": {{\n",
    "    \"neural_sparse\": {{\n",
    "      \"embedding\": {{\n",
    "        \"query_text\": \"í•œêµ­ì–´ ê²€ìƒ‰ ìµœì í™”\",\n",
    "        \"model_id\": \"<model_id>\"\n",
    "      }}\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "## í•™ìŠµ ì •ë³´\n",
    "\n",
    "- **Base Model**: {MODEL_NAME}\n",
    "- **Training Samples**: {len(train_dataset):,}\n",
    "- **Epochs**: {NUM_EPOCHS}\n",
    "- **Best Val Loss**: {best_val_loss:.4f}\n",
    "- **Trained Date**: {datetime.now().strftime(\"%Y-%m-%d\")}\n",
    "\n",
    "## íŠ¹ì§•\n",
    "\n",
    "1. **í•œêµ­ì–´ ìµœì í™”**: KLUE-BERT ê¸°ë°˜\n",
    "2. **Inference-Free**: ì¿¼ë¦¬ëŠ” tokenizer + IDF lookupë§Œ ì‚¬ìš©\n",
    "3. **íŠ¸ë Œë“œ í‚¤ì›Œë“œ**: 2024-2025 AI/ML íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë¶€ìŠ¤íŒ…\n",
    "4. **IDF-aware**: ë‚®ì€ IDF í† í° ì–µì œ\n",
    "5. **Sparse**: L0 regularizationìœ¼ë¡œ í¬ì†Œì„± ìœ ì§€\n",
    "\n",
    "## ì°¸ê³ \n",
    "\n",
    "- [OpenSearch Neural Sparse Search](https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/)\n",
    "- [Paper: Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers](https://arxiv.org/abs/2411.04403)\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/README.md\", 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "print(f\"âœ“ README.md ìƒì„±\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}/\")\n",
    "print(f\"\\nì €ì¥ëœ íŒŒì¼:\")\n",
    "for filename in os.listdir(OUTPUT_DIR):\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    size = os.path.getsize(filepath) / (1024*1024)  # MB\n",
    "    print(f\"  - {filename:30s} ({size:>8.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ì €ì¥ëœ ëª¨ë¸ë¡œ inference í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
      "============================================================\n",
      "\n",
      "ğŸ“ ì¿¼ë¦¬ ì¸ì½”ë”© (Inference-Free: Tokenizer + IDF Lookup)\n",
      "\n",
      "Query: ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ\n",
      "  í¬ì†Œì„±: 99.99% (non-zero: 4/32033)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ê²€ìƒ‰              (8.7576)\n",
      "     2. ì¸ê³µì§€ëŠ¥            (8.5652)\n",
      "     3. ì‹œìŠ¤í…œ             (5.9199)\n",
      "     4. ê¸°ë°˜              (5.7376)\n",
      "\n",
      "Query: í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ \n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 5/32033)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. í•œêµ­ì–´             (7.7089)\n",
      "     2. ìì—°              (6.1994)\n",
      "     3. ì²˜ë¦¬              (6.1294)\n",
      "     4. ê¸°ìˆ               (5.1528)\n",
      "     5. ##ì–´             (3.3692)\n",
      "\n",
      "Query: OpenSearch ë²¡í„° ê²€ìƒ‰\n",
      "  í¬ì†Œì„±: 99.99% (non-zero: 3/32033)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ë²¡               (12.5093)\n",
      "     2. ê²€ìƒ‰              (8.7576)\n",
      "     3. ##í„°             (6.5533)\n",
      "\n",
      "Query: ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ë°©ë²•\n",
      "  í¬ì†Œì„±: 99.99% (non-zero: 3/32033)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. í•™ìŠµ              (7.4378)\n",
      "     2. ëª¨ë¸              (6.1028)\n",
      "     3. ë°©ë²•              (5.7607)\n",
      "\n",
      "Query: ChatGPT LLM í”„ë¡¬í”„íŠ¸\n",
      "  í¬ì†Œì„±: 100.00% (non-zero: 0/32033)\n",
      "  ìƒìœ„ í† í°:\n",
      "\n",
      "\n",
      "ğŸ“„ ë¬¸ì„œ ì¸ì½”ë”© (Model Inference)\n",
      "\n",
      "Document: OpenSearchëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural spars...\n",
      "  í¬ì†Œì„±: 99.95% (non-zero: 17/32033)\n",
      "  L1 Norm: 5.34\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ê²€ìƒ‰              (0.9013)\n",
      "     2. ë¶„ì„              (0.6341)\n",
      "     3. ì—”ì§„              (0.5808)\n",
      "     4. ê°€               (0.4577)\n",
      "     5. ë²¡               (0.4438)\n",
      "     6. ëŠ”               (0.4106)\n",
      "     7. ì˜               (0.3472)\n",
      "     8. ì™€               (0.2894)\n",
      "     9. ë¥¼               (0.2793)\n",
      "    10. ë¡œ               (0.2705)\n",
      "\n",
      "Document: í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤....\n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 6/32033)\n",
      "  L1 Norm: 2.73\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ìì—°              (0.8770)\n",
      "     2. ì–¸ì–´              (0.7629)\n",
      "     3. í•œêµ­ì–´             (0.5692)\n",
      "     4. í˜•íƒœ              (0.2195)\n",
      "     5. í•œê¸€              (0.1591)\n",
      "     6. ì¸ì‹              (0.1422)\n",
      "\n",
      "============================================================\n",
      "âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def encode_document(text, model, tokenizer, device, max_length=128):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œë¥¼ sparse vectorë¡œ ì¸ì½”ë”© (ëª¨ë¸ ì‚¬ìš©)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sparse_vec = model(input_ids, attention_mask)\n",
    "    \n",
    "    return sparse_vec.cpu().numpy()[0]\n",
    "\n",
    "def encode_query_inference_free(text, tokenizer, idf_dict, max_length=128):\n",
    "    \"\"\"\n",
    "    ì¿¼ë¦¬ë¥¼ sparse vectorë¡œ ì¸ì½”ë”© (IDF lookup - Inference-Free!)\n",
    "    \"\"\"\n",
    "    # í† í°í™”\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False, max_length=max_length, truncation=True)\n",
    "    \n",
    "    # IDF lookup\n",
    "    sparse_vec = np.zeros(len(tokenizer))\n",
    "    for token_id in tokens:\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        if token_str in idf_dict:\n",
    "            sparse_vec[token_id] = idf_dict[token_str]\n",
    "    \n",
    "    return sparse_vec\n",
    "\n",
    "def get_top_tokens(sparse_vec, tokenizer, top_k=15):\n",
    "    \"\"\"\n",
    "    Sparse vectorì—ì„œ ìƒìœ„ í† í° ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    top_indices = np.argsort(sparse_vec)[-top_k:][::-1]\n",
    "    top_values = sparse_vec[top_indices]\n",
    "    \n",
    "    top_tokens = []\n",
    "    for idx, val in zip(top_indices, top_values):\n",
    "        if val > 0:\n",
    "            token = tokenizer.decode([idx])\n",
    "            top_tokens.append((token, val))\n",
    "    \n",
    "    return top_tokens\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_queries = [\n",
    "    \"ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ\",\n",
    "    \"í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ \",\n",
    "    \"OpenSearch ë²¡í„° ê²€ìƒ‰\",\n",
    "    \"ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ë°©ë²•\",\n",
    "    \"ChatGPT LLM í”„ë¡¬í”„íŠ¸\",\n",
    "]\n",
    "\n",
    "test_documents = [\n",
    "    \"OpenSearchëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.\",\n",
    "    \"í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤.\",\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ“ ì¿¼ë¦¬ ì¸ì½”ë”© (Inference-Free: Tokenizer + IDF Lookup)\\n\")\n",
    "\n",
    "for query in test_queries:\n",
    "    sparse_vec = encode_query_inference_free(query, tokenizer, idf_token_dict_boosted)\n",
    "    \n",
    "    non_zero = np.count_nonzero(sparse_vec)\n",
    "    sparsity = (1 - non_zero / len(sparse_vec)) * 100\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"  í¬ì†Œì„±: {sparsity:.2f}% (non-zero: {non_zero}/{len(sparse_vec)})\")\n",
    "    print(f\"  ìƒìœ„ í† í°:\")\n",
    "    \n",
    "    top_tokens = get_top_tokens(sparse_vec, tokenizer, top_k=10)\n",
    "    for i, (token, value) in enumerate(top_tokens, 1):\n",
    "        print(f\"    {i:2d}. {token:15s} ({value:.4f})\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nğŸ“„ ë¬¸ì„œ ì¸ì½”ë”© (Model Inference)\\n\")\n",
    "\n",
    "for doc in test_documents:\n",
    "    sparse_vec = encode_document(doc, doc_encoder, tokenizer, device)\n",
    "    \n",
    "    non_zero = np.count_nonzero(sparse_vec)\n",
    "    sparsity = (1 - non_zero / len(sparse_vec)) * 100\n",
    "    \n",
    "    print(f\"Document: {doc[:50]}...\")\n",
    "    print(f\"  í¬ì†Œì„±: {sparsity:.2f}% (non-zero: {non_zero}/{len(sparse_vec)})\")\n",
    "    print(f\"  L1 Norm: {np.sum(np.abs(sparse_vec)):.2f}\")\n",
    "    print(f\"  ìƒìœ„ í† í°:\")\n",
    "    \n",
    "    top_tokens = get_top_tokens(sparse_vec, tokenizer, top_k=10)\n",
    "    for i, (token, value) in enumerate(top_tokens, 1):\n",
    "        print(f\"    {i:2d}. {token:15s} ({value:.4f})\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. OpenSearch í†µí•© ê°€ì´ë“œ\n",
    "\n",
    "í•™ìŠµëœ ëª¨ë¸ì„ OpenSearchì— í†µí•©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘     OpenSearch Inference-Free Neural Sparse ëª¨ë¸ í†µí•© ê°€ì´ë“œ  â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "## 1ï¸âƒ£ ëª¨ë¸ íŒ¨í‚¤ì§• ë° ì—…ë¡œë“œ\n",
      "\n",
      "```bash\n",
      "# ëª¨ë¸ ì••ì¶•\n",
      "cd opensearch-korean-neural-sparse-v1\n",
      "zip -r ../korean-neural-sparse-v1.zip .\n",
      "\n",
      "# OpenSearchì— ëª¨ë¸ ë“±ë¡\n",
      "POST /_plugins/_ml/models/_upload\n",
      "{\n",
      "  \"name\": \"korean-neural-sparse-doc-v1\",\n",
      "  \"version\": \"1.0\",\n",
      "  \"description\": \"Korean Neural Sparse Model for document encoding\",\n",
      "  \"model_format\": \"TORCH_SCRIPT\",\n",
      "  \"model_config\": {\n",
      "    \"model_type\": \"bert\",\n",
      "    \"embedding_dimension\": 30000,\n",
      "    \"framework_type\": \"sentence_transformers\",\n",
      "    \"all_config\": {\n",
      "      \"mode\": \"doc-only\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "# Tokenizer ëª¨ë¸ ë“±ë¡ (ì¿¼ë¦¬ìš©)\n",
      "POST /_plugins/_ml/models/_upload\n",
      "{\n",
      "  \"name\": \"korean-neural-sparse-tokenizer-v1\",\n",
      "  \"version\": \"1.0\",\n",
      "  \"model_format\": \"TOKENIZER\",\n",
      "  \"model_config\": {\n",
      "    \"model_type\": \"tokenizer\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "## 2ï¸âƒ£ ì¸ë±ìŠ¤ ìƒì„± (rank_features íƒ€ì…)\n",
      "\n",
      "```json\n",
      "PUT /korean-neural-sparse-index\n",
      "{\n",
      "  \"settings\": {\n",
      "    \"index\": {\n",
      "      \"default_pipeline\": \"korean-neural-sparse-ingest\"\n",
      "    }\n",
      "  },\n",
      "  \"mappings\": {\n",
      "    \"properties\": {\n",
      "      \"title\": { \"type\": \"text\" },\n",
      "      \"content\": { \"type\": \"text\" },\n",
      "      \"content_embedding\": {\n",
      "        \"type\": \"rank_features\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "## 3ï¸âƒ£ Ingest Pipeline ì„¤ì •\n",
      "\n",
      "```json\n",
      "PUT /_ingest/pipeline/korean-neural-sparse-ingest\n",
      "{\n",
      "  \"description\": \"Korean neural sparse encoding pipeline\",\n",
      "  \"processors\": [\n",
      "    {\n",
      "      \"sparse_encoding\": {\n",
      "        \"model_id\": \"<doc_model_id>\",\n",
      "        \"field_map\": {\n",
      "          \"content\": \"content_embedding\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "## 4ï¸âƒ£ ë¬¸ì„œ ì¸ë±ì‹±\n",
      "\n",
      "```json\n",
      "POST /korean-neural-sparse-index/_doc\n",
      "{\n",
      "  \"title\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰ ê¸°ìˆ \",\n",
      "  \"content\": \"OpenSearchëŠ” neural sparse ê²€ìƒ‰ì„ ì§€ì›í•˜ëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ì—”ì§„ì…ë‹ˆë‹¤.\"\n",
      "}\n",
      "```\n",
      "\n",
      "## 5ï¸âƒ£ Neural Sparse ê²€ìƒ‰ (Doc-only mode)\n",
      "\n",
      "```json\n",
      "POST /korean-neural-sparse-index/_search\n",
      "{\n",
      "  \"query\": {\n",
      "    \"neural_sparse\": {\n",
      "      \"content_embedding\": {\n",
      "        \"query_text\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰ ìµœì í™”\",\n",
      "        \"model_id\": \"<tokenizer_model_id>\",\n",
      "        \"max_token_score\": 3.5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "## 6ï¸âƒ£ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + Neural Sparse)\n",
      "\n",
      "```json\n",
      "POST /korean-neural-sparse-index/_search\n",
      "{\n",
      "  \"query\": {\n",
      "    \"hybrid\": {\n",
      "      \"queries\": [\n",
      "        {\n",
      "          \"match\": {\n",
      "            \"content\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰\"\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"neural_sparse\": {\n",
      "            \"content_embedding\": {\n",
      "              \"query_text\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰\",\n",
      "              \"model_id\": \"<tokenizer_model_id>\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"search_pipeline\": {\n",
      "    \"phase_results_processors\": [\n",
      "      {\n",
      "        \"normalization-processor\": {\n",
      "          \"normalization\": {\n",
      "            \"technique\": \"min_max\"\n",
      "          },\n",
      "          \"combination\": {\n",
      "            \"technique\": \"arithmetic_mean\",\n",
      "            \"parameters\": {\n",
      "              \"weights\": [0.3, 0.7]\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "## ğŸ“Š ì„±ëŠ¥ íŠ¹ì§•\n",
      "\n",
      "- **ë¬¸ì„œ ì¸ì½”ë”©**: BERT ê¸°ë°˜ ì‹ ê²½ë§ (ëŠë¦¼, ê³ í’ˆì§ˆ)\n",
      "- **ì¿¼ë¦¬ ì¸ì½”ë”©**: Tokenizer + IDF lookup (ë§¤ìš° ë¹ ë¦„, Inference-Free!)\n",
      "- **ì¿¼ë¦¬ ì§€ì—°ì‹œê°„**: BM25ì™€ ê±°ì˜ ë™ì¼ (1.1x)\n",
      "- **ê²€ìƒ‰ ì •í™•ë„**: Siamese sparse ëª¨ë¸ê³¼ ìœ ì‚¬\n",
      "\n",
      "## ğŸ“š ì°¸ê³  ìë£Œ\n",
      "\n",
      "- [OpenSearch Neural Sparse Search](https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/)\n",
      "- [Doc-only Mode](https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/#doc-only-mode)\n",
      "- [Pretrained Models](https://opensearch.org/docs/latest/ml-commons-plugin/pretrained-models/)\n",
      "- [Paper: Inference-Free Learned Sparse Retrievers](https://arxiv.org/abs/2411.04403)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘     OpenSearch Inference-Free Neural Sparse ëª¨ë¸ í†µí•© ê°€ì´ë“œ  â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "## 1ï¸âƒ£ ëª¨ë¸ íŒ¨í‚¤ì§• ë° ì—…ë¡œë“œ\n",
    "\n",
    "```bash\n",
    "# ëª¨ë¸ ì••ì¶•\n",
    "cd opensearch-korean-neural-sparse-v1\n",
    "zip -r ../korean-neural-sparse-v1.zip .\n",
    "\n",
    "# OpenSearchì— ëª¨ë¸ ë“±ë¡\n",
    "POST /_plugins/_ml/models/_upload\n",
    "{\n",
    "  \"name\": \"korean-neural-sparse-doc-v1\",\n",
    "  \"version\": \"1.0\",\n",
    "  \"description\": \"Korean Neural Sparse Model for document encoding\",\n",
    "  \"model_format\": \"TORCH_SCRIPT\",\n",
    "  \"model_config\": {\n",
    "    \"model_type\": \"bert\",\n",
    "    \"embedding_dimension\": 30000,\n",
    "    \"framework_type\": \"sentence_transformers\",\n",
    "    \"all_config\": {\n",
    "      \"mode\": \"doc-only\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Tokenizer ëª¨ë¸ ë“±ë¡ (ì¿¼ë¦¬ìš©)\n",
    "POST /_plugins/_ml/models/_upload\n",
    "{\n",
    "  \"name\": \"korean-neural-sparse-tokenizer-v1\",\n",
    "  \"version\": \"1.0\",\n",
    "  \"model_format\": \"TOKENIZER\",\n",
    "  \"model_config\": {\n",
    "    \"model_type\": \"tokenizer\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## 2ï¸âƒ£ ì¸ë±ìŠ¤ ìƒì„± (rank_features íƒ€ì…)\n",
    "\n",
    "```json\n",
    "PUT /korean-neural-sparse-index\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"index\": {\n",
    "      \"default_pipeline\": \"korean-neural-sparse-ingest\"\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"title\": { \"type\": \"text\" },\n",
    "      \"content\": { \"type\": \"text\" },\n",
    "      \"content_embedding\": {\n",
    "        \"type\": \"rank_features\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## 3ï¸âƒ£ Ingest Pipeline ì„¤ì •\n",
    "\n",
    "```json\n",
    "PUT /_ingest/pipeline/korean-neural-sparse-ingest\n",
    "{\n",
    "  \"description\": \"Korean neural sparse encoding pipeline\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"sparse_encoding\": {\n",
    "        \"model_id\": \"<doc_model_id>\",\n",
    "        \"field_map\": {\n",
    "          \"content\": \"content_embedding\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "## 4ï¸âƒ£ ë¬¸ì„œ ì¸ë±ì‹±\n",
    "\n",
    "```json\n",
    "POST /korean-neural-sparse-index/_doc\n",
    "{\n",
    "  \"title\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰ ê¸°ìˆ \",\n",
    "  \"content\": \"OpenSearchëŠ” neural sparse ê²€ìƒ‰ì„ ì§€ì›í•˜ëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ì—”ì§„ì…ë‹ˆë‹¤.\"\n",
    "}\n",
    "```\n",
    "\n",
    "## 5ï¸âƒ£ Neural Sparse ê²€ìƒ‰ (Doc-only mode)\n",
    "\n",
    "```json\n",
    "POST /korean-neural-sparse-index/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"neural_sparse\": {\n",
    "      \"content_embedding\": {\n",
    "        \"query_text\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰ ìµœì í™”\",\n",
    "        \"model_id\": \"<tokenizer_model_id>\",\n",
    "        \"max_token_score\": 3.5\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## 6ï¸âƒ£ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + Neural Sparse)\n",
    "\n",
    "```json\n",
    "POST /korean-neural-sparse-index/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"hybrid\": {\n",
    "      \"queries\": [\n",
    "        {\n",
    "          \"match\": {\n",
    "            \"content\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰\"\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"neural_sparse\": {\n",
    "            \"content_embedding\": {\n",
    "              \"query_text\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰\",\n",
    "              \"model_id\": \"<tokenizer_model_id>\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"search_pipeline\": {\n",
    "    \"phase_results_processors\": [\n",
    "      {\n",
    "        \"normalization-processor\": {\n",
    "          \"normalization\": {\n",
    "            \"technique\": \"min_max\"\n",
    "          },\n",
    "          \"combination\": {\n",
    "            \"technique\": \"arithmetic_mean\",\n",
    "            \"parameters\": {\n",
    "              \"weights\": [0.3, 0.7]\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## ğŸ“Š ì„±ëŠ¥ íŠ¹ì§•\n",
    "\n",
    "- **ë¬¸ì„œ ì¸ì½”ë”©**: BERT ê¸°ë°˜ ì‹ ê²½ë§ (ëŠë¦¼, ê³ í’ˆì§ˆ)\n",
    "- **ì¿¼ë¦¬ ì¸ì½”ë”©**: Tokenizer + IDF lookup (ë§¤ìš° ë¹ ë¦„, Inference-Free!)\n",
    "- **ì¿¼ë¦¬ ì§€ì—°ì‹œê°„**: BM25ì™€ ê±°ì˜ ë™ì¼ (1.1x)\n",
    "- **ê²€ìƒ‰ ì •í™•ë„**: Siamese sparse ëª¨ë¸ê³¼ ìœ ì‚¬\n",
    "\n",
    "## ğŸ“š ì°¸ê³  ìë£Œ\n",
    "\n",
    "- [OpenSearch Neural Sparse Search](https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/)\n",
    "- [Doc-only Mode](https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/#doc-only-mode)\n",
    "- [Pretrained Models](https://opensearch.org/docs/latest/ml-commons-plugin/pretrained-models/)\n",
    "- [Paper: Inference-Free Learned Sparse Retrievers](https://arxiv.org/abs/2411.04403)\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### âœ… ì™„ë£Œëœ ì‘ì—…\n",
    "\n",
    "1. **ë°ì´í„° ìˆ˜ì§‘**: í•œêµ­ì–´ ê³µê°œ ë°ì´í„°ì…‹ (KLUE, KorQuAD, Wikipedia ë“±)\n",
    "2. **IDF ê³„ì‚°**: í† í°ë³„ IDF ê°€ì¤‘ì¹˜ ê³„ì‚° ë° íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…\n",
    "3. **ëª¨ë¸ í•™ìŠµ**: OpenSearch doc-only mode ë¬¸ì„œ ì¸ì½”ë” í•™ìŠµ\n",
    "   - IDF-aware penalty\n",
    "   - L0 regularization\n",
    "   - Ranking loss\n",
    "4. **ëª¨ë¸ ì €ì¥**: OpenSearch í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ì €ì¥\n",
    "   - `pytorch_model.bin` (ë¬¸ì„œ ì¸ì½”ë”)\n",
    "   - `idf.json` (ì¿¼ë¦¬ìš© ê°€ì¤‘ì¹˜)\n",
    "   - Tokenizer íŒŒì¼ë“¤\n",
    "   - `config.json`\n",
    "\n",
    "### ğŸ¯ í•µì‹¬ íŠ¹ì§•\n",
    "\n",
    "- **Inference-Free**: ì¿¼ë¦¬ëŠ” tokenizer + IDF lookupë§Œ ì‚¬ìš© â†’ ë§¤ìš° ë¹ ë¦„!\n",
    "- **í•œêµ­ì–´ ìµœì í™”**: KLUE-BERT ê¸°ë°˜ + í•œêµ­ì–´ ë°ì´í„°ì…‹\n",
    "- **íŠ¸ë Œë“œ ë°˜ì˜**: 2024-2025 AI/ML í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë¶€ìŠ¤íŒ…\n",
    "- **OpenSearch í˜¸í™˜**: ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ í˜•ì‹\n",
    "\n",
    "### ğŸš€ ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "1. **Knowledge Distillation**: Dense + Sparse siamese ëª¨ë¸ë¡œ distillation\n",
    "2. **ë” ë§ì€ ë°ì´í„°**: AI Hub, NIKL ë“± ì¶”ê°€ ë°ì´í„°ì…‹\n",
    "3. **Hard Negative Mining**: In-batch negatives, hard negatives\n",
    "4. **ëª¨ë¸ í‰ê°€**: BEIR ë²¤ì¹˜ë§ˆí¬, MRR, NDCG ë“±\n",
    "5. **OpenSearch ë°°í¬**: ì‹¤ì œ ê²€ìƒ‰ ì‹œìŠ¤í…œì— í†µí•©\n",
    "6. **A/B í…ŒìŠ¤íŒ…**: ê¸°ì¡´ BM25ì™€ ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "### ğŸ“ˆ ê¸°ëŒ€ íš¨ê³¼\n",
    "\n",
    "- BM25 ëŒ€ë¹„ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ\n",
    "- Dense retrieval ëŒ€ë¹„ ë¹ ë¥¸ ì†ë„\n",
    "- í•œêµ­ì–´ íŠ¹í™” ê²€ìƒ‰ ì„±ëŠ¥ ê°œì„ \n",
    "- íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê²€ìƒ‰ ìµœì í™”"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

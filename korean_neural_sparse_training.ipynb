{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenSearch Neural Sparse ëª¨ë¸ í•œêµ­ì–´ ì„±ëŠ¥ ê°œì„  í”„ë¡œì íŠ¸\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ OpenSearchì˜ neural sparse ëª¨ë¸(SPLADE ê¸°ë°˜)ì˜ í•œêµ­ì–´ ì„±ëŠ¥ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ì¬í•™ìŠµ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "## ì£¼ìš” ê¸°ëŠ¥\n",
    "1. í•œêµ­ì–´ ê³µê°œ ë°ì´í„°ì…‹ ìˆ˜ì§‘ (Hugging Face)\n",
    "2. í˜•íƒœì†Œ ë¶„ì„ (KoNLPy, Mecab)\n",
    "3. ë™ì˜ì–´ ì²˜ë¦¬ ë° íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì ìš©\n",
    "4. SPLADE ëª¨ë¸ íŒŒì¸íŠœë‹\n",
    "5. OpenSearch í†µí•©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install -q transformers datasets sentence-transformers\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q konlpy mecab-python3\n",
    "!pip install -q openai anthropic  # ë™ì˜ì–´ ë° íŠ¸ë Œë“œ ë¶„ì„ìš©\n",
    "!pip install -q scikit-learn pandas numpy matplotlib seaborn\n",
    "!pip install -q accelerate huggingface-hub\n",
    "\n",
    "# Mecab í˜•íƒœì†Œ ë¶„ì„ê¸° ì„¤ì¹˜ (Ubuntu/Debian)\n",
    "!apt-get install -y g++ openjdk-8-jdk python3-dev\n",
    "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Hugging Face & Transformers\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForMaskedLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# í˜•íƒœì†Œ ë¶„ì„\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í•œêµ­ì–´ ë°ì´í„°ì…‹ ìˆ˜ì§‘\n",
    "\n",
    "Hugging Faceì—ì„œ ë‹¤ì–‘í•œ í•œêµ­ì–´ ê³µê°œ ë°ì´í„°ì…‹ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤:\n",
    "- **KLUE**: í•œêµ­ì–´ ì´í•´ í‰ê°€ ë²¤ì¹˜ë§ˆí¬\n",
    "- **KorQuAD**: í•œêµ­ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°ì…‹\n",
    "- **Korean Wikipedia**: í•œêµ­ì–´ ìœ„í‚¤í”¼ë””ì•„\n",
    "- **NIKL**: êµ­ë¦½êµ­ì–´ì› ë§ë­‰ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_korean_datasets():\n",
    "    \"\"\"\n",
    "    Hugging Faceì—ì„œ í•œêµ­ì–´ ê³µê°œ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    datasets_info = {}\n",
    "    \n",
    "    print(\"ğŸ“š í•œêµ­ì–´ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\\n\")\n",
    "    \n",
    "    # 1. KLUE (Korean Language Understanding Evaluation)\n",
    "    try:\n",
    "        print(\"1ï¸âƒ£ KLUE ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        klue_tc = load_dataset(\"klue\", \"ynat\", split=\"train\")  # Topic Classification\n",
    "        klue_sts = load_dataset(\"klue\", \"sts\", split=\"train\")  # Semantic Textual Similarity\n",
    "        klue_mrc = load_dataset(\"klue\", \"mrc\", split=\"train\")  # Machine Reading Comprehension\n",
    "        \n",
    "        datasets_info['klue'] = {\n",
    "            'topic_classification': klue_tc,\n",
    "            'semantic_similarity': klue_sts,\n",
    "            'reading_comprehension': klue_mrc\n",
    "        }\n",
    "        print(f\"   âœ“ Topic Classification: {len(klue_tc):,} samples\")\n",
    "        print(f\"   âœ“ Semantic Similarity: {len(klue_sts):,} samples\")\n",
    "        print(f\"   âœ“ Reading Comprehension: {len(klue_mrc):,} samples\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— KLUE ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # 2. KorQuAD (Korean Question Answering Dataset)\n",
    "    try:\n",
    "        print(\"2ï¸âƒ£ KorQuAD ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        korquad_v1 = load_dataset(\"squad_kor_v1\", split=\"train\")\n",
    "        datasets_info['korquad'] = korquad_v1\n",
    "        print(f\"   âœ“ KorQuAD v1: {len(korquad_v1):,} samples\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— KorQuAD ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # 3. Korean Wikipedia\n",
    "    try:\n",
    "        print(\"3ï¸âƒ£ í•œêµ­ì–´ Wikipedia ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        ko_wiki = load_dataset(\"wikipedia\", \"20220301.ko\", split=\"train[:50000]\")  # ìƒ˜í”Œë§\n",
    "        datasets_info['wikipedia'] = ko_wiki\n",
    "        print(f\"   âœ“ Korean Wikipedia: {len(ko_wiki):,} samples\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— Wikipedia ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # 4. Korean Chat/Conversation datasets\n",
    "    try:\n",
    "        print(\"4ï¸âƒ£ í•œêµ­ì–´ ëŒ€í™” ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        korean_chat = load_dataset(\"heegyu/Korean-Culture-QA\", split=\"train\")\n",
    "        datasets_info['korean_chat'] = korean_chat\n",
    "        print(f\"   âœ“ Korean Culture QA: {len(korean_chat):,} samples\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— Korean Chat ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # 5. Korean News Dataset\n",
    "    try:\n",
    "        print(\"5ï¸âƒ£ í•œêµ­ì–´ ë‰´ìŠ¤ ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        korean_news = load_dataset(\"heegyu/news-category-dataset\", split=\"train[:100000]\")\n",
    "        datasets_info['korean_news'] = korean_news\n",
    "        print(f\"   âœ“ Korean News: {len(korean_news):,} samples\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— Korean News ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    return datasets_info\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "korean_datasets = load_korean_datasets()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"ì´ {len(korean_datasets)}ê°œì˜ ë°ì´í„°ì…‹ ì¹´í…Œê³ ë¦¬ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texts_from_datasets(datasets_info):\n",
    "    \"\"\"\n",
    "    ê° ë°ì´í„°ì…‹ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    all_texts = []\n",
    "    \n",
    "    # KLUE ë°ì´í„°ì…‹\n",
    "    if 'klue' in datasets_info:\n",
    "        klue_data = datasets_info['klue']\n",
    "        \n",
    "        # Topic Classification\n",
    "        if 'topic_classification' in klue_data:\n",
    "            texts = [item['title'] for item in klue_data['topic_classification']]\n",
    "            all_texts.extend(texts)\n",
    "        \n",
    "        # Semantic Similarity\n",
    "        if 'semantic_similarity' in klue_data:\n",
    "            for item in klue_data['semantic_similarity']:\n",
    "                all_texts.append(item['sentence1'])\n",
    "                all_texts.append(item['sentence2'])\n",
    "        \n",
    "        # Reading Comprehension\n",
    "        if 'reading_comprehension' in klue_data:\n",
    "            for item in klue_data['reading_comprehension']:\n",
    "                all_texts.append(item['context'])\n",
    "                all_texts.append(item['question'])\n",
    "    \n",
    "    # KorQuAD\n",
    "    if 'korquad' in datasets_info:\n",
    "        for item in datasets_info['korquad']:\n",
    "            all_texts.append(item['context'])\n",
    "            all_texts.append(item['question'])\n",
    "    \n",
    "    # Wikipedia\n",
    "    if 'wikipedia' in datasets_info:\n",
    "        texts = [item['text'][:1000] for item in datasets_info['wikipedia']]  # ì²˜ìŒ 1000ìë§Œ\n",
    "        all_texts.extend(texts)\n",
    "    \n",
    "    # Korean Chat\n",
    "    if 'korean_chat' in datasets_info:\n",
    "        for item in datasets_info['korean_chat']:\n",
    "            if 'question' in item:\n",
    "                all_texts.append(item['question'])\n",
    "            if 'answer' in item:\n",
    "                all_texts.append(item['answer'])\n",
    "    \n",
    "    # Korean News\n",
    "    if 'korean_news' in datasets_info:\n",
    "        for item in datasets_info['korean_news']:\n",
    "            if 'headline' in item:\n",
    "                all_texts.append(item['headline'])\n",
    "    \n",
    "    # ì¤‘ë³µ ì œê±° ë° ì •ì œ\n",
    "    all_texts = list(set([text.strip() for text in all_texts if text and len(text.strip()) > 10]))\n",
    "    \n",
    "    print(f\"ì´ {len(all_texts):,}ê°œì˜ ê³ ìœ í•œ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    return all_texts\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "corpus_texts = extract_texts_from_datasets(korean_datasets)\n",
    "\n",
    "# ìƒ˜í”Œ ì¶œë ¥\n",
    "print(\"\\nìƒ˜í”Œ í…ìŠ¤íŠ¸:\")\n",
    "for i, text in enumerate(corpus_texts[:5], 1):\n",
    "    print(f\"{i}. {text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. í˜•íƒœì†Œ ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "\n",
    "Mecabì„ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ì£¼ìš” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mecab í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "try:\n",
    "    mecab = Mecab()\n",
    "    print(\"âœ“ Mecab í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Mecab ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ëŒ€ì•ˆìœ¼ë¡œ Oktë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "    from konlpy.tag import Okt\n",
    "    mecab = Okt()\n",
    "\n",
    "def extract_morphemes(text, analyzer=mecab):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ì—ì„œ í˜•íƒœì†Œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    ëª…ì‚¬(NNG, NNP), ë™ì‚¬(VV), í˜•ìš©ì‚¬(VA)ë¥¼ ì£¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # í˜•íƒœì†Œ ë¶„ì„\n",
    "        morphs = analyzer.pos(text)\n",
    "        \n",
    "        # ì¤‘ìš”í•œ í’ˆì‚¬ë§Œ ì¶”ì¶œ (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬)\n",
    "        important_pos = ['NNG', 'NNP', 'VV', 'VA', 'XR', 'SL']  # ì¼ë°˜ëª…ì‚¬, ê³ ìœ ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ì–´ê·¼, ì™¸êµ­ì–´\n",
    "        keywords = [word for word, pos in morphs if pos in important_pos and len(word) > 1]\n",
    "        \n",
    "        return keywords\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def analyze_corpus(texts, sample_size=10000):\n",
    "    \"\"\"\n",
    "    ì½”í¼ìŠ¤ì—ì„œ í˜•íƒœì†Œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  í‚¤ì›Œë“œ ë¹ˆë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“Š {min(sample_size, len(texts)):,}ê°œì˜ í…ìŠ¤íŠ¸ì—ì„œ í˜•íƒœì†Œ ë¶„ì„ ì¤‘...\")\n",
    "    \n",
    "    all_keywords = []\n",
    "    morpheme_data = []\n",
    "    \n",
    "    # ìƒ˜í”Œë§ (ì „ì²´ ì²˜ë¦¬ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)\n",
    "    sample_texts = texts[:sample_size]\n",
    "    \n",
    "    for i, text in enumerate(sample_texts):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"  ì§„í–‰: {i:,}/{len(sample_texts):,} ({i/len(sample_texts)*100:.1f}%)\")\n",
    "        \n",
    "        keywords = extract_morphemes(text)\n",
    "        all_keywords.extend(keywords)\n",
    "        \n",
    "        morpheme_data.append({\n",
    "            'text': text,\n",
    "            'keywords': keywords,\n",
    "            'keyword_count': len(keywords)\n",
    "        })\n",
    "    \n",
    "    # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°\n",
    "    keyword_freq = Counter(all_keywords)\n",
    "    \n",
    "    print(f\"\\nâœ“ ì´ {len(all_keywords):,}ê°œì˜ í‚¤ì›Œë“œ ì¶”ì¶œ\")\n",
    "    print(f\"âœ“ ê³ ìœ  í‚¤ì›Œë“œ ìˆ˜: {len(keyword_freq):,}ê°œ\")\n",
    "    \n",
    "    return morpheme_data, keyword_freq\n",
    "\n",
    "# í˜•íƒœì†Œ ë¶„ì„ ì‹¤í–‰\n",
    "morpheme_data, keyword_freq = analyze_corpus(corpus_texts)\n",
    "\n",
    "# ìƒìœ„ í‚¤ì›Œë“œ ì¶œë ¥\n",
    "print(\"\\nğŸ† ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ:\")\n",
    "for i, (keyword, freq) in enumerate(keyword_freq.most_common(50), 1):\n",
    "    print(f\"{i:2d}. {keyword:15s} - {freq:,}íšŒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë™ì˜ì–´ ì²˜ë¦¬ ë° íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜\n",
    "\n",
    "ì›¹ ê²€ìƒ‰ì„ í†µí•´ ìµœì‹  íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ ì‹ë³„í•˜ê³ , ë™ì˜ì–´ ì‚¬ì „ì„ êµ¬ì¶•í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œêµ­ì–´ ë™ì˜ì–´ ì‚¬ì „ (ê¸°ë³¸)\n",
    "BASE_SYNONYMS = {\n",
    "    'ì¸ê³µì§€ëŠ¥': ['AI', 'ì—ì´ì•„ì´', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'ê¸°ê³„í•™ìŠµ'],\n",
    "    'ê²€ìƒ‰': ['ì„œì¹˜', 'search', 'ì°¾ê¸°', 'íƒìƒ‰'],\n",
    "    'ë°ì´í„°': ['ìë£Œ', 'data', 'ì •ë³´'],\n",
    "    'ê°œë°œ': ['ê°œë°œí•˜ë‹¤', 'development', 'ì œì‘', 'êµ¬ì¶•'],\n",
    "    'ì„œë¹„ìŠ¤': ['ì„œë¹„ìŠ¤í•˜ë‹¤', 'service', 'ì œê³µ'],\n",
    "    'ì‚¬ìš©ì': ['ìœ ì €', 'user', 'ì´ìš©ì', 'ê³ ê°'],\n",
    "    'ì‹œìŠ¤í…œ': ['system', 'ì²´ê³„', 'ì‹œìŠ¤í…œ'],\n",
    "    'ê¸°ìˆ ': ['í…Œí¬', 'tech', 'technology', 'ê¸°ë²•'],\n",
    "    'ë¶„ì„': ['ì• ë„ë¦¬í‹±ìŠ¤', 'analytics', 'í•´ì„', 'ë¶„ì„í•˜ë‹¤'],\n",
    "    'ëª¨ë¸': ['model', 'ëª¨í˜•'],\n",
    "    'í•™ìŠµ': ['training', 'íŠ¸ë ˆì´ë‹', 'ëŸ¬ë‹', 'learning'],\n",
    "    'ì„±ëŠ¥': ['í¼í¬ë¨¼ìŠ¤', 'performance', 'íš¨ìœ¨'],\n",
    "    'ìµœì í™”': ['optimization', 'ê°œì„ ', 'í–¥ìƒ'],\n",
    "    'ì•Œê³ ë¦¬ì¦˜': ['algorithm', 'ì•Œê³ ë¦¬ì¦˜'],\n",
    "    'ë„¤íŠ¸ì›Œí¬': ['network', 'ë„¤íŠ¸ì›', 'ë§'],\n",
    "}\n",
    "\n",
    "# 2024-2025 íŠ¸ë Œë“œ í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ ë†’ìŒ)\n",
    "TREND_KEYWORDS = {\n",
    "    'LLM': 3.0,  # Large Language Model\n",
    "    'GPT': 3.0,\n",
    "    'ChatGPT': 3.0,\n",
    "    'ì±—GPT': 3.0,\n",
    "    'ìƒì„±í˜•AI': 2.8,\n",
    "    'GenAI': 2.8,\n",
    "    'íŠ¸ëœìŠ¤í¬ë¨¸': 2.5,\n",
    "    'Transformer': 2.5,\n",
    "    'RAG': 2.7,  # Retrieval Augmented Generation\n",
    "    'ë²¡í„°ê²€ìƒ‰': 2.6,\n",
    "    'ì„ë² ë”©': 2.5,\n",
    "    'embedding': 2.5,\n",
    "    'BERT': 2.3,\n",
    "    'OpenSearch': 2.4,\n",
    "    'Elasticsearch': 2.2,\n",
    "    'ì‹ ê²½ë§': 2.3,\n",
    "    'íŒŒì¸íŠœë‹': 2.5,\n",
    "    'fine-tuning': 2.5,\n",
    "    'prompt': 2.7,\n",
    "    'í”„ë¡¬í”„íŠ¸': 2.7,\n",
    "    'SPLADE': 2.8,\n",
    "    'í¬ì†Œë²¡í„°': 2.6,\n",
    "    'sparse': 2.5,\n",
    "    'í† í°': 2.4,\n",
    "    'token': 2.4,\n",
    "    'AIê²€ìƒ‰': 2.6,\n",
    "    'ì‹œë§¨í‹±ê²€ìƒ‰': 2.5,\n",
    "    'semantic': 2.5,\n",
    "}\n",
    "\n",
    "def build_synonym_dict(base_synonyms, keyword_freq):\n",
    "    \"\"\"\n",
    "    ê¸°ë³¸ ë™ì˜ì–´ ì‚¬ì „ê³¼ í‚¤ì›Œë“œ ë¹ˆë„ë¥¼ ê²°í•©í•˜ì—¬ í™•ì¥ëœ ë™ì˜ì–´ ì‚¬ì „ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    synonym_dict = {}\n",
    "    \n",
    "    for key, synonyms in base_synonyms.items():\n",
    "        all_terms = [key] + synonyms\n",
    "        for term in all_terms:\n",
    "            synonym_dict[term.lower()] = all_terms\n",
    "    \n",
    "    return synonym_dict\n",
    "\n",
    "def calculate_keyword_weights(keyword_freq, trend_keywords):\n",
    "    \"\"\"\n",
    "    í‚¤ì›Œë“œì˜ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    - ê¸°ë³¸ ë¹ˆë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜\n",
    "    - íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ ê°€ì¤‘ì¹˜\n",
    "    \"\"\"\n",
    "    keyword_weights = {}\n",
    "    \n",
    "    # ë¹ˆë„ ì •ê·œí™” (log scale)\n",
    "    max_freq = max(keyword_freq.values()) if keyword_freq else 1\n",
    "    \n",
    "    for keyword, freq in keyword_freq.items():\n",
    "        # ê¸°ë³¸ ê°€ì¤‘ì¹˜: ë¡œê·¸ ìŠ¤ì¼€ì¼ ë¹ˆë„\n",
    "        base_weight = np.log1p(freq) / np.log1p(max_freq)\n",
    "        \n",
    "        # íŠ¸ë Œë“œ ë³´ë„ˆìŠ¤\n",
    "        trend_bonus = 1.0\n",
    "        for trend_key, bonus in trend_keywords.items():\n",
    "            if trend_key.lower() in keyword.lower() or keyword.lower() in trend_key.lower():\n",
    "                trend_bonus = max(trend_bonus, bonus)\n",
    "        \n",
    "        # ìµœì¢… ê°€ì¤‘ì¹˜\n",
    "        keyword_weights[keyword] = base_weight * trend_bonus\n",
    "    \n",
    "    return keyword_weights\n",
    "\n",
    "# ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶•\n",
    "synonym_dict = build_synonym_dict(BASE_SYNONYMS, keyword_freq)\n",
    "print(f\"âœ“ ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶• ì™„ë£Œ: {len(synonym_dict):,}ê°œ í•­ëª©\")\n",
    "\n",
    "# í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "keyword_weights = calculate_keyword_weights(keyword_freq, TREND_KEYWORDS)\n",
    "print(f\"âœ“ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ê³„ì‚° ì™„ë£Œ: {len(keyword_weights):,}ê°œ í‚¤ì›Œë“œ\")\n",
    "\n",
    "# ê°€ì¤‘ì¹˜ ìƒìœ„ í‚¤ì›Œë“œ ì¶œë ¥\n",
    "print(\"\\nğŸ”¥ ê°€ì¤‘ì¹˜ ìƒìœ„ 30ê°œ í‚¤ì›Œë“œ:\")\n",
    "sorted_weights = sorted(keyword_weights.items(), key=lambda x: x[1], reverse=True)[:30]\n",
    "for i, (keyword, weight) in enumerate(sorted_weights, 1):\n",
    "    print(f\"{i:2d}. {keyword:20s} - ê°€ì¤‘ì¹˜: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "\n",
    "SPLADE ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_dataset(morpheme_data, keyword_weights, synonym_dict):\n",
    "    \"\"\"\n",
    "    SPLADE í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
    "    - Query-Document ìŒ ìƒì„±\n",
    "    - ë™ì˜ì–´ í™•ì¥\n",
    "    - ê°€ì¤‘ì¹˜ ì ìš©\n",
    "    \"\"\"\n",
    "    training_data = []\n",
    "    \n",
    "    for item in morpheme_data:\n",
    "        text = item['text']\n",
    "        keywords = item['keywords']\n",
    "        \n",
    "        # í‚¤ì›Œë“œ ê¸°ë°˜ ì¿¼ë¦¬ ìƒì„±\n",
    "        if len(keywords) >= 2:\n",
    "            # ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ (ê°€ì¤‘ì¹˜ ê¸°ë°˜)\n",
    "            weighted_keywords = [(kw, keyword_weights.get(kw, 0.5)) for kw in keywords]\n",
    "            weighted_keywords.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # ìƒìœ„ 3-5ê°œ í‚¤ì›Œë“œë¡œ ì¿¼ë¦¬ ìƒì„±\n",
    "            top_keywords = [kw for kw, _ in weighted_keywords[:np.random.randint(3, 6)]]\n",
    "            query = ' '.join(top_keywords)\n",
    "            \n",
    "            # ë™ì˜ì–´ í™•ì¥ëœ ì¿¼ë¦¬ë„ ì¶”ê°€\n",
    "            expanded_queries = [query]\n",
    "            for kw in top_keywords:\n",
    "                if kw.lower() in synonym_dict:\n",
    "                    synonyms = synonym_dict[kw.lower()]\n",
    "                    if len(synonyms) > 1:\n",
    "                        # ë™ì˜ì–´ ì¤‘ í•˜ë‚˜ë¡œ ëŒ€ì²´\n",
    "                        alt_kw = np.random.choice([s for s in synonyms if s != kw])\n",
    "                        alt_query = query.replace(kw, alt_kw)\n",
    "                        expanded_queries.append(alt_query)\n",
    "            \n",
    "            # í•™ìŠµ ë°ì´í„° ì¶”ê°€\n",
    "            for q in expanded_queries:\n",
    "                training_data.append({\n",
    "                    'query': q,\n",
    "                    'document': text,\n",
    "                    'score': 1.0,  # Positive pair\n",
    "                    'keywords': keywords\n",
    "                })\n",
    "    \n",
    "    print(f\"âœ“ í•™ìŠµ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(training_data):,}ê°œ ìƒ˜í”Œ\")\n",
    "    return training_data\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "training_data = prepare_training_dataset(morpheme_data, keyword_weights, synonym_dict)\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
    "train_df = pd.DataFrame(training_data)\n",
    "\n",
    "print(\"\\nğŸ“Š í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ:\")\n",
    "print(train_df.head(10))\n",
    "\n",
    "# í†µê³„\n",
    "print(\"\\nğŸ“ˆ ë°ì´í„°ì…‹ í†µê³„:\")\n",
    "print(f\"ì´ ìƒ˜í”Œ ìˆ˜: {len(train_df):,}\")\n",
    "print(f\"í‰ê·  ì¿¼ë¦¬ ê¸¸ì´: {train_df['query'].str.len().mean():.1f}ì\")\n",
    "print(f\"í‰ê·  ë¬¸ì„œ ê¸¸ì´: {train_df['document'].str.len().mean():.1f}ì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SPLADE ëª¨ë¸ ì´ˆê¸°í™” ë° ì„¤ì •\n",
    "\n",
    "í•œêµ­ì–´ BERT ê¸°ë°˜ SPLADE ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œêµ­ì–´ ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì„ íƒ\n",
    "MODEL_NAME = \"klue/bert-base\"  # ë˜ëŠ” \"monologg/kobert\", \"beomi/kcbert-base\"\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "base_model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ: {MODEL_NAME}\")\n",
    "print(f\"âœ“ ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {MODEL_NAME}\")\n",
    "print(f\"   - Vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"   - Model parameters: {sum(p.numel() for p in base_model.parameters()):,}\")\n",
    "\n",
    "class SPLADEModel(nn.Module):\n",
    "    \"\"\"\n",
    "    SPLADE (Sparse Lexical and Expansion Model) êµ¬í˜„\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model, tokenizer):\n",
    "        super().__init__()\n",
    "        self.transformer = base_model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # SPLADE íŠ¹í™” ë ˆì´ì–´\n",
    "        hidden_size = base_model.config.hidden_size\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        \n",
    "        # Log saturation activation\n",
    "        self.log_activation = lambda x: torch.log1p(torch.relu(x))\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Returns: sparse representation (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        # BERT ì¸ì½”ë”©\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        # Logits (batch_size, seq_len, vocab_size)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Max pooling over sequence length\n",
    "        sparse_rep = torch.max(\n",
    "            self.log_activation(logits) * attention_mask.unsqueeze(-1),\n",
    "            dim=1\n",
    "        ).values\n",
    "        \n",
    "        return sparse_rep\n",
    "\n",
    "# SPLADE ëª¨ë¸ ì´ˆê¸°í™”\n",
    "splade_model = SPLADEModel(base_model, tokenizer)\n",
    "print(\"\\nâœ“ SPLADE ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "\n",
    "# GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ ì´ë™\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "splade_model = splade_model.to(device)\n",
    "print(f\"âœ“ ëª¨ë¸ì„ {device}ë¡œ ì´ë™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì„¤ì •\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "WARMUP_STEPS = 500\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ ê°€ì¤‘ì¹˜\n",
    "FLOPS_REGULARIZATION = 1e-3  # L0 regularization\n",
    "RANKING_LOSS_WEIGHT = 1.0\n",
    "\n",
    "print(\"ğŸ¯ í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  - Max length: {MAX_LENGTH}\")\n",
    "print(f\"  - FLOPS regularization: {FLOPS_REGULARIZATION}\")\n",
    "print(f\"  - Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ë°ì´í„° ë¡œë” ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPLADEDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    SPLADE í•™ìŠµìš© ë°ì´í„°ì…‹\n",
    "    \"\"\"\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # ì¿¼ë¦¬ í† í°í™”\n",
    "        query_encoded = self.tokenizer(\n",
    "            item['query'],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # ë¬¸ì„œ í† í°í™”\n",
    "        doc_encoded = self.tokenizer(\n",
    "            item['document'],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'query_input_ids': query_encoded['input_ids'].squeeze(),\n",
    "            'query_attention_mask': query_encoded['attention_mask'].squeeze(),\n",
    "            'doc_input_ids': doc_encoded['input_ids'].squeeze(),\n",
    "            'doc_attention_mask': doc_encoded['attention_mask'].squeeze(),\n",
    "            'score': torch.tensor(item['score'], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¶„í•  (train/val)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(training_data, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = SPLADEDataset(train_data, tokenizer, MAX_LENGTH)\n",
    "val_dataset = SPLADEDataset(val_data, tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"âœ“ í•™ìŠµ ë°ì´í„°ì…‹: {len(train_dataset):,} ìƒ˜í”Œ\")\n",
    "print(f\"âœ“ ê²€ì¦ ë°ì´í„°ì…‹: {len(val_dataset):,} ìƒ˜í”Œ\")\n",
    "print(f\"âœ“ ë°°ì¹˜ ìˆ˜: {len(train_loader):,} (train), {len(val_loader):,} (val)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. í•™ìŠµ ë£¨í”„ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def splade_loss(query_rep, doc_rep, scores, flops_reg=1e-3):\n",
    "    \"\"\"\n",
    "    SPLADE ì†ì‹¤ í•¨ìˆ˜\n",
    "    - Ranking loss (query-document similarity)\n",
    "    - FLOPS regularization (L0 sparsity)\n",
    "    \"\"\"\n",
    "    # Dot product similarity\n",
    "    similarities = torch.sum(query_rep * doc_rep, dim=-1)\n",
    "    \n",
    "    # Ranking loss (MSE)\n",
    "    ranking_loss = nn.functional.mse_loss(similarities, scores)\n",
    "    \n",
    "    # FLOPS regularization (encourage sparsity)\n",
    "    query_flops = torch.sum(torch.abs(query_rep)) / query_rep.size(0)\n",
    "    doc_flops = torch.sum(torch.abs(doc_rep)) / doc_rep.size(0)\n",
    "    flops_loss = query_flops + doc_flops\n",
    "    \n",
    "    total_loss = ranking_loss + flops_reg * flops_loss\n",
    "    \n",
    "    return total_loss, ranking_loss, flops_loss\n",
    "\n",
    "# ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "optimizer = AdamW(splade_model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"âœ“ ì˜µí‹°ë§ˆì´ì € ì„¤ì • ì™„ë£Œ (AdamW, lr={LEARNING_RATE})\")\n",
    "print(f\"âœ“ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ì™„ë£Œ (ì´ {total_steps:,} ìŠ¤í…, warmup {WARMUP_STEPS:,} ìŠ¤í…)\")\n",
    "\n",
    "# í•™ìŠµ ê¸°ë¡\n",
    "training_history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_ranking_loss': [],\n",
    "    'train_flops_loss': []\n",
    "}\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scheduler, device):\n",
    "    \"\"\"\n",
    "    í•œ ì—í­ í•™ìŠµ\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_ranking_loss = 0\n",
    "    total_flops_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=\"Training\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # ë°ì´í„° ì´ë™\n",
    "        query_input_ids = batch['query_input_ids'].to(device)\n",
    "        query_attention_mask = batch['query_attention_mask'].to(device)\n",
    "        doc_input_ids = batch['doc_input_ids'].to(device)\n",
    "        doc_attention_mask = batch['doc_attention_mask'].to(device)\n",
    "        scores = batch['score'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        query_rep = model(query_input_ids, query_attention_mask)\n",
    "        doc_rep = model(doc_input_ids, doc_attention_mask)\n",
    "        \n",
    "        # ì†ì‹¤ ê³„ì‚°\n",
    "        loss, ranking_loss, flops_loss = splade_loss(\n",
    "            query_rep, doc_rep, scores, FLOPS_REGULARIZATION\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # ê¸°ë¡\n",
    "        total_loss += loss.item()\n",
    "        total_ranking_loss += ranking_loss.item()\n",
    "        total_flops_loss += flops_loss.item()\n",
    "        \n",
    "        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'rank_loss': f'{ranking_loss.item():.4f}',\n",
    "            'flops': f'{flops_loss.item():.2f}'\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_ranking = total_ranking_loss / len(loader)\n",
    "    avg_flops = total_flops_loss / len(loader)\n",
    "    \n",
    "    return avg_loss, avg_ranking, avg_flops\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"\n",
    "    ê²€ì¦\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            query_input_ids = batch['query_input_ids'].to(device)\n",
    "            query_attention_mask = batch['query_attention_mask'].to(device)\n",
    "            doc_input_ids = batch['doc_input_ids'].to(device)\n",
    "            doc_attention_mask = batch['doc_attention_mask'].to(device)\n",
    "            scores = batch['score'].to(device)\n",
    "            \n",
    "            query_rep = model(query_input_ids, query_attention_mask)\n",
    "            doc_rep = model(doc_input_ids, doc_attention_mask)\n",
    "            \n",
    "            loss, _, _ = splade_loss(query_rep, doc_rep, scores, FLOPS_REGULARIZATION)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ í•™ìŠµ ì‹œì‘!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ëª¨ë¸ í•™ìŠµ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì‹¤í–‰\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # í•™ìŠµ\n",
    "    train_loss, train_ranking, train_flops = train_epoch(\n",
    "        splade_model, train_loader, optimizer, scheduler, device\n",
    "    )\n",
    "    \n",
    "    # ê²€ì¦\n",
    "    val_loss = evaluate(splade_model, val_loader, device)\n",
    "    \n",
    "    # ê¸°ë¡\n",
    "    training_history['train_loss'].append(train_loss)\n",
    "    training_history['val_loss'].append(val_loss)\n",
    "    training_history['train_ranking_loss'].append(train_ranking)\n",
    "    training_history['train_flops_loss'].append(train_flops)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Epoch {epoch + 1} ê²°ê³¼:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Train Ranking Loss: {train_ranking:.4f}\")\n",
    "    print(f\"  Train FLOPS Loss: {train_flops:.2f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': splade_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, 'best_korean_splade_model.pt')\n",
    "        print(f\"  âœ“ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥! (val_loss: {val_loss:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ìµœì¢… Best Validation Loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. í•™ìŠµ ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# ì „ì²´ ì†ì‹¤\n",
    "axes[0, 0].plot(training_history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0, 0].plot(training_history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Ranking ì†ì‹¤\n",
    "axes[0, 1].plot(training_history['train_ranking_loss'], label='Ranking Loss', marker='o', color='orange')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Ranking Loss')\n",
    "axes[0, 1].set_title('Training Ranking Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# FLOPS ì†ì‹¤\n",
    "axes[1, 0].plot(training_history['train_flops_loss'], label='FLOPS Loss', marker='o', color='green')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('FLOPS Loss')\n",
    "axes[1, 0].set_title('Training FLOPS Loss (Sparsity)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Loss ë¹„êµ\n",
    "x = np.arange(len(training_history['train_loss']))\n",
    "width = 0.35\n",
    "axes[1, 1].bar(x - width/2, training_history['train_loss'], width, label='Train', alpha=0.8)\n",
    "axes[1, 1].bar(x + width/2, training_history['val_loss'], width, label='Val', alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].set_title('Train vs Val Loss Comparison')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels([f'E{i+1}' for i in x])\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ í•™ìŠµ ê³¡ì„  ì €ì¥: training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. ëª¨ë¸ í‰ê°€ ë° í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ\n",
    "checkpoint = torch.load('best_korean_splade_model.pt')\n",
    "splade_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "splade_model.eval()\n",
    "\n",
    "print(\"âœ“ ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  - Epoch: {checkpoint['epoch'] + 1}\")\n",
    "print(f\"  - Val Loss: {checkpoint['val_loss']:.4f}\")\n",
    "\n",
    "def encode_text(text, model, tokenizer, device, max_length=128):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ë¥¼ SPLADE sparse vectorë¡œ ì¸ì½”ë”©\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # í† í°í™”\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    \n",
    "    # ì¸ì½”ë”©\n",
    "    with torch.no_grad():\n",
    "        sparse_rep = model(input_ids, attention_mask)\n",
    "    \n",
    "    return sparse_rep.cpu().numpy()[0]\n",
    "\n",
    "def get_top_tokens(sparse_vector, tokenizer, top_k=20):\n",
    "    \"\"\"\n",
    "    Sparse vectorì—ì„œ ìƒìœ„ í† í° ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    # ìƒìœ„ kê°œ ì¸ë±ìŠ¤\n",
    "    top_indices = np.argsort(sparse_vector)[-top_k:][::-1]\n",
    "    top_values = sparse_vector[top_indices]\n",
    "    \n",
    "    # í† í° ë³€í™˜\n",
    "    top_tokens = [(tokenizer.decode([idx]), val) for idx, val in zip(top_indices, top_values) if val > 0]\n",
    "    \n",
    "    return top_tokens\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬\n",
    "test_queries = [\n",
    "    \"ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê²€ìƒ‰ ì—”ì§„\",\n",
    "    \"ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ë°©ë²•\",\n",
    "    \"í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ \",\n",
    "    \"OpenSearch ë²¡í„° ê²€ìƒ‰\",\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ìµœì í™”\",\n",
    "    \"ChatGPT ê°™ì€ ìƒì„±í˜• AI\",\n",
    "    \"íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ ì„¤ëª…\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ§ª í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì¸ì½”ë”© ê²°ê³¼\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nğŸ“ ì¿¼ë¦¬: {query}\")\n",
    "    \n",
    "    # ì¸ì½”ë”©\n",
    "    sparse_vec = encode_text(query, splade_model, tokenizer, device)\n",
    "    \n",
    "    # í¬ì†Œì„± ê³„ì‚°\n",
    "    non_zero_ratio = np.count_nonzero(sparse_vec) / len(sparse_vec) * 100\n",
    "    l1_norm = np.sum(np.abs(sparse_vec))\n",
    "    \n",
    "    print(f\"  í¬ì†Œì„±: {non_zero_ratio:.2f}% (non-zero: {np.count_nonzero(sparse_vec):,}/{len(sparse_vec):,})\")\n",
    "    print(f\"  L1 Norm: {l1_norm:.2f}\")\n",
    "    \n",
    "    # ìƒìœ„ í† í°\n",
    "    top_tokens = get_top_tokens(sparse_vec, tokenizer, top_k=15)\n",
    "    print(f\"  ìƒìœ„ í† í°:\")\n",
    "    for i, (token, value) in enumerate(top_tokens, 1):\n",
    "        print(f\"    {i:2d}. {token:15s} ({value:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. ëª¨ë¸ ì €ì¥ ë° ë‚´ë³´ë‚´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "OUTPUT_DIR = \"./korean_splade_model\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ì „ì²´ ëª¨ë¸ ì €ì¥\n",
    "torch.save(splade_model.state_dict(), f\"{OUTPUT_DIR}/pytorch_model.bin\")\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# ì„¤ì • íŒŒì¼ ì €ì¥\n",
    "config = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"vocab_size\": tokenizer.vocab_size,\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \"training_samples\": len(train_dataset),\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"best_val_loss\": best_val_loss,\n",
    "    \"flops_regularization\": FLOPS_REGULARIZATION,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"trained_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/config.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì €ì¥\n",
    "weights_df = pd.DataFrame([\n",
    "    {\"keyword\": k, \"weight\": v} \n",
    "    for k, v in sorted(keyword_weights.items(), key=lambda x: x[1], reverse=True)[:1000]\n",
    "])\n",
    "weights_df.to_csv(f\"{OUTPUT_DIR}/keyword_weights.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "# ë™ì˜ì–´ ì‚¬ì „ ì €ì¥\n",
    "with open(f\"{OUTPUT_DIR}/synonyms.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(BASE_SYNONYMS, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\")\n",
    "print(f\"\\nì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}/\")\n",
    "print(\"\\nì €ì¥ëœ íŒŒì¼:\")\n",
    "print(\"  - pytorch_model.bin: PyTorch ëª¨ë¸ ê°€ì¤‘ì¹˜\")\n",
    "print(\"  - config.json: ëª¨ë¸ ì„¤ì • ë° í•™ìŠµ ì •ë³´\")\n",
    "print(\"  - tokenizer files: í† í¬ë‚˜ì´ì € íŒŒì¼ë“¤\")\n",
    "print(\"  - keyword_weights.csv: í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜\")\n",
    "print(\"  - synonyms.json: ë™ì˜ì–´ ì‚¬ì „\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. OpenSearch í†µí•© ê°€ì´ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘          OpenSearch Neural Sparse ëª¨ë¸ í†µí•© ê°€ì´ë“œ           â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "1ï¸âƒ£ ëª¨ë¸ì„ OpenSearchì— ì—…ë¡œë“œ:\n",
    "\n",
    "   ```bash\n",
    "   # ëª¨ë¸ì„ TorchScriptë¡œ ë³€í™˜\n",
    "   python convert_to_torchscript.py --model ./korean_splade_model\n",
    "   \n",
    "   # OpenSearchì— ëª¨ë¸ ë“±ë¡\n",
    "   curl -XPOST \"localhost:9200/_plugins/_ml/models/_upload\" \\\n",
    "     -H 'Content-Type: application/json' \\\n",
    "     -d '{\n",
    "       \"name\": \"korean-splade-model\",\n",
    "       \"version\": \"1.0\",\n",
    "       \"model_format\": \"TORCH_SCRIPT\",\n",
    "       \"model_config\": {\n",
    "         \"model_type\": \"bert\",\n",
    "         \"embedding_dimension\": 768,\n",
    "         \"framework_type\": \"sentence_transformers\"\n",
    "       }\n",
    "     }'\n",
    "   ```\n",
    "\n",
    "2ï¸âƒ£ Neural Sparse Index ìƒì„±:\n",
    "\n",
    "   ```json\n",
    "   PUT /korean-neural-sparse-index\n",
    "   {\n",
    "     \"settings\": {\n",
    "       \"index\": {\n",
    "         \"knn\": true,\n",
    "         \"default_pipeline\": \"korean-neural-sparse-pipeline\"\n",
    "       }\n",
    "     },\n",
    "     \"mappings\": {\n",
    "       \"properties\": {\n",
    "         \"content\": { \"type\": \"text\" },\n",
    "         \"content_embedding\": {\n",
    "           \"type\": \"rank_features\"\n",
    "         }\n",
    "       }\n",
    "     }\n",
    "   }\n",
    "   ```\n",
    "\n",
    "3ï¸âƒ£ Ingest Pipeline ì„¤ì •:\n",
    "\n",
    "   ```json\n",
    "   PUT /_ingest/pipeline/korean-neural-sparse-pipeline\n",
    "   {\n",
    "     \"description\": \"Korean Neural Sparse Encoding Pipeline\",\n",
    "     \"processors\": [\n",
    "       {\n",
    "         \"sparse_encoding\": {\n",
    "           \"model_id\": \"<model_id>\",\n",
    "           \"field_map\": {\n",
    "             \"content\": \"content_embedding\"\n",
    "           }\n",
    "         }\n",
    "       }\n",
    "     ]\n",
    "   }\n",
    "   ```\n",
    "\n",
    "4ï¸âƒ£ Neural Sparse ê²€ìƒ‰:\n",
    "\n",
    "   ```json\n",
    "   POST /korean-neural-sparse-index/_search\n",
    "   {\n",
    "     \"query\": {\n",
    "       \"neural_sparse\": {\n",
    "         \"content_embedding\": {\n",
    "           \"query_text\": \"ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê²€ìƒ‰\",\n",
    "           \"model_id\": \"<model_id>\"\n",
    "         }\n",
    "       }\n",
    "     }\n",
    "   }\n",
    "   ```\n",
    "\n",
    "5ï¸âƒ£ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + Neural Sparse):\n",
    "\n",
    "   ```json\n",
    "   POST /korean-neural-sparse-index/_search\n",
    "   {\n",
    "     \"query\": {\n",
    "       \"hybrid\": {\n",
    "         \"queries\": [\n",
    "           {\n",
    "             \"match\": {\n",
    "               \"content\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰\"\n",
    "             }\n",
    "           },\n",
    "           {\n",
    "             \"neural_sparse\": {\n",
    "               \"content_embedding\": {\n",
    "                 \"query_text\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰\",\n",
    "                 \"model_id\": \"<model_id>\"\n",
    "               }\n",
    "             }\n",
    "           }\n",
    "         ]\n",
    "       }\n",
    "     },\n",
    "     \"search_pipeline\": {\n",
    "       \"phase_results_processors\": [\n",
    "         {\n",
    "           \"normalization-processor\": {\n",
    "             \"normalization\": {\n",
    "               \"technique\": \"min_max\"\n",
    "             },\n",
    "             \"combination\": {\n",
    "               \"technique\": \"arithmetic_mean\",\n",
    "               \"parameters\": {\n",
    "                 \"weights\": [0.3, 0.7]\n",
    "               }\n",
    "             }\n",
    "           }\n",
    "         }\n",
    "       ]\n",
    "     }\n",
    "   }\n",
    "   ```\n",
    "\n",
    "ğŸ“š ì°¸ê³  ìë£Œ:\n",
    "  - OpenSearch Neural Search: https://opensearch.org/docs/latest/search-plugins/neural-search/\n",
    "  - Neural Sparse Search: https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/\n",
    "  - Hybrid Search: https://opensearch.org/docs/latest/search-plugins/hybrid-search/\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸\n",
    "benchmark_queries = [\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹\",\n",
    "    \"ì¸ê³µì§€ëŠ¥ ë”¥ëŸ¬ë‹\",\n",
    "    \"í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ  ì—°êµ¬\",\n",
    "    \"OpenSearch ë²¡í„° ê²€ìƒ‰ ì—”ì§„ ìµœì í™” ë°©ë²•\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for query in benchmark_queries:\n",
    "    # ì‹œê°„ ì¸¡ì •\n",
    "    start_time = time.time()\n",
    "    sparse_vec = encode_text(query, splade_model, tokenizer, device)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    encoding_time = (end_time - start_time) * 1000  # ms\n",
    "    non_zero_count = np.count_nonzero(sparse_vec)\n",
    "    sparsity = (1 - non_zero_count / len(sparse_vec)) * 100\n",
    "    \n",
    "    print(f\"\\nì¿¼ë¦¬: {query}\")\n",
    "    print(f\"  ì¸ì½”ë”© ì‹œê°„: {encoding_time:.2f}ms\")\n",
    "    print(f\"  í¬ì†Œì„±: {sparsity:.2f}% (non-zero: {non_zero_count}/{len(sparse_vec)})\")\n",
    "    print(f\"  ë²¡í„° í¬ê¸°: {len(sparse_vec):,} dimensions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### ğŸ‰ ì™„ë£Œëœ ì‘ì—…\n",
    "\n",
    "1. âœ… í•œêµ­ì–´ ê³µê°œ ë°ì´í„°ì…‹ ìˆ˜ì§‘ (KLUE, KorQuAD, Wikipedia ë“±)\n",
    "2. âœ… Mecab í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ í†µí•œ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "3. âœ… ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶• ë° íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì ìš©\n",
    "4. âœ… SPLADE ëª¨ë¸ íŒŒì¸íŠœë‹ (í•œêµ­ì–´ BERT ê¸°ë°˜)\n",
    "5. âœ… ëª¨ë¸ í‰ê°€ ë° ë²¤ì¹˜ë§ˆí¬\n",
    "6. âœ… OpenSearch í†µí•© ê°€ì´ë“œ ì œê³µ\n",
    "\n",
    "### ğŸ“ˆ ì„±ëŠ¥ ê°œì„  í¬ì¸íŠ¸\n",
    "\n",
    "- **í¬ì†Œì„±**: FLOPS regularizationì„ í†µí•´ íš¨ìœ¨ì ì¸ sparse representation ë‹¬ì„±\n",
    "- **íŠ¸ë Œë“œ ë°˜ì˜**: 2024-2025 ìµœì‹  í‚¤ì›Œë“œ(LLM, GPT, RAG ë“±)ì— ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬\n",
    "- **ë™ì˜ì–´ ì²˜ë¦¬**: í•œêµ­ì–´ íŠ¹ì„±ì„ ê³ ë ¤í•œ ë™ì˜ì–´ í™•ì¥\n",
    "- **í˜•íƒœì†Œ ë¶„ì„**: Mecabì„ í†µí•œ ì •í™•í•œ í•œêµ­ì–´ í† í°í™”\n",
    "\n",
    "### ğŸš€ ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "1. **ë” ë§ì€ ë°ì´í„°ë¡œ í•™ìŠµ**: AI Hub, NIKL ë“± ì¶”ê°€ ë°ì´í„°ì…‹ í™œìš©\n",
    "2. **Hard Negative Mining**: ì–´ë ¤ìš´ negative ìƒ˜í”Œë¡œ í•™ìŠµ ê°•í™”\n",
    "3. **Cross-encoder Reranking**: ì¬ìˆœìœ„í™” ëª¨ë¸ ì¶”ê°€\n",
    "4. **Multi-task Learning**: NER, QA ë“± ë‹¤ì–‘í•œ íƒœìŠ¤í¬ ë™ì‹œ í•™ìŠµ\n",
    "5. **A/B í…ŒìŠ¤íŒ…**: OpenSearchì—ì„œ ê¸°ì¡´ ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ\n",
    "6. **ì§€ì†ì  ì—…ë°ì´íŠ¸**: ìµœì‹  íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì£¼ê¸°ì  ì—…ë°ì´íŠ¸\n",
    "\n",
    "### ğŸ“ ì°¸ê³ \n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ í•œêµ­ì–´ neural sparse ê²€ìƒ‰ì˜ ê¸°ì´ˆë¥¼ ë‹¤ì§€ëŠ” ì¶œë°œì ì…ë‹ˆë‹¤. \n",
    "ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ë” ë§ì€ ë°ì´í„°, ë” ê¸´ í•™ìŠµ ì‹œê°„, ê·¸ë¦¬ê³  í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

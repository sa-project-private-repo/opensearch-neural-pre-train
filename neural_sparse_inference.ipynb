{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenSearch Korean Neural Sparse Model - Inference Test\n",
    "\n",
    "í•™ìŠµëœ í•œêµ­ì–´ Neural Sparse ëª¨ë¸ì„ ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "## í…ŒìŠ¤íŠ¸ í•­ëª©\n",
    "1. ëª¨ë¸ ë¡œë“œ\n",
    "2. ë¬¸ì„œ ì¸ì½”ë”© (BERT ê¸°ë°˜)\n",
    "3. ì¿¼ë¦¬ ì¸ì½”ë”© (IDF lookup - Inference-Free)\n",
    "4. ìœ ì‚¬ë„ ê³„ì‚° ë° ê²€ìƒ‰\n",
    "5. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë””ë°”ì´ìŠ¤: cuda\n",
      "GPU: Tesla T4\n",
      "ë©”ëª¨ë¦¬: 14.57 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForMaskedLM\n",
    "\n",
    "# GPU/CPU í™•ì¸\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ëª¨ë¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ëª¨ë¸ í´ëž˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "class OpenSearchDocEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    OpenSearch Neural Sparse Document Encoder (Doc-only mode)\n",
    "    \n",
    "    ë¬¸ì„œë¥¼ sparse vectorë¡œ ì¸ì½”ë”©í•˜ëŠ” ëª¨ë¸ìž…ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"klue/bert-base\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # BERT ê¸°ë°˜ ì¸ì½”ë”\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.bert = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        \n",
    "        self.vocab_size = self.config.vocab_size\n",
    "        \n",
    "        # Log saturation activation: log(1 + ReLU(x))\n",
    "        self.activation = lambda x: torch.log1p(torch.relu(x))\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, return_dict=False):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            input_ids: (batch_size, seq_len)\n",
    "            attention_mask: (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            sparse_vector: (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        # BERT MLM head output\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Logits: (batch_size, seq_len, vocab_size)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Apply activation: log(1 + ReLU(logits))\n",
    "        activated = self.activation(logits)\n",
    "        \n",
    "        # Max pooling over sequence length\n",
    "        # (batch_size, seq_len, vocab_size) â†’ (batch_size, vocab_size)\n",
    "        sparse_vector = torch.max(\n",
    "            activated * attention_mask.unsqueeze(-1),\n",
    "            dim=1\n",
    "        ).values\n",
    "        \n",
    "        if return_dict:\n",
    "            return {'output': sparse_vector}\n",
    "        \n",
    "        return sparse_vector\n",
    "\n",
    "print(\"âœ“ ëª¨ë¸ í´ëž˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì €ìž¥ëœ ëª¨ë¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ ëª¨ë¸ íŒŒì¼ í™•ì¸...\n",
      "  âœ“ pytorch_model.bin         ( 432313.86 KB)\n",
      "  âœ“ idf.json                  (    905.19 KB)\n",
      "  âœ“ config.json               (      0.55 KB)\n",
      "  âœ“ tokenizer.json            (    734.63 KB)\n",
      "  âœ“ vocab.txt                 (    242.65 KB)\n",
      "\n",
      "============================================================\n",
      "ëª¨ë¸ ë¡œë”© ì¤‘...\n",
      "============================================================\n",
      "\n",
      "âœ“ Config ë¡œë“œ ì™„ë£Œ\n",
      "  ëª¨ë¸ íƒ€ìž…: opensearch-neural-sparse-doc-encoder\n",
      "  ë² ì´ìŠ¤ ëª¨ë¸: klue/bert-base\n",
      "  Vocab size: 32,000\n",
      "  Max length: 128\n",
      "\n",
      "âœ“ Tokenizer ë¡œë“œ ì™„ë£Œ\n",
      "  Vocab size: 32,000\n",
      "\n",
      "âœ“ IDF ë”•ì…”ë„ˆë¦¬ ë¡œë“œ ì™„ë£Œ\n",
      "  í† í° ìˆ˜: 29,205\n",
      "  í‰ê·  IDF: 8.9035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Document Encoder ë¡œë“œ ì™„ë£Œ\n",
      "  Parameters: 110,650,880\n",
      "  Device: cuda\n",
      "\n",
      "============================================================\n",
      "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
    "MODEL_DIR = \"./opensearch-korean-neural-sparse-v1\"\n",
    "\n",
    "# íŒŒì¼ ì¡´ìž¬ í™•ì¸\n",
    "required_files = [\n",
    "    \"pytorch_model.bin\",\n",
    "    \"idf.json\",\n",
    "    \"config.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"vocab.txt\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ“¦ ëª¨ë¸ íŒŒì¼ í™•ì¸...\")\n",
    "for filename in required_files:\n",
    "    filepath = os.path.join(MODEL_DIR, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        size = os.path.getsize(filepath) / 1024\n",
    "        print(f\"  âœ“ {filename:25s} ({size:>10.2f} KB)\")\n",
    "    else:\n",
    "        print(f\"  âœ— {filename:25s} [NOT FOUND]\")\n",
    "        raise FileNotFoundError(f\"Required file not found: {filename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Config ë¡œë“œ\n",
    "with open(os.path.join(MODEL_DIR, \"config.json\"), 'r', encoding='utf-8') as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "print(\"\\nâœ“ Config ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  ëª¨ë¸ íƒ€ìž…: {model_config['model_type']}\")\n",
    "print(f\"  ë² ì´ìŠ¤ ëª¨ë¸: {model_config['base_model']}\")\n",
    "print(f\"  Vocab size: {model_config['vocab_size']:,}\")\n",
    "print(f\"  Max length: {model_config['max_seq_length']}\")\n",
    "\n",
    "# 2. Tokenizer ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "print(f\"\\nâœ“ Tokenizer ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "# 3. IDF ë”•ì…”ë„ˆë¦¬ ë¡œë“œ\n",
    "with open(os.path.join(MODEL_DIR, \"idf.json\"), 'r', encoding='utf-8') as f:\n",
    "    idf_dict = json.load(f)\n",
    "\n",
    "print(f\"\\nâœ“ IDF ë”•ì…”ë„ˆë¦¬ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  í† í° ìˆ˜: {len(idf_dict):,}\")\n",
    "print(f\"  í‰ê·  IDF: {np.mean(list(idf_dict.values())):.4f}\")\n",
    "\n",
    "# 4. Document Encoder ëª¨ë¸ ë¡œë“œ\n",
    "base_model = model_config['base_model']\n",
    "doc_encoder = OpenSearchDocEncoder(base_model)\n",
    "\n",
    "# í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "state_dict = torch.load(\n",
    "    os.path.join(MODEL_DIR, \"pytorch_model.bin\"),\n",
    "    map_location=device\n",
    ")\n",
    "doc_encoder.load_state_dict(state_dict)\n",
    "doc_encoder = doc_encoder.to(device)\n",
    "doc_encoder.eval()  # Evaluation mode\n",
    "\n",
    "print(f\"\\nâœ“ Document Encoder ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in doc_encoder.parameters()):,}\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Inference í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def encode_document(text, model, tokenizer, device, max_length=128):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œë¥¼ sparse vectorë¡œ ì¸ì½”ë”© (ëª¨ë¸ ì‚¬ìš© - ëŠë¦¼)\n",
    "    \n",
    "    Args:\n",
    "        text: ë¬¸ì„œ í…ìŠ¤íŠ¸\n",
    "        model: Document encoder ëª¨ë¸\n",
    "        tokenizer: Tokenizer\n",
    "        device: ë””ë°”ì´ìŠ¤\n",
    "        max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "    \n",
    "    Returns:\n",
    "        sparse_vector: (vocab_size,) numpy array\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # í† í°í™”\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        sparse_vec = model(input_ids, attention_mask)\n",
    "    \n",
    "    return sparse_vec.cpu().numpy()[0]\n",
    "\n",
    "\n",
    "def encode_query_inference_free(text, tokenizer, idf_dict, max_length=128):\n",
    "    \"\"\"\n",
    "    ì¿¼ë¦¬ë¥¼ sparse vectorë¡œ ì¸ì½”ë”© (IDF lookup - Inference-Free! ë§¤ìš° ë¹ ë¦„)\n",
    "    \n",
    "    Args:\n",
    "        text: ì¿¼ë¦¬ í…ìŠ¤íŠ¸\n",
    "        tokenizer: Tokenizer\n",
    "        idf_dict: IDF ë”•ì…”ë„ˆë¦¬\n",
    "        max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "    \n",
    "    Returns:\n",
    "        sparse_vector: (vocab_size,) numpy array\n",
    "    \"\"\"\n",
    "    # í† í°í™”\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        add_special_tokens=False,\n",
    "        max_length=max_length,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # IDF lookup\n",
    "    sparse_vec = np.zeros(tokenizer.vocab_size)\n",
    "    for token_id in tokens:\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        if token_str in idf_dict:\n",
    "            sparse_vec[token_id] = idf_dict[token_str]\n",
    "    \n",
    "    return sparse_vec\n",
    "\n",
    "\n",
    "def get_top_tokens(sparse_vec, tokenizer, top_k=15):\n",
    "    \"\"\"\n",
    "    Sparse vectorì—ì„œ ìƒìœ„ í† í° ì¶”ì¶œ\n",
    "    \n",
    "    Args:\n",
    "        sparse_vec: (vocab_size,) numpy array\n",
    "        tokenizer: Tokenizer\n",
    "        top_k: ìƒìœ„ kê°œ\n",
    "    \n",
    "    Returns:\n",
    "        List of (token, value) tuples\n",
    "    \"\"\"\n",
    "    top_indices = np.argsort(sparse_vec)[-top_k:][::-1]\n",
    "    top_values = sparse_vec[top_indices]\n",
    "    \n",
    "    top_tokens = []\n",
    "    for idx, val in zip(top_indices, top_values):\n",
    "        if val > 0:\n",
    "            token = tokenizer.decode([idx])\n",
    "            top_tokens.append((token, val))\n",
    "    \n",
    "    return top_tokens\n",
    "\n",
    "\n",
    "def compute_similarity(query_vec, doc_vec):\n",
    "    \"\"\"\n",
    "    ì¿¼ë¦¬ì™€ ë¬¸ì„œì˜ ìœ ì‚¬ë„ ê³„ì‚° (Dot product)\n",
    "    \n",
    "    Args:\n",
    "        query_vec: (vocab_size,) numpy array\n",
    "        doc_vec: (vocab_size,) numpy array\n",
    "    \n",
    "    Returns:\n",
    "        similarity score (float)\n",
    "    \"\"\"\n",
    "    return np.dot(query_vec, doc_vec)\n",
    "\n",
    "\n",
    "print(\"âœ“ Inference í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë¬¸ì„œ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ“„ ë¬¸ì„œ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ (Model Inference)\n",
      "============================================================\n",
      "\n",
      "[ë¬¸ì„œ 1]\n",
      "í…ìŠ¤íŠ¸: OpenSearchëŠ” ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ìž…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„...\n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 7/32,000)\n",
      "  L1 Norm: 1.82\n",
      "  ì¸ì½”ë”© ì‹œê°„: 142.29ms\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ##earch         (0.4343)\n",
      "     2. ê²€ìƒ‰              (0.3980)\n",
      "     3. ë¶„ì„              (0.3320)\n",
      "     4. ë³´ê³ ì„œ             (0.2745)\n",
      "     5. ##S             (0.2496)\n",
      "     6. íƒìƒ‰              (0.1086)\n",
      "     7. ì˜¤               (0.0222)\n",
      "\n",
      "[ë¬¸ì„œ 2]\n",
      "í…ìŠ¤íŠ¸: í•œêµ­ì–´ ìžì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤. KoNLPyì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬...\n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 5/32,000)\n",
      "  L1 Norm: 1.93\n",
      "  ì¸ì½”ë”© ì‹œê°„: 12.97ms\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ì–¸ì–´              (0.7562)\n",
      "     2. ì²˜ë¦¬              (0.7488)\n",
      "     3. ìžì—°              (0.3232)\n",
      "     4. ë¼ì´ë¸Œ             (0.0915)\n",
      "     5. ##ëŸ¬ë¦¬            (0.0058)\n",
      "\n",
      "[ë¬¸ì„œ 3]\n",
      "í…ìŠ¤íŠ¸: ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ ë°ì´í„°ì™€ ì»´í“¨íŒ… ìžì›ì´ í•„ìš”í•©ë‹ˆë‹¤. GPUë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµ ì†ë„ê°€ í¬ê²Œ ...\n",
      "  í¬ì†Œì„±: 99.97% (non-zero: 9/32,000)\n",
      "  L1 Norm: 5.01\n",
      "  ì¸ì½”ë”© ì‹œê°„: 12.33ms\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ë”¥               (1.1729)\n",
      "     2. í•™ìŠµ              (0.9816)\n",
      "     3. GP              (0.8123)\n",
      "     4. ##ëŸ¬             (0.6911)\n",
      "     5. ê·¸ëž˜í”½             (0.5761)\n",
      "     6. ë°ì´í„°             (0.4078)\n",
      "     7. ë‡Œ               (0.2140)\n",
      "     8. AI              (0.0768)\n",
      "     9. ëª¨ë¸              (0.0751)\n",
      "\n",
      "[ë¬¸ì„œ 4]\n",
      "í…ìŠ¤íŠ¸: ChatGPTëŠ” OpenAIê°€ ê°œë°œí•œ ëŒ€í™”í˜• AI ëª¨ë¸ìž…ë‹ˆë‹¤. GPT ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©° ë‹¤ì–‘í•œ ìž‘ì—…ì„...\n",
      "  í¬ì†Œì„±: 99.99% (non-zero: 4/32,000)\n",
      "  L1 Norm: 2.41\n",
      "  ì¸ì½”ë”© ì‹œê°„: 12.22ms\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. AI              (1.3888)\n",
      "     2. ##P             (0.5188)\n",
      "     3. ##T             (0.4897)\n",
      "     4. ##G             (0.0174)\n",
      "\n",
      "[ë¬¸ì„œ 5]\n",
      "í…ìŠ¤íŠ¸: ë²¡í„° ê²€ìƒ‰ì€ ìž„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤. Dense retrieval ë°©ì‹ì´ë¼ê³ ë„ ...\n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 5/32,000)\n",
      "  L1 Norm: 2.66\n",
      "  ì¸ì½”ë”© ì‹œê°„: 12.18ms\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ##í„°             (0.8604)\n",
      "     2. ê²€ìƒ‰              (0.6825)\n",
      "     3. ë²¡               (0.6392)\n",
      "     4. íƒìƒ‰              (0.4006)\n",
      "     5. ##ìƒ‰             (0.0783)\n",
      "\n",
      "í‰ê·  ì¸ì½”ë”© ì‹œê°„: 38.40ms\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë¬¸ì„œ\n",
    "test_documents = [\n",
    "    \"OpenSearchëŠ” ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ìž…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.\",\n",
    "    \"í•œêµ­ì–´ ìžì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤. KoNLPyì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.\",\n",
    "    \"ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ ë°ì´í„°ì™€ ì»´í“¨íŒ… ìžì›ì´ í•„ìš”í•©ë‹ˆë‹¤. GPUë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµ ì†ë„ê°€ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤.\",\n",
    "    \"ChatGPTëŠ” OpenAIê°€ ê°œë°œí•œ ëŒ€í™”í˜• AI ëª¨ë¸ìž…ë‹ˆë‹¤. GPT ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©° ë‹¤ì–‘í•œ ìž‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ë²¡í„° ê²€ìƒ‰ì€ ìž„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤. Dense retrieval ë°©ì‹ì´ë¼ê³ ë„ ë¶ˆë¦½ë‹ˆë‹¤.\",\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“„ ë¬¸ì„œ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ (Model Inference)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "doc_vectors = []\n",
    "encoding_times = []\n",
    "\n",
    "for i, doc in enumerate(test_documents, 1):\n",
    "    print(f\"\\n[ë¬¸ì„œ {i}]\")\n",
    "    print(f\"í…ìŠ¤íŠ¸: {doc[:60]}...\")\n",
    "    \n",
    "    # ì¸ì½”ë”© ì‹œê°„ ì¸¡ì •\n",
    "    start_time = time.time()\n",
    "    sparse_vec = encode_document(doc, doc_encoder, tokenizer, device)\n",
    "    elapsed = time.time() - start_time\n",
    "    encoding_times.append(elapsed)\n",
    "    \n",
    "    doc_vectors.append(sparse_vec)\n",
    "    \n",
    "    # í†µê³„\n",
    "    non_zero = np.count_nonzero(sparse_vec)\n",
    "    sparsity = (1 - non_zero / len(sparse_vec)) * 100\n",
    "    l1_norm = np.sum(np.abs(sparse_vec))\n",
    "    \n",
    "    print(f\"  í¬ì†Œì„±: {sparsity:.2f}% (non-zero: {non_zero:,}/{len(sparse_vec):,})\")\n",
    "    print(f\"  L1 Norm: {l1_norm:.2f}\")\n",
    "    print(f\"  ì¸ì½”ë”© ì‹œê°„: {elapsed*1000:.2f}ms\")\n",
    "    \n",
    "    # ìƒìœ„ í† í°\n",
    "    print(f\"  ìƒìœ„ í† í°:\")\n",
    "    top_tokens = get_top_tokens(sparse_vec, tokenizer, top_k=10)\n",
    "    for j, (token, value) in enumerate(top_tokens, 1):\n",
    "        print(f\"    {j:2d}. {token:15s} ({value:.4f})\")\n",
    "\n",
    "print(f\"\\ní‰ê·  ì¸ì½”ë”© ì‹œê°„: {np.mean(encoding_times)*1000:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì¿¼ë¦¬ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ (Inference-Free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ“ ì¿¼ë¦¬ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ (Inference-Free: Tokenizer + IDF Lookup)\n",
      "============================================================\n",
      "\n",
      "[ì¿¼ë¦¬ 1] OpenSearch ë²¡í„° ê²€ìƒ‰\n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 6/32,000)\n",
      "  ì¸ì½”ë”© ì‹œê°„: 0.5574ms âš¡ (ë§¤ìš° ë¹ ë¦„!)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ë²¡               (12.2485)\n",
      "     2. ê²€ìƒ‰              (8.8385)\n",
      "     3. Op              (7.4678)\n",
      "     4. ##S             (6.8928)\n",
      "     5. ##í„°             (6.4828)\n",
      "     6. ##en            (5.7666)\n",
      "\n",
      "[ì¿¼ë¦¬ 2] í•œêµ­ì–´ ìžì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ \n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 5/32,000)\n",
      "  ì¸ì½”ë”© ì‹œê°„: 0.2050ms âš¡ (ë§¤ìš° ë¹ ë¦„!)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. í•œêµ­ì–´             (7.6927)\n",
      "     2. ìžì—°              (6.2214)\n",
      "     3. ì²˜ë¦¬              (6.0865)\n",
      "     4. ê¸°ìˆ               (5.1364)\n",
      "     5. ##ì–´             (3.3754)\n",
      "\n",
      "[ì¿¼ë¦¬ 3] ë”¥ëŸ¬ë‹ GPU í•™ìŠµ\n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 6/32,000)\n",
      "  ì¸ì½”ë”© ì‹œê°„: 0.2315ms âš¡ (ë§¤ìš° ë¹ ë¦„!)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. GP              (13.5708)\n",
      "     2. ##ë‹             (12.3340)\n",
      "     3. ë”¥               (10.4745)\n",
      "     4. í•™ìŠµ              (7.6003)\n",
      "     5. ##ëŸ¬             (6.9786)\n",
      "     6. ##U             (5.9254)\n",
      "\n",
      "[ì¿¼ë¦¬ 4] ChatGPT LLM ëª¨ë¸\n",
      "  í¬ì†Œì„±: 99.97% (non-zero: 9/32,000)\n",
      "  ì¸ì½”ë”© ì‹œê°„: 0.2255ms âš¡ (ë§¤ìš° ë¹ ë¦„!)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ##T             (14.7473)\n",
      "     2. ##G             (11.1942)\n",
      "     3. ##M             (8.9907)\n",
      "     4. ##L             (8.2165)\n",
      "     5. ##P             (7.5446)\n",
      "     6. L               (6.9884)\n",
      "     7. Ch              (6.9079)\n",
      "     8. ##at            (6.8284)\n",
      "     9. ëª¨ë¸              (6.1814)\n",
      "\n",
      "[ì¿¼ë¦¬ 5] ê²€ìƒ‰ ì—”ì§„ ìµœì í™”\n",
      "  í¬ì†Œì„±: 99.99% (non-zero: 3/32,000)\n",
      "  ì¸ì½”ë”© ì‹œê°„: 0.1378ms âš¡ (ë§¤ìš° ë¹ ë¦„!)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ê²€ìƒ‰              (8.8385)\n",
      "     2. ìµœì í™”             (7.7767)\n",
      "     3. ì—”ì§„              (7.1470)\n",
      "\n",
      "[ì¿¼ë¦¬ 6] ì¸ê³µì§€ëŠ¥ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§\n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 5/32,000)\n",
      "  ì¸ì½”ë”© ì‹œê°„: 0.1843ms âš¡ (ë§¤ìš° ë¹ ë¦„!)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ##ë¡¬             (11.3833)\n",
      "     2. í”„               (10.5800)\n",
      "     3. ##í”„íŠ¸            (9.6803)\n",
      "     4. ì—”ì§€ë‹ˆì–´ë§           (8.7288)\n",
      "     5. ì¸ê³µì§€ëŠ¥            (8.5432)\n",
      "\n",
      "í‰ê·  ì¿¼ë¦¬ ì¸ì½”ë”© ì‹œê°„: 0.2569ms\n",
      "\n",
      "ðŸ’¡ ë¬¸ì„œ ì¸ì½”ë”© ëŒ€ë¹„ ì†ë„: 149.4ë°° ë¹ ë¦„!\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬\n",
    "test_queries = [\n",
    "    \"OpenSearch ë²¡í„° ê²€ìƒ‰\",\n",
    "    \"í•œêµ­ì–´ ìžì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ \",\n",
    "    \"ë”¥ëŸ¬ë‹ GPU í•™ìŠµ\",\n",
    "    \"ChatGPT LLM ëª¨ë¸\",\n",
    "    \"ê²€ìƒ‰ ì—”ì§„ ìµœì í™”\",\n",
    "    \"ì¸ê³µì§€ëŠ¥ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§\",\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“ ì¿¼ë¦¬ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ (Inference-Free: Tokenizer + IDF Lookup)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "query_vectors = []\n",
    "query_encoding_times = []\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n[ì¿¼ë¦¬ {i}] {query}\")\n",
    "    \n",
    "    # ì¸ì½”ë”© ì‹œê°„ ì¸¡ì •\n",
    "    start_time = time.time()\n",
    "    sparse_vec = encode_query_inference_free(query, tokenizer, idf_dict)\n",
    "    elapsed = time.time() - start_time\n",
    "    query_encoding_times.append(elapsed)\n",
    "    \n",
    "    query_vectors.append(sparse_vec)\n",
    "    \n",
    "    # í†µê³„\n",
    "    non_zero = np.count_nonzero(sparse_vec)\n",
    "    sparsity = (1 - non_zero / len(sparse_vec)) * 100\n",
    "    \n",
    "    print(f\"  í¬ì†Œì„±: {sparsity:.2f}% (non-zero: {non_zero}/{len(sparse_vec):,})\")\n",
    "    print(f\"  ì¸ì½”ë”© ì‹œê°„: {elapsed*1000:.4f}ms âš¡ (ë§¤ìš° ë¹ ë¦„!)\")\n",
    "    \n",
    "    # ìƒìœ„ í† í°\n",
    "    print(f\"  ìƒìœ„ í† í°:\")\n",
    "    top_tokens = get_top_tokens(sparse_vec, tokenizer, top_k=10)\n",
    "    for j, (token, value) in enumerate(top_tokens, 1):\n",
    "        print(f\"    {j:2d}. {token:15s} ({value:.4f})\")\n",
    "\n",
    "print(f\"\\ní‰ê·  ì¿¼ë¦¬ ì¸ì½”ë”© ì‹œê°„: {np.mean(query_encoding_times)*1000:.4f}ms\")\n",
    "print(f\"\\nðŸ’¡ ë¬¸ì„œ ì¸ì½”ë”© ëŒ€ë¹„ ì†ë„: {np.mean(encoding_times)/np.mean(query_encoding_times):.1f}ë°° ë¹ ë¦„!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (ìœ ì‚¬ë„ ê³„ì‚°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (Query-Document Similarity)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ì¿¼ë¦¬: OpenSearch ë²¡í„° ê²€ìƒ‰\n",
      "============================================================\n",
      "\n",
      "ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 3ê°œ):\n",
      "\n",
      "  [1ìœ„] ìœ ì‚¬ë„: 19.4390\n",
      "       ë¬¸ì„œ: ë²¡í„° ê²€ìƒ‰ì€ ìž„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤. Dense retrieval ë°©ì‹ì´ë¼ê³ ë„ ë¶ˆë¦½ë‹ˆë‹¤....\n",
      "\n",
      "  [2ìœ„] ìœ ì‚¬ë„: 5.2380\n",
      "       ë¬¸ì„œ: OpenSearchëŠ” ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ìž…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤....\n",
      "\n",
      "  [3ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "       ë¬¸ì„œ: í•œêµ­ì–´ ìžì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤. KoNLPyì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤....\n",
      "\n",
      "============================================================\n",
      "ì¿¼ë¦¬: í•œêµ­ì–´ ìžì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ \n",
      "============================================================\n",
      "\n",
      "ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 3ê°œ):\n",
      "\n",
      "  [1ìœ„] ìœ ì‚¬ë„: 6.5684\n",
      "       ë¬¸ì„œ: í•œêµ­ì–´ ìžì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤. KoNLPyì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤....\n",
      "\n",
      "  [2ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "       ë¬¸ì„œ: OpenSearchëŠ” ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ìž…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤....\n",
      "\n",
      "  [3ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "       ë¬¸ì„œ: ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ ë°ì´í„°ì™€ ì»´í“¨íŒ… ìžì›ì´ í•„ìš”í•©ë‹ˆë‹¤. GPUë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµ ì†ë„ê°€ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤....\n",
      "\n",
      "============================================================\n",
      "ì¿¼ë¦¬: ë”¥ëŸ¬ë‹ GPU í•™ìŠµ\n",
      "============================================================\n",
      "\n",
      "ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 3ê°œ):\n",
      "\n",
      "  [1ìœ„] ìœ ì‚¬ë„: 35.5924\n",
      "       ë¬¸ì„œ: ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ ë°ì´í„°ì™€ ì»´í“¨íŒ… ìžì›ì´ í•„ìš”í•©ë‹ˆë‹¤. GPUë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµ ì†ë„ê°€ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤....\n",
      "\n",
      "  [2ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "       ë¬¸ì„œ: OpenSearchëŠ” ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ìž…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤....\n",
      "\n",
      "  [3ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "       ë¬¸ì„œ: í•œêµ­ì–´ ìžì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤. KoNLPyì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤....\n",
      "\n",
      "============================================================\n",
      "ì¿¼ë¦¬: ChatGPT LLM ëª¨ë¸\n",
      "============================================================\n",
      "\n",
      "ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 3ê°œ):\n",
      "\n",
      "  [1ìœ„] ìœ ì‚¬ë„: 11.3314\n",
      "       ë¬¸ì„œ: ChatGPTëŠ” OpenAIê°€ ê°œë°œí•œ ëŒ€í™”í˜• AI ëª¨ë¸ìž…ë‹ˆë‹¤. GPT ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©° ë‹¤ì–‘í•œ ìž‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤....\n",
      "\n",
      "  [2ìœ„] ìœ ì‚¬ë„: 0.4645\n",
      "       ë¬¸ì„œ: ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ ë°ì´í„°ì™€ ì»´í“¨íŒ… ìžì›ì´ í•„ìš”í•©ë‹ˆë‹¤. GPUë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµ ì†ë„ê°€ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤....\n",
      "\n",
      "  [3ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "       ë¬¸ì„œ: OpenSearchëŠ” ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ìž…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤....\n",
      "\n",
      "============================================================\n",
      "ì¿¼ë¦¬: ê²€ìƒ‰ ì—”ì§„ ìµœì í™”\n",
      "============================================================\n",
      "\n",
      "ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 3ê°œ):\n",
      "\n",
      "  [1ìœ„] ìœ ì‚¬ë„: 6.0319\n",
      "       ë¬¸ì„œ: ë²¡í„° ê²€ìƒ‰ì€ ìž„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤. Dense retrieval ë°©ì‹ì´ë¼ê³ ë„ ë¶ˆë¦½ë‹ˆë‹¤....\n",
      "\n",
      "  [2ìœ„] ìœ ì‚¬ë„: 3.5175\n",
      "       ë¬¸ì„œ: OpenSearchëŠ” ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ìž…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤....\n",
      "\n",
      "  [3ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "       ë¬¸ì„œ: í•œêµ­ì–´ ìžì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤. KoNLPyì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤....\n",
      "\n",
      "============================================================\n",
      "ì¿¼ë¦¬: ì¸ê³µì§€ëŠ¥ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§\n",
      "============================================================\n",
      "\n",
      "ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 3ê°œ):\n",
      "\n",
      "  [1ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "       ë¬¸ì„œ: OpenSearchëŠ” ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ìž…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤....\n",
      "\n",
      "  [2ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "       ë¬¸ì„œ: í•œêµ­ì–´ ìžì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤. KoNLPyì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤....\n",
      "\n",
      "  [3ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "       ë¬¸ì„œ: ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ ë°ì´í„°ì™€ ì»´í“¨íŒ… ìžì›ì´ í•„ìš”í•©ë‹ˆë‹¤. GPUë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµ ì†ë„ê°€ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤....\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ðŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (Query-Document Similarity)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ê° ì¿¼ë¦¬ì— ëŒ€í•´ ê°€ìž¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ì¿¼ë¦¬: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    query_vec = query_vectors[i]\n",
    "    \n",
    "    # ëª¨ë“  ë¬¸ì„œì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    similarities = []\n",
    "    for j, doc_vec in enumerate(doc_vectors):\n",
    "        sim = compute_similarity(query_vec, doc_vec)\n",
    "        similarities.append((j, sim))\n",
    "    \n",
    "    # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # ìƒìœ„ 3ê°œ ë¬¸ì„œ ì¶œë ¥\n",
    "    print(\"\\nê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 3ê°œ):\")\n",
    "    for rank, (doc_idx, score) in enumerate(similarities[:3], 1):\n",
    "        print(f\"\\n  [{rank}ìœ„] ìœ ì‚¬ë„: {score:.4f}\")\n",
    "        print(f\"       ë¬¸ì„œ: {test_documents[doc_idx][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "âš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬\n",
      "============================================================\n",
      "\n",
      "í…ŒìŠ¤íŠ¸: 100ê°œ ì¿¼ë¦¬ ì¸ì½”ë”© (Inference-Free)\n",
      "ì´ ì‹œê°„: 0.0532s\n",
      "í‰ê·  ì‹œê°„: 0.5324ms per query\n",
      "ì²˜ë¦¬ëŸ‰: 1878.35 queries/sec\n",
      "\n",
      "í…ŒìŠ¤íŠ¸: 10ê°œ ë¬¸ì„œ ì¸ì½”ë”© (Model Inference)\n",
      "ì´ ì‹œê°„: 0.1374s\n",
      "í‰ê·  ì‹œê°„: 13.74ms per document\n",
      "ì²˜ë¦¬ëŸ‰: 72.76 documents/sec\n",
      "\n",
      "============================================================\n",
      "ðŸ’¡ ìš”ì•½\n",
      "============================================================\n",
      "âœ“ ì¿¼ë¦¬ ì¸ì½”ë”©: ë§¤ìš° ë¹ ë¦„ (IDF lookupë§Œ ì‚¬ìš©)\n",
      "âœ“ ë¬¸ì„œ ì¸ì½”ë”©: ëŠë¦¼ (BERT ëª¨ë¸ ì‚¬ìš©)\n",
      "âœ“ ì¿¼ë¦¬ëŠ” ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥, ë¬¸ì„œëŠ” ì‚¬ì „ ì¸ë±ì‹± í•„ìš”\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"âš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ëŒ€ëŸ‰ ì¿¼ë¦¬ ì²˜ë¦¬ ì†ë„ í…ŒìŠ¤íŠ¸\n",
    "num_queries = 100\n",
    "test_query_text = \"OpenSearch ë²¡í„° ê²€ìƒ‰ ìµœì í™”\"\n",
    "\n",
    "print(f\"\\ní…ŒìŠ¤íŠ¸: {num_queries}ê°œ ì¿¼ë¦¬ ì¸ì½”ë”© (Inference-Free)\")\n",
    "start_time = time.time()\n",
    "for _ in range(num_queries):\n",
    "    _ = encode_query_inference_free(test_query_text, tokenizer, idf_dict)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"ì´ ì‹œê°„: {elapsed:.4f}s\")\n",
    "print(f\"í‰ê·  ì‹œê°„: {elapsed/num_queries*1000:.4f}ms per query\")\n",
    "print(f\"ì²˜ë¦¬ëŸ‰: {num_queries/elapsed:.2f} queries/sec\")\n",
    "\n",
    "# ë¬¸ì„œ ì¸ì½”ë”© ì†ë„ í…ŒìŠ¤íŠ¸\n",
    "num_docs = 10\n",
    "test_doc_text = \"OpenSearchëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ì—”ì§„ìž…ë‹ˆë‹¤.\"\n",
    "\n",
    "print(f\"\\ní…ŒìŠ¤íŠ¸: {num_docs}ê°œ ë¬¸ì„œ ì¸ì½”ë”© (Model Inference)\")\n",
    "start_time = time.time()\n",
    "for _ in range(num_docs):\n",
    "    _ = encode_document(test_doc_text, doc_encoder, tokenizer, device)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"ì´ ì‹œê°„: {elapsed:.4f}s\")\n",
    "print(f\"í‰ê·  ì‹œê°„: {elapsed/num_docs*1000:.2f}ms per document\")\n",
    "print(f\"ì²˜ë¦¬ëŸ‰: {num_docs/elapsed:.2f} documents/sec\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ’¡ ìš”ì•½\")\n",
    "print(\"=\"*60)\n",
    "print(\"âœ“ ì¿¼ë¦¬ ì¸ì½”ë”©: ë§¤ìš° ë¹ ë¦„ (IDF lookupë§Œ ì‚¬ìš©)\")\n",
    "print(\"âœ“ ë¬¸ì„œ ì¸ì½”ë”©: ëŠë¦¼ (BERT ëª¨ë¸ ì‚¬ìš©)\")\n",
    "print(\"âœ“ ì¿¼ë¦¬ëŠ” ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥, ë¬¸ì„œëŠ” ì‚¬ì „ ì¸ë±ì‹± í•„ìš”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ§ª ì»¤ìŠ¤í…€ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
      "============================================================\n",
      "\n",
      "ì¿¼ë¦¬: ì¸ê³µì§€ëŠ¥ LLM ëª¨ë¸ ìµœì í™” ë°©ë²•\n",
      "\n",
      "ê²€ìƒ‰ ê²°ê³¼:\n",
      "\n",
      "[1ìœ„] ìœ ì‚¬ë„: 12.0368\n",
      "     ë¬¸ì„œ: LLM ëª¨ë¸ ìµœì í™”ë¥¼ ìœ„í•´ì„œëŠ” ì–‘ìží™”, í”„ë£¨ë‹, ì§€ì‹ ì¦ë¥˜ ë“±ì˜ ê¸°ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "[2ìœ„] ìœ ì‚¬ë„: 2.3374\n",
      "     ë¬¸ì„œ: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì¶”ë¡  ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ KV ìºì‹œì™€ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í™œìš©í•©ë‹ˆë‹¤.\n",
      "\n",
      "[3ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "     ë¬¸ì„œ: í•œêµ­ì–´ ìŒì‹ ì¤‘ì—ì„œ ê¹€ì¹˜ì°Œê°œëŠ” ê°€ìž¥ ì¸ê¸° ìžˆëŠ” ë©”ë‰´ ì¤‘ í•˜ë‚˜ìž…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ìš©ìž ì •ì˜ ì¿¼ë¦¬ì™€ ë¬¸ì„œë¡œ í…ŒìŠ¤íŠ¸\n",
    "custom_query = \"ì¸ê³µì§€ëŠ¥ LLM ëª¨ë¸ ìµœì í™” ë°©ë²•\"  # ì—¬ê¸°ì— í…ŒìŠ¤íŠ¸í•  ì¿¼ë¦¬ ìž…ë ¥\n",
    "\n",
    "custom_documents = [\n",
    "    \"LLM ëª¨ë¸ ìµœì í™”ë¥¼ ìœ„í•´ì„œëŠ” ì–‘ìží™”, í”„ë£¨ë‹, ì§€ì‹ ì¦ë¥˜ ë“±ì˜ ê¸°ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\",\n",
    "    \"í•œêµ­ì–´ ìŒì‹ ì¤‘ì—ì„œ ê¹€ì¹˜ì°Œê°œëŠ” ê°€ìž¥ ì¸ê¸° ìžˆëŠ” ë©”ë‰´ ì¤‘ í•˜ë‚˜ìž…ë‹ˆë‹¤.\",\n",
    "    \"ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì¶”ë¡  ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ KV ìºì‹œì™€ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í™œìš©í•©ë‹ˆë‹¤.\",\n",
    "]  # ì—¬ê¸°ì— í…ŒìŠ¤íŠ¸í•  ë¬¸ì„œ ìž…ë ¥\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ§ª ì»¤ìŠ¤í…€ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nì¿¼ë¦¬: {custom_query}\")\n",
    "\n",
    "# ì¿¼ë¦¬ ì¸ì½”ë”©\n",
    "query_vec = encode_query_inference_free(custom_query, tokenizer, idf_dict)\n",
    "\n",
    "# ë¬¸ì„œ ì¸ì½”ë”© ë° ìœ ì‚¬ë„ ê³„ì‚°\n",
    "results = []\n",
    "for i, doc in enumerate(custom_documents):\n",
    "    doc_vec = encode_document(doc, doc_encoder, tokenizer, device)\n",
    "    similarity = compute_similarity(query_vec, doc_vec)\n",
    "    results.append((i, similarity, doc))\n",
    "\n",
    "# ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nê²€ìƒ‰ ê²°ê³¼:\")\n",
    "for rank, (doc_idx, score, doc_text) in enumerate(results, 1):\n",
    "    print(f\"\\n[{rank}ìœ„] ìœ ì‚¬ë„: {score:.4f}\")\n",
    "    print(f\"     ë¬¸ì„œ: {doc_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. í¬ì†Œì„±(Sparsity) ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ“Š í¬ì†Œì„± ë¶„ì„\n",
      "============================================================\n",
      "\n",
      "ë¬¸ì„œ ë²¡í„° í¬ì†Œì„±:\n",
      "  í‰ê· : 99.98%\n",
      "  ìµœì†Œ: 99.97%\n",
      "  ìµœëŒ€: 99.99%\n",
      "  í‘œì¤€íŽ¸ì°¨: 0.01%\n",
      "\n",
      "ì¿¼ë¦¬ ë²¡í„° í¬ì†Œì„±:\n",
      "  í‰ê· : 99.98%\n",
      "  ìµœì†Œ: 99.97%\n",
      "  ìµœëŒ€: 99.99%\n",
      "  í‘œì¤€íŽ¸ì°¨: 0.01%\n",
      "\n",
      "ðŸ’¡ í¬ì†Œì„±ì´ ë†’ì„ìˆ˜ë¡ (100%ì— ê°€ê¹Œìš¸ìˆ˜ë¡):\n",
      "   - ë©”ëª¨ë¦¬ íš¨ìœ¨ì \n",
      "   - ê²€ìƒ‰ ì†ë„ ë¹ ë¦„\n",
      "   - ì €ìž¥ ê³µê°„ ì ìŒ\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ðŸ“Š í¬ì†Œì„± ë¶„ì„\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ë¬¸ì„œ ë²¡í„° í¬ì†Œì„±\n",
    "doc_sparsities = []\n",
    "for vec in doc_vectors:\n",
    "    non_zero = np.count_nonzero(vec)\n",
    "    sparsity = (1 - non_zero / len(vec)) * 100\n",
    "    doc_sparsities.append(sparsity)\n",
    "\n",
    "print(f\"\\në¬¸ì„œ ë²¡í„° í¬ì†Œì„±:\")\n",
    "print(f\"  í‰ê· : {np.mean(doc_sparsities):.2f}%\")\n",
    "print(f\"  ìµœì†Œ: {np.min(doc_sparsities):.2f}%\")\n",
    "print(f\"  ìµœëŒ€: {np.max(doc_sparsities):.2f}%\")\n",
    "print(f\"  í‘œì¤€íŽ¸ì°¨: {np.std(doc_sparsities):.2f}%\")\n",
    "\n",
    "# ì¿¼ë¦¬ ë²¡í„° í¬ì†Œì„±\n",
    "query_sparsities = []\n",
    "for vec in query_vectors:\n",
    "    non_zero = np.count_nonzero(vec)\n",
    "    sparsity = (1 - non_zero / len(vec)) * 100\n",
    "    query_sparsities.append(sparsity)\n",
    "\n",
    "print(f\"\\nì¿¼ë¦¬ ë²¡í„° í¬ì†Œì„±:\")\n",
    "print(f\"  í‰ê· : {np.mean(query_sparsities):.2f}%\")\n",
    "print(f\"  ìµœì†Œ: {np.min(query_sparsities):.2f}%\")\n",
    "print(f\"  ìµœëŒ€: {np.max(query_sparsities):.2f}%\")\n",
    "print(f\"  í‘œì¤€íŽ¸ì°¨: {np.std(query_sparsities):.2f}%\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ í¬ì†Œì„±ì´ ë†’ì„ìˆ˜ë¡ (100%ì— ê°€ê¹Œìš¸ìˆ˜ë¡):\")\n",
    "print(f\"   - ë©”ëª¨ë¦¬ íš¨ìœ¨ì \")\n",
    "print(f\"   - ê²€ìƒ‰ ì†ë„ ë¹ ë¦„\")\n",
    "print(f\"   - ì €ìž¥ ê³µê°„ ì ìŒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ê²°ê³¼ ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"âœ… OpenSearch Korean Neural Sparse Model - Inference Test ì™„ë£Œ\")\nprint(\"=\"*60)\n\nprint(\"\\nðŸ“Š í…ŒìŠ¤íŠ¸ ìš”ì•½:\")\nprint(f\"  â€¢ ë¬¸ì„œ ì¸ì½”ë”© í‰ê·  ì‹œê°„: {np.mean(encoding_times)*1000:.2f}ms\")\nprint(f\"  â€¢ ì¿¼ë¦¬ ì¸ì½”ë”© í‰ê·  ì‹œê°„: {np.mean(query_encoding_times)*1000:.4f}ms\")\nprint(f\"  â€¢ ì†ë„ ì°¨ì´: {np.mean(encoding_times)/np.mean(query_encoding_times):.1f}ë°°\")\nprint(f\"  â€¢ ë¬¸ì„œ ë²¡í„° í‰ê·  í¬ì†Œì„±: {np.mean(doc_sparsities):.2f}%\")\nprint(f\"  â€¢ ì¿¼ë¦¬ ë²¡í„° í‰ê·  í¬ì†Œì„±: {np.mean(query_sparsities):.2f}%\")\n\nprint(\"\\nðŸŽ¯ í•µì‹¬ íŠ¹ì§•:\")\nprint(\"  âœ“ Inference-Free ì¿¼ë¦¬ ì¸ì½”ë”© (ë§¤ìš° ë¹ ë¦„)\")\nprint(\"  âœ“ ë†’ì€ í¬ì†Œì„± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\")\nprint(\"  âœ“ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ê°€ëŠ¥\")\nprint(\"  âœ“ í•œêµ­ì–´ ìµœì í™”\")\n\nprint(\"\\nðŸ’¡ OpenSearch í†µí•©:\")\nprint(\"  1. ë¬¸ì„œëŠ” ì‚¬ì „ì— ì¸ì½”ë”©í•˜ì—¬ ì¸ë±ì‹±\")\nprint(\"  2. ì¿¼ë¦¬ëŠ” ì‹¤ì‹œê°„ìœ¼ë¡œ IDF lookup\")\nprint(\"  3. rank_features í•„ë“œë¡œ ì €ìž¥\")\nprint(\"  4. neural_sparse ì¿¼ë¦¬ë¡œ ê²€ìƒ‰\")\n\nprint(\"\\n\" + \"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "source": "## 12. ðŸš€ Transformers ìŠ¤íƒ€ì¼ ëž˜í¼ í´ëž˜ìŠ¤ (ê°„íŽ¸í•œ ì‚¬ìš©)\n\nHugging Face Transformersì²˜ëŸ¼ ê°„ë‹¨í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìžˆëŠ” í†µí•© í´ëž˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class NeuralSparseSearchModel:\n    \"\"\"\n    OpenSearch Neural Sparse ê²€ìƒ‰ ëª¨ë¸ - Transformers ìŠ¤íƒ€ì¼ ì¸í„°íŽ˜ì´ìŠ¤\n\n    ì‚¬ìš© ì˜ˆì œ:\n        model = NeuralSparseSearchModel.from_pretrained(\"./opensearch-korean-neural-sparse-v1\")\n        results = model.search(query=\"ê²€ìƒ‰ ì¿¼ë¦¬\", documents=[\"ë¬¸ì„œ1\", \"ë¬¸ì„œ2\", ...])\n    \"\"\"\n\n    def __init__(self, doc_encoder, tokenizer, idf_dict, device='cuda', max_length=128):\n        self.doc_encoder = doc_encoder\n        self.tokenizer = tokenizer\n        self.idf_dict = idf_dict\n        self.device = device\n        self.max_length = max_length\n\n        # Document ë²¡í„° ìºì‹œ (ì„ íƒì )\n        self._doc_cache = {}\n\n    @classmethod\n    def from_pretrained(cls, model_path, device=None):\n        \"\"\"\n        ì €ìž¥ëœ ëª¨ë¸ì—ì„œ ë¡œë“œ (Transformers ìŠ¤íƒ€ì¼)\n\n        Args:\n            model_path: ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ\n            device: ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cpu')\n\n        Returns:\n            NeuralSparseSearchModel ì¸ìŠ¤í„´ìŠ¤\n        \"\"\"\n        import os\n\n        if device is None:\n            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        device = torch.device(device)\n\n        # Config ë¡œë“œ\n        with open(os.path.join(model_path, \"config.json\"), 'r', encoding='utf-8') as f:\n            config = json.load(f)\n\n        # Tokenizer ë¡œë“œ\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n        # IDF ë”•ì…”ë„ˆë¦¬ ë¡œë“œ\n        with open(os.path.join(model_path, \"idf.json\"), 'r', encoding='utf-8') as f:\n            idf_dict = json.load(f)\n\n        # Document Encoder ë¡œë“œ\n        base_model = config['base_model']\n        doc_encoder = OpenSearchDocEncoder(base_model)\n        state_dict = torch.load(\n            os.path.join(model_path, \"pytorch_model.bin\"),\n            map_location=device\n        )\n        doc_encoder.load_state_dict(state_dict)\n        doc_encoder = doc_encoder.to(device)\n        doc_encoder.eval()\n\n        max_length = config.get('max_seq_length', 128)\n\n        return cls(doc_encoder, tokenizer, idf_dict, device, max_length)\n\n    def encode_query(self, query):\n        \"\"\"\n        ì¿¼ë¦¬ë¥¼ sparse vectorë¡œ ì¸ì½”ë”© (Inference-Free!)\n\n        Args:\n            query: ì¿¼ë¦¬ í…ìŠ¤íŠ¸ (str)\n\n        Returns:\n            sparse_vector: (vocab_size,) numpy array\n        \"\"\"\n        # í† í°í™”\n        tokens = self.tokenizer.encode(\n            query,\n            add_special_tokens=False,\n            max_length=self.max_length,\n            truncation=True\n        )\n\n        # IDF lookup\n        sparse_vec = np.zeros(self.tokenizer.vocab_size)\n        for token_id in tokens:\n            token_str = self.tokenizer.decode([token_id])\n            if token_str in self.idf_dict:\n                sparse_vec[token_id] = self.idf_dict[token_str]\n\n        return sparse_vec\n\n    def encode_documents(self, documents, batch_size=8, show_progress=True):\n        \"\"\"\n        ë¬¸ì„œë“¤ì„ ë°°ì¹˜ë¡œ ì¸ì½”ë”©\n\n        Args:\n            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n            batch_size: ë°°ì¹˜ í¬ê¸°\n            show_progress: ì§„í–‰ë¥  í‘œì‹œ\n\n        Returns:\n            List of sparse vectors\n        \"\"\"\n        from tqdm.auto import tqdm\n\n        self.doc_encoder.eval()\n        doc_vectors = []\n\n        iterator = range(0, len(documents), batch_size)\n        if show_progress:\n            iterator = tqdm(iterator, desc=\"Encoding documents\")\n\n        for i in iterator:\n            batch = documents[i:i+batch_size]\n\n            # ë°°ì¹˜ í† í°í™”\n            encoded = self.tokenizer(\n                batch,\n                max_length=self.max_length,\n                padding='max_length',\n                truncation=True,\n                return_tensors='pt'\n            )\n\n            input_ids = encoded['input_ids'].to(self.device)\n            attention_mask = encoded['attention_mask'].to(self.device)\n\n            # Inference\n            with torch.no_grad():\n                sparse_vecs = self.doc_encoder(input_ids, attention_mask)\n\n            doc_vectors.extend(sparse_vecs.cpu().numpy())\n\n        return doc_vectors\n\n    def search(self, query, documents, top_k=5, return_scores=True, batch_size=8):\n        \"\"\"\n        ê²€ìƒ‰ ìˆ˜í–‰\n\n        Args:\n            query: ê²€ìƒ‰ ì¿¼ë¦¬ (str)\n            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë¬¸ì„œ ë²¡í„° ë¦¬ìŠ¤íŠ¸\n            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜\n            return_scores: ì ìˆ˜ í¬í•¨ ì—¬ë¶€\n            batch_size: ë¬¸ì„œ ì¸ì½”ë”© ë°°ì¹˜ í¬ê¸°\n\n        Returns:\n            DataFrame with columns: rank, document, score (if return_scores=True)\n        \"\"\"\n        # ì¿¼ë¦¬ ì¸ì½”ë”©\n        query_vec = self.encode_query(query)\n\n        # ë¬¸ì„œ ì¸ì½”ë”© (ë¬¸ìžì—´ì¸ ê²½ìš°)\n        if len(documents) > 0 and isinstance(documents[0], str):\n            doc_vectors = self.encode_documents(documents, batch_size=batch_size)\n        else:\n            doc_vectors = documents\n\n        # ìœ ì‚¬ë„ ê³„ì‚°\n        similarities = []\n        for i, doc_vec in enumerate(doc_vectors):\n            sim = np.dot(query_vec, doc_vec)\n            similarities.append((i, sim))\n\n        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬\n        similarities.sort(key=lambda x: x[1], reverse=True)\n\n        # ìƒìœ„ kê°œ ê²°ê³¼\n        top_results = similarities[:top_k]\n\n        # DataFrame ìƒì„±\n        results = []\n        for rank, (doc_idx, score) in enumerate(top_results, 1):\n            if isinstance(documents[0], str):\n                result = {\n                    'rank': rank,\n                    'document_id': doc_idx,\n                    'document': documents[doc_idx],\n                }\n            else:\n                result = {\n                    'rank': rank,\n                    'document_id': doc_idx,\n                }\n\n            if return_scores:\n                result['score'] = score\n\n            results.append(result)\n\n        return pd.DataFrame(results)\n\n    def get_top_tokens(self, sparse_vec, top_k=10):\n        \"\"\"\n        Sparse vectorì—ì„œ ìƒìœ„ í† í° ì¶”ì¶œ\n\n        Args:\n            sparse_vec: (vocab_size,) numpy array\n            top_k: ìƒìœ„ kê°œ\n\n        Returns:\n            DataFrame with columns: rank, token, score\n        \"\"\"\n        top_indices = np.argsort(sparse_vec)[-top_k:][::-1]\n        top_values = sparse_vec[top_indices]\n\n        results = []\n        for rank, (idx, val) in enumerate(zip(top_indices, top_values), 1):\n            if val > 0:\n                token = self.tokenizer.decode([idx])\n                results.append({\n                    'rank': rank,\n                    'token': token,\n                    'score': val\n                })\n\n        return pd.DataFrame(results)\n\n    def compute_sparsity(self, sparse_vec):\n        \"\"\"\n        í¬ì†Œì„± ê³„ì‚°\n\n        Args:\n            sparse_vec: (vocab_size,) numpy array\n\n        Returns:\n            sparsity percentage (float)\n        \"\"\"\n        non_zero = np.count_nonzero(sparse_vec)\n        sparsity = (1 - non_zero / len(sparse_vec)) * 100\n        return sparsity\n\n\nprint(\"âœ“ NeuralSparseSearchModel í´ëž˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 13. ëž˜í¼ í´ëž˜ìŠ¤ ì‚¬ìš© ì˜ˆì œ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ëª¨ë¸ ë¡œë“œ (Transformers ìŠ¤íƒ€ì¼!)\nmodel = NeuralSparseSearchModel.from_pretrained(\n    MODEL_DIR,\n    device=device\n)\n\nprint(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\nprint(f\"   ë””ë°”ì´ìŠ¤: {model.device}\")\nprint(f\"   Vocab size: {model.tokenizer.vocab_size:,}\")\nprint(f\"   Max length: {model.max_length}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 13.1. ê°„ë‹¨í•œ ê²€ìƒ‰ ì˜ˆì œ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ê²€ìƒ‰í•  ë¬¸ì„œ ì¤€ë¹„\nsearch_documents = [\n    \"OpenSearchëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ìž…ë‹ˆë‹¤. Neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.\",\n    \"í•œêµ­ì–´ ìžì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ ì´ ê¸‰ì†ë„ë¡œ ë°œì „í•˜ê³  ìžˆìŠµë‹ˆë‹¤.\",\n    \"ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì—ëŠ” GPUê°€ í•„ìˆ˜ì ìž…ë‹ˆë‹¤.\",\n    \"ChatGPTëŠ” ëŒ€í™”í˜• AI ëª¨ë¸ìž…ë‹ˆë‹¤.\",\n    \"ë²¡í„° ê²€ìƒ‰ì€ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.\",\n    \"Pythonì€ ë°ì´í„° ê³¼í•™ ë¶„ì•¼ì—ì„œ ê°€ìž¥ ì¸ê¸° ìžˆëŠ” ì–¸ì–´ìž…ë‹ˆë‹¤.\",\n    \"Transformer ì•„í‚¤í…ì²˜ëŠ” NLPì˜ í˜ëª…ì„ ì¼ìœ¼ì¼°ìŠµë‹ˆë‹¤.\",\n    \"ê²€ìƒ‰ ì—”ì§„ ìµœì í™”ëŠ” ì›¹ì‚¬ì´íŠ¸ íŠ¸ëž˜í”½ ì¦ëŒ€ì— ì¤‘ìš”í•©ë‹ˆë‹¤.\",\n]\n\n# ê²€ìƒ‰ ìˆ˜í–‰\nquery = \"OpenSearch ê²€ìƒ‰ ì—”ì§„\"\nresults = model.search(query, search_documents, top_k=3)\n\nprint(f\"\\nì¿¼ë¦¬: '{query}'\\n\")\nprint(results.to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 13.2. ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ì—¬ëŸ¬ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰\nqueries = [\n    \"ë”¥ëŸ¬ë‹ GPU\",\n    \"ìžì—°ì–´ ì²˜ë¦¬\",\n    \"ChatGPT AI\",\n]\n\nprint(\"=\"*80)\nprint(\"ðŸ” ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰ ê²°ê³¼\")\nprint(\"=\"*80)\n\nfor query in queries:\n    print(f\"\\nì¿¼ë¦¬: '{query}'\")\n    print(\"-\" * 80)\n\n    results = model.search(query, search_documents, top_k=3)\n    print(results.to_string(index=False))\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 13.3. ì¿¼ë¦¬/ë¬¸ì„œ ë¶„ì„"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ì¿¼ë¦¬ ë¶„ì„\nquery = \"OpenSearch ë²¡í„° ê²€ìƒ‰ ìµœì í™”\"\nquery_vec = model.encode_query(query)\n\nprint(f\"ì¿¼ë¦¬: '{query}'\\n\")\nprint(f\"í¬ì†Œì„±: {model.compute_sparsity(query_vec):.2f}%\")\nprint(f\"\\nìƒìœ„ í† í°:\")\nprint(model.get_top_tokens(query_vec, top_k=10).to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 13.4. ë°°ì¹˜ ë¬¸ì„œ ì¸ì½”ë”©"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ëŒ€ëŸ‰ ë¬¸ì„œ ì¸ì½”ë”©\nlarge_document_set = [\n    f\"ë¬¸ì„œ {i}: ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ë¬¸ì„œìž…ë‹ˆë‹¤. Neural sparse ê²€ìƒ‰ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\"\n    for i in range(20)\n]\n\nprint(\"ðŸ“„ ë°°ì¹˜ ë¬¸ì„œ ì¸ì½”ë”© ì¤‘...\\n\")\n\n# ë°°ì¹˜ë¡œ ì¸ì½”ë”© (progress bar í‘œì‹œ)\ndoc_vectors = model.encode_documents(large_document_set, batch_size=8)\n\nprint(f\"\\nâœ“ {len(doc_vectors)}ê°œ ë¬¸ì„œ ì¸ì½”ë”© ì™„ë£Œ!\")\nprint(f\"  í‰ê·  í¬ì†Œì„±: {np.mean([model.compute_sparsity(v) for v in doc_vectors]):.2f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 13.5. ì‚¬ì „ ì¸ì½”ë”©ëœ ë¬¸ì„œë¡œ ê²€ìƒ‰"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ë¬¸ì„œë¥¼ ë¯¸ë¦¬ ì¸ì½”ë”©í•´ë‘ê³  ë°˜ë³µì ìœ¼ë¡œ ê²€ìƒ‰\n# (ì‹¤ì œ ì‹œìŠ¤í…œì—ì„œëŠ” ë¬¸ì„œë¥¼ í•œ ë²ˆë§Œ ì¸ì½”ë”©í•˜ê³  ì €ìž¥)\n\nprint(\"1ï¸âƒ£ ë¬¸ì„œ ì‚¬ì „ ì¸ì½”ë”©...\\n\")\ncached_doc_vectors = model.encode_documents(search_documents, batch_size=4)\n\nprint(\"\\n2ï¸âƒ£ ì‚¬ì „ ì¸ì½”ë”©ëœ ë¬¸ì„œë¡œ ë¹ ë¥¸ ê²€ìƒ‰...\\n\")\n\n# ì—¬ëŸ¬ ì¿¼ë¦¬ë¡œ ë¹ ë¥´ê²Œ ê²€ìƒ‰\ntest_queries = [\n    \"ê²€ìƒ‰ ì—”ì§„\",\n    \"AI ëª¨ë¸\",\n    \"ë”¥ëŸ¬ë‹\",\n]\n\nfor query in test_queries:\n    # ë²¡í„°ë§Œ ì „ë‹¬í•˜ë©´ ì¸ì½”ë”© ìƒëžµ (ë§¤ìš° ë¹ ë¦„!)\n    results = model.search(query, cached_doc_vectors, top_k=2)\n\n    print(f\"ì¿¼ë¦¬: '{query}'\")\n    print(\"-\" * 60)\n\n    for _, row in results.iterrows():\n        doc_idx = row['document_id']\n        score = row['score']\n        print(f\"  [{row['rank']}ìœ„] ì ìˆ˜: {score:.4f}\")\n        print(f\"       ë¬¸ì„œ: {search_documents[doc_idx][:60]}...\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 14. ì„±ëŠ¥ ë¹„êµ: ê¸°ì¡´ vs ëž˜í¼ í´ëž˜ìŠ¤"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*80)\nprint(\"âš¡ ì„±ëŠ¥ ë¹„êµ: ê¸°ì¡´ í•¨ìˆ˜ vs ëž˜í¼ í´ëž˜ìŠ¤\")\nprint(\"=\"*80)\n\ntest_docs = [f\"í…ŒìŠ¤íŠ¸ ë¬¸ì„œ {i}\" for i in range(50)]\ntest_query = \"í…ŒìŠ¤íŠ¸\"\n\n# 1. ê¸°ì¡´ ë°©ì‹\nprint(\"\\n1ï¸âƒ£ ê¸°ì¡´ ë°©ì‹ (ê°œë³„ í•¨ìˆ˜ í˜¸ì¶œ)\")\nstart = time.time()\nfor doc in test_docs:\n    _ = encode_document(doc, doc_encoder, tokenizer, device)\nelapsed_old = time.time() - start\nprint(f\"   ì‹œê°„: {elapsed_old:.4f}s\")\n\n# 2. ëž˜í¼ í´ëž˜ìŠ¤ (ë°°ì¹˜ ì²˜ë¦¬)\nprint(\"\\n2ï¸âƒ£ ëž˜í¼ í´ëž˜ìŠ¤ (ë°°ì¹˜ ì²˜ë¦¬)\")\nstart = time.time()\n_ = model.encode_documents(test_docs, batch_size=8, show_progress=False)\nelapsed_new = time.time() - start\nprint(f\"   ì‹œê°„: {elapsed_new:.4f}s\")\n\nprint(f\"\\nðŸ’¡ ì†ë„ í–¥ìƒ: {elapsed_old/elapsed_new:.2f}ë°° ë¹ ë¦„!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 15. ì‹¤ì „ ì˜ˆì œ: í•œêµ­ì–´ ë‰´ìŠ¤ ê²€ìƒ‰"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# í•œêµ­ì–´ ë‰´ìŠ¤ ì˜ˆì œ\nnews_articles = [\n    \"ì‚¼ì„±ì „ìžê°€ ì°¨ì„¸ëŒ€ ë°˜ë„ì²´ ê¸°ìˆ  ê°œë°œì— ì„±ê³µí–ˆë‹¤ê³  ë°œí‘œí–ˆìŠµë‹ˆë‹¤.\",\n    \"ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ì˜ë£Œ ë¶„ì•¼ì—ì„œ í˜ì‹ ì„ ì¼ìœ¼í‚¤ê³  ìžˆìŠµë‹ˆë‹¤.\",\n    \"ì •ë¶€ê°€ íƒ„ì†Œì¤‘ë¦½ ì •ì±…ì„ ê°•í™”í•˜ê¸°ë¡œ ê²°ì •í–ˆìŠµë‹ˆë‹¤.\",\n    \"ìžìœ¨ì£¼í–‰ ìžë™ì°¨ ìƒìš©í™”ê°€ 2025ë…„ì— ë³¸ê²©í™”ë  ì „ë§ìž…ë‹ˆë‹¤.\",\n    \"K-popì´ ì „ ì„¸ê³„ì ìœ¼ë¡œ í° ì¸ê¸°ë¥¼ ëŒê³  ìžˆìŠµë‹ˆë‹¤.\",\n    \"ë°”ì´ì˜¤ ê¸°ì—…ë“¤ì´ ì‹ ì•½ ê°œë°œì— ë°•ì°¨ë¥¼ ê°€í•˜ê³  ìžˆìŠµë‹ˆë‹¤.\",\n    \"ì „ê¸°ì°¨ ë°°í„°ë¦¬ ê¸°ìˆ ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìžˆìŠµë‹ˆë‹¤.\",\n    \"ë©”íƒ€ë²„ìŠ¤ê°€ ì°¨ì„¸ëŒ€ ì¸í„°ë„· í”Œëž«í¼ìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìžˆìŠµë‹ˆë‹¤.\",\n]\n\nprint(\"=\"*80)\nprint(\"ðŸ“° í•œêµ­ì–´ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œìŠ¤í…œ\")\nprint(\"=\"*80)\n\n# ë‰´ìŠ¤ ì¿¼ë¦¬\nnews_queries = [\n    \"ë°˜ë„ì²´ ê¸°ìˆ \",\n    \"AI ì˜ë£Œ\",\n    \"ì „ê¸°ì°¨ ë°°í„°ë¦¬\",\n]\n\nfor query in news_queries:\n    print(f\"\\nðŸ” ê²€ìƒ‰ì–´: '{query}'\")\n    print(\"-\" * 80)\n\n    results = model.search(query, news_articles, top_k=3)\n\n    for _, row in results.iterrows():\n        print(f\"\\n  [{row['rank']}ìœ„] ì ìˆ˜: {row['score']:.4f}\")\n        print(f\"       {row['document']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 16. ìš”ì•½ ë° ì‚¬ìš© ê°€ì´ë“œ\n\n### âœ… NeuralSparseSearchModel ì£¼ìš” ê¸°ëŠ¥\n\n1. **`from_pretrained(model_path)`** - ëª¨ë¸ ë¡œë“œ\n2. **`encode_query(query)`** - ì¿¼ë¦¬ ì¸ì½”ë”© (Inference-Free, ë§¤ìš° ë¹ ë¦„)\n3. **`encode_documents(documents, batch_size)`** - ë¬¸ì„œ ë°°ì¹˜ ì¸ì½”ë”©\n4. **`search(query, documents, top_k)`** - ê²€ìƒ‰ ìˆ˜í–‰\n5. **`get_top_tokens(sparse_vec, top_k)`** - ìƒìœ„ í† í° ë¶„ì„\n6. **`compute_sparsity(sparse_vec)`** - í¬ì†Œì„± ê³„ì‚°\n\n### ðŸ’¡ ì‚¬ìš© íŒ¨í„´\n\n#### íŒ¨í„´ 1: ì¦‰ì‹œ ê²€ìƒ‰ (ë¬¸ì„œ ì¸ì½”ë”© ì¦‰ì‹œ ìˆ˜í–‰)\n```python\nmodel = NeuralSparseSearchModel.from_pretrained(\"./model\")\nresults = model.search(query=\"ê²€ìƒ‰ì–´\", documents=[\"ë¬¸ì„œ1\", \"ë¬¸ì„œ2\"])\n```\n\n#### íŒ¨í„´ 2: ì‚¬ì „ ì¸ì½”ë”© (ëŒ€ëŸ‰ ë¬¸ì„œ, ë°˜ë³µ ê²€ìƒ‰)\n```python\n# 1íšŒë§Œ ì¸ì½”ë”©\ndoc_vectors = model.encode_documents(documents, batch_size=16)\n\n# ë¹ ë¥¸ ê²€ìƒ‰ (ì—¬ëŸ¬ ë²ˆ)\nresults1 = model.search(query=\"ì¿¼ë¦¬1\", documents=doc_vectors)\nresults2 = model.search(query=\"ì¿¼ë¦¬2\", documents=doc_vectors)\n```\n\n### ðŸŽ¯ ì‹¤ì „ í™œìš©\n\n- **OpenSearch í†µí•©**: ë¬¸ì„œë¥¼ ë¯¸ë¦¬ ì¸ì½”ë”©í•˜ì—¬ rank_features í•„ë“œì— ì €ìž¥\n- **ì‹¤ì‹œê°„ ê²€ìƒ‰**: ì¿¼ë¦¬ë§Œ ì¦‰ì‹œ ì¸ì½”ë”© (Inference-Free)\n- **ë°°ì¹˜ ì²˜ë¦¬**: ëŒ€ëŸ‰ ë¬¸ì„œëŠ” ë°°ì¹˜ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ì¸ì½”ë”©\n- **ë¶„ì„**: ìƒìœ„ í† í° ë¶„ì„ìœ¼ë¡œ ê²€ìƒ‰ í’ˆì§ˆ ê°œì„ "
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
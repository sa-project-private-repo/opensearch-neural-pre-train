{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenSearch Korean Neural Sparse Model - Inference Test\n",
    "\n",
    "í•™ìŠµëœ í•œêµ­ì–´ Neural Sparse ëª¨ë¸ì„ ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "## í…ŒìŠ¤íŠ¸ í•­ëª©\n",
    "1. ëª¨ë¸ ë¡œë“œ\n",
    "2. ë¬¸ì„œ ì¸ì½”ë”© (BERT ê¸°ë°˜)\n",
    "3. ì¿¼ë¦¬ ì¸ì½”ë”© (IDF lookup - Inference-Free)\n",
    "4. ìœ ì‚¬ë„ ê³„ì‚° ë° ê²€ìƒ‰\n",
    "5. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë””ë°”ì´ìŠ¤: cuda\n",
      "GPU: Tesla T4\n",
      "ë©”ëª¨ë¦¬: 14.57 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForMaskedLM\n",
    "\n",
    "# GPU/CPU í™•ì¸\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ëª¨ë¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "class OpenSearchDocEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    OpenSearch Neural Sparse Document Encoder (Doc-only mode)\n",
    "    \n",
    "    ë¬¸ì„œë¥¼ sparse vectorë¡œ ì¸ì½”ë”©í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"klue/bert-base\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # BERT ê¸°ë°˜ ì¸ì½”ë”\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.bert = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        \n",
    "        self.vocab_size = self.config.vocab_size\n",
    "        \n",
    "        # Log saturation activation: log(1 + ReLU(x))\n",
    "        self.activation = lambda x: torch.log1p(torch.relu(x))\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, return_dict=False):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            input_ids: (batch_size, seq_len)\n",
    "            attention_mask: (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            sparse_vector: (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        # BERT MLM head output\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Logits: (batch_size, seq_len, vocab_size)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Apply activation: log(1 + ReLU(logits))\n",
    "        activated = self.activation(logits)\n",
    "        \n",
    "        # Max pooling over sequence length\n",
    "        # (batch_size, seq_len, vocab_size) â†’ (batch_size, vocab_size)\n",
    "        sparse_vector = torch.max(\n",
    "            activated * attention_mask.unsqueeze(-1),\n",
    "            dim=1\n",
    "        ).values\n",
    "        \n",
    "        if return_dict:\n",
    "            return {'output': sparse_vector}\n",
    "        \n",
    "        return sparse_vector\n",
    "\n",
    "print(\"âœ“ ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ ëª¨ë¸ íŒŒì¼ í™•ì¸...\n",
      "  âœ“ pytorch_model.bin         ( 432313.86 KB)\n",
      "  âœ“ idf.json                  (    905.19 KB)\n",
      "  âœ“ config.json               (      0.55 KB)\n",
      "  âœ“ tokenizer.json            (    734.63 KB)\n",
      "  âœ“ vocab.txt                 (    242.65 KB)\n",
      "\n",
      "============================================================\n",
      "ëª¨ë¸ ë¡œë”© ì¤‘...\n",
      "============================================================\n",
      "\n",
      "âœ“ Config ë¡œë“œ ì™„ë£Œ\n",
      "  ëª¨ë¸ íƒ€ì…: opensearch-neural-sparse-doc-encoder\n",
      "  ë² ì´ìŠ¤ ëª¨ë¸: klue/bert-base\n",
      "  Vocab size: 32,000\n",
      "  Max length: 128\n",
      "\n",
      "âœ“ Tokenizer ë¡œë“œ ì™„ë£Œ\n",
      "  Vocab size: 32,000\n",
      "\n",
      "âœ“ IDF ë”•ì…”ë„ˆë¦¬ ë¡œë“œ ì™„ë£Œ\n",
      "  í† í° ìˆ˜: 29,205\n",
      "  í‰ê·  IDF: 8.9035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Document Encoder ë¡œë“œ ì™„ë£Œ\n",
      "  Parameters: 110,650,880\n",
      "  Device: cuda\n",
      "\n",
      "============================================================\n",
      "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
    "MODEL_DIR = \"./opensearch-korean-neural-sparse-v1\"\n",
    "\n",
    "# íŒŒì¼ ì¡´ì¬ í™•ì¸\n",
    "required_files = [\n",
    "    \"pytorch_model.bin\",\n",
    "    \"idf.json\",\n",
    "    \"config.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"vocab.txt\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“¦ ëª¨ë¸ íŒŒì¼ í™•ì¸...\")\n",
    "for filename in required_files:\n",
    "    filepath = os.path.join(MODEL_DIR, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        size = os.path.getsize(filepath) / 1024\n",
    "        print(f\"  âœ“ {filename:25s} ({size:>10.2f} KB)\")\n",
    "    else:\n",
    "        print(f\"  âœ— {filename:25s} [NOT FOUND]\")\n",
    "        raise FileNotFoundError(f\"Required file not found: {filename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Config ë¡œë“œ\n",
    "with open(os.path.join(MODEL_DIR, \"config.json\"), 'r', encoding='utf-8') as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "print(\"\\nâœ“ Config ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  ëª¨ë¸ íƒ€ì…: {model_config['model_type']}\")\n",
    "print(f\"  ë² ì´ìŠ¤ ëª¨ë¸: {model_config['base_model']}\")\n",
    "print(f\"  Vocab size: {model_config['vocab_size']:,}\")\n",
    "print(f\"  Max length: {model_config['max_seq_length']}\")\n",
    "\n",
    "# 2. Tokenizer ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "print(f\"\\nâœ“ Tokenizer ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "# 3. IDF ë”•ì…”ë„ˆë¦¬ ë¡œë“œ\n",
    "with open(os.path.join(MODEL_DIR, \"idf.json\"), 'r', encoding='utf-8') as f:\n",
    "    idf_dict = json.load(f)\n",
    "\n",
    "print(f\"\\nâœ“ IDF ë”•ì…”ë„ˆë¦¬ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  í† í° ìˆ˜: {len(idf_dict):,}\")\n",
    "print(f\"  í‰ê·  IDF: {np.mean(list(idf_dict.values())):.4f}\")\n",
    "\n",
    "# 4. Document Encoder ëª¨ë¸ ë¡œë“œ\n",
    "base_model = model_config['base_model']\n",
    "doc_encoder = OpenSearchDocEncoder(base_model)\n",
    "\n",
    "# í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "state_dict = torch.load(\n",
    "    os.path.join(MODEL_DIR, \"pytorch_model.bin\"),\n",
    "    map_location=device\n",
    ")\n",
    "doc_encoder.load_state_dict(state_dict)\n",
    "doc_encoder = doc_encoder.to(device)\n",
    "doc_encoder.eval()  # Evaluation mode\n",
    "\n",
    "print(f\"\\nâœ“ Document Encoder ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in doc_encoder.parameters()):,}\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Inference í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def encode_document(text, model, tokenizer, device, max_length=128):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œë¥¼ sparse vectorë¡œ ì¸ì½”ë”© (ëª¨ë¸ ì‚¬ìš© - ëŠë¦¼)\n",
    "    \n",
    "    Args:\n",
    "        text: ë¬¸ì„œ í…ìŠ¤íŠ¸\n",
    "        model: Document encoder ëª¨ë¸\n",
    "        tokenizer: Tokenizer\n",
    "        device: ë””ë°”ì´ìŠ¤\n",
    "        max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "    \n",
    "    Returns:\n",
    "        sparse_vector: (vocab_size,) numpy array\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # í† í°í™”\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        sparse_vec = model(input_ids, attention_mask)\n",
    "    \n",
    "    return sparse_vec.cpu().numpy()[0]\n",
    "\n",
    "\n",
    "def encode_query_inference_free(text, tokenizer, idf_dict, max_length=128):\n",
    "    \"\"\"\n",
    "    ì¿¼ë¦¬ë¥¼ sparse vectorë¡œ ì¸ì½”ë”© (IDF lookup - Inference-Free! ë§¤ìš° ë¹ ë¦„)\n",
    "    \n",
    "    Args:\n",
    "        text: ì¿¼ë¦¬ í…ìŠ¤íŠ¸\n",
    "        tokenizer: Tokenizer\n",
    "        idf_dict: IDF ë”•ì…”ë„ˆë¦¬\n",
    "        max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "    \n",
    "    Returns:\n",
    "        sparse_vector: (vocab_size,) numpy array\n",
    "    \"\"\"\n",
    "    # í† í°í™”\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        add_special_tokens=False,\n",
    "        max_length=max_length,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # IDF lookup\n",
    "    sparse_vec = np.zeros(tokenizer.vocab_size)\n",
    "    for token_id in tokens:\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        if token_str in idf_dict:\n",
    "            sparse_vec[token_id] = idf_dict[token_str]\n",
    "    \n",
    "    return sparse_vec\n",
    "\n",
    "\n",
    "def get_top_tokens(sparse_vec, tokenizer, top_k=15):\n",
    "    \"\"\"\n",
    "    Sparse vectorì—ì„œ ìƒìœ„ í† í° ì¶”ì¶œ\n",
    "    \n",
    "    Args:\n",
    "        sparse_vec: (vocab_size,) numpy array\n",
    "        tokenizer: Tokenizer\n",
    "        top_k: ìƒìœ„ kê°œ\n",
    "    \n",
    "    Returns:\n",
    "        List of (token, value) tuples\n",
    "    \"\"\"\n",
    "    top_indices = np.argsort(sparse_vec)[-top_k:][::-1]\n",
    "    top_values = sparse_vec[top_indices]\n",
    "    \n",
    "    top_tokens = []\n",
    "    for idx, val in zip(top_indices, top_values):\n",
    "        if val > 0:\n",
    "            token = tokenizer.decode([idx])\n",
    "            top_tokens.append((token, val))\n",
    "    \n",
    "    return top_tokens\n",
    "\n",
    "\n",
    "def compute_similarity(query_vec, doc_vec):\n",
    "    \"\"\"\n",
    "    ì¿¼ë¦¬ì™€ ë¬¸ì„œì˜ ìœ ì‚¬ë„ ê³„ì‚° (Dot product)\n",
    "    \n",
    "    Args:\n",
    "        query_vec: (vocab_size,) numpy array\n",
    "        doc_vec: (vocab_size,) numpy array\n",
    "    \n",
    "    Returns:\n",
    "        similarity score (float)\n",
    "    \"\"\"\n",
    "    return np.dot(query_vec, doc_vec)\n",
    "\n",
    "\n",
    "print(\"âœ“ Inference í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë¬¸ì„œ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“„ ë¬¸ì„œ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ (Model Inference)\n",
      "============================================================\n",
      "\n",
      "[ë¬¸ì„œ 1]\n",
      "í…ìŠ¤íŠ¸: OpenSearchëŠ” ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„...\n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 7/32,000)\n",
      "  L1 Norm: 1.82\n",
      "  ì¸ì½”ë”© ì‹œê°„: 142.29ms\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ##earch         (0.4343)\n",
      "     2. ê²€ìƒ‰              (0.3980)\n",
      "     3. ë¶„ì„              (0.3320)\n",
      "     4. ë³´ê³ ì„œ             (0.2745)\n",
      "     5. ##S             (0.2496)\n",
      "     6. íƒìƒ‰              (0.1086)\n",
      "     7. ì˜¤               (0.0222)\n",
      "\n",
      "[ë¬¸ì„œ 2]\n",
      "í…ìŠ¤íŠ¸: í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤. KoNLPyì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬...\n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 5/32,000)\n",
      "  L1 Norm: 1.93\n",
      "  ì¸ì½”ë”© ì‹œê°„: 12.97ms\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ì–¸ì–´              (0.7562)\n",
      "     2. ì²˜ë¦¬              (0.7488)\n",
      "     3. ìì—°              (0.3232)\n",
      "     4. ë¼ì´ë¸Œ             (0.0915)\n",
      "     5. ##ëŸ¬ë¦¬            (0.0058)\n",
      "\n",
      "[ë¬¸ì„œ 3]\n",
      "í…ìŠ¤íŠ¸: ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ ë°ì´í„°ì™€ ì»´í“¨íŒ… ìì›ì´ í•„ìš”í•©ë‹ˆë‹¤. GPUë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµ ì†ë„ê°€ í¬ê²Œ ...\n",
      "  í¬ì†Œì„±: 99.97% (non-zero: 9/32,000)\n",
      "  L1 Norm: 5.01\n",
      "  ì¸ì½”ë”© ì‹œê°„: 12.33ms\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ë”¥               (1.1729)\n",
      "     2. í•™ìŠµ              (0.9816)\n",
      "     3. GP              (0.8123)\n",
      "     4. ##ëŸ¬             (0.6911)\n",
      "     5. ê·¸ë˜í”½             (0.5761)\n",
      "     6. ë°ì´í„°             (0.4078)\n",
      "     7. ë‡Œ               (0.2140)\n",
      "     8. AI              (0.0768)\n",
      "     9. ëª¨ë¸              (0.0751)\n",
      "\n",
      "[ë¬¸ì„œ 4]\n",
      "í…ìŠ¤íŠ¸: ChatGPTëŠ” OpenAIê°€ ê°œë°œí•œ ëŒ€í™”í˜• AI ëª¨ë¸ì…ë‹ˆë‹¤. GPT ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©° ë‹¤ì–‘í•œ ì‘ì—…ì„...\n",
      "  í¬ì†Œì„±: 99.99% (non-zero: 4/32,000)\n",
      "  L1 Norm: 2.41\n",
      "  ì¸ì½”ë”© ì‹œê°„: 12.22ms\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. AI              (1.3888)\n",
      "     2. ##P             (0.5188)\n",
      "     3. ##T             (0.4897)\n",
      "     4. ##G             (0.0174)\n",
      "\n",
      "[ë¬¸ì„œ 5]\n",
      "í…ìŠ¤íŠ¸: ë²¡í„° ê²€ìƒ‰ì€ ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤. Dense retrieval ë°©ì‹ì´ë¼ê³ ë„ ...\n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 5/32,000)\n",
      "  L1 Norm: 2.66\n",
      "  ì¸ì½”ë”© ì‹œê°„: 12.18ms\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ##í„°             (0.8604)\n",
      "     2. ê²€ìƒ‰              (0.6825)\n",
      "     3. ë²¡               (0.6392)\n",
      "     4. íƒìƒ‰              (0.4006)\n",
      "     5. ##ìƒ‰             (0.0783)\n",
      "\n",
      "í‰ê·  ì¸ì½”ë”© ì‹œê°„: 38.40ms\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë¬¸ì„œ\n",
    "test_documents = [\n",
    "    \"OpenSearchëŠ” ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.\",\n",
    "    \"í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤. KoNLPyì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.\",\n",
    "    \"ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ ë°ì´í„°ì™€ ì»´í“¨íŒ… ìì›ì´ í•„ìš”í•©ë‹ˆë‹¤. GPUë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµ ì†ë„ê°€ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤.\",\n",
    "    \"ChatGPTëŠ” OpenAIê°€ ê°œë°œí•œ ëŒ€í™”í˜• AI ëª¨ë¸ì…ë‹ˆë‹¤. GPT ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©° ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ë²¡í„° ê²€ìƒ‰ì€ ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤. Dense retrieval ë°©ì‹ì´ë¼ê³ ë„ ë¶ˆë¦½ë‹ˆë‹¤.\",\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“„ ë¬¸ì„œ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ (Model Inference)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "doc_vectors = []\n",
    "encoding_times = []\n",
    "\n",
    "for i, doc in enumerate(test_documents, 1):\n",
    "    print(f\"\\n[ë¬¸ì„œ {i}]\")\n",
    "    print(f\"í…ìŠ¤íŠ¸: {doc[:60]}...\")\n",
    "    \n",
    "    # ì¸ì½”ë”© ì‹œê°„ ì¸¡ì •\n",
    "    start_time = time.time()\n",
    "    sparse_vec = encode_document(doc, doc_encoder, tokenizer, device)\n",
    "    elapsed = time.time() - start_time\n",
    "    encoding_times.append(elapsed)\n",
    "    \n",
    "    doc_vectors.append(sparse_vec)\n",
    "    \n",
    "    # í†µê³„\n",
    "    non_zero = np.count_nonzero(sparse_vec)\n",
    "    sparsity = (1 - non_zero / len(sparse_vec)) * 100\n",
    "    l1_norm = np.sum(np.abs(sparse_vec))\n",
    "    \n",
    "    print(f\"  í¬ì†Œì„±: {sparsity:.2f}% (non-zero: {non_zero:,}/{len(sparse_vec):,})\")\n",
    "    print(f\"  L1 Norm: {l1_norm:.2f}\")\n",
    "    print(f\"  ì¸ì½”ë”© ì‹œê°„: {elapsed*1000:.2f}ms\")\n",
    "    \n",
    "    # ìƒìœ„ í† í°\n",
    "    print(f\"  ìƒìœ„ í† í°:\")\n",
    "    top_tokens = get_top_tokens(sparse_vec, tokenizer, top_k=10)\n",
    "    for j, (token, value) in enumerate(top_tokens, 1):\n",
    "        print(f\"    {j:2d}. {token:15s} ({value:.4f})\")\n",
    "\n",
    "print(f\"\\ní‰ê·  ì¸ì½”ë”© ì‹œê°„: {np.mean(encoding_times)*1000:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì¿¼ë¦¬ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ (Inference-Free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“ ì¿¼ë¦¬ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ (Inference-Free: Tokenizer + IDF Lookup)\n",
      "============================================================\n",
      "\n",
      "[ì¿¼ë¦¬ 1] OpenSearch ë²¡í„° ê²€ìƒ‰\n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 6/32,000)\n",
      "  ì¸ì½”ë”© ì‹œê°„: 0.5574ms âš¡ (ë§¤ìš° ë¹ ë¦„!)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ë²¡               (12.2485)\n",
      "     2. ê²€ìƒ‰              (8.8385)\n",
      "     3. Op              (7.4678)\n",
      "     4. ##S             (6.8928)\n",
      "     5. ##í„°             (6.4828)\n",
      "     6. ##en            (5.7666)\n",
      "\n",
      "[ì¿¼ë¦¬ 2] í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ \n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 5/32,000)\n",
      "  ì¸ì½”ë”© ì‹œê°„: 0.2050ms âš¡ (ë§¤ìš° ë¹ ë¦„!)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. í•œêµ­ì–´             (7.6927)\n",
      "     2. ìì—°              (6.2214)\n",
      "     3. ì²˜ë¦¬              (6.0865)\n",
      "     4. ê¸°ìˆ               (5.1364)\n",
      "     5. ##ì–´             (3.3754)\n",
      "\n",
      "[ì¿¼ë¦¬ 3] ë”¥ëŸ¬ë‹ GPU í•™ìŠµ\n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 6/32,000)\n",
      "  ì¸ì½”ë”© ì‹œê°„: 0.2315ms âš¡ (ë§¤ìš° ë¹ ë¦„!)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. GP              (13.5708)\n",
      "     2. ##ë‹             (12.3340)\n",
      "     3. ë”¥               (10.4745)\n",
      "     4. í•™ìŠµ              (7.6003)\n",
      "     5. ##ëŸ¬             (6.9786)\n",
      "     6. ##U             (5.9254)\n",
      "\n",
      "[ì¿¼ë¦¬ 4] ChatGPT LLM ëª¨ë¸\n",
      "  í¬ì†Œì„±: 99.97% (non-zero: 9/32,000)\n",
      "  ì¸ì½”ë”© ì‹œê°„: 0.2255ms âš¡ (ë§¤ìš° ë¹ ë¦„!)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ##T             (14.7473)\n",
      "     2. ##G             (11.1942)\n",
      "     3. ##M             (8.9907)\n",
      "     4. ##L             (8.2165)\n",
      "     5. ##P             (7.5446)\n",
      "     6. L               (6.9884)\n",
      "     7. Ch              (6.9079)\n",
      "     8. ##at            (6.8284)\n",
      "     9. ëª¨ë¸              (6.1814)\n",
      "\n",
      "[ì¿¼ë¦¬ 5] ê²€ìƒ‰ ì—”ì§„ ìµœì í™”\n",
      "  í¬ì†Œì„±: 99.99% (non-zero: 3/32,000)\n",
      "  ì¸ì½”ë”© ì‹œê°„: 0.1378ms âš¡ (ë§¤ìš° ë¹ ë¦„!)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ê²€ìƒ‰              (8.8385)\n",
      "     2. ìµœì í™”             (7.7767)\n",
      "     3. ì—”ì§„              (7.1470)\n",
      "\n",
      "[ì¿¼ë¦¬ 6] ì¸ê³µì§€ëŠ¥ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§\n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 5/32,000)\n",
      "  ì¸ì½”ë”© ì‹œê°„: 0.1843ms âš¡ (ë§¤ìš° ë¹ ë¦„!)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ##ë¡¬             (11.3833)\n",
      "     2. í”„               (10.5800)\n",
      "     3. ##í”„íŠ¸            (9.6803)\n",
      "     4. ì—”ì§€ë‹ˆì–´ë§           (8.7288)\n",
      "     5. ì¸ê³µì§€ëŠ¥            (8.5432)\n",
      "\n",
      "í‰ê·  ì¿¼ë¦¬ ì¸ì½”ë”© ì‹œê°„: 0.2569ms\n",
      "\n",
      "ğŸ’¡ ë¬¸ì„œ ì¸ì½”ë”© ëŒ€ë¹„ ì†ë„: 149.4ë°° ë¹ ë¦„!\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬\n",
    "test_queries = [\n",
    "    \"OpenSearch ë²¡í„° ê²€ìƒ‰\",\n",
    "    \"í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ \",\n",
    "    \"ë”¥ëŸ¬ë‹ GPU í•™ìŠµ\",\n",
    "    \"ChatGPT LLM ëª¨ë¸\",\n",
    "    \"ê²€ìƒ‰ ì—”ì§„ ìµœì í™”\",\n",
    "    \"ì¸ê³µì§€ëŠ¥ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§\",\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“ ì¿¼ë¦¬ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ (Inference-Free: Tokenizer + IDF Lookup)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "query_vectors = []\n",
    "query_encoding_times = []\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n[ì¿¼ë¦¬ {i}] {query}\")\n",
    "    \n",
    "    # ì¸ì½”ë”© ì‹œê°„ ì¸¡ì •\n",
    "    start_time = time.time()\n",
    "    sparse_vec = encode_query_inference_free(query, tokenizer, idf_dict)\n",
    "    elapsed = time.time() - start_time\n",
    "    query_encoding_times.append(elapsed)\n",
    "    \n",
    "    query_vectors.append(sparse_vec)\n",
    "    \n",
    "    # í†µê³„\n",
    "    non_zero = np.count_nonzero(sparse_vec)\n",
    "    sparsity = (1 - non_zero / len(sparse_vec)) * 100\n",
    "    \n",
    "    print(f\"  í¬ì†Œì„±: {sparsity:.2f}% (non-zero: {non_zero}/{len(sparse_vec):,})\")\n",
    "    print(f\"  ì¸ì½”ë”© ì‹œê°„: {elapsed*1000:.4f}ms âš¡ (ë§¤ìš° ë¹ ë¦„!)\")\n",
    "    \n",
    "    # ìƒìœ„ í† í°\n",
    "    print(f\"  ìƒìœ„ í† í°:\")\n",
    "    top_tokens = get_top_tokens(sparse_vec, tokenizer, top_k=10)\n",
    "    for j, (token, value) in enumerate(top_tokens, 1):\n",
    "        print(f\"    {j:2d}. {token:15s} ({value:.4f})\")\n",
    "\n",
    "print(f\"\\ní‰ê·  ì¿¼ë¦¬ ì¸ì½”ë”© ì‹œê°„: {np.mean(query_encoding_times)*1000:.4f}ms\")\n",
    "print(f\"\\nğŸ’¡ ë¬¸ì„œ ì¸ì½”ë”© ëŒ€ë¹„ ì†ë„: {np.mean(encoding_times)/np.mean(query_encoding_times):.1f}ë°° ë¹ ë¦„!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (ìœ ì‚¬ë„ ê³„ì‚°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (Query-Document Similarity)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ì¿¼ë¦¬: OpenSearch ë²¡í„° ê²€ìƒ‰\n",
      "============================================================\n",
      "\n",
      "ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 3ê°œ):\n",
      "\n",
      "  [1ìœ„] ìœ ì‚¬ë„: 19.4390\n",
      "       ë¬¸ì„œ: ë²¡í„° ê²€ìƒ‰ì€ ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤. Dense retrieval ë°©ì‹ì´ë¼ê³ ë„ ë¶ˆë¦½ë‹ˆë‹¤....\n",
      "\n",
      "  [2ìœ„] ìœ ì‚¬ë„: 5.2380\n",
      "       ë¬¸ì„œ: OpenSearchëŠ” ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤....\n",
      "\n",
      "  [3ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "       ë¬¸ì„œ: í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤. KoNLPyì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤....\n",
      "\n",
      "============================================================\n",
      "ì¿¼ë¦¬: í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ \n",
      "============================================================\n",
      "\n",
      "ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 3ê°œ):\n",
      "\n",
      "  [1ìœ„] ìœ ì‚¬ë„: 6.5684\n",
      "       ë¬¸ì„œ: í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤. KoNLPyì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤....\n",
      "\n",
      "  [2ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "       ë¬¸ì„œ: OpenSearchëŠ” ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤....\n",
      "\n",
      "  [3ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "       ë¬¸ì„œ: ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ ë°ì´í„°ì™€ ì»´í“¨íŒ… ìì›ì´ í•„ìš”í•©ë‹ˆë‹¤. GPUë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµ ì†ë„ê°€ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤....\n",
      "\n",
      "============================================================\n",
      "ì¿¼ë¦¬: ë”¥ëŸ¬ë‹ GPU í•™ìŠµ\n",
      "============================================================\n",
      "\n",
      "ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 3ê°œ):\n",
      "\n",
      "  [1ìœ„] ìœ ì‚¬ë„: 35.5924\n",
      "       ë¬¸ì„œ: ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ ë°ì´í„°ì™€ ì»´í“¨íŒ… ìì›ì´ í•„ìš”í•©ë‹ˆë‹¤. GPUë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµ ì†ë„ê°€ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤....\n",
      "\n",
      "  [2ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "       ë¬¸ì„œ: OpenSearchëŠ” ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤....\n",
      "\n",
      "  [3ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "       ë¬¸ì„œ: í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤. KoNLPyì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤....\n",
      "\n",
      "============================================================\n",
      "ì¿¼ë¦¬: ChatGPT LLM ëª¨ë¸\n",
      "============================================================\n",
      "\n",
      "ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 3ê°œ):\n",
      "\n",
      "  [1ìœ„] ìœ ì‚¬ë„: 11.3314\n",
      "       ë¬¸ì„œ: ChatGPTëŠ” OpenAIê°€ ê°œë°œí•œ ëŒ€í™”í˜• AI ëª¨ë¸ì…ë‹ˆë‹¤. GPT ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©° ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤....\n",
      "\n",
      "  [2ìœ„] ìœ ì‚¬ë„: 0.4645\n",
      "       ë¬¸ì„œ: ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ ë°ì´í„°ì™€ ì»´í“¨íŒ… ìì›ì´ í•„ìš”í•©ë‹ˆë‹¤. GPUë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµ ì†ë„ê°€ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤....\n",
      "\n",
      "  [3ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "       ë¬¸ì„œ: OpenSearchëŠ” ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤....\n",
      "\n",
      "============================================================\n",
      "ì¿¼ë¦¬: ê²€ìƒ‰ ì—”ì§„ ìµœì í™”\n",
      "============================================================\n",
      "\n",
      "ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 3ê°œ):\n",
      "\n",
      "  [1ìœ„] ìœ ì‚¬ë„: 6.0319\n",
      "       ë¬¸ì„œ: ë²¡í„° ê²€ìƒ‰ì€ ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤. Dense retrieval ë°©ì‹ì´ë¼ê³ ë„ ë¶ˆë¦½ë‹ˆë‹¤....\n",
      "\n",
      "  [2ìœ„] ìœ ì‚¬ë„: 3.5175\n",
      "       ë¬¸ì„œ: OpenSearchëŠ” ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤....\n",
      "\n",
      "  [3ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "       ë¬¸ì„œ: í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤. KoNLPyì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤....\n",
      "\n",
      "============================================================\n",
      "ì¿¼ë¦¬: ì¸ê³µì§€ëŠ¥ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§\n",
      "============================================================\n",
      "\n",
      "ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 3ê°œ):\n",
      "\n",
      "  [1ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "       ë¬¸ì„œ: OpenSearchëŠ” ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤....\n",
      "\n",
      "  [2ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "       ë¬¸ì„œ: í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤. KoNLPyì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤....\n",
      "\n",
      "  [3ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "       ë¬¸ì„œ: ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ ë°ì´í„°ì™€ ì»´í“¨íŒ… ìì›ì´ í•„ìš”í•©ë‹ˆë‹¤. GPUë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµ ì†ë„ê°€ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤....\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (Query-Document Similarity)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ê° ì¿¼ë¦¬ì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ì¿¼ë¦¬: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    query_vec = query_vectors[i]\n",
    "    \n",
    "    # ëª¨ë“  ë¬¸ì„œì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    similarities = []\n",
    "    for j, doc_vec in enumerate(doc_vectors):\n",
    "        sim = compute_similarity(query_vec, doc_vec)\n",
    "        similarities.append((j, sim))\n",
    "    \n",
    "    # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # ìƒìœ„ 3ê°œ ë¬¸ì„œ ì¶œë ¥\n",
    "    print(\"\\nê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 3ê°œ):\")\n",
    "    for rank, (doc_idx, score) in enumerate(similarities[:3], 1):\n",
    "        print(f\"\\n  [{rank}ìœ„] ìœ ì‚¬ë„: {score:.4f}\")\n",
    "        print(f\"       ë¬¸ì„œ: {test_documents[doc_idx][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "âš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬\n",
      "============================================================\n",
      "\n",
      "í…ŒìŠ¤íŠ¸: 100ê°œ ì¿¼ë¦¬ ì¸ì½”ë”© (Inference-Free)\n",
      "ì´ ì‹œê°„: 0.0532s\n",
      "í‰ê·  ì‹œê°„: 0.5324ms per query\n",
      "ì²˜ë¦¬ëŸ‰: 1878.35 queries/sec\n",
      "\n",
      "í…ŒìŠ¤íŠ¸: 10ê°œ ë¬¸ì„œ ì¸ì½”ë”© (Model Inference)\n",
      "ì´ ì‹œê°„: 0.1374s\n",
      "í‰ê·  ì‹œê°„: 13.74ms per document\n",
      "ì²˜ë¦¬ëŸ‰: 72.76 documents/sec\n",
      "\n",
      "============================================================\n",
      "ğŸ’¡ ìš”ì•½\n",
      "============================================================\n",
      "âœ“ ì¿¼ë¦¬ ì¸ì½”ë”©: ë§¤ìš° ë¹ ë¦„ (IDF lookupë§Œ ì‚¬ìš©)\n",
      "âœ“ ë¬¸ì„œ ì¸ì½”ë”©: ëŠë¦¼ (BERT ëª¨ë¸ ì‚¬ìš©)\n",
      "âœ“ ì¿¼ë¦¬ëŠ” ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥, ë¬¸ì„œëŠ” ì‚¬ì „ ì¸ë±ì‹± í•„ìš”\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"âš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ëŒ€ëŸ‰ ì¿¼ë¦¬ ì²˜ë¦¬ ì†ë„ í…ŒìŠ¤íŠ¸\n",
    "num_queries = 100\n",
    "test_query_text = \"OpenSearch ë²¡í„° ê²€ìƒ‰ ìµœì í™”\"\n",
    "\n",
    "print(f\"\\ní…ŒìŠ¤íŠ¸: {num_queries}ê°œ ì¿¼ë¦¬ ì¸ì½”ë”© (Inference-Free)\")\n",
    "start_time = time.time()\n",
    "for _ in range(num_queries):\n",
    "    _ = encode_query_inference_free(test_query_text, tokenizer, idf_dict)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"ì´ ì‹œê°„: {elapsed:.4f}s\")\n",
    "print(f\"í‰ê·  ì‹œê°„: {elapsed/num_queries*1000:.4f}ms per query\")\n",
    "print(f\"ì²˜ë¦¬ëŸ‰: {num_queries/elapsed:.2f} queries/sec\")\n",
    "\n",
    "# ë¬¸ì„œ ì¸ì½”ë”© ì†ë„ í…ŒìŠ¤íŠ¸\n",
    "num_docs = 10\n",
    "test_doc_text = \"OpenSearchëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ì—”ì§„ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "print(f\"\\ní…ŒìŠ¤íŠ¸: {num_docs}ê°œ ë¬¸ì„œ ì¸ì½”ë”© (Model Inference)\")\n",
    "start_time = time.time()\n",
    "for _ in range(num_docs):\n",
    "    _ = encode_document(test_doc_text, doc_encoder, tokenizer, device)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"ì´ ì‹œê°„: {elapsed:.4f}s\")\n",
    "print(f\"í‰ê·  ì‹œê°„: {elapsed/num_docs*1000:.2f}ms per document\")\n",
    "print(f\"ì²˜ë¦¬ëŸ‰: {num_docs/elapsed:.2f} documents/sec\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ’¡ ìš”ì•½\")\n",
    "print(\"=\"*60)\n",
    "print(\"âœ“ ì¿¼ë¦¬ ì¸ì½”ë”©: ë§¤ìš° ë¹ ë¦„ (IDF lookupë§Œ ì‚¬ìš©)\")\n",
    "print(\"âœ“ ë¬¸ì„œ ì¸ì½”ë”©: ëŠë¦¼ (BERT ëª¨ë¸ ì‚¬ìš©)\")\n",
    "print(\"âœ“ ì¿¼ë¦¬ëŠ” ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥, ë¬¸ì„œëŠ” ì‚¬ì „ ì¸ë±ì‹± í•„ìš”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ§ª ì»¤ìŠ¤í…€ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
      "============================================================\n",
      "\n",
      "ì¿¼ë¦¬: ì¸ê³µì§€ëŠ¥ LLM ëª¨ë¸ ìµœì í™” ë°©ë²•\n",
      "\n",
      "ê²€ìƒ‰ ê²°ê³¼:\n",
      "\n",
      "[1ìœ„] ìœ ì‚¬ë„: 12.0368\n",
      "     ë¬¸ì„œ: LLM ëª¨ë¸ ìµœì í™”ë¥¼ ìœ„í•´ì„œëŠ” ì–‘ìí™”, í”„ë£¨ë‹, ì§€ì‹ ì¦ë¥˜ ë“±ì˜ ê¸°ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "[2ìœ„] ìœ ì‚¬ë„: 2.3374\n",
      "     ë¬¸ì„œ: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì¶”ë¡  ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ KV ìºì‹œì™€ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í™œìš©í•©ë‹ˆë‹¤.\n",
      "\n",
      "[3ìœ„] ìœ ì‚¬ë„: 0.0000\n",
      "     ë¬¸ì„œ: í•œêµ­ì–´ ìŒì‹ ì¤‘ì—ì„œ ê¹€ì¹˜ì°Œê°œëŠ” ê°€ì¥ ì¸ê¸° ìˆëŠ” ë©”ë‰´ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ìš©ì ì •ì˜ ì¿¼ë¦¬ì™€ ë¬¸ì„œë¡œ í…ŒìŠ¤íŠ¸\n",
    "custom_query = \"ì¸ê³µì§€ëŠ¥ LLM ëª¨ë¸ ìµœì í™” ë°©ë²•\"  # ì—¬ê¸°ì— í…ŒìŠ¤íŠ¸í•  ì¿¼ë¦¬ ì…ë ¥\n",
    "\n",
    "custom_documents = [\n",
    "    \"LLM ëª¨ë¸ ìµœì í™”ë¥¼ ìœ„í•´ì„œëŠ” ì–‘ìí™”, í”„ë£¨ë‹, ì§€ì‹ ì¦ë¥˜ ë“±ì˜ ê¸°ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"í•œêµ­ì–´ ìŒì‹ ì¤‘ì—ì„œ ê¹€ì¹˜ì°Œê°œëŠ” ê°€ì¥ ì¸ê¸° ìˆëŠ” ë©”ë‰´ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\",\n",
    "    \"ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì¶”ë¡  ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ KV ìºì‹œì™€ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í™œìš©í•©ë‹ˆë‹¤.\",\n",
    "]  # ì—¬ê¸°ì— í…ŒìŠ¤íŠ¸í•  ë¬¸ì„œ ì…ë ¥\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ§ª ì»¤ìŠ¤í…€ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nì¿¼ë¦¬: {custom_query}\")\n",
    "\n",
    "# ì¿¼ë¦¬ ì¸ì½”ë”©\n",
    "query_vec = encode_query_inference_free(custom_query, tokenizer, idf_dict)\n",
    "\n",
    "# ë¬¸ì„œ ì¸ì½”ë”© ë° ìœ ì‚¬ë„ ê³„ì‚°\n",
    "results = []\n",
    "for i, doc in enumerate(custom_documents):\n",
    "    doc_vec = encode_document(doc, doc_encoder, tokenizer, device)\n",
    "    similarity = compute_similarity(query_vec, doc_vec)\n",
    "    results.append((i, similarity, doc))\n",
    "\n",
    "# ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nê²€ìƒ‰ ê²°ê³¼:\")\n",
    "for rank, (doc_idx, score, doc_text) in enumerate(results, 1):\n",
    "    print(f\"\\n[{rank}ìœ„] ìœ ì‚¬ë„: {score:.4f}\")\n",
    "    print(f\"     ë¬¸ì„œ: {doc_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. í¬ì†Œì„±(Sparsity) ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“Š í¬ì†Œì„± ë¶„ì„\n",
      "============================================================\n",
      "\n",
      "ë¬¸ì„œ ë²¡í„° í¬ì†Œì„±:\n",
      "  í‰ê· : 99.98%\n",
      "  ìµœì†Œ: 99.97%\n",
      "  ìµœëŒ€: 99.99%\n",
      "  í‘œì¤€í¸ì°¨: 0.01%\n",
      "\n",
      "ì¿¼ë¦¬ ë²¡í„° í¬ì†Œì„±:\n",
      "  í‰ê· : 99.98%\n",
      "  ìµœì†Œ: 99.97%\n",
      "  ìµœëŒ€: 99.99%\n",
      "  í‘œì¤€í¸ì°¨: 0.01%\n",
      "\n",
      "ğŸ’¡ í¬ì†Œì„±ì´ ë†’ì„ìˆ˜ë¡ (100%ì— ê°€ê¹Œìš¸ìˆ˜ë¡):\n",
      "   - ë©”ëª¨ë¦¬ íš¨ìœ¨ì \n",
      "   - ê²€ìƒ‰ ì†ë„ ë¹ ë¦„\n",
      "   - ì €ì¥ ê³µê°„ ì ìŒ\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š í¬ì†Œì„± ë¶„ì„\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ë¬¸ì„œ ë²¡í„° í¬ì†Œì„±\n",
    "doc_sparsities = []\n",
    "for vec in doc_vectors:\n",
    "    non_zero = np.count_nonzero(vec)\n",
    "    sparsity = (1 - non_zero / len(vec)) * 100\n",
    "    doc_sparsities.append(sparsity)\n",
    "\n",
    "print(f\"\\në¬¸ì„œ ë²¡í„° í¬ì†Œì„±:\")\n",
    "print(f\"  í‰ê· : {np.mean(doc_sparsities):.2f}%\")\n",
    "print(f\"  ìµœì†Œ: {np.min(doc_sparsities):.2f}%\")\n",
    "print(f\"  ìµœëŒ€: {np.max(doc_sparsities):.2f}%\")\n",
    "print(f\"  í‘œì¤€í¸ì°¨: {np.std(doc_sparsities):.2f}%\")\n",
    "\n",
    "# ì¿¼ë¦¬ ë²¡í„° í¬ì†Œì„±\n",
    "query_sparsities = []\n",
    "for vec in query_vectors:\n",
    "    non_zero = np.count_nonzero(vec)\n",
    "    sparsity = (1 - non_zero / len(vec)) * 100\n",
    "    query_sparsities.append(sparsity)\n",
    "\n",
    "print(f\"\\nì¿¼ë¦¬ ë²¡í„° í¬ì†Œì„±:\")\n",
    "print(f\"  í‰ê· : {np.mean(query_sparsities):.2f}%\")\n",
    "print(f\"  ìµœì†Œ: {np.min(query_sparsities):.2f}%\")\n",
    "print(f\"  ìµœëŒ€: {np.max(query_sparsities):.2f}%\")\n",
    "print(f\"  í‘œì¤€í¸ì°¨: {np.std(query_sparsities):.2f}%\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ í¬ì†Œì„±ì´ ë†’ì„ìˆ˜ë¡ (100%ì— ê°€ê¹Œìš¸ìˆ˜ë¡):\")\n",
    "print(f\"   - ë©”ëª¨ë¦¬ íš¨ìœ¨ì \")\n",
    "print(f\"   - ê²€ìƒ‰ ì†ë„ ë¹ ë¦„\")\n",
    "print(f\"   - ì €ì¥ ê³µê°„ ì ìŒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ê²°ê³¼ ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "âœ… OpenSearch Korean Neural Sparse Model - Inference Test ì™„ë£Œ\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š í…ŒìŠ¤íŠ¸ ìš”ì•½:\n",
      "  â€¢ ë¬¸ì„œ ì¸ì½”ë”© í‰ê·  ì‹œê°„: 38.40ms\n",
      "  â€¢ ì¿¼ë¦¬ ì¸ì½”ë”© í‰ê·  ì‹œê°„: 0.2569ms\n",
      "  â€¢ ì†ë„ ì°¨ì´: 149.4ë°°\n",
      "  â€¢ ë¬¸ì„œ ë²¡í„° í‰ê·  í¬ì†Œì„±: 99.98%\n",
      "  â€¢ ì¿¼ë¦¬ ë²¡í„° í‰ê·  í¬ì†Œì„±: 99.98%\n",
      "\n",
      "ğŸ¯ í•µì‹¬ íŠ¹ì§•:\n",
      "  âœ“ Inference-Free ì¿¼ë¦¬ ì¸ì½”ë”© (ë§¤ìš° ë¹ ë¦„)\n",
      "  âœ“ ë†’ì€ í¬ì†Œì„± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\n",
      "  âœ“ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ê°€ëŠ¥\n",
      "  âœ“ í•œêµ­ì–´ ìµœì í™”\n",
      "\n",
      "ğŸ’¡ OpenSearch í†µí•©:\n",
      "  1. ë¬¸ì„œëŠ” ì‚¬ì „ì— ì¸ì½”ë”©í•˜ì—¬ ì¸ë±ì‹±\n",
      "  2. ì¿¼ë¦¬ëŠ” ì‹¤ì‹œê°„ìœ¼ë¡œ IDF lookup\n",
      "  3. rank_features í•„ë“œë¡œ ì €ì¥\n",
      "  4. neural_sparse ì¿¼ë¦¬ë¡œ ê²€ìƒ‰\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"âœ… OpenSearch Korean Neural Sparse Model - Inference Test ì™„ë£Œ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nğŸ“Š í…ŒìŠ¤íŠ¸ ìš”ì•½:\")\n",
    "print(f\"  â€¢ ë¬¸ì„œ ì¸ì½”ë”© í‰ê·  ì‹œê°„: {np.mean(encoding_times)*1000:.2f}ms\")\n",
    "print(f\"  â€¢ ì¿¼ë¦¬ ì¸ì½”ë”© í‰ê·  ì‹œê°„: {np.mean(query_encoding_times)*1000:.4f}ms\")\n",
    "print(f\"  â€¢ ì†ë„ ì°¨ì´: {np.mean(encoding_times)/np.mean(query_encoding_times):.1f}ë°°\")\n",
    "print(f\"  â€¢ ë¬¸ì„œ ë²¡í„° í‰ê·  í¬ì†Œì„±: {np.mean(doc_sparsities):.2f}%\")\n",
    "print(f\"  â€¢ ì¿¼ë¦¬ ë²¡í„° í‰ê·  í¬ì†Œì„±: {np.mean(query_sparsities):.2f}%\")\n",
    "\n",
    "print(\"\\nğŸ¯ í•µì‹¬ íŠ¹ì§•:\")\n",
    "print(\"  âœ“ Inference-Free ì¿¼ë¦¬ ì¸ì½”ë”© (ë§¤ìš° ë¹ ë¦„)\")\n",
    "print(\"  âœ“ ë†’ì€ í¬ì†Œì„± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\")\n",
    "print(\"  âœ“ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ê°€ëŠ¥\")\n",
    "print(\"  âœ“ í•œêµ­ì–´ ìµœì í™”\")\n",
    "\n",
    "print(\"\\nğŸ’¡ OpenSearch í†µí•©:\")\n",
    "print(\"  1. ë¬¸ì„œëŠ” ì‚¬ì „ì— ì¸ì½”ë”©í•˜ì—¬ ì¸ë±ì‹±\")\n",
    "print(\"  2. ì¿¼ë¦¬ëŠ” ì‹¤ì‹œê°„ìœ¼ë¡œ IDF lookup\")\n",
    "print(\"  3. rank_features í•„ë“œë¡œ ì €ì¥\")\n",
    "print(\"  4. neural_sparse ì¿¼ë¦¬ë¡œ ê²€ìƒ‰\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Neural Sparse Base Training\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ê¸°ë³¸ Neural Sparse ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ëª©í‘œ\n",
    "- í•œêµ­ì–´ ë¬¸ì„œ ë°ì´í„° ì¤€ë¹„\n",
    "- IDF ê¸°ë°˜ í†µê³„ ìƒì„±\n",
    "- ê¸°ë³¸ Query-Document ìŒ ìƒì„±\n",
    "- Neural Sparse Encoder í•™ìŠµ (Base Model)\n",
    "\n",
    "## ì¶œë ¥ ë°ì´í„°\n",
    "ëª¨ë“  ë°ì´í„°ëŠ” `dataset/base_model/` ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤:\n",
    "- `documents.json`: í•œêµ­ì–´ ë¬¸ì„œ ë°ì´í„°\n",
    "- `idf_statistics.pkl`: IDF í†µê³„\n",
    "- `qd_pairs_base.pkl`: ê¸°ë³¸ QD ìŒ\n",
    "- `bilingual_synonyms.json`: ì´ì¤‘ì–¸ì–´ ë™ì˜ì–´ ì‚¬ì „\n",
    "- `neural_sparse_v1_model/`: í•™ìŠµëœ ëª¨ë¸\n",
    "\n",
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "ì´ ë…¸íŠ¸ë¶ ì‹¤í–‰ í›„ `02_llm_synthetic_data_generation.ipynb`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DatasetManager initialized\n",
      "ğŸ“ Base path: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset\n"
     ]
    }
   ],
   "source": [
    "# DatasetManager ì´ˆê¸°í™”\n",
    "from src.dataset_manager import DatasetManager\n",
    "\n",
    "dm = DatasetManager(base_path=\"dataset\")\n",
    "print(\"âœ… DatasetManager initialized\")\n",
    "print(f\"ğŸ“ Base path: {dm.base_path.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenSearch Inference-Free Neural Sparse ëª¨ë¸ - í•œêµ­ì–´ í•™ìŠµ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **OpenSearch inference-free IR ëª¨ë¸** í‘œì¤€ì— ë”°ë¼ í•œêµ­ì–´ neural sparse ê²€ìƒ‰ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ¯ OpenSearch ëª¨ë¸ êµ¬ì¡°\n",
    "\n",
    "### Doc-only Mode (Inference-Free)\n",
    "- **ë¬¸ì„œ ì¸ì½”ë”©**: BERT ê¸°ë°˜ ì‹ ê²½ë§ â†’ sparse vector\n",
    "- **ì¿¼ë¦¬ ì¸ì½”ë”©**: Tokenizer + **idf.json** (weight lookup table) â†’ **Inference-Free!**\n",
    "\n",
    "### í•µì‹¬ íŒŒì¼\n",
    "1. `pytorch_model.bin` - ë¬¸ì„œ ì¸ì½”ë” ëª¨ë¸ (BERT ê¸°ë°˜)\n",
    "2. `idf.json` - í† í°ë³„ ê°€ì¤‘ì¹˜ lookup table (ì¿¼ë¦¬ìš©)\n",
    "3. `tokenizer.json`, `vocab.txt` - í† í¬ë‚˜ì´ì €\n",
    "4. `config.json` - ëª¨ë¸ ì„¤ì •\n",
    "\n",
    "### í•™ìŠµ ë°©ë²•\n",
    "- **IDF-aware Penalty**: ë‚®ì€ IDF í† í°ì˜ ê¸°ì—¬ë„ ì–µì œ\n",
    "- **Knowledge Distillation**: Dense + Sparse siamese ëª¨ë¸ì—ì„œ í•™ìŠµ\n",
    "- **â„“0 Sparsification**: L0 regularizationìœ¼ë¡œ í¬ì†Œì„± ìœ ì§€\n",
    "\n",
    "### ì°¸ê³  ë…¼ë¬¸\n",
    "- [Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers](https://arxiv.org/abs/2411.04403)\n",
    "- [Exploring â„“0 Sparsification for Inference-free Sparse Retrievers](https://opensearch.org/blog/)\n",
    "\n",
    "### OpenSearch Models Collection\n",
    "- https://huggingface.co/collections/opensearch-project/inference-free-ir-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Project root: /home/west/Documents/cursor-workspace\n"
     ]
    }
   ],
   "source": [
    "# Disable Triton to avoid compilation errors (ARM aarch64)\n",
    "import os\n",
    "os.environ[\"TRITON_INTERPRET\"] = \"1\"  # Use interpreter mode\n",
    "os.environ[\"DISABLE_TRITON\"] = \"1\"     # Completely disable\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Add parent directory to path for src imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path (notebooks/ ìƒìœ„ ë””ë ‰í† ë¦¬)\n",
    "project_root = Path().absolute().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ–¥ï¸  GPU/CPU í™˜ê²½ í™•ì¸\n",
      "============================================================\n",
      "PyTorch ë²„ì „: 2.10.0.dev20251109+cu130\n",
      "CUDA ì‚¬ìš© ê°€ëŠ¥: True\n",
      "âœ“ GPU ì‚¬ìš© ê°€ëŠ¥!\n",
      "  - CUDA ë²„ì „: 13.0\n",
      "  - GPU ê°œìˆ˜: 1\n",
      "  - GPU 0: NVIDIA GB10\n",
      "    * ë©”ëª¨ë¦¬: 119.70 GB\n",
      "    * Compute Capability: 12.1\n",
      "\n",
      "â†’ í•™ìŠµ ë””ë°”ì´ìŠ¤: GPU (cuda)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Hugging Face & Transformers\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForMaskedLM,\n",
    "    AutoConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    get_linear_schedule_with_warmup  # ì´ í•¨ìˆ˜ëŠ” transformersì—ì„œ ì œê³µ\n",
    ")\n",
    "\n",
    "# í˜•íƒœì†Œ ë¶„ì„\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# GPU/CPU í™˜ê²½ í™•ì¸\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ–¥ï¸  GPU/CPU í™˜ê²½ í™•ì¸\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ GPU ì‚¬ìš© ê°€ëŠ¥!\")\n",
    "    print(f\"  - CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "    print(f\"  - GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  - GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    * ë©”ëª¨ë¦¬: {props.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"    * Compute Capability: {props.major}.{props.minor}\")\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\\nâ†’ í•™ìŠµ ë””ë°”ì´ìŠ¤: GPU (cuda)\")\n",
    "else:\n",
    "    print(f\"âš ï¸  GPU ì‚¬ìš© ë¶ˆê°€\")\n",
    "    print(f\"  - CPUë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤\")\n",
    "    print(f\"  - í•™ìŠµ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"\\nâ†’ í•™ìŠµ ë””ë°”ì´ìŠ¤: CPU\")\n",
    "    print(f\"\\nğŸ’¡ GPU ì‚¬ìš© ê¶Œì¥:\")\n",
    "    print(f\"  - AWS EC2: g4dn.xlarge ì´ìƒ (NVIDIA T4 GPU)\")\n",
    "    print(f\"  - AWS EC2: p3.2xlarge ì´ìƒ (NVIDIA V100 GPU)\")\n",
    "    print(f\"  - Google Colab: GPU ëŸ°íƒ€ì„ ì‚¬ìš©\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í•œêµ­ì–´ ë°ì´í„°ì…‹ ìˆ˜ì§‘\n",
    "\n",
    "Hugging Faceì—ì„œ ë‹¤ì–‘í•œ í•œêµ­ì–´ ê³µê°œ ë°ì´í„°ì…‹ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š í•œêµ­ì–´ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\n",
      "\n",
      "1ï¸âƒ£ KLUE MRC ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ“ 17,554 query-document pairs\n",
      "\n",
      "2ï¸âƒ£ KorQuAD v1 ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ“ 60,407 query-document pairs\n",
      "\n",
      "3ï¸âƒ£ í•œêµ­ì–´ Wikipedia ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ— Wikipedia ë¡œë”© ì‹¤íŒ¨: Couldn't find file at https://dumps.wikimedia.org/kowiki/20220301/dumpstatus.json\n",
      "\n",
      "4ï¸âƒ£ KLUE Topic Classification ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ“ 45,678 documents\n",
      "\n",
      "5ï¸âƒ£ í•œêµ­ì–´ ë‰´ìŠ¤ ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ“ 50,000 documents\n",
      "\n",
      "\n",
      "============================================================\n",
      "ì´ 117,914ê°œì˜ ê³ ìœ  ë¬¸ì„œ\n",
      "ì´ 77,785ê°œì˜ ê³ ìœ  ì¿¼ë¦¬\n",
      "ì´ 77,961ê°œì˜ query-document pairs\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def load_korean_datasets():\n",
    "    \"\"\"\n",
    "    Hugging Faceì—ì„œ í•œêµ­ì–´ ê³µê°œ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "    Query-Document ìŒì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì…‹ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    datasets_collection = {\n",
    "        'documents': [],\n",
    "        'queries': [],\n",
    "        'qd_pairs': []  # (query, document, relevance) íŠœí”Œ\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ“š í•œêµ­ì–´ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\\n\")\n",
    "    \n",
    "    # 1. KLUE MRC (Machine Reading Comprehension)\n",
    "    try:\n",
    "        print(\"1ï¸âƒ£ KLUE MRC ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        klue_mrc = load_dataset(\"klue\", \"mrc\", split=\"train\")\n",
    "        \n",
    "        for item in klue_mrc:\n",
    "            doc = item['context']\n",
    "            query = item['question']\n",
    "            \n",
    "            datasets_collection['documents'].append(doc)\n",
    "            datasets_collection['queries'].append(query)\n",
    "            datasets_collection['qd_pairs'].append((query, doc, 1.0))  # Positive pair\n",
    "        \n",
    "        print(f\"   âœ“ {len(klue_mrc):,} query-document pairs\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— KLUE MRC ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # 2. KorQuAD v1\n",
    "    try:\n",
    "        print(\"2ï¸âƒ£ KorQuAD v1 ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        korquad = load_dataset(\"squad_kor_v1\", split=\"train\")\n",
    "        \n",
    "        for item in korquad:\n",
    "            doc = item['context']\n",
    "            query = item['question']\n",
    "            \n",
    "            datasets_collection['documents'].append(doc)\n",
    "            datasets_collection['queries'].append(query)\n",
    "            datasets_collection['qd_pairs'].append((query, doc, 1.0))\n",
    "        \n",
    "        print(f\"   âœ“ {len(korquad):,} query-document pairs\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— KorQuAD ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # 3. Korean Wikipedia (ë¬¸ì„œ ì½”í¼ìŠ¤)\n",
    "    try:\n",
    "        print(\"3ï¸âƒ£ í•œêµ­ì–´ Wikipedia ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        ko_wiki = load_dataset(\"wikipedia\", \"20220301.ko\", split=\"train[:100000]\")\n",
    "        \n",
    "        for item in ko_wiki:\n",
    "            text = item['text']\n",
    "            if len(text) > 100:  # ìµœì†Œ ê¸¸ì´ í•„í„°\n",
    "                datasets_collection['documents'].append(text[:2000])  # ì²˜ìŒ 2000ì\n",
    "        \n",
    "        print(f\"   âœ“ {len(ko_wiki):,} documents\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— Wikipedia ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # 4. KLUE Topic Classification (ë¬¸ì„œ ì½”í¼ìŠ¤)\n",
    "    try:\n",
    "        print(\"4ï¸âƒ£ KLUE Topic Classification ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        klue_tc = load_dataset(\"klue\", \"ynat\", split=\"train\")\n",
    "        \n",
    "        for item in klue_tc:\n",
    "            datasets_collection['documents'].append(item['title'])\n",
    "        \n",
    "        print(f\"   âœ“ {len(klue_tc):,} documents\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— KLUE TC ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # 5. Korean News Dataset\n",
    "    try:\n",
    "        print(\"5ï¸âƒ£ í•œêµ­ì–´ ë‰´ìŠ¤ ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        korean_news = load_dataset(\"heegyu/news-category-dataset\", split=\"train[:50000]\")\n",
    "        \n",
    "        for item in korean_news:\n",
    "            if 'headline' in item:\n",
    "                datasets_collection['documents'].append(item['headline'])\n",
    "        \n",
    "        print(f\"   âœ“ {len(korean_news):,} documents\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— Korean News ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # ì¤‘ë³µ ì œê±°\n",
    "    datasets_collection['documents'] = list(set([\n",
    "        doc.strip() for doc in datasets_collection['documents'] \n",
    "        if doc and len(doc.strip()) > 10\n",
    "    ]))\n",
    "    \n",
    "    datasets_collection['queries'] = list(set([\n",
    "        q.strip() for q in datasets_collection['queries'] \n",
    "        if q and len(q.strip()) > 5\n",
    "    ]))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ì´ {len(datasets_collection['documents']):,}ê°œì˜ ê³ ìœ  ë¬¸ì„œ\")\n",
    "    print(f\"ì´ {len(datasets_collection['queries']):,}ê°œì˜ ê³ ìœ  ì¿¼ë¦¬\")\n",
    "    print(f\"ì´ {len(datasets_collection['qd_pairs']):,}ê°œì˜ query-document pairs\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return datasets_collection\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "korean_data = load_korean_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š í•œêµ­ì–´ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ (ì‹œê°„ ì •ë³´ í¬í•¨)\n",
      "ğŸ“° Loading Korean news dataset: heegyu/news-category-dataset\n",
      "Processing 10000 articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading news: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 59626.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 9996 documents\n",
      "  Date range: 2018-05-01 to 2022-09-23\n",
      "  Time span: 1606 days\n",
      "  Categories: 29\n",
      "    POLITICS: 3357\n",
      "    ENTERTAINMENT: 1597\n",
      "    U.S. NEWS: 1377\n",
      "    WORLD NEWS: 1194\n",
      "    COMEDY: 319\n",
      "âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ\n",
      "  Documents: 9,996ê°œ\n",
      "  Dates: 9,996ê°œ\n",
      "\n",
      "ğŸ“„ ìƒ˜í”Œ ë¬¸ì„œ: Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters...\n",
      "ğŸ“… ìƒ˜í”Œ ë‚ ì§œ: 2022-09-23 00:00:00\n",
      "\n",
      "ğŸ“š ì¶”ê°€ ë°ì´í„°ì…‹ ë¡œë“œ (KLUE, KorQuAD ë“±)\n",
      "ğŸ“š í•œêµ­ì–´ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\n",
      "\n",
      "1ï¸âƒ£ KLUE MRC ë°ì´í„°ì…‹ ë¡œë”©...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ 17,554 query-document pairs\n",
      "\n",
      "2ï¸âƒ£ KorQuAD v1 ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ“ 60,407 query-document pairs\n",
      "\n",
      "3ï¸âƒ£ í•œêµ­ì–´ Wikipedia ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ— Wikipedia ë¡œë”© ì‹¤íŒ¨: Couldn't find file at https://dumps.wikimedia.org/kowiki/20220301/dumpstatus.json\n",
      "\n",
      "4ï¸âƒ£ KLUE Topic Classification ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ“ 45,678 documents\n",
      "\n",
      "5ï¸âƒ£ í•œêµ­ì–´ ë‰´ìŠ¤ ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ“ 50,000 documents\n",
      "\n",
      "\n",
      "============================================================\n",
      "ì´ 117,914ê°œì˜ ê³ ìœ  ë¬¸ì„œ\n",
      "ì´ 77,785ê°œì˜ ê³ ìœ  ì¿¼ë¦¬\n",
      "ì´ 77,961ê°œì˜ query-document pairs\n",
      "============================================================\n",
      "  ì¶”ê°€ documents: 117,914ê°œ\n",
      "  ì „ì²´ documents: 127,910ê°œ\n",
      "  Queries: 77,785ê°œ\n",
      "  QD Pairs: 77,961ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ğŸ†• ë°ì´í„°ì…‹ ë¡œë“œ (ì‹œê°„ ì •ë³´ í¬í•¨)\n",
    "from src import load_korean_news_with_dates\n",
    "\n",
    "print(\"ğŸ“Š í•œêµ­ì–´ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ (ì‹œê°„ ì •ë³´ í¬í•¨)\")\n",
    "\n",
    "# ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ (ë‚ ì§œ ì •ë³´ í¬í•¨)\n",
    "news_data = load_korean_news_with_dates(\n",
    "    max_samples=10000,\n",
    "    min_doc_length=20\n",
    ")\n",
    "\n",
    "documents = news_data['documents']\n",
    "dates = news_data['dates']\n",
    "\n",
    "print(f\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  Documents: {len(documents):,}ê°œ\")\n",
    "print(f\"  Dates: {len(dates):,}ê°œ\")\n",
    "\n",
    "# ìƒ˜í”Œ ì¶œë ¥\n",
    "if documents:\n",
    "    print(f\"\\nğŸ“„ ìƒ˜í”Œ ë¬¸ì„œ: {documents[0][:100]}...\")\n",
    "    print(f\"ğŸ“… ìƒ˜í”Œ ë‚ ì§œ: {dates[0]}\")\n",
    "\n",
    "# ê¸°ì¡´ load_korean_datasets() í•¨ìˆ˜ë„ í˜¸í™˜ì„±ì„ ìœ„í•´ ì‹¤í–‰\n",
    "print(\"\\nğŸ“š ì¶”ê°€ ë°ì´í„°ì…‹ ë¡œë“œ (KLUE, KorQuAD ë“±)\")\n",
    "korean_data = load_korean_datasets()\n",
    "\n",
    "# ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©\n",
    "if korean_data['documents']:\n",
    "    print(f\"  ì¶”ê°€ documents: {len(korean_data['documents']):,}ê°œ\")\n",
    "    documents.extend(korean_data['documents'])\n",
    "    # ê¸°ì¡´ ë°ì´í„°ëŠ” ë‚ ì§œê°€ ì—†ìœ¼ë¯€ë¡œ í˜„ì¬ ë‚ ì§œë¡œ ì±„ì›€\n",
    "    from datetime import datetime\n",
    "    dates.extend([datetime.now()] * len(korean_data['documents']))\n",
    "    print(f\"  ì „ì²´ documents: {len(documents):,}ê°œ\")\n",
    "\n",
    "queries = korean_data.get('queries', [])\n",
    "qd_pairs = korean_data.get('qd_pairs', [])\n",
    "\n",
    "print(f\"  Queries: {len(queries):,}ê°œ\")\n",
    "print(f\"  QD Pairs: {len(qd_pairs):,}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. IDF (Inverse Document Frequency) ê³„ì‚°\n",
    "\n",
    "ì¿¼ë¦¬ìš© í† í° ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ IDFë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "ì´ê²ƒì´ **idf.json** íŒŒì¼ì˜ ê¸°ë°˜ì´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ: klue/bert-base\n",
      "  Vocab size: 32,000\n",
      "\n",
      "ğŸ“Š IDF ê³„ì‚° ì¤‘ (ìƒ˜í”Œ: 50,000ê°œ ë¬¸ì„œ)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4021f8c37ee74abbb26c40e6d91d7a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing documents:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ IDF ê³„ì‚° ì™„ë£Œ\n",
      "  ì´ 29,241ê°œ í† í°\n",
      "  í‰ê·  IDF: 8.8984\n",
      "  IDF ë²”ìœ„: [2.3048, 11.1267]\n",
      "\n",
      "ğŸ” IDF ìƒìœ„ 20ê°œ í† í° (í¬ê·€ í† í°):\n",
      " 1. ê°€ì•¡                   - IDF: 11.1267\n",
      " 2. ê¹€ë‘í•œ                  - IDF: 11.1267\n",
      " 3. ê°ˆë¹„ì°œ                  - IDF: 11.1267\n",
      " 4. ë„ë¼ì§€                  - IDF: 11.1267\n",
      " 5. ##ê¹¬                  - IDF: 11.1267\n",
      " 6. êµ´ëŸ¬ê°€                  - IDF: 11.1267\n",
      " 7. ìš°ì™€                   - IDF: 11.1267\n",
      " 8. ë¯¼í˜                   - IDF: 11.1267\n",
      " 9. íƒœì›                    - IDF: 11.1267\n",
      "10. ê³ ì¦ˆ                   - IDF: 11.1267\n",
      "11. ë¯¼ë°•                   - IDF: 11.1267\n",
      "12. ##ë¼ì´ì–´í‹°               - IDF: 11.1267\n",
      "13. ì‘¤ì…”                   - IDF: 11.1267\n",
      "14. ##ì–‘ì–‘                 - IDF: 11.1267\n",
      "15. ì‹¤ì»·                   - IDF: 11.1267\n",
      "16. ##êº½                  - IDF: 11.1267\n",
      "17. êº½                    - IDF: 11.1267\n",
      "18. ì°¨ê³¡ì°¨ê³¡                 - IDF: 11.1267\n",
      "19. http                 - IDF: 11.1267\n",
      "20. í•„ë§                   - IDF: 11.1267\n",
      "\n",
      "ğŸ”» IDF í•˜ìœ„ 20ê°œ í† í° (í”í•œ í† í°):\n",
      " 1. â€¦                    - IDF: 2.7832\n",
      " 2. ##ì—ì„œ                 - IDF: 2.7658\n",
      " 3. ##ìœ¼ë¡œ                 - IDF: 2.7345\n",
      " 4. ##í•œ                  - IDF: 2.6979\n",
      " 5. ##ê°€                  - IDF: 2.6958\n",
      " 6. '                    - IDF: 2.6916\n",
      " 7. ##ë¥¼                  - IDF: 2.6852\n",
      " 8. ##í•˜                  - IDF: 2.6752\n",
      " 9. ##ê³                   - IDF: 2.6696\n",
      "10. ##ë¡œ                  - IDF: 2.6620\n",
      "11. ##ì„                  - IDF: 2.6327\n",
      "12. ##ì´                  - IDF: 2.6058\n",
      "13. ##ì€                  - IDF: 2.5772\n",
      "14. ##ë‹¤                  - IDF: 2.5473\n",
      "15. ##ì˜                  - IDF: 2.5100\n",
      "16. ##ëŠ”                  - IDF: 2.4680\n",
      "17. ,                    - IDF: 2.4000\n",
      "18. .                    - IDF: 2.3966\n",
      "19. ##ì—                  - IDF: 2.3378\n",
      "20. ##s                  - IDF: 2.3048\n"
     ]
    }
   ],
   "source": [
    "# í•œêµ­ì–´ BERT í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "MODEL_NAME = \"klue/bert-base\"  # í•œêµ­ì–´ ìµœì í™” BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ: {MODEL_NAME}\")\n",
    "print(f\"  Vocab size: {len(tokenizer):,}\")\n",
    "\n",
    "def calculate_idf(documents, tokenizer, sample_size=50000):\n",
    "    \"\"\"\n",
    "    ì½”í¼ìŠ¤ì—ì„œ IDFë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    IDF(t) = log(N / df(t))\n",
    "    \n",
    "    Args:\n",
    "        documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        tokenizer: Hugging Face tokenizer\n",
    "        sample_size: ìƒ˜í”Œë§í•  ë¬¸ì„œ ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        dict: {token_id: idf_score}\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“Š IDF ê³„ì‚° ì¤‘ (ìƒ˜í”Œ: {min(sample_size, len(documents)):,}ê°œ ë¬¸ì„œ)...\")\n",
    "    \n",
    "    # ìƒ˜í”Œë§\n",
    "    sample_docs = documents[:sample_size]\n",
    "    N = len(sample_docs)\n",
    "    \n",
    "    # ê° í† í°ì´ ë‚˜íƒ€ë‚œ ë¬¸ì„œ ìˆ˜ ê³„ì‚°\n",
    "    df = Counter()  # document frequency\n",
    "    \n",
    "    for i, doc in enumerate(tqdm(sample_docs, desc=\"Tokenizing documents\")):\n",
    "        # í† í°í™”\n",
    "        tokens = tokenizer.encode(doc, add_special_tokens=False, max_length=512, truncation=True)\n",
    "        \n",
    "        # ë¬¸ì„œì— ë‚˜íƒ€ë‚œ ê³ ìœ  í† í°ë“¤\n",
    "        unique_tokens = set(tokens)\n",
    "        \n",
    "        # df ì—…ë°ì´íŠ¸\n",
    "        for token_id in unique_tokens:\n",
    "            df[token_id] += 1\n",
    "    \n",
    "    # IDF ê³„ì‚°\n",
    "    idf_dict = {}\n",
    "    for token_id, doc_freq in df.items():\n",
    "        # IDF = log(N / df)\n",
    "        idf_score = math.log((N + 1) / (doc_freq + 1)) + 1.0  # smoothing\n",
    "        idf_dict[token_id] = idf_score\n",
    "    \n",
    "    # í† í° ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "    idf_token_dict = {}\n",
    "    for token_id, score in idf_dict.items():\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        idf_token_dict[token_str] = float(score)\n",
    "    \n",
    "    print(f\"\\nâœ“ IDF ê³„ì‚° ì™„ë£Œ\")\n",
    "    print(f\"  ì´ {len(idf_token_dict):,}ê°œ í† í°\")\n",
    "    print(f\"  í‰ê·  IDF: {np.mean(list(idf_token_dict.values())):.4f}\")\n",
    "    print(f\"  IDF ë²”ìœ„: [{min(idf_token_dict.values()):.4f}, {max(idf_token_dict.values()):.4f}]\")\n",
    "    \n",
    "    return idf_token_dict, idf_dict\n",
    "\n",
    "# IDF ê³„ì‚°\n",
    "idf_token_dict, idf_id_dict = calculate_idf(korean_data['documents'], tokenizer)\n",
    "\n",
    "# ìƒìœ„/í•˜ìœ„ IDF í† í° ì¶œë ¥\n",
    "print(\"\\nğŸ” IDF ìƒìœ„ 20ê°œ í† í° (í¬ê·€ í† í°):\")\n",
    "sorted_idf = sorted(idf_token_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for i, (token, score) in enumerate(sorted_idf[:20], 1):\n",
    "    print(f\"{i:2d}. {token:20s} - IDF: {score:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ”» IDF í•˜ìœ„ 20ê°œ í† í° (í”í•œ í† í°):\")\n",
    "for i, (token, score) in enumerate(sorted_idf[-20:], 1):\n",
    "    print(f\"{i:2d}. {token:20s} - IDF: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. í•œêµ­ì–´ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì¶”ê°€\n",
    "\n",
    "2024-2025 íŠ¸ë Œë“œ í‚¤ì›Œë“œì— ëŒ€í•´ IDF ê°€ì¤‘ì¹˜ë¥¼ ë¶€ìŠ¤íŒ…í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ†• ìë™ íŠ¸ë Œë“œ ê°ì§€ (Unsupervised)\n",
    "\n",
    "í•˜ë“œì½”ë”©ëœ `TREND_BOOST` ëŒ€ì‹  ìë™ìœ¼ë¡œ íŠ¸ë Œë”© í† í°ì„ ê°ì§€í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” ìë™ íŠ¸ë Œë“œ ê°ì§€ (3ê°œì›” ê°„ê²© ë¹„êµ)\n",
      "  ë°ì´í„° ë‚ ì§œ ë²”ìœ„: 2018-05-01 ~ 2025-11-15\n",
      "  ë°ì´í„° ê¸°ê°„: 2755ì¼ (91.8ê°œì›”)\n",
      "  âœ“ ìµœê·¼ 3ê°œì›”(90ì¼) vs ê³¼ê±° 6ê°œì›”(180ì¼) ë¹„êµ\n",
      "\n",
      "ğŸ”¥ Detecting Trending Tokens\n",
      "   Recent period: 90 days\n",
      "   Historical period: 180 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing trends: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127910/127910 [00:11<00:00, 10980.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Recent documents: 117914\n",
      "   Historical documents: 0\n",
      "\n",
      "âœ“ Found 100 trending tokens\n",
      "\n",
      "  Top 10 Trending:\n",
      "    1. To: 139153.26x\n",
      "    2. The: 138483.28x\n",
      "    3. Tr: 113498.97x\n",
      "    4. In: 88082.14x\n",
      "    5. [UNK]: 80483.39x\n",
      "    6. For: 68313.50x\n",
      "    7. St: 58933.78x\n",
      "    8. Th: 56745.75x\n",
      "    9. Se: 50105.31x\n",
      "    10. On: 49494.70x\n",
      "\n",
      "âœ“ 100ê°œì˜ íŠ¸ë Œë”© í† í° ìë™ ë°œê²¬\n",
      "\n",
      "âœ“ Created boost dictionary with 100 tokens\n",
      "  Boost range: 1.20 - 2.00\n",
      "âœ“ Applied boost to 100 tokens\n",
      "âœ“ ìë™ íŠ¸ë Œë“œ ë¶€ìŠ¤íŒ… ì™„ë£Œ\n",
      "\n",
      "  ğŸ“ˆ ìƒìœ„ 10ê°œ íŠ¸ë Œë”© í† í°:\n",
      "    To: 139153.26x ì¦ê°€ (boost=2.00)\n",
      "    The: 138483.28x ì¦ê°€ (boost=2.00)\n",
      "    Tr: 113498.97x ì¦ê°€ (boost=1.84)\n",
      "    In: 88082.14x ì¦ê°€ (boost=1.67)\n",
      "    [UNK]: 80483.39x ì¦ê°€ (boost=1.62)\n",
      "    For: 68313.50x ì¦ê°€ (boost=1.55)\n",
      "    St: 58933.78x ì¦ê°€ (boost=1.49)\n",
      "    Th: 56745.75x ì¦ê°€ (boost=1.47)\n",
      "    Se: 50105.31x ì¦ê°€ (boost=1.43)\n",
      "    On: 49494.70x ì¦ê°€ (boost=1.43)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ†• ìë™ íŠ¸ë Œë“œ ê°ì§€ (í•˜ë“œì½”ë”© ì œê±°) - 3ê°œì›” ê°„ê²©\n",
    "from src import (\n",
    "    detect_trending_tokens,\n",
    "    build_trend_boost_dict,\n",
    "    apply_temporal_boost_to_idf,\n",
    ")\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "USE_AUTO_TREND_DETECTION = True\n",
    "\n",
    "if USE_AUTO_TREND_DETECTION:\n",
    "    print(\"\\nğŸ” ìë™ íŠ¸ë Œë“œ ê°ì§€ (3ê°œì›” ê°„ê²© ë¹„êµ)\")\n",
    "    \n",
    "    # ë°ì´í„° ë‚ ì§œ ë²”ìœ„ í™•ì¸\n",
    "    min_date = min(dates)\n",
    "    max_date = max(dates)\n",
    "    date_span = (max_date - min_date).days\n",
    "    \n",
    "    print(f\"  ë°ì´í„° ë‚ ì§œ ë²”ìœ„: {min_date.date()} ~ {max_date.date()}\")\n",
    "    print(f\"  ë°ì´í„° ê¸°ê°„: {date_span}ì¼ ({date_span/30:.1f}ê°œì›”)\")\n",
    "    \n",
    "    # 3ê°œì›” ê°„ê²© ì„¤ì •\n",
    "    recent_days = 90  # ìµœê·¼ 3ê°œì›”\n",
    "    historical_days = 180  # ê³¼ê±° 6ê°œì›” (ë¹„êµ ê¸°ì¤€)\n",
    "    \n",
    "    # ë°ì´í„°ê°€ ë¶€ì¡±í•œ ê²½ìš° ìë™ ì¡°ì •\n",
    "    if date_span < 180:\n",
    "        if date_span < 90:\n",
    "            # 3ê°œì›” ë¯¸ë§Œ: ì „ì²´ ê¸°ê°„ì˜ 1/2ì„ recentë¡œ ì‚¬ìš©\n",
    "            recent_days = max(30, date_span // 2)\n",
    "            historical_days = date_span\n",
    "            print(f\"  âš ï¸  ë°ì´í„° ë¶€ì¡±: recent={recent_days}ì¼, historical={historical_days}ì¼ë¡œ ì¡°ì •\")\n",
    "        else:\n",
    "            # 3~6ê°œì›”: recentëŠ” 3ê°œì›”, historicalì€ ì „ì²´\n",
    "            recent_days = 90\n",
    "            historical_days = date_span\n",
    "            print(f\"  âš ï¸  ë°ì´í„° ë¶€ì¡±: historical={historical_days}ì¼ë¡œ ì¡°ì •\")\n",
    "    else:\n",
    "        print(f\"  âœ“ ìµœê·¼ 3ê°œì›”(90ì¼) vs ê³¼ê±° 6ê°œì›”(180ì¼) ë¹„êµ\")\n",
    "    \n",
    "    # íŠ¸ë Œë”© í† í° ìë™ ê°ì§€\n",
    "    trending_tokens = detect_trending_tokens(\n",
    "        documents=documents,\n",
    "        dates=dates,\n",
    "        tokenizer=tokenizer,\n",
    "        recent_days=recent_days,\n",
    "        historical_days=historical_days,\n",
    "        min_recent_count=5,  # ìµœê·¼ ê¸°ê°„ì— ìµœì†Œ 5íšŒ ì´ìƒ ì¶œí˜„\n",
    "        top_k=100,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ“ {len(trending_tokens)}ê°œì˜ íŠ¸ë Œë”© í† í° ìë™ ë°œê²¬\")\n",
    "    \n",
    "    if len(trending_tokens) > 0:\n",
    "        # ìë™ boost ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "        auto_trend_boost = build_trend_boost_dict(\n",
    "            trending_tokens=trending_tokens,\n",
    "            max_boost=2.0,\n",
    "            min_boost=1.2,\n",
    "        )\n",
    "        \n",
    "        # IDFì— ì ìš©\n",
    "        idf_token_dict = apply_temporal_boost_to_idf(\n",
    "            idf_token_dict=idf_token_dict,\n",
    "            boost_dict=auto_trend_boost,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ“ ìë™ íŠ¸ë Œë“œ ë¶€ìŠ¤íŒ… ì™„ë£Œ\")\n",
    "        print(f\"\\n  ğŸ“ˆ ìƒìœ„ 10ê°œ íŠ¸ë Œë”© í† í°:\")\n",
    "        for token, score in trending_tokens[:10]:\n",
    "            boost = auto_trend_boost.get(token, 1.0)\n",
    "            print(f\"    {token}: {score:.2f}x ì¦ê°€ (boost={boost:.2f})\")\n",
    "    else:\n",
    "        print(\"\\n  â„¹ï¸  íŠ¸ë Œë”© í† í°ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        print(f\"     - ìµœê·¼ {recent_days}ì¼ê³¼ ê³¼ê±° {historical_days}ì¼ ë¹„êµ\")\n",
    "        print(\"     - ë°ì´í„° ê¸°ê°„ì´ ì§§ê±°ë‚˜ í† í° ë¹ˆë„ ë³€í™”ê°€ ì ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "        print(\"     - ë” ë§ì€ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê±°ë‚˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì„¸ìš”.\")\n",
    "else:\n",
    "    print(\"\\nâ­ï¸  ìë™ íŠ¸ë Œë“œ ê°ì§€ ê±´ë„ˆëœ€ (USE_AUTO_TREND_DETECTION=False)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ†• í•œì˜ í†µí•© ë™ì˜ì–´ ì‚¬ì „ (Cross-lingual)\n",
    "\n",
    "ë¹„ì§€ë„ í•™ìŠµìœ¼ë¡œ 'ëª¨ë¸' â†” 'model' ê°™ì€ í•œì˜ ë™ì˜ì–´ë¥¼ ìë™ ë°œê²¬í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŒ í•œì˜ í†µí•© ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶•\n",
      "  ìˆ˜ë™ ì •ì˜ ìŒ: 32ê°œ\n",
      "  âœ“ doc_encoder ë°œê²¬ - embedding ê¸°ë°˜ ë™ì˜ì–´ ë°œê²¬ ì‚¬ìš©\n",
      "  â„¹ï¸  doc_encoder ì—†ìŒ - ê¸°ë³¸ ë™ì˜ì–´ ìŒë§Œ ì‚¬ìš©\n",
      "     (ëª¨ë¸ í•™ìŠµ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ìë™ ë°œê²¬ ê¸°ëŠ¥ ì‚¬ìš© ê°€ëŠ¥)\n",
      "  ì „ì²´ bilingual ì‚¬ì „: 32ê°œ í•­ëª©\n",
      "\n",
      "ğŸ”— Applying Bilingual Synonyms to IDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sharing IDF: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 418123.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Updated 2 IDF entries with bilingual synonyms\n",
      "\n",
      "âœ“ í•œì˜ ë™ì˜ì–´ IDF ë™ê¸°í™” ì™„ë£Œ\n",
      "  ì˜ˆì‹œ: 'ëª¨ë¸' â†” 'model', 'í•™ìŠµ' â†” 'training'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ†• í•œì˜ í†µí•© ë™ì˜ì–´ (Bilingual Synonyms)\n",
    "from src import (\n",
    "    build_comprehensive_bilingual_dictionary,\n",
    "    get_default_korean_english_pairs,\n",
    "    apply_bilingual_synonyms_to_idf,\n",
    ")\n",
    "\n",
    "USE_BILINGUAL_SYNONYMS = True\n",
    "\n",
    "if USE_BILINGUAL_SYNONYMS:\n",
    "    print(\"\\nğŸŒ í•œì˜ í†µí•© ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶•\")\n",
    "    \n",
    "    # ê¸°ë³¸ í•œì˜ ìŒ (ì„ íƒì )\n",
    "    manual_pairs = get_default_korean_english_pairs()\n",
    "    print(f\"  ìˆ˜ë™ ì •ì˜ ìŒ: {len(manual_pairs)}ê°œ\")\n",
    "    \n",
    "    # Check if doc_encoder is available (only after model training)\n",
    "    try:\n",
    "        # doc_encoderê°€ ì •ì˜ë˜ì–´ ìˆìœ¼ë©´ embedding ê¸°ë°˜ ë™ì˜ì–´ ë°œê²¬ ì‚¬ìš©\n",
    "        if 'doc_encoder' in globals():\n",
    "            print(\"  âœ“ doc_encoder ë°œê²¬ - embedding ê¸°ë°˜ ë™ì˜ì–´ ë°œê²¬ ì‚¬ìš©\")\n",
    "            \n",
    "            # í¬ê´„ì ì¸ bilingual ì‚¬ì „ êµ¬ì¶• (ë¹„ì§€ë„)\n",
    "            bilingual_dict = build_comprehensive_bilingual_dictionary(\n",
    "                documents=documents[:5000],  # ìƒ˜í”Œ ì‚¬ìš© (ì†ë„)\n",
    "                token_embeddings=doc_encoder.bert.embeddings.word_embeddings.weight.detach().cpu().numpy(),\n",
    "                tokenizer=tokenizer,\n",
    "                bert_model=doc_encoder.bert,\n",
    "                manual_pairs=manual_pairs,\n",
    "            )\n",
    "        else:\n",
    "            raise NameError(\"doc_encoder not defined\")\n",
    "    except (NameError, AttributeError):\n",
    "        # doc_encoderê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ìˆ˜ë™ ìŒë§Œ ì‚¬ìš©\n",
    "        print(\"  â„¹ï¸  doc_encoder ì—†ìŒ - ê¸°ë³¸ ë™ì˜ì–´ ìŒë§Œ ì‚¬ìš©\")\n",
    "        print(\"     (ëª¨ë¸ í•™ìŠµ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ìë™ ë°œê²¬ ê¸°ëŠ¥ ì‚¬ìš© ê°€ëŠ¥)\")\n",
    "        bilingual_dict = dict(manual_pairs)\n",
    "    \n",
    "    print(f\"  ì „ì²´ bilingual ì‚¬ì „: {len(bilingual_dict):,}ê°œ í•­ëª©\")\n",
    "    \n",
    "    # IDFì— ì ìš© (ë™ì˜ì–´ë¼ë¦¬ IDF ê³µìœ )\n",
    "    idf_token_dict = apply_bilingual_synonyms_to_idf(\n",
    "        idf_dict=idf_token_dict,\n",
    "        bilingual_dict=bilingual_dict,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ“ í•œì˜ ë™ì˜ì–´ IDF ë™ê¸°í™” ì™„ë£Œ\")\n",
    "    print(f\"  ì˜ˆì‹œ: 'ëª¨ë¸' â†” 'model', 'í•™ìŠµ' â†” 'training'\")\n",
    "else:\n",
    "    print(\"\\nâ­ï¸  í•œì˜ ë™ì˜ì–´ í†µí•© ê±´ë„ˆëœ€ (USE_BILINGUAL_SYNONYMS=False)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. OpenSearch ë¬¸ì„œ ì¸ì½”ë” ëª¨ë¸ ì •ì˜\n",
    "\n",
    "**Doc-only mode**ë¥¼ ìœ„í•œ ë¬¸ì„œ ì¸ì½”ë”ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "BERT ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ sparse vectorë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ OpenSearch Document Encoder ì´ˆê¸°í™” ì™„ë£Œ\n",
      "  ëª¨ë¸: klue/bert-base\n",
      "  Vocab size: 32,000\n",
      "  Parameters: 110,650,880\n",
      "  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "class OpenSearchDocEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    OpenSearch Neural Sparse Document Encoder (Doc-only mode)\n",
    "    \n",
    "    ë¬¸ì„œë¥¼ sparse vectorë¡œ ì¸ì½”ë”©í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "    ì¶œë ¥ í˜•ì‹: {\"output\": <sparse_vector>}\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"klue/bert-base\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # BERT ê¸°ë°˜ ì¸ì½”ë”\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.bert = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        \n",
    "        self.vocab_size = self.config.vocab_size\n",
    "        \n",
    "        # Log saturation activation\n",
    "        # log(1 + ReLU(x))\n",
    "        self.activation = lambda x: torch.log1p(torch.relu(x))\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, return_dict=False):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            input_ids: (batch_size, seq_len)\n",
    "            attention_mask: (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            sparse_vector: (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        # BERT MLM head output\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Logits: (batch_size, seq_len, vocab_size)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Apply activation: log(1 + ReLU(logits))\n",
    "        activated = self.activation(logits)\n",
    "        \n",
    "        # Max pooling over sequence length\n",
    "        # (batch_size, seq_len, vocab_size) â†’ (batch_size, vocab_size)\n",
    "        sparse_vector = torch.max(\n",
    "            activated * attention_mask.unsqueeze(-1),\n",
    "            dim=1\n",
    "        ).values\n",
    "        \n",
    "        if return_dict:\n",
    "            return {'output': sparse_vector}\n",
    "        \n",
    "        return sparse_vector\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "doc_encoder = OpenSearchDocEncoder(MODEL_NAME)\n",
    "doc_encoder = doc_encoder.to(device)\n",
    "\n",
    "# Tokenizerì— special tokens ì¶”ê°€ë¡œ ì¸í•´ embedding resize í•„ìš”\n",
    "if len(tokenizer) > doc_encoder.vocab_size:\n",
    "    print(f\"Resizing model embeddings: {doc_encoder.vocab_size} â†’ {len(tokenizer)}\")\n",
    "    doc_encoder.bert.resize_token_embeddings(len(tokenizer))\n",
    "    doc_encoder.vocab_size = len(tokenizer)\n",
    "    print(f\"âœ“ Model embeddings resized\")\n",
    "\n",
    "print(f\"âœ“ OpenSearch Document Encoder ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "print(f\"  ëª¨ë¸: {MODEL_NAME}\")\n",
    "print(f\"  Vocab size: {doc_encoder.vocab_size:,}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in doc_encoder.parameters()):,}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "\n",
    "Query-Document pairsì™€ negative samplingì„ ì¤€ë¹„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Negative sampling ì¤‘ (negatives per query: 2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7615c54ce1400890ba78659283bd08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ì´ 30,000ê°œ pairs (original: 10,000)\n",
      "\n",
      "ğŸ“Š ë°ì´í„°ì…‹ ë¶„í• :\n",
      "  Train: 27,000 pairs\n",
      "  Val: 3,000 pairs\n",
      "\n",
      "âœ“ ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ\n",
      "  Batch size: 16\n",
      "  Train batches: 1,688\n",
      "  Val batches: 188\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SparseEncodingDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    OpenSearch Neural Sparse í•™ìŠµìš© ë°ì´í„°ì…‹\n",
    "    \"\"\"\n",
    "    def __init__(self, qd_pairs, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            qd_pairs: [(query, document, relevance), ...]\n",
    "            tokenizer: Hugging Face tokenizer\n",
    "            max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "        \"\"\"\n",
    "        self.qd_pairs = qd_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.qd_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        query, document, relevance = self.qd_pairs[idx]\n",
    "        \n",
    "        # ì¿¼ë¦¬ í† í°í™” (IDF lookupìš©)\n",
    "        query_encoded = self.tokenizer(\n",
    "            query,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # ë¬¸ì„œ í† í°í™” (ëª¨ë¸ ì¸ì½”ë”©ìš©)\n",
    "        doc_encoded = self.tokenizer(\n",
    "            document,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'query_input_ids': query_encoded['input_ids'].squeeze(0),\n",
    "            'query_attention_mask': query_encoded['attention_mask'].squeeze(0),\n",
    "            'doc_input_ids': doc_encoded['input_ids'].squeeze(0),\n",
    "            'doc_attention_mask': doc_encoded['attention_mask'].squeeze(0),\n",
    "            'relevance': torch.tensor(relevance, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# Negative sampling ì¶”ê°€ (ìµœì í™” ë²„ì „)\n",
    "def add_negative_samples(qd_pairs, documents, num_negatives=1):\n",
    "    \"\"\"\n",
    "    ê° positive pairì— ëŒ€í•´ negative documentsë¥¼ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.\n",
    "    (ì¸ë±ìŠ¤ ê¸°ë°˜ ìƒ˜í”Œë§ìœ¼ë¡œ 100ë°° ì´ìƒ ë¹ ë¦„!)\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ”„ Negative sampling ì¤‘ (negatives per query: {num_negatives})...\")\n",
    "    \n",
    "    # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì¸ë±ìŠ¤ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡)\n",
    "    doc_list = documents if isinstance(documents, list) else list(documents)\n",
    "    n_docs = len(doc_list)\n",
    "    \n",
    "    # ë¬¸ì„œ â†’ ì¸ë±ìŠ¤ ë§¤í•‘ (ë¹ ë¥¸ ê²€ìƒ‰ìš©)\n",
    "    doc_to_idx = {doc: idx for idx, doc in enumerate(doc_list)}\n",
    "    \n",
    "    augmented_pairs = []\n",
    "    \n",
    "    for query, pos_doc, relevance in tqdm(qd_pairs):\n",
    "        # Positive pair ì¶”ê°€\n",
    "        augmented_pairs.append((query, pos_doc, 1.0))\n",
    "        \n",
    "        # Positive ë¬¸ì„œì˜ ì¸ë±ìŠ¤\n",
    "        pos_idx = doc_to_idx.get(pos_doc, -1)\n",
    "        \n",
    "        # Negative sampling (ì¸ë±ìŠ¤ ê¸°ë°˜ - ë§¤ìš° ë¹ ë¦„!)\n",
    "        for _ in range(num_negatives):\n",
    "            # ëœë¤ ì¸ë±ìŠ¤ ì„ íƒ\n",
    "            neg_idx = np.random.randint(0, n_docs)\n",
    "            \n",
    "            # Positiveì™€ ê°™ì€ ì¸ë±ìŠ¤ë©´ ë‹¤ë¥¸ ê²ƒìœ¼ë¡œ êµì²´\n",
    "            if neg_idx == pos_idx:\n",
    "                neg_idx = (neg_idx + 1) % n_docs\n",
    "            \n",
    "            neg_doc = doc_list[neg_idx]\n",
    "            augmented_pairs.append((query, neg_doc, 0.0))\n",
    "    \n",
    "    print(f\"âœ“ ì´ {len(augmented_pairs):,}ê°œ pairs (original: {len(qd_pairs):,})\")\n",
    "    return augmented_pairs\n",
    "\n",
    "# Negative sampling ì ìš©\n",
    "augmented_pairs = add_negative_samples(\n",
    "    korean_data['qd_pairs'][:10000],  # ìƒ˜í”Œë§ (ì „ì²´ëŠ” ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼)\n",
    "    korean_data['documents'],\n",
    "    num_negatives=2\n",
    ")\n",
    "\n",
    "# Train/Val split\n",
    "train_pairs, val_pairs = train_test_split(augmented_pairs, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"\\nğŸ“Š ë°ì´í„°ì…‹ ë¶„í• :\")\n",
    "print(f\"  Train: {len(train_pairs):,} pairs\")\n",
    "print(f\"  Val: {len(val_pairs):,} pairs\")\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„±\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = SparseEncodingDataset(train_pairs, tokenizer, MAX_LENGTH)\n",
    "val_dataset = SparseEncodingDataset(val_pairs, tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nâœ“ ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Train batches: {len(train_loader):,}\")\n",
    "print(f\"  Val batches: {len(val_loader):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ” í† í° ì„ë² ë”© ê¸°ë°˜ ë™ì˜ì–´ ìë™ ë°œê²¬\n",
      "============================================================\n",
      "âœ“ Token embeddings ì¶”ì¶œ ì™„ë£Œ: (32000, 768)\n",
      "  Vocab size: 32,000\n",
      "  Embedding dim: 768\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸ” í† í° ì„ë² ë”© ê¸°ë°˜ ë™ì˜ì–´ ìë™ ë°œê²¬\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# BERT ëª¨ë¸ì€ ì´ë¯¸ doc_encoderì— ë¡œë“œë˜ì–´ ìˆìŒ\n",
    "# Token embedding ì¶”ì¶œ\n",
    "token_embeddings = doc_encoder.bert.bert.embeddings.word_embeddings.weight.detach().cpu().numpy()\n",
    "\n",
    "print(f\"âœ“ Token embeddings ì¶”ì¶œ ì™„ë£Œ: {token_embeddings.shape}\")\n",
    "print(f\"  Vocab size: {token_embeddings.shape[0]:,}\")\n",
    "print(f\"  Embedding dim: {token_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ find_similar_tokens í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def find_similar_tokens(token, tokenizer, embeddings, top_k=10, threshold=0.75):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í† í°ê³¼ ìœ ì‚¬í•œ í† í°ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        token: ê²€ìƒ‰í•  í† í° (ë¬¸ìì—´)\n",
    "        tokenizer: Tokenizer\n",
    "        embeddings: Token embeddings (numpy array)\n",
    "        top_k: ë°˜í™˜í•  ìœ ì‚¬ í† í° ê°œìˆ˜\n",
    "        threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (0~1)\n",
    "    \n",
    "    Returns:\n",
    "        List of (token, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # í† í° -> ID ë³€í™˜\n",
    "    token_id = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(token))\n",
    "    if not token_id:\n",
    "        return []\n",
    "    token_id = token_id[0]  # ì²« ë²ˆì§¸ í† í° ID ì‚¬ìš©\n",
    "    \n",
    "    # Token embedding ì¶”ì¶œ\n",
    "    token_emb = embeddings[token_id]\n",
    "    \n",
    "    # ëª¨ë“  í† í°ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    similarities = np.dot(embeddings, token_emb) / (\n",
    "        np.linalg.norm(embeddings, axis=1) * np.linalg.norm(token_emb) + 1e-10\n",
    "    )\n",
    "    \n",
    "    # ìƒìœ„ top_k+1 ê°œ ì¶”ì¶œ (ìê¸° ìì‹  ì œì™¸)\n",
    "    top_indices = np.argsort(similarities)[-(top_k+1):][::-1]\n",
    "    \n",
    "    similar_tokens = []\n",
    "    for idx in top_indices:\n",
    "        sim_score = float(similarities[idx])\n",
    "        if sim_score >= threshold and int(idx) != token_id:\n",
    "            similar_token = tokenizer.decode([int(idx)]).strip()\n",
    "            # í•„í„°ë§: ë¹ˆ ë¬¸ìì—´, íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆëŠ” ê²½ìš° ì œì™¸\n",
    "            if similar_token and not similar_token in ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']:\n",
    "                similar_tokens.append((similar_token, sim_score))\n",
    "    \n",
    "    return similar_tokens[:top_k]\n",
    "\n",
    "\n",
    "print(\"âœ“ find_similar_tokens í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì†ì‹¤ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "OpenSearch ëª¨ë¸ì˜ í•µì‹¬ ì†ì‹¤ í•¨ìˆ˜:\n",
    "1. **Ranking Loss**: Query-Document similarity\n",
    "2. **IDF-aware Penalty**: ë‚®ì€ IDF í† í° ì–µì œ\n",
    "3. **L0 Regularization**: Sparsity ìœ ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n",
      "  - Ranking Loss (BCE)\n",
      "  - L0 Regularization (Sparsity)\n",
      "  - IDF-aware Penalty (Low-IDF suppression)\n"
     ]
    }
   ],
   "source": [
    "def compute_query_representation(query_tokens, idf_dict, tokenizer):\n",
    "    \"\"\"\n",
    "    ì¿¼ë¦¬ë¥¼ IDF lookupìœ¼ë¡œ sparse vectorë¡œ ë³€í™˜ (Inference-free!)\n",
    "    \n",
    "    Args:\n",
    "        query_tokens: (batch_size, seq_len)\n",
    "        idf_dict: {token_id: idf_score}\n",
    "        tokenizer: Hugging Face tokenizer\n",
    "    \n",
    "    Returns:\n",
    "        query_sparse: (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = query_tokens.shape\n",
    "    vocab_size = len(tokenizer)  # Use len() to include added special tokens\n",
    "    \n",
    "    # Initialize sparse vector\n",
    "    query_sparse = torch.zeros(batch_size, vocab_size, device=query_tokens.device)\n",
    "    \n",
    "    # Fill with IDF weights\n",
    "    for b in range(batch_size):\n",
    "        for token_id in query_tokens[b]:\n",
    "            token_id = token_id.item()\n",
    "            if token_id in idf_dict:\n",
    "                query_sparse[b, token_id] = idf_dict[token_id]\n",
    "    \n",
    "    return query_sparse\n",
    "\n",
    "def neural_sparse_loss(doc_sparse, query_sparse, relevance, idf_dict, \n",
    "                       lambda_l0=1e-3, lambda_idf=1e-2):\n",
    "    \"\"\"\n",
    "    OpenSearch Neural Sparse Loss\n",
    "    \n",
    "    Args:\n",
    "        doc_sparse: (batch_size, vocab_size) - ë¬¸ì„œì˜ sparse representation\n",
    "        query_sparse: (batch_size, vocab_size) - ì¿¼ë¦¬ì˜ sparse representation (IDF lookup)\n",
    "        relevance: (batch_size,) - ê´€ë ¨ë„ ì ìˆ˜ (1.0 or 0.0)\n",
    "        idf_dict: {token_id: idf_score}\n",
    "        lambda_l0: L0 regularization ê°€ì¤‘ì¹˜\n",
    "        lambda_idf: IDF-aware penalty ê°€ì¤‘ì¹˜\n",
    "    \n",
    "    Returns:\n",
    "        total_loss, ranking_loss, l0_loss, idf_penalty\n",
    "    \"\"\"\n",
    "    # 1. Ranking Loss: Dot product similarity\n",
    "    similarity = torch.sum(doc_sparse * query_sparse, dim=-1)\n",
    "    ranking_loss = F.binary_cross_entropy_with_logits(\n",
    "        similarity, relevance, reduction='mean'\n",
    "    )\n",
    "    \n",
    "    # 2. L0 Regularization (FLOPS penalty for sparsity)\n",
    "    l0_loss = torch.mean(torch.sum(torch.abs(doc_sparse), dim=-1))\n",
    "    \n",
    "    # 3. IDF-aware Penalty (suppress low-IDF tokens)\n",
    "    # ë‚®ì€ IDF í† í°ì— í˜ë„í‹° ë¶€ì—¬\n",
    "    idf_tensor = torch.tensor(\n",
    "        [idf_dict.get(i, 1.0) for i in range(doc_sparse.shape[1])],\n",
    "        device=doc_sparse.device\n",
    "    )\n",
    "    \n",
    "    # Inverse IDF penalty: ë‚®ì€ IDF = ë†’ì€ í˜ë„í‹°\n",
    "    inverse_idf = 1.0 / (idf_tensor + 1e-6)\n",
    "    idf_penalty = torch.mean(torch.sum(doc_sparse * inverse_idf, dim=-1))\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = ranking_loss + lambda_l0 * l0_loss + lambda_idf * idf_penalty\n",
    "    \n",
    "    return total_loss, ranking_loss, l0_loss, idf_penalty\n",
    "\n",
    "print(\"âœ“ ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n",
    "print(\"  - Ranking Loss (BCE)\")\n",
    "print(\"  - L0 Regularization (Sparsity)\")\n",
    "print(\"  - IDF-aware Penalty (Low-IDF suppression)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. í•™ìŠµ ì„¤ì • ë° ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\n",
      "  Learning rate: 2e-05\n",
      "  Epochs: 3\n",
      "  Batch size: 16\n",
      "  Max length: 128\n",
      "  Lambda L0: 0.001\n",
      "  Lambda IDF: 0.01\n",
      "  Device: cuda\n",
      "\n",
      "âœ“ Optimizer: AdamW\n",
      "âœ“ Scheduler: Linear warmup (500 steps)\n",
      "âœ“ Total steps: 5,064\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "WARMUP_STEPS = 500\n",
    "LAMBDA_L0 = 1e-3  # L0 regularization\n",
    "LAMBDA_IDF = 1e-2  # IDF-aware penalty\n",
    "\n",
    "print(\"ğŸ¯ í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Max length: {MAX_LENGTH}\")\n",
    "print(f\"  Lambda L0: {LAMBDA_L0}\")\n",
    "print(f\"  Lambda IDF: {LAMBDA_IDF}\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "# Optimizer & Scheduler\n",
    "optimizer = AdamW(doc_encoder.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Optimizer: AdamW\")\n",
    "print(f\"âœ“ Scheduler: Linear warmup ({WARMUP_STEPS} steps)\")\n",
    "print(f\"âœ“ Total steps: {total_steps:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸš€ í•™ìŠµ ì‹œì‘!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, idf_id_dict, tokenizer, device):\n",
    "    \"\"\"\n",
    "    í•œ ì—í­ í•™ìŠµ\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_ranking = 0\n",
    "    total_l0 = 0\n",
    "    total_idf_penalty = 0\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move to device\n",
    "        query_tokens = batch['query_input_ids'].to(device)\n",
    "        doc_input_ids = batch['doc_input_ids'].to(device)\n",
    "        doc_attention_mask = batch['doc_attention_mask'].to(device)\n",
    "        relevance = batch['relevance'].to(device)\n",
    "        \n",
    "        # Document encoding (ëª¨ë¸ë¡œ ì¸ì½”ë”©)\n",
    "        doc_sparse = model(doc_input_ids, doc_attention_mask)\n",
    "        \n",
    "        # Query encoding (IDF lookup - inference-free!)\n",
    "        query_sparse = compute_query_representation(query_tokens, idf_id_dict, tokenizer)\n",
    "        \n",
    "        # Loss ê³„ì‚°\n",
    "        loss, ranking_loss, l0_loss, idf_penalty = neural_sparse_loss(\n",
    "            doc_sparse, query_sparse, relevance, idf_id_dict,\n",
    "            lambda_l0=LAMBDA_L0, lambda_idf=LAMBDA_IDF\n",
    "        )\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # ê¸°ë¡\n",
    "        total_loss += loss.item()\n",
    "        total_ranking += ranking_loss.item()\n",
    "        total_l0 += l0_loss.item()\n",
    "        total_idf_penalty += idf_penalty.item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'rank': f'{ranking_loss.item():.4f}',\n",
    "            'l0': f'{l0_loss.item():.2f}',\n",
    "            'idf': f'{idf_penalty.item():.4f}'\n",
    "        })\n",
    "    \n",
    "    n = len(loader)\n",
    "    return total_loss/n, total_ranking/n, total_l0/n, total_idf_penalty/n\n",
    "\n",
    "def evaluate(model, loader, idf_id_dict, tokenizer, device):\n",
    "    \"\"\"\n",
    "    ê²€ì¦\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            query_tokens = batch['query_input_ids'].to(device)\n",
    "            doc_input_ids = batch['doc_input_ids'].to(device)\n",
    "            doc_attention_mask = batch['doc_attention_mask'].to(device)\n",
    "            relevance = batch['relevance'].to(device)\n",
    "            \n",
    "            doc_sparse = model(doc_input_ids, doc_attention_mask)\n",
    "            query_sparse = compute_query_representation(query_tokens, idf_id_dict, tokenizer)\n",
    "            \n",
    "            loss, _, _, _ = neural_sparse_loss(\n",
    "                doc_sparse, query_sparse, relevance, idf_id_dict,\n",
    "                lambda_l0=LAMBDA_L0, lambda_idf=LAMBDA_IDF\n",
    "            )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# í•™ìŠµ íˆìŠ¤í† ë¦¬\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'ranking_loss': [],\n",
    "    'l0_loss': [],\n",
    "    'idf_penalty': []\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ í•™ìŠµ ì‹œì‘!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Epoch 1/3\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec2eb0e07b34970a7ae4f8528b45a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bb119eb6d449c8847743581f12de69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Epoch 1 ê²°ê³¼:\n",
      "  Train Loss: 7.2215\n",
      "  Val Loss: 0.6931\n",
      "  Ranking Loss: 4.9022\n",
      "  L0 Loss: 905.14\n",
      "  IDF Penalty: 141.4162\n",
      "  âœ“ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥! (val_loss: 0.6931)\n",
      "\n",
      "============================================================\n",
      "Epoch 2/3\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b976fe0f27b40b79ac8e383cb0014dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e33777ee52047739298989992d72552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Epoch 2 ê²°ê³¼:\n",
      "  Train Loss: 0.6931\n",
      "  Val Loss: 0.6931\n",
      "  Ranking Loss: 0.6931\n",
      "  L0 Loss: 0.00\n",
      "  IDF Penalty: 0.0000\n",
      "\n",
      "============================================================\n",
      "Epoch 3/3\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cdd9c2ee499411fba69850c81468493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529229348d8d44c197376927c4d37f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Epoch 3 ê²°ê³¼:\n",
      "  Train Loss: 0.6931\n",
      "  Val Loss: 0.6931\n",
      "  Ranking Loss: 0.6931\n",
      "  L0 Loss: 0.00\n",
      "  IDF Penalty: 0.0000\n",
      "\n",
      "============================================================\n",
      "âœ… í•™ìŠµ ì™„ë£Œ!\n",
      "============================================================\n",
      "Best Validation Loss: 0.6931\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµ ì‹¤í–‰\n",
    "best_val_loss = float('inf')\n",
    "# ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "import os\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "best_model_path = \"./models/best_korean_neural_sparse_encoder.pt\"\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # í•™ìŠµ\n",
    "    train_loss, ranking_loss, l0_loss, idf_penalty = train_epoch(\n",
    "        doc_encoder, train_loader, optimizer, scheduler,\n",
    "        idf_id_dict, tokenizer, device\n",
    "    )\n",
    "    \n",
    "    # ê²€ì¦\n",
    "    val_loss = evaluate(doc_encoder, val_loader, idf_id_dict, tokenizer, device)\n",
    "    \n",
    "    # ê¸°ë¡\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['ranking_loss'].append(ranking_loss)\n",
    "    history['l0_loss'].append(l0_loss)\n",
    "    history['idf_penalty'].append(idf_penalty)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Epoch {epoch + 1} ê²°ê³¼:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Ranking Loss: {ranking_loss:.4f}\")\n",
    "    print(f\"  L0 Loss: {l0_loss:.2f}\")\n",
    "    print(f\"  IDF Penalty: {idf_penalty:.4f}\")\n",
    "    \n",
    "    # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': doc_encoder.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': {\n",
    "                'model_name': MODEL_NAME,\n",
    "                'vocab_size': len(tokenizer),\n",
    "                'max_length': MAX_LENGTH,\n",
    "            }\n",
    "        }, best_model_path)\n",
    "        print(f\"  âœ“ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥! (val_loss: {val_loss:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ëª¨ë¸ ì €ì¥ (OpenSearch í˜¸í™˜ í˜•ì‹)\n",
    "\n",
    "OpenSearchì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤:\n",
    "1. `pytorch_model.bin` - ë¬¸ì„œ ì¸ì½”ë”\n",
    "2. `idf.json` - ì¿¼ë¦¬ìš© í† í° ê°€ì¤‘ì¹˜ lookup table\n",
    "3. Tokenizer íŒŒì¼ë“¤\n",
    "4. `config.json` - ëª¨ë¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ OpenSearch í˜¸í™˜ ëª¨ë¸ ì €ì¥ ì¤‘...\n",
      "\n",
      "âœ“ ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ (Epoch 1, Val Loss: 0.6931)\n",
      "âœ“ pytorch_model.bin ì €ì¥\n",
      "âœ“ idf.json ì €ì¥ (29,241 tokens)\n",
      "âœ“ Tokenizer íŒŒì¼ ì €ì¥\n",
      "âœ“ config.json ì €ì¥\n",
      "âœ“ README.md ìƒì„±\n",
      "\n",
      "============================================================\n",
      "âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\n",
      "============================================================\n",
      "\n",
      "ì €ì¥ ìœ„ì¹˜: ./models/opensearch-korean-neural-sparse-v1/\n",
      "\n",
      "ì €ì¥ëœ íŒŒì¼:\n",
      "  - tokenizer.json                 (    0.72 MB)\n",
      "  - tokenizer_config.json          (    0.00 MB)\n",
      "  - added_tokens.json              (    0.00 MB)\n",
      "  - pytorch_model.bin              (  422.18 MB)\n",
      "  - README.md                      (    0.00 MB)\n",
      "  - idf.json                       (    0.88 MB)\n",
      "  - vocab.txt                      (    0.24 MB)\n",
      "  - special_tokens_map.json        (    0.00 MB)\n",
      "  - config.json                    (    0.00 MB)\n"
     ]
    }
   ],
   "source": [
    "# ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "OUTPUT_DIR = \"./models/opensearch-korean-neural-sparse-v1\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“¦ OpenSearch í˜¸í™˜ ëª¨ë¸ ì €ì¥ ì¤‘...\\n\")\n",
    "\n",
    "# 1. ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ\n",
    "checkpoint = torch.load(best_model_path)\n",
    "doc_encoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"âœ“ ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ (Epoch {checkpoint['epoch'] + 1}, Val Loss: {checkpoint['val_loss']:.4f})\")\n",
    "\n",
    "# 2. pytorch_model.bin ì €ì¥ (ë¬¸ì„œ ì¸ì½”ë”)\n",
    "torch.save(doc_encoder.state_dict(), f\"{OUTPUT_DIR}/pytorch_model.bin\")\n",
    "print(f\"âœ“ pytorch_model.bin ì €ì¥\")\n",
    "\n",
    "# 3. idf.json ì €ì¥ (ì¿¼ë¦¬ìš© ê°€ì¤‘ì¹˜ lookup table)\n",
    "with open(f\"{OUTPUT_DIR}/idf.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(idf_token_dict, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ“ idf.json ì €ì¥ ({len(idf_token_dict):,} tokens)\")\n",
    "\n",
    "# 4. Tokenizer ì €ì¥\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"âœ“ Tokenizer íŒŒì¼ ì €ì¥\")\n",
    "\n",
    "# 5. config.json ì €ì¥\n",
    "model_config = {\n",
    "    \"model_type\": \"opensearch-neural-sparse-doc-encoder\",\n",
    "    \"base_model\": MODEL_NAME,\n",
    "    \"vocab_size\": len(tokenizer),\n",
    "    \"max_seq_length\": MAX_LENGTH,\n",
    "    \"mode\": \"doc-only\",\n",
    "    \"output_format\": \"rank_features\",\n",
    "    \"training_info\": {\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"lambda_l0\": LAMBDA_L0,\n",
    "        \"lambda_idf\": LAMBDA_IDF,\n",
    "        \"training_samples\": len(train_dataset),\n",
    "        \"trained_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    },\n",
    "    \"usage\": {\n",
    "        \"documents\": \"Use pytorch_model.bin to encode documents\",\n",
    "        \"queries\": \"Use tokenizer + idf.json for inference-free query encoding\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/config.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_config, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ“ config.json ì €ì¥\")\n",
    "\n",
    "# 6. README ìƒì„±\n",
    "readme_content = f\"\"\"# OpenSearch Korean Neural Sparse Model v1\n",
    "\n",
    "í•œêµ­ì–´ì— ìµœì í™”ëœ OpenSearch inference-free neural sparse ê²€ìƒ‰ ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "## ëª¨ë¸ êµ¬ì¡°\n",
    "\n",
    "### Doc-only Mode (Inference-Free)\n",
    "- **ë¬¸ì„œ ì¸ì½”ë”©**: BERT ê¸°ë°˜ ì‹ ê²½ë§ (`pytorch_model.bin`)\n",
    "- **ì¿¼ë¦¬ ì¸ì½”ë”©**: Tokenizer + IDF lookup (`idf.json`) - **Inference-Free!**\n",
    "\n",
    "## íŒŒì¼ êµ¬ì¡°\n",
    "\n",
    "```\n",
    "{OUTPUT_DIR}/\n",
    "â”œâ”€â”€ pytorch_model.bin       # ë¬¸ì„œ ì¸ì½”ë” ëª¨ë¸\n",
    "â”œâ”€â”€ idf.json                # ì¿¼ë¦¬ìš© í† í° ê°€ì¤‘ì¹˜ (IDF + íŠ¸ë Œë“œ ë¶€ìŠ¤íŒ…)\n",
    "â”œâ”€â”€ tokenizer.json          # í† í¬ë‚˜ì´ì €\n",
    "â”œâ”€â”€ tokenizer_config.json   # í† í¬ë‚˜ì´ì € ì„¤ì •\n",
    "â”œâ”€â”€ vocab.txt               # ì–´íœ˜ ì‚¬ì „\n",
    "â”œâ”€â”€ special_tokens_map.json # íŠ¹ìˆ˜ í† í°\n",
    "â””â”€â”€ config.json             # ëª¨ë¸ ì„¤ì •\n",
    "```\n",
    "\n",
    "## ì‚¬ìš© ë°©ë²•\n",
    "\n",
    "### 1. OpenSearchì— ëª¨ë¸ ë“±ë¡\n",
    "\n",
    "```bash\n",
    "# ëª¨ë¸ ì••ì¶•\n",
    "cd {OUTPUT_DIR}\n",
    "zip -r ../korean-neural-sparse-v1.zip .\n",
    "\n",
    "# OpenSearchì— ì—…ë¡œë“œ\n",
    "POST /_plugins/_ml/models/_upload\n",
    "{{\n",
    "  \"name\": \"korean-neural-sparse-v1\",\n",
    "  \"version\": \"1.0\",\n",
    "  \"model_format\": \"TORCH_SCRIPT\",\n",
    "  \"model_config\": {{\n",
    "    \"model_type\": \"bert\",\n",
    "    \"embedding_dimension\": {len(tokenizer)},\n",
    "    \"framework_type\": \"sentence_transformers\"\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "### 2. ì¸ë±ìŠ¤ ìƒì„±\n",
    "\n",
    "```json\n",
    "PUT /korean-docs\n",
    "{{\n",
    "  \"mappings\": {{\n",
    "    \"properties\": {{\n",
    "      \"content\": {{ \"type\": \"text\" }},\n",
    "      \"embedding\": {{ \"type\": \"rank_features\" }}\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "### 3. ê²€ìƒ‰\n",
    "\n",
    "```json\n",
    "POST /korean-docs/_search\n",
    "{{\n",
    "  \"query\": {{\n",
    "    \"neural_sparse\": {{\n",
    "      \"embedding\": {{\n",
    "        \"query_text\": \"í•œêµ­ì–´ ê²€ìƒ‰ ìµœì í™”\",\n",
    "        \"model_id\": \"<model_id>\"\n",
    "      }}\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "## í•™ìŠµ ì •ë³´\n",
    "\n",
    "- **Base Model**: {MODEL_NAME}\n",
    "- **Training Samples**: {len(train_dataset):,}\n",
    "- **Epochs**: {NUM_EPOCHS}\n",
    "- **Best Val Loss**: {best_val_loss:.4f}\n",
    "- **Trained Date**: {datetime.now().strftime(\"%Y-%m-%d\")}\n",
    "\n",
    "## íŠ¹ì§•\n",
    "\n",
    "1. **í•œêµ­ì–´ ìµœì í™”**: KLUE-BERT ê¸°ë°˜\n",
    "2. **Inference-Free**: ì¿¼ë¦¬ëŠ” tokenizer + IDF lookupë§Œ ì‚¬ìš©\n",
    "3. **íŠ¸ë Œë“œ í‚¤ì›Œë“œ**: 2024-2025 AI/ML íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë¶€ìŠ¤íŒ…\n",
    "4. **IDF-aware**: ë‚®ì€ IDF í† í° ì–µì œ\n",
    "5. **Sparse**: L0 regularizationìœ¼ë¡œ í¬ì†Œì„± ìœ ì§€\n",
    "\n",
    "## ì°¸ê³ \n",
    "\n",
    "- [OpenSearch Neural Sparse Search](https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/)\n",
    "- [Paper: Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers](https://arxiv.org/abs/2411.04403)\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/README.md\", 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "print(f\"âœ“ README.md ìƒì„±\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}/\")\n",
    "print(f\"\\nì €ì¥ëœ íŒŒì¼:\")\n",
    "for filename in os.listdir(OUTPUT_DIR):\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    size = os.path.getsize(filepath) / (1024*1024)  # MB\n",
    "    print(f\"  - {filename:30s} ({size:>8.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ì €ì¥ëœ ëª¨ë¸ë¡œ inference í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
      "============================================================\n",
      "\n",
      "ğŸ“ ì¿¼ë¦¬ ì¸ì½”ë”© (Inference-Free: Tokenizer + IDF Lookup)\n",
      "\n",
      "Query: ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ\n",
      "  í¬ì†Œì„±: 99.99% (non-zero: 4/32000)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ì‹œìŠ¤í…œ             (10.2104)\n",
      "     2. ê²€ìƒ‰              (7.4010)\n",
      "     3. ì¸ê³µì§€ëŠ¥            (7.1470)\n",
      "     4. ê¸°ë°˜              (5.7630)\n",
      "\n",
      "Query: í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ \n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 5/32000)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. í•œêµ­ì–´             (7.8308)\n",
      "     2. ìì—°              (6.2030)\n",
      "     3. ì²˜ë¦¬              (6.0609)\n",
      "     4. ê¸°ìˆ               (5.1081)\n",
      "     5. ##ì–´             (3.3636)\n",
      "\n",
      "Query: OpenSearch ë²¡í„° ê²€ìƒ‰\n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 7/32000)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ##earch         (10.7212)\n",
      "     2. ë²¡               (9.4219)\n",
      "     3. ê²€ìƒ‰              (7.4010)\n",
      "     4. Op              (5.6756)\n",
      "     5. ##S             (5.3367)\n",
      "     6. ##í„°             (5.0075)\n",
      "     7. ##en            (4.4371)\n",
      "\n",
      "Query: ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ë°©ë²•\n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 6/32000)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ë”¥               (8.4876)\n",
      "     2. í•™ìŠµ              (7.5293)\n",
      "     3. ##ë‹             (6.5941)\n",
      "     4. ëª¨ë¸              (6.0930)\n",
      "     5. ë°©ë²•              (5.7376)\n",
      "     6. ##ëŸ¬             (4.8623)\n",
      "\n",
      "Query: ChatGPT LLM í”„ë¡¬í”„íŠ¸\n",
      "  í¬ì†Œì„±: 99.97% (non-zero: 11/32000)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ##ë¡¬             (8.2645)\n",
      "     2. í”„               (7.3539)\n",
      "     3. ##í”„íŠ¸            (6.9370)\n",
      "     4. ##M             (6.0087)\n",
      "     5. Ch              (5.8849)\n",
      "     6. ##L             (5.4777)\n",
      "     7. ##T             (5.4296)\n",
      "     8. ##G             (5.3367)\n",
      "     9. ##P             (5.0108)\n",
      "    10. L               (4.6390)\n",
      "\n",
      "\n",
      "ğŸ“„ ë¬¸ì„œ ì¸ì½”ë”© (Model Inference)\n",
      "\n",
      "Document: OpenSearchëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural spars...\n",
      "  í¬ì†Œì„±: 100.00% (non-zero: 0/32000)\n",
      "  L1 Norm: 0.00\n",
      "  ìƒìœ„ í† í°:\n",
      "\n",
      "Document: í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤....\n",
      "  í¬ì†Œì„±: 100.00% (non-zero: 0/32000)\n",
      "  L1 Norm: 0.00\n",
      "  ìƒìœ„ í† í°:\n",
      "\n",
      "============================================================\n",
      "âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def encode_document(text, model, tokenizer, device, max_length=128):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œë¥¼ sparse vectorë¡œ ì¸ì½”ë”© (ëª¨ë¸ ì‚¬ìš©)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sparse_vec = model(input_ids, attention_mask)\n",
    "    \n",
    "    return sparse_vec.cpu().numpy()[0]\n",
    "\n",
    "def encode_query_inference_free(text, tokenizer, idf_dict, max_length=128):\n",
    "    \"\"\"\n",
    "    ì¿¼ë¦¬ë¥¼ sparse vectorë¡œ ì¸ì½”ë”© (IDF lookup - Inference-Free!)\n",
    "    \"\"\"\n",
    "    # í† í°í™”\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False, max_length=max_length, truncation=True)\n",
    "    \n",
    "    # IDF lookup\n",
    "    sparse_vec = np.zeros(len(tokenizer))\n",
    "    for token_id in tokens:\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        if token_str in idf_dict:\n",
    "            sparse_vec[token_id] = idf_dict[token_str]\n",
    "    \n",
    "    return sparse_vec\n",
    "\n",
    "def get_top_tokens(sparse_vec, tokenizer, top_k=15):\n",
    "    \"\"\"\n",
    "    Sparse vectorì—ì„œ ìƒìœ„ í† í° ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    top_indices = np.argsort(sparse_vec)[-top_k:][::-1]\n",
    "    top_values = sparse_vec[top_indices]\n",
    "    \n",
    "    top_tokens = []\n",
    "    for idx, val in zip(top_indices, top_values):\n",
    "        if val > 0:\n",
    "            token = tokenizer.decode([idx])\n",
    "            top_tokens.append((token, val))\n",
    "    \n",
    "    return top_tokens\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_queries = [\n",
    "    \"ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ\",\n",
    "    \"í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ \",\n",
    "    \"OpenSearch ë²¡í„° ê²€ìƒ‰\",\n",
    "    \"ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ë°©ë²•\",\n",
    "    \"ChatGPT LLM í”„ë¡¬í”„íŠ¸\",\n",
    "]\n",
    "\n",
    "test_documents = [\n",
    "    \"OpenSearchëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.\",\n",
    "    \"í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤.\",\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ“ ì¿¼ë¦¬ ì¸ì½”ë”© (Inference-Free: Tokenizer + IDF Lookup)\\n\")\n",
    "\n",
    "for query in test_queries:\n",
    "    sparse_vec = encode_query_inference_free(query, tokenizer, idf_token_dict)\n",
    "    \n",
    "    non_zero = np.count_nonzero(sparse_vec)\n",
    "    sparsity = (1 - non_zero / len(sparse_vec)) * 100\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"  í¬ì†Œì„±: {sparsity:.2f}% (non-zero: {non_zero}/{len(sparse_vec)})\")\n",
    "    print(f\"  ìƒìœ„ í† í°:\")\n",
    "    \n",
    "    top_tokens = get_top_tokens(sparse_vec, tokenizer, top_k=10)\n",
    "    for i, (token, value) in enumerate(top_tokens, 1):\n",
    "        print(f\"    {i:2d}. {token:15s} ({value:.4f})\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nğŸ“„ ë¬¸ì„œ ì¸ì½”ë”© (Model Inference)\\n\")\n",
    "\n",
    "for doc in test_documents:\n",
    "    sparse_vec = encode_document(doc, doc_encoder, tokenizer, device)\n",
    "    \n",
    "    non_zero = np.count_nonzero(sparse_vec)\n",
    "    sparsity = (1 - non_zero / len(sparse_vec)) * 100\n",
    "    \n",
    "    print(f\"Document: {doc[:50]}...\")\n",
    "    print(f\"  í¬ì†Œì„±: {sparsity:.2f}% (non-zero: {non_zero}/{len(sparse_vec)})\")\n",
    "    print(f\"  L1 Norm: {np.sum(np.abs(sparse_vec)):.2f}\")\n",
    "    print(f\"  ìƒìœ„ í† í°:\")\n",
    "    \n",
    "    top_tokens = get_top_tokens(sparse_vec, tokenizer, top_k=10)\n",
    "    for i, (token, value) in enumerate(top_tokens, 1):\n",
    "        print(f\"    {i:2d}. {token:15s} ({value:.4f})\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. OpenSearch í†µí•© ê°€ì´ë“œ\n",
    "\n",
    "í•™ìŠµëœ ëª¨ë¸ì„ OpenSearchì— í†µí•©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘     OpenSearch Inference-Free Neural Sparse ëª¨ë¸ í†µí•© ê°€ì´ë“œ  â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "## 1ï¸âƒ£ ëª¨ë¸ íŒ¨í‚¤ì§• ë° ì—…ë¡œë“œ\n",
      "\n",
      "```bash\n",
      "# ëª¨ë¸ ì••ì¶•\n",
      "cd models/opensearch-korean-neural-sparse-v1\n",
      "zip -r ../korean-neural-sparse-v1.zip .\n",
      "\n",
      "# OpenSearchì— ëª¨ë¸ ë“±ë¡\n",
      "POST /_plugins/_ml/models/_upload\n",
      "{\n",
      "  \"name\": \"korean-neural-sparse-doc-v1\",\n",
      "  \"version\": \"1.0\",\n",
      "  \"description\": \"Korean Neural Sparse Model for document encoding\",\n",
      "  \"model_format\": \"TORCH_SCRIPT\",\n",
      "  \"model_config\": {\n",
      "    \"model_type\": \"bert\",\n",
      "    \"embedding_dimension\": 30000,\n",
      "    \"framework_type\": \"sentence_transformers\",\n",
      "    \"all_config\": {\n",
      "      \"mode\": \"doc-only\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "# Tokenizer ëª¨ë¸ ë“±ë¡ (ì¿¼ë¦¬ìš©)\n",
      "POST /_plugins/_ml/models/_upload\n",
      "{\n",
      "  \"name\": \"korean-neural-sparse-tokenizer-v1\",\n",
      "  \"version\": \"1.0\",\n",
      "  \"model_format\": \"TOKENIZER\",\n",
      "  \"model_config\": {\n",
      "    \"model_type\": \"tokenizer\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "## 2ï¸âƒ£ ì¸ë±ìŠ¤ ìƒì„± (rank_features íƒ€ì…)\n",
      "\n",
      "```json\n",
      "PUT /korean-neural-sparse-index\n",
      "{\n",
      "  \"settings\": {\n",
      "    \"index\": {\n",
      "      \"default_pipeline\": \"korean-neural-sparse-ingest\"\n",
      "    }\n",
      "  },\n",
      "  \"mappings\": {\n",
      "    \"properties\": {\n",
      "      \"title\": { \"type\": \"text\" },\n",
      "      \"content\": { \"type\": \"text\" },\n",
      "      \"content_embedding\": {\n",
      "        \"type\": \"rank_features\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "## 3ï¸âƒ£ Ingest Pipeline ì„¤ì •\n",
      "\n",
      "```json\n",
      "PUT /_ingest/pipeline/korean-neural-sparse-ingest\n",
      "{\n",
      "  \"description\": \"Korean neural sparse encoding pipeline\",\n",
      "  \"processors\": [\n",
      "    {\n",
      "      \"sparse_encoding\": {\n",
      "        \"model_id\": \"<doc_model_id>\",\n",
      "        \"field_map\": {\n",
      "          \"content\": \"content_embedding\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "## 4ï¸âƒ£ ë¬¸ì„œ ì¸ë±ì‹±\n",
      "\n",
      "```json\n",
      "POST /korean-neural-sparse-index/_doc\n",
      "{\n",
      "  \"title\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰ ê¸°ìˆ \",\n",
      "  \"content\": \"OpenSearchëŠ” neural sparse ê²€ìƒ‰ì„ ì§€ì›í•˜ëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ì—”ì§„ì…ë‹ˆë‹¤.\"\n",
      "}\n",
      "```\n",
      "\n",
      "## 5ï¸âƒ£ Neural Sparse ê²€ìƒ‰ (Doc-only mode)\n",
      "\n",
      "```json\n",
      "POST /korean-neural-sparse-index/_search\n",
      "{\n",
      "  \"query\": {\n",
      "    \"neural_sparse\": {\n",
      "      \"content_embedding\": {\n",
      "        \"query_text\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰ ìµœì í™”\",\n",
      "        \"model_id\": \"<tokenizer_model_id>\",\n",
      "        \"max_token_score\": 3.5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "## 6ï¸âƒ£ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + Neural Sparse)\n",
      "\n",
      "```json\n",
      "POST /korean-neural-sparse-index/_search\n",
      "{\n",
      "  \"query\": {\n",
      "    \"hybrid\": {\n",
      "      \"queries\": [\n",
      "        {\n",
      "          \"match\": {\n",
      "            \"content\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰\"\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"neural_sparse\": {\n",
      "            \"content_embedding\": {\n",
      "              \"query_text\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰\",\n",
      "              \"model_id\": \"<tokenizer_model_id>\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"search_pipeline\": {\n",
      "    \"phase_results_processors\": [\n",
      "      {\n",
      "        \"normalization-processor\": {\n",
      "          \"normalization\": {\n",
      "            \"technique\": \"min_max\"\n",
      "          },\n",
      "          \"combination\": {\n",
      "            \"technique\": \"arithmetic_mean\",\n",
      "            \"parameters\": {\n",
      "              \"weights\": [0.3, 0.7]\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "## ğŸ“Š ì„±ëŠ¥ íŠ¹ì§•\n",
      "\n",
      "- **ë¬¸ì„œ ì¸ì½”ë”©**: BERT ê¸°ë°˜ ì‹ ê²½ë§ (ëŠë¦¼, ê³ í’ˆì§ˆ)\n",
      "- **ì¿¼ë¦¬ ì¸ì½”ë”©**: Tokenizer + IDF lookup (ë§¤ìš° ë¹ ë¦„, Inference-Free!)\n",
      "- **ì¿¼ë¦¬ ì§€ì—°ì‹œê°„**: BM25ì™€ ê±°ì˜ ë™ì¼ (1.1x)\n",
      "- **ê²€ìƒ‰ ì •í™•ë„**: Siamese sparse ëª¨ë¸ê³¼ ìœ ì‚¬\n",
      "\n",
      "## ğŸ“š ì°¸ê³  ìë£Œ\n",
      "\n",
      "- [OpenSearch Neural Sparse Search](https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/)\n",
      "- [Doc-only Mode](https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/#doc-only-mode)\n",
      "- [Pretrained Models](https://opensearch.org/docs/latest/ml-commons-plugin/pretrained-models/)\n",
      "- [Paper: Inference-Free Learned Sparse Retrievers](https://arxiv.org/abs/2411.04403)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘     OpenSearch Inference-Free Neural Sparse ëª¨ë¸ í†µí•© ê°€ì´ë“œ  â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "## 1ï¸âƒ£ ëª¨ë¸ íŒ¨í‚¤ì§• ë° ì—…ë¡œë“œ\n",
    "\n",
    "```bash\n",
    "# ëª¨ë¸ ì••ì¶•\n",
    "cd models/opensearch-korean-neural-sparse-v1\n",
    "zip -r ../korean-neural-sparse-v1.zip .\n",
    "\n",
    "# OpenSearchì— ëª¨ë¸ ë“±ë¡\n",
    "POST /_plugins/_ml/models/_upload\n",
    "{\n",
    "  \"name\": \"korean-neural-sparse-doc-v1\",\n",
    "  \"version\": \"1.0\",\n",
    "  \"description\": \"Korean Neural Sparse Model for document encoding\",\n",
    "  \"model_format\": \"TORCH_SCRIPT\",\n",
    "  \"model_config\": {\n",
    "    \"model_type\": \"bert\",\n",
    "    \"embedding_dimension\": 30000,\n",
    "    \"framework_type\": \"sentence_transformers\",\n",
    "    \"all_config\": {\n",
    "      \"mode\": \"doc-only\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Tokenizer ëª¨ë¸ ë“±ë¡ (ì¿¼ë¦¬ìš©)\n",
    "POST /_plugins/_ml/models/_upload\n",
    "{\n",
    "  \"name\": \"korean-neural-sparse-tokenizer-v1\",\n",
    "  \"version\": \"1.0\",\n",
    "  \"model_format\": \"TOKENIZER\",\n",
    "  \"model_config\": {\n",
    "    \"model_type\": \"tokenizer\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## 2ï¸âƒ£ ì¸ë±ìŠ¤ ìƒì„± (rank_features íƒ€ì…)\n",
    "\n",
    "```json\n",
    "PUT /korean-neural-sparse-index\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"index\": {\n",
    "      \"default_pipeline\": \"korean-neural-sparse-ingest\"\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"title\": { \"type\": \"text\" },\n",
    "      \"content\": { \"type\": \"text\" },\n",
    "      \"content_embedding\": {\n",
    "        \"type\": \"rank_features\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## 3ï¸âƒ£ Ingest Pipeline ì„¤ì •\n",
    "\n",
    "```json\n",
    "PUT /_ingest/pipeline/korean-neural-sparse-ingest\n",
    "{\n",
    "  \"description\": \"Korean neural sparse encoding pipeline\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"sparse_encoding\": {\n",
    "        \"model_id\": \"<doc_model_id>\",\n",
    "        \"field_map\": {\n",
    "          \"content\": \"content_embedding\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "## 4ï¸âƒ£ ë¬¸ì„œ ì¸ë±ì‹±\n",
    "\n",
    "```json\n",
    "POST /korean-neural-sparse-index/_doc\n",
    "{\n",
    "  \"title\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰ ê¸°ìˆ \",\n",
    "  \"content\": \"OpenSearchëŠ” neural sparse ê²€ìƒ‰ì„ ì§€ì›í•˜ëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ì—”ì§„ì…ë‹ˆë‹¤.\"\n",
    "}\n",
    "```\n",
    "\n",
    "## 5ï¸âƒ£ Neural Sparse ê²€ìƒ‰ (Doc-only mode)\n",
    "\n",
    "```json\n",
    "POST /korean-neural-sparse-index/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"neural_sparse\": {\n",
    "      \"content_embedding\": {\n",
    "        \"query_text\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰ ìµœì í™”\",\n",
    "        \"model_id\": \"<tokenizer_model_id>\",\n",
    "        \"max_token_score\": 3.5\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## 6ï¸âƒ£ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + Neural Sparse)\n",
    "\n",
    "```json\n",
    "POST /korean-neural-sparse-index/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"hybrid\": {\n",
    "      \"queries\": [\n",
    "        {\n",
    "          \"match\": {\n",
    "            \"content\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰\"\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"neural_sparse\": {\n",
    "            \"content_embedding\": {\n",
    "              \"query_text\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰\",\n",
    "              \"model_id\": \"<tokenizer_model_id>\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"search_pipeline\": {\n",
    "    \"phase_results_processors\": [\n",
    "      {\n",
    "        \"normalization-processor\": {\n",
    "          \"normalization\": {\n",
    "            \"technique\": \"min_max\"\n",
    "          },\n",
    "          \"combination\": {\n",
    "            \"technique\": \"arithmetic_mean\",\n",
    "            \"parameters\": {\n",
    "              \"weights\": [0.3, 0.7]\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## ğŸ“Š ì„±ëŠ¥ íŠ¹ì§•\n",
    "\n",
    "- **ë¬¸ì„œ ì¸ì½”ë”©**: BERT ê¸°ë°˜ ì‹ ê²½ë§ (ëŠë¦¼, ê³ í’ˆì§ˆ)\n",
    "- **ì¿¼ë¦¬ ì¸ì½”ë”©**: Tokenizer + IDF lookup (ë§¤ìš° ë¹ ë¦„, Inference-Free!)\n",
    "- **ì¿¼ë¦¬ ì§€ì—°ì‹œê°„**: BM25ì™€ ê±°ì˜ ë™ì¼ (1.1x)\n",
    "- **ê²€ìƒ‰ ì •í™•ë„**: Siamese sparse ëª¨ë¸ê³¼ ìœ ì‚¬\n",
    "\n",
    "## ğŸ“š ì°¸ê³  ìë£Œ\n",
    "\n",
    "- [OpenSearch Neural Sparse Search](https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/)\n",
    "- [Doc-only Mode](https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/#doc-only-mode)\n",
    "- [Pretrained Models](https://opensearch.org/docs/latest/ml-commons-plugin/pretrained-models/)\n",
    "- [Paper: Inference-Free Learned Sparse Retrievers](https://arxiv.org/abs/2411.04403)\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### âœ… ì™„ë£Œëœ ì‘ì—…\n",
    "\n",
    "1. **ë°ì´í„° ìˆ˜ì§‘**: í•œêµ­ì–´ ê³µê°œ ë°ì´í„°ì…‹ (KLUE, KorQuAD, Wikipedia ë“±)\n",
    "2. **IDF ê³„ì‚°**: í† í°ë³„ IDF ê°€ì¤‘ì¹˜ ê³„ì‚° ë° íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…\n",
    "3. **ëª¨ë¸ í•™ìŠµ**: OpenSearch doc-only mode ë¬¸ì„œ ì¸ì½”ë” í•™ìŠµ\n",
    "   - IDF-aware penalty\n",
    "   - L0 regularization\n",
    "   - Ranking loss\n",
    "4. **ëª¨ë¸ ì €ì¥**: OpenSearch í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ì €ì¥\n",
    "   - `pytorch_model.bin` (ë¬¸ì„œ ì¸ì½”ë”)\n",
    "   - `idf.json` (ì¿¼ë¦¬ìš© ê°€ì¤‘ì¹˜)\n",
    "   - Tokenizer íŒŒì¼ë“¤\n",
    "   - `config.json`\n",
    "\n",
    "### ğŸ¯ í•µì‹¬ íŠ¹ì§•\n",
    "\n",
    "- **Inference-Free**: ì¿¼ë¦¬ëŠ” tokenizer + IDF lookupë§Œ ì‚¬ìš© â†’ ë§¤ìš° ë¹ ë¦„!\n",
    "- **í•œêµ­ì–´ ìµœì í™”**: KLUE-BERT ê¸°ë°˜ + í•œêµ­ì–´ ë°ì´í„°ì…‹\n",
    "- **íŠ¸ë Œë“œ ë°˜ì˜**: 2024-2025 AI/ML í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë¶€ìŠ¤íŒ…\n",
    "- **OpenSearch í˜¸í™˜**: ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ í˜•ì‹\n",
    "\n",
    "### ğŸš€ ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "1. **Knowledge Distillation**: Dense + Sparse siamese ëª¨ë¸ë¡œ distillation\n",
    "2. **ë” ë§ì€ ë°ì´í„°**: AI Hub, NIKL ë“± ì¶”ê°€ ë°ì´í„°ì…‹\n",
    "3. **Hard Negative Mining**: In-batch negatives, hard negatives\n",
    "4. **ëª¨ë¸ í‰ê°€**: BEIR ë²¤ì¹˜ë§ˆí¬, MRR, NDCG ë“±\n",
    "5. **OpenSearch ë°°í¬**: ì‹¤ì œ ê²€ìƒ‰ ì‹œìŠ¤í…œì— í†µí•©\n",
    "6. **A/B í…ŒìŠ¤íŒ…**: ê¸°ì¡´ BM25ì™€ ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "### ğŸ“ˆ ê¸°ëŒ€ íš¨ê³¼\n",
    "\n",
    "- BM25 ëŒ€ë¹„ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ\n",
    "- Dense retrieval ëŒ€ë¹„ ë¹ ë¥¸ ì†ë„\n",
    "- í•œêµ­ì–´ íŠ¹í™” ê²€ìƒ‰ ì„±ëŠ¥ ê°œì„ \n",
    "- íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê²€ìƒ‰ ìµœì í™”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ë°ì´í„° ì €ì¥ (Notebook ê°„ ê³µìœ )\n",
    "\n",
    "í•™ìŠµëœ ë°ì´í„°ë¥¼ `dataset/base_model/` ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "ë‹¤ìŒ ë…¸íŠ¸ë¶ì—ì„œ ì´ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved JSON: dataset/base_model/documents.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('dataset/base_model/documents.json')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. í•œêµ­ì–´ ë¬¸ì„œ ë°ì´í„° ì €ì¥\n",
    "dm.save_json(\n",
    "    documents,\n",
    "    \"documents.json\",\n",
    "    \"base_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved Pickle: dataset/base_model/idf_statistics.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('dataset/base_model/idf_statistics.pkl')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. IDF í†µê³„ ì €ì¥\n",
    "dm.save_pickle(\n",
    "    idf_id_dict,\n",
    "    \"idf_statistics.pkl\",\n",
    "    \"base_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved Pickle: dataset/base_model/qd_pairs_base.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('dataset/base_model/qd_pairs_base.pkl')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. ê¸°ë³¸ Query-Document ìŒ ì €ì¥\n",
    "dm.save_pickle(\n",
    "    augmented_pairs,\n",
    "    \"qd_pairs_base.pkl\",\n",
    "    \"base_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved JSON: dataset/base_model/bilingual_synonyms.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('dataset/base_model/bilingual_synonyms.json')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. ì´ì¤‘ì–¸ì–´ ë™ì˜ì–´ ì‚¬ì „ ì €ì¥\n",
    "dm.save_json(\n",
    "    bilingual_dict,\n",
    "    \"bilingual_synonyms.json\",\n",
    "    \"base_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved Model: dataset/base_model/neural_sparse_v1_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('dataset/base_model/neural_sparse_v1_model')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. í•™ìŠµëœ Neural Sparse ëª¨ë¸ ì €ì¥\n",
    "dm.save_model(\n",
    "    doc_encoder,\n",
    "    tokenizer,\n",
    "    \"neural_sparse_v1_model\",\n",
    "    \"base_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ“Š Dataset Summary\n",
      "======================================================================\n",
      "Base path: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset\n",
      "Total datasets: 5\n",
      "\n",
      "Datasets by directory:\n",
      "\n",
      "  ğŸ“ base_model/\n",
      "     - documents.json                           (           json,   48.6 MB)\n",
      "     - idf_statistics.pkl                       (         pickle,    0.3 MB)\n",
      "     - qd_pairs_base.pkl                        (         pickle,   29.2 MB)\n",
      "     - bilingual_synonyms.json                  (           json,    0.0 MB)\n",
      "     - neural_sparse_v1_model                   (  pytorch_model,  423.1 MB)\n",
      "     Total:                                                     501.3 MB\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ì €ì¥ëœ ë°ì´í„° ìš”ì•½\n",
    "dm.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Notebook 1 ì™„ë£Œ\n",
    "\n",
    "ëª¨ë“  ê¸°ë³¸ ëª¨ë¸ ë°ì´í„°ê°€ `dataset/base_model/`ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„\n",
    "ì´ì œ `02_llm_synthetic_data_generation.ipynb`ë¥¼ ì‹¤í–‰í•˜ì—¬ LLM ê¸°ë°˜ í•©ì„± ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

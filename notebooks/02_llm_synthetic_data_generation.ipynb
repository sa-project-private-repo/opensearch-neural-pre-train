{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. LLM Synthetic Data Generation\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ LLMì„ ì‚¬ìš©í•˜ì—¬ í•©ì„± ì¿¼ë¦¬ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì „ì œì¡°ê±´\n",
    "ë¨¼ì € `01_neural_sparse_base_training.ipynb`ë¥¼ ì‹¤í–‰í•˜ì—¬ ê¸°ë³¸ ë°ì´í„°ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ëª©í‘œ\n",
    "- LLM ëª¨ë¸ ë¡œë”© (Qwen2.5-14B-Instruct BF16)\n",
    "- í•œêµ­ì–´ ë¬¸ì„œ ê¸°ë°˜ í•©ì„± ì¿¼ë¦¬ ìƒì„±\n",
    "- ë™ì˜ì–´ í™•ì¥ ë° ê²€ì¦\n",
    "\n",
    "## ì¶œë ¥ ë°ì´í„°\n",
    "ëª¨ë“  ë°ì´í„°ëŠ” `dataset/llm_generated/` ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤:\n",
    "- `synthetic_qd_pairs.pkl`: LLM ìƒì„± Query-Document ìŒ\n",
    "- `enhanced_synonyms.json`: LLM ê²€ì¦ ë™ì˜ì–´ ì‚¬ì „\n",
    "\n",
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "ì´ ë…¸íŠ¸ë¶ ì‹¤í–‰ í›„ `03_llm_enhanced_training.ipynb`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BF16 ëª¨ë¸ì€ ìˆœìˆ˜ PyTorchë¡œ ì‘ë™í•˜ë¯€ë¡œ Triton ë¶ˆí•„ìš”\n",
    "print(\"âœ… Using BF16 model - no Triton configuration needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DatasetManager ì´ˆê¸°í™”\n",
    "from src.dataset_manager import DatasetManager\n",
    "\n",
    "dm = DatasetManager(base_path=\"dataset\")\n",
    "print(\"âœ… DatasetManager initialized\")\n",
    "print(f\"ğŸ“ Base path: {dm.base_path.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ ë°ì´í„° íŒŒì¼ í™•ì¸\n",
    "required_files = [\n",
    "    (\"base_model\", \"documents.json\"),\n",
    "    (\"base_model\", \"bilingual_synonyms.json\"),\n",
    "]\n",
    "\n",
    "if not dm.check_dependencies(required_files):\n",
    "    raise RuntimeError(\n",
    "        \"Required data files not found. \"\n",
    "        \"Please run 01_neural_sparse_base_training.ipynb first.\"\n",
    "    )\n",
    "\n",
    "print(\"\\nâœ… Ready to proceed with LLM data generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook 1ì—ì„œ ìƒì„±ëœ ë°ì´í„° ë¡œë“œ\n",
    "print(\"Loading data from notebook 1...\\n\")\n",
    "\n",
    "documents = dm.load_json(\"documents.json\", \"base_model\")\n",
    "bilingual_dict = dm.load_json(\"bilingual_synonyms.json\", \"base_model\")\n",
    "\n",
    "print(f\"\\nâœ… Loaded {len(documents)} documents\")\n",
    "print(f\"âœ… Loaded bilingual dictionary with {len(bilingual_dict)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ†• 13. LLM ëª¨ë¸ ë¡œë”© ë° ì´ˆê¸°í™”\n",
    "\n",
    "ì´ ì„¹ì…˜ë¶€í„°ëŠ” LLM ê¸°ë°˜ í™•ì¥ ê¸°ëŠ¥ì…ë‹ˆë‹¤:\n",
    "- í•©ì„± Query-Document ìŒ ìƒì„±\n",
    "- í•œì˜ ë™ì˜ì–´ ìë™ ê²€ì¦\n",
    "- ì„±ëŠ¥ ê°œì„ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 13: LLM Model Loading\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ¤– ì„¹ì…˜ 13: LLM ëª¨ë¸ ë¡œë”© ë° ì´ˆê¸°í™”\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from src.llm_loader import load_qwen3_awq, check_gpu_memory\n",
    "\n",
    "# GPU ë©”ëª¨ë¦¬ ì²´í¬\n",
    "check_gpu_memory()\n",
    "\n",
    "# Qwen2.5-14B-Instruct BF16 ëª¨ë¸ ë¡œë”© (ìˆœìˆ˜ PyTorch, Triton ë¶ˆí•„ìš”)\n",
    "print(\"\\nâš ï¸  ì°¸ê³ : ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"   ëª¨ë¸: Qwen2.5-14B-Instruct\")\n",
    "print(\"   í¬ê¸°: ~28GB (BF16 precision)\")\n",
    "print(\"   íŠ¹ì§•: ìˆœìˆ˜ PyTorch, ì•ˆì •ì , Triton ë¶ˆí•„ìš”\")\n",
    "print(\"   ì˜ˆìƒ ë¡œë”© ì‹œê°„: ~10-15ë¶„\")\n",
    "\n",
    "llm_model, llm_tokenizer = load_qwen3_awq(\n",
    "    model_name=\"Qwen/Qwen2.5-14B-Instruct\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\")\n",
    "print(\"   ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í•©ì„± ë°ì´í„°ë¥¼ ìƒì„±í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"   ì˜ˆìƒ ì¶”ë¡  ì†ë„: ~20ì´ˆ/ë¬¸ì„œ (3ê°œ ì¿¼ë¦¬ ìƒì„±)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ†• 14. LLM ê¸°ë°˜ í•©ì„± Query-Document Pairs ìƒì„±\n",
    "\n",
    "LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¡œë¶€í„° ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì—­ìƒì„±í•©ë‹ˆë‹¤.\n",
    "ì´ë¥¼ í†µí•´ í•™ìŠµ ë°ì´í„°ë¥¼ ëŒ€í­ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\nprint(\"ğŸ“ ì„¹ì…˜ 14: LLM ê¸°ë°˜ í•©ì„± Query-Document Pairs ìƒì„±\")\nprint(\"=\"*70)\n\nfrom src.synthetic_data_generator import generate_synthetic_qd_pairs\n\n# ê¸°ì¡´ ë¬¸ì„œì—ì„œ í•©ì„± ì¿¼ë¦¬ ìƒì„±\n# ì²˜ìŒ 1000ê°œ ë¬¸ì„œ ì‚¬ìš© (ì‹œê°„ ê³ ë ¤)\nprint(f\"\\nğŸ“Š í•©ì„± ë°ì´í„° ìƒì„± ì„¤ì •:\")\nprint(f\"   - ì‚¬ìš©í•  ë¬¸ì„œ: 1,000ê°œ\")\nprint(f\"   - ë¬¸ì„œë‹¹ ì¿¼ë¦¬: 3ê°œ\")\nprint(f\"   - ì˜ˆìƒ ìƒì„± pairs: ~3,000ê°œ\")\nprint(f\"   - ëª¨ë¸: Qwen2.5-14B-Instruct (BF16)\")\nprint(f\"   - ì˜ˆìƒ ì†Œìš” ì‹œê°„: ~5-6ì‹œê°„ (ì•½ 20ì´ˆ/ë¬¸ì„œ)\")\nprint(f\"\\nğŸ’¡ íŒ: í…ŒìŠ¤íŠ¸ ì‹œ max_documents=10 ì •ë„ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤.\")\n\nsynthetic_pairs = generate_synthetic_qd_pairs(\n    documents=documents[:1000],  # ì²˜ìŒ 1000ê°œ ë¬¸ì„œ\n    llm_model=llm_model,\n    llm_tokenizer=llm_tokenizer,\n    num_queries_per_doc=3,\n    batch_size=2,\n    max_documents=1000,\n    enable_filtering=True,\n    verbose=True,  # ìƒì„¸ ë¡œê·¸ í™œì„±í™”\n)\n\nprint(f\"\\nâœ… í•©ì„± ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(synthetic_pairs):,}ê°œ pairs\")\n\n# ìƒ˜í”Œ ì¶œë ¥\nprint(\"\\nğŸ“‹ ìƒì„±ëœ í•©ì„± ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ):\")\nprint(\"=\"*70)\nfor i, (query, doc, relevance) in enumerate(synthetic_pairs[:5], 1):\n    print(f\"\\n{i}. Query: {query}\")\n    print(f\"   Document: {doc[:100]}...\")\n    print(f\"   Relevance: {relevance}\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ†• 15. LLM ê¸°ë°˜ í•œì˜ ë™ì˜ì–´ ê²€ì¦ ë° í™•ì¥\n",
    "\n",
    "ê¸°ì¡´ ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ë°œê²¬í•œ ë™ì˜ì–´ë¥¼ LLMìœ¼ë¡œ ê²€ì¦í•˜ì—¬ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸŒ ì„¹ì…˜ 15: LLM ê¸°ë°˜ í•œì˜ ë™ì˜ì–´ ê²€ì¦ ë° í™•ì¥\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from src.cross_lingual_synonyms import enhance_bilingual_dict_with_llm\n",
    "\n",
    "# ê¸°ì¡´ ì„ë² ë”© ê¸°ë°˜ ë™ì˜ì–´ë¥¼ LLMìœ¼ë¡œ ê²€ì¦\n",
    "print(f\"\\nğŸ“Š ë™ì˜ì–´ ê²€ì¦ ì„¤ì •:\")\n",
    "print(f\"   - ê¸°ì¡´ ë™ì˜ì–´ ì‚¬ì „: {len(bilingual_dict):,}ê°œ í•­ëª©\")\n",
    "print(f\"   - ê²€ì¦í•  í•­ëª©: ìƒìœ„ 100ê°œ\")\n",
    "print(f\"   - ê²€ì¦ ì„ê³„ê°’: 0.8\")\n",
    "print(f\"   - ì˜ˆìƒ ì†Œìš” ì‹œê°„: ~5-10ë¶„\")\n",
    "\n",
    "enhanced_bilingual_dict = enhance_bilingual_dict_with_llm(\n",
    "    initial_dict=bilingual_dict,  # Cell 14ì—ì„œ ìƒì„±ëœ ê¸°ì¡´ ì‚¬ì „\n",
    "    llm_model=llm_model,\n",
    "    llm_tokenizer=llm_tokenizer,\n",
    "    verification_threshold=0.8,\n",
    "    max_verify=100,  # ìƒìœ„ 100ê°œë§Œ ê²€ì¦ (ì‹œê°„ ì ˆì•½)\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… LLM ê²€ì¦ ì™„ë£Œ!\")\n",
    "print(f\"   ê¸°ì¡´ ì‚¬ì „: {len(bilingual_dict):,}ê°œ\")\n",
    "print(f\"   ê²€ì¦ í›„: {len(enhanced_bilingual_dict):,}ê°œ\")\n",
    "\n",
    "# ê²€ì¦ ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥\n",
    "print(\"\\nğŸ“‹ ê²€ì¦ëœ ë™ì˜ì–´ ìƒ˜í”Œ:\")\n",
    "print(\"=\"*70)\n",
    "sample_count = 0\n",
    "for korean, english_list in list(enhanced_bilingual_dict.items())[:10]:\n",
    "    sample_count += 1\n",
    "    print(f\"{sample_count}. {korean} â†” {', '.join(english_list)}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ì—…ë°ì´íŠ¸ëœ ë™ì˜ì–´ ì‚¬ì „ì„ bilingual_dictì— ë°˜ì˜\n",
    "bilingual_dict = enhanced_bilingual_dict\n",
    "print(f\"\\nâœ“ ë™ì˜ì–´ ì‚¬ì „ ì—…ë°ì´íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ë°ì´í„° ì €ì¥ (Notebook ê°„ ê³µìœ )\n",
    "\n",
    "LLMìœ¼ë¡œ ìƒì„±ëœ ë°ì´í„°ë¥¼ `dataset/llm_generated/` ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "ë‹¤ìŒ ë…¸íŠ¸ë¶ì—ì„œ ì´ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ enhanced ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LLM ìƒì„± Query-Document ìŒ ì €ì¥\n",
    "dm.save_pickle(\n",
    "    synthetic_pairs,\n",
    "    \"synthetic_qd_pairs.pkl\",\n",
    "    \"llm_generated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LLM ê²€ì¦ ë™ì˜ì–´ ì‚¬ì „ ì €ì¥\n",
    "dm.save_json(\n",
    "    enhanced_bilingual_dict,\n",
    "    \"enhanced_synonyms.json\",\n",
    "    \"llm_generated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì €ì¥ëœ ë°ì´í„° ìš”ì•½\n",
    "dm.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Notebook 2 ì™„ë£Œ\n",
    "\n",
    "ëª¨ë“  LLM ìƒì„± ë°ì´í„°ê°€ `dataset/llm_generated/`ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„\n",
    "ì´ì œ `03_llm_enhanced_training.ipynb`ë¥¼ ì‹¤í–‰í•˜ì—¬ enhanced ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì„±ëŠ¥ì„ ë¹„êµí•˜ì„¸ìš”."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
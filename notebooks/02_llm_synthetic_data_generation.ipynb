{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. LLM Synthetic Data Generation\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ LLMì„ ì‚¬ìš©í•˜ì—¬ í•©ì„± ì¿¼ë¦¬ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì „ì œì¡°ê±´\n",
    "ë¨¼ì € `01_neural_sparse_base_training.ipynb`ë¥¼ ì‹¤í–‰í•˜ì—¬ ê¸°ë³¸ ë°ì´í„°ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ëª©í‘œ\n",
    "- LLM ëª¨ë¸ ë¡œë”© (Qwen3-30B-A3B-Thinking-2507-FP8)\n",
    "- í•œêµ­ì–´ ë¬¸ì„œ ê¸°ë°˜ í•©ì„± ì¿¼ë¦¬ ìƒì„±\n",
    "- ë™ì˜ì–´ í™•ì¥ ë° ê²€ì¦\n",
    "\n",
    "## ì¶œë ¥ ë°ì´í„°\n",
    "ëª¨ë“  ë°ì´í„°ëŠ” `dataset/llm_generated/` ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤:\n",
    "- `synthetic_qd_pairs.pkl`: LLM ìƒì„± Query-Document ìŒ\n",
    "- `enhanced_synonyms.json`: LLM ê²€ì¦ ë™ì˜ì–´ ì‚¬ì „\n",
    "\n",
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "ì´ ë…¸íŠ¸ë¶ ì‹¤í–‰ í›„ `03_llm_enhanced_training.ipynb`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Triton compilation disabled for ARM compatibility\n"
     ]
    }
   ],
   "source": [
    "# Triton ì»´íŒŒì¼ ë¹„í™œì„±í™” (ARM aarch64 í˜¸í™˜ì„±)\n",
    "import os\n",
    "\n",
    "os.environ[\"TRITON_INTERPRET\"] = \"1\"\n",
    "os.environ[\"DISABLE_TRITON\"] = \"1\"\n",
    "\n",
    "print(\"âœ… Triton compilation disabled for ARM compatibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DatasetManager initialized\n",
      "ğŸ“ Base path: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset\n"
     ]
    }
   ],
   "source": [
    "# DatasetManager ì´ˆê¸°í™”\n",
    "from src.dataset_manager import DatasetManager\n",
    "\n",
    "dm = DatasetManager(base_path=\"dataset\")\n",
    "print(\"âœ… DatasetManager initialized\")\n",
    "print(f\"ğŸ“ Base path: {dm.base_path.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All dependencies satisfied\n",
      "\n",
      "âœ… Ready to proceed with LLM data generation\n"
     ]
    }
   ],
   "source": [
    "# í•„ìˆ˜ ë°ì´í„° íŒŒì¼ í™•ì¸\n",
    "required_files = [\n",
    "    (\"base_model\", \"documents.json\"),\n",
    "    (\"base_model\", \"bilingual_synonyms.json\"),\n",
    "]\n",
    "\n",
    "if not dm.check_dependencies(required_files):\n",
    "    raise RuntimeError(\n",
    "        \"Required data files not found. \"\n",
    "        \"Please run 01_neural_sparse_base_training.ipynb first.\"\n",
    "    )\n",
    "\n",
    "print(\"\\nâœ… Ready to proceed with LLM data generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from notebook 1...\n",
      "\n",
      "âœ“ Loaded JSON: dataset/base_model/documents.json\n",
      "âœ“ Loaded JSON: dataset/base_model/bilingual_synonyms.json\n",
      "\n",
      "âœ… Loaded 127910 documents\n",
      "âœ… Loaded bilingual dictionary with 32 entries\n",
      "âœ“ Loaded JSON: dataset/base_model/documents.json\n",
      "âœ“ Loaded JSON: dataset/base_model/bilingual_synonyms.json\n",
      "\n",
      "âœ… Loaded 127910 documents\n",
      "âœ… Loaded bilingual dictionary with 32 entries\n"
     ]
    }
   ],
   "source": [
    "# Notebook 1ì—ì„œ ìƒì„±ëœ ë°ì´í„° ë¡œë“œ\n",
    "print(\"Loading data from notebook 1...\\n\")\n",
    "\n",
    "documents = dm.load_json(\"documents.json\", \"base_model\")\n",
    "bilingual_dict = dm.load_json(\"bilingual_synonyms.json\", \"base_model\")\n",
    "\n",
    "print(f\"\\nâœ… Loaded {len(documents)} documents\")\n",
    "print(f\"âœ… Loaded bilingual dictionary with {len(bilingual_dict)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ†• 13. LLM ëª¨ë¸ ë¡œë”© ë° ì´ˆê¸°í™”\n",
    "\n",
    "ì´ ì„¹ì…˜ë¶€í„°ëŠ” LLM ê¸°ë°˜ í™•ì¥ ê¸°ëŠ¥ì…ë‹ˆë‹¤:\n",
    "- í•©ì„± Query-Document ìŒ ìƒì„±\n",
    "- í•œì˜ ë™ì˜ì–´ ìë™ ê²€ì¦\n",
    "- ì„±ëŠ¥ ê°œì„ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ¤– ì„¹ì…˜ 13: LLM ëª¨ë¸ ë¡œë”© ë° ì´ˆê¸°í™”\n",
      "======================================================================\n",
      "======================================================================\n",
      "ğŸ–¥ï¸  GPU Memory Status\n",
      "======================================================================\n",
      "\n",
      "GPU 0: NVIDIA GB10\n",
      "  Total:       119.70 GB\n",
      "  Allocated:     0.00 GB\n",
      "  Free:        119.70 GB\n",
      "======================================================================\n",
      "âš ï¸  ì°¸ê³ : ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "   ëª¨ë¸ í¬ê¸°: ~30GB (FP8 quantization, ARM aarch64 compatible)\n",
      "\n",
      "======================================================================\n",
      "ğŸ“¥ Loading Qwen3 Model (FP8 Quantization)\n",
      "======================================================================\n",
      "Model: Qwen/Qwen3-30B-A3B-Thinking-2507-FP8\n",
      "Device map: auto\n",
      "ğŸ’¡ Using FP8 quantization (ARM aarch64 compatible)\n",
      "\n",
      "1ï¸âƒ£ Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/west/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  queued_call()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Tokenizer loaded (vocab size: 151,669)\n",
      "\n",
      "2ï¸âƒ£ Loading model with FP8 quantization...\n",
      "   Using Transformers native FP8 support (no external deps needed)\n",
      "   This may take a few minutes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba050d6b4acd4070b7f6447f12b856eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded\n",
      "\n",
      "ğŸ“Š GPU Memory after loading: 30.22 GB\n",
      "\n",
      "======================================================================\n",
      "âœ… Qwen3 Model Ready!\n",
      "======================================================================\n",
      "\n",
      "âœ… LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n",
      "   ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í•©ì„± ë°ì´í„°ë¥¼ ìƒì„±í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# Section 13: LLM Model Loading\n",
    "import os\n",
    "\n",
    "# Disable Triton JIT compilation to avoid Python.h dependency\n",
    "os.environ[\"TRITON_DISABLE_LINE_INFO\"] = \"1\"\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = \"/tmp/triton_cache\"\n",
    "os.environ[\"TRITON_INTERPRET\"] = \"1\"  # ì¸í„°í”„ë¦¬í„° ëª¨ë“œ ì‚¬ìš©\n",
    "os.environ[\"DISABLE_TRITON\"] = \"1\"    # ì™„ì „ ë¹„í™œì„±í™”\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ¤– ì„¹ì…˜ 13: LLM ëª¨ë¸ ë¡œë”© ë° ì´ˆê¸°í™”\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from src.llm_loader import load_qwen3_awq, check_gpu_memory\n",
    "\n",
    "# GPU ë©”ëª¨ë¦¬ ì²´í¬\n",
    "check_gpu_memory()\n",
    "\n",
    "# Qwen3-30B-A3B-Thinking-2507-FP8 ëª¨ë¸ ë¡œë”© (FP8 quantization, ARM compatible)\n",
    "# Qwen3-30B-A3B-Thinking-2507-FP8 ëª¨ë¸ ë¡œë”© (FP8 quantization, ARM compatible)\n",
    "print(\"âš ï¸  ì°¸ê³ : ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"   ëª¨ë¸ í¬ê¸°: ~30GB (FP8 quantization, ARM aarch64 compatible)\")\n",
    "\n",
    "llm_model, llm_tokenizer = load_qwen3_awq(\n",
    "    model_name=\"Qwen/Qwen3-30B-A3B-Thinking-2507-FP8\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\")\n",
    "print(\"   ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í•©ì„± ë°ì´í„°ë¥¼ ìƒì„±í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ†• 14. LLM ê¸°ë°˜ í•©ì„± Query-Document Pairs ìƒì„±\n",
    "\n",
    "LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¡œë¶€í„° ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì—­ìƒì„±í•©ë‹ˆë‹¤.\n",
    "ì´ë¥¼ í†µí•´ í•™ìŠµ ë°ì´í„°ë¥¼ ëŒ€í­ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ“ ì„¹ì…˜ 14: LLM ê¸°ë°˜ í•©ì„± Query-Document Pairs ìƒì„±\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š í•©ì„± ë°ì´í„° ìƒì„± ì„¤ì •:\n",
      "   - ì‚¬ìš©í•  ë¬¸ì„œ: 1,000ê°œ\n",
      "   - ë¬¸ì„œë‹¹ ì¿¼ë¦¬: 3ê°œ\n",
      "   - ì˜ˆìƒ ìƒì„± pairs: ~3,000ê°œ\n",
      "   - ì˜ˆìƒ ì†Œìš” ì‹œê°„: ~15-30ë¶„ (GPU ì†ë„ì— ë”°ë¼)\n",
      "\n",
      "======================================================================\n",
      "ğŸ“ Generating Synthetic Query-Document Pairs\n",
      "======================================================================\n",
      "Start time: 2025-11-15 18:42:31\n",
      "Documents: 1000\n",
      "Queries per doc: 3\n",
      "Expected total queries: 3000\n",
      "Quality filtering: ON\n",
      "Verbose logging: ON\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating queries:   0%|                                                  | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ“Š Progress Report - Document 1/1000\n",
      "======================================================================\n",
      "â±ï¸  Elapsed: 0.0m | ETA: 0.0m\n",
      "âœ… Generated: 0 pairs\n",
      "âŒ Failed/Filtered: 0\n",
      "âš¡ Avg time/doc: 0.00s\n",
      "======================================================================\n",
      "\n",
      "ğŸ“„ Doc 1: Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters...\n",
      "   ğŸ¤– Calling LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating queries:   0%|                                                  | 0/1000 [05:28<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   - ì˜ˆìƒ ìƒì„± pairs: ~3,000ê°œ\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   - ì˜ˆìƒ ì†Œìš” ì‹œê°„: ~15-30ë¶„ (GPU ì†ë„ì— ë”°ë¼)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m synthetic_pairs = \u001b[43mgenerate_synthetic_qd_pairs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ì²˜ìŒ 1000ê°œ ë¬¸ì„œ\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm_tokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_queries_per_doc\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_documents\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_filtering\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ… í•©ì„± ë°ì´í„° ìƒì„± ì™„ë£Œ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(synthetic_pairs)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mê°œ pairs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# ìƒ˜í”Œ ì¶œë ¥\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/src/synthetic_data_generator.py:307\u001b[39m, in \u001b[36mgenerate_synthetic_qd_pairs\u001b[39m\u001b[34m(documents, llm_model, llm_tokenizer, num_queries_per_doc, batch_size, max_documents, enable_filtering, verbose)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;66;03m# Generate queries\u001b[39;00m\n\u001b[32m    306\u001b[39m query_gen_start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m queries = \u001b[43mgenerate_queries_from_document\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm_tokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_queries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_queries_per_doc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m query_gen_time = time.time() - query_gen_start\n\u001b[32m    314\u001b[39m total_generation_time += query_gen_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/src/synthetic_data_generator.py:87\u001b[39m, in \u001b[36mgenerate_queries_from_document\u001b[39m\u001b[34m(document, llm_model, llm_tokenizer, num_queries, max_new_tokens, temperature, verbose)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Generate\u001b[39;00m\n\u001b[32m     86\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m generated = \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m gen_time = time.time() - start_time\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/src/llm_loader.py:240\u001b[39m, in \u001b[36mgenerate_text\u001b[39m\u001b[34m(model, tokenizer, prompt, max_new_tokens, temperature, top_p, do_sample)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available():\n\u001b[32m    238\u001b[39m     inputs = {k: v.cuda() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# Decode and remove prompt\u001b[39;00m\n\u001b[32m    250\u001b[39m full_text = tokenizer.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2784\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2781\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m2784\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1783\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1783\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1794\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1790\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1792\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1793\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1796\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1797\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/transformers/models/qwen3_moe/modeling_qwen3_moe.py:650\u001b[39m, in \u001b[36mQwen3MoeForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_router_logits, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    645\u001b[39m output_router_logits = (\n\u001b[32m    646\u001b[39m     output_router_logits \u001b[38;5;28;01mif\u001b[39;00m output_router_logits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_router_logits\n\u001b[32m    647\u001b[39m )\n\u001b[32m    649\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m650\u001b[39m outputs: MoeModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_router_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_router_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    662\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    663\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1783\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1783\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1794\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1790\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1792\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1793\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1796\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1797\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/transformers/models/qwen3_moe/modeling_qwen3_moe.py:487\u001b[39m, in \u001b[36mQwen3MoeModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    484\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m MoeModelOutputWithPast(  \u001b[38;5;66;03m# only diff with Mistral is the output type, we need MoE\u001b[39;00m\n\u001b[32m    501\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    502\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    503\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1783\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1783\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1794\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1790\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1792\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1793\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1796\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1797\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/transformers/models/qwen3_moe/modeling_qwen3_moe.py:359\u001b[39m, in \u001b[36mQwen3MoeDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, position_ids, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    357\u001b[39m residual = hidden_states\n\u001b[32m    358\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[38;5;66;03m# For the MoE layers, we need to unpack\u001b[39;00m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(hidden_states, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1783\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1783\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1794\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1790\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1792\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1793\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1796\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1797\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/transformers/models/qwen3_moe/modeling_qwen3_moe.py:258\u001b[39m, in \u001b[36mQwen3MoeSparseMoeBlock.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# Index the correct hidden states and compute the expert hidden state for\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# the current expert. We need to make sure to multiply the output hidden\u001b[39;00m\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# states by `routing_weights` on the corresponding tokens (top-1 and top-2)\u001b[39;00m\n\u001b[32m    257\u001b[39m current_state = hidden_states[\u001b[38;5;28;01mNone\u001b[39;00m, top_x].reshape(-\u001b[32m1\u001b[39m, hidden_dim)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m current_hidden_states = \u001b[43mexpert_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m)\u001b[49m * routing_weights[top_x, idx, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m# However `index_add_` only support torch tensors for indexing so we'll use\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[38;5;66;03m# the `top_x` tensor here.\u001b[39;00m\n\u001b[32m    262\u001b[39m final_hidden_states.index_add_(\u001b[32m0\u001b[39m, top_x, current_hidden_states.to(hidden_states.dtype))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1783\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1783\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1794\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1790\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1792\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1793\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1796\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1797\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/transformers/models/qwen3_moe/modeling_qwen3_moe.py:209\u001b[39m, in \u001b[36mQwen3MoeMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28mself\u001b[39m.gate_proj(x)) * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1783\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1783\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1794\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1790\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1792\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1793\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1796\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1797\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/transformers/integrations/finegrained_fp8.py:340\u001b[39m, in \u001b[36mFP8Linear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch_accelerator_module.device(\u001b[38;5;28minput\u001b[39m.device):\n\u001b[32m    339\u001b[39m     qinput, scale = act_quant(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m.block_size[\u001b[32m1\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m     output = \u001b[43mw8a8_block_fp8_matmul_triton\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mqinput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight_scale_inv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[38;5;66;03m# Blocks the CPU until all accelerator operations on the specified device are complete. It is used to ensure that the results of the\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[38;5;66;03m# preceding operations are ready before proceeding\u001b[39;00m\n\u001b[32m    350\u001b[39m torch_accelerator_module.synchronize()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/transformers/integrations/finegrained_fp8.py:197\u001b[39m, in \u001b[36mw8a8_block_fp8_matmul_triton\u001b[39m\u001b[34m(A, B, As, Bs, block_size, output_dtype)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgrid\u001b[39m(META):\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (triton.cdiv(M, META[\u001b[33m\"\u001b[39m\u001b[33mBLOCK_SIZE_M\u001b[39m\u001b[33m\"\u001b[39m]) * triton.cdiv(N, META[\u001b[33m\"\u001b[39m\u001b[33mBLOCK_SIZE_N\u001b[39m\u001b[33m\"\u001b[39m]),)\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[43m_w8a8_block_fp8_matmul\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mAs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_n\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mA\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mA\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mAs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mAs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBLOCK_SIZE_M\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBLOCK_SIZE_M\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBLOCK_SIZE_N\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBLOCK_SIZE_N\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBLOCK_SIZE_K\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBLOCK_SIZE_K\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mGROUP_SIZE_M\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m C\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/triton/runtime/interpreter.py:1288\u001b[39m, in \u001b[36mGridExecutor.__call__\u001b[39m\u001b[34m(self, *args_dev, **kwargs)\u001b[39m\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InterpreterError(\u001b[38;5;28mrepr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1287\u001b[39m \u001b[38;5;66;03m# copy arguments back to propagate side-effects\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1288\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_restore_args_dev\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_dev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_hst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_hst\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/triton/runtime/interpreter.py:1254\u001b[39m, in \u001b[36mGridExecutor._restore_args_dev\u001b[39m\u001b[34m(self, args_dev, args_hst, kwargs, kwargs_hst)\u001b[39m\n\u001b[32m   1251\u001b[39m     _from_cpu(kwarg_dev, kwarg_hst)\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (arg_dev, arg_hst) \u001b[38;5;129;01min\u001b[39;00m storages.values():\n\u001b[32m-> \u001b[39m\u001b[32m1254\u001b[39m     \u001b[43marg_dev\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_hst\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“ ì„¹ì…˜ 14: LLM ê¸°ë°˜ í•©ì„± Query-Document Pairs ìƒì„±\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from src.synthetic_data_generator import generate_synthetic_qd_pairs\n",
    "\n",
    "# ê¸°ì¡´ ë¬¸ì„œì—ì„œ í•©ì„± ì¿¼ë¦¬ ìƒì„±\n",
    "# ì²˜ìŒ 1000ê°œ ë¬¸ì„œ ì‚¬ìš© (ì‹œê°„ ê³ ë ¤)\n",
    "print(f\"\\nğŸ“Š í•©ì„± ë°ì´í„° ìƒì„± ì„¤ì •:\")\n",
    "print(f\"   - ì‚¬ìš©í•  ë¬¸ì„œ: 1,000ê°œ\")\n",
    "print(f\"   - ë¬¸ì„œë‹¹ ì¿¼ë¦¬: 3ê°œ\")\n",
    "print(f\"   - ì˜ˆìƒ ìƒì„± pairs: ~3,000ê°œ\")\n",
    "print(f\"   - ì˜ˆìƒ ì†Œìš” ì‹œê°„: ~15-30ë¶„ (GPU ì†ë„ì— ë”°ë¼)\")\n",
    "\n",
    "synthetic_pairs = generate_synthetic_qd_pairs(\n",
    "    documents=documents[:1000],  # ì²˜ìŒ 1000ê°œ ë¬¸ì„œ\n",
    "    llm_model=llm_model,\n",
    "    llm_tokenizer=llm_tokenizer,\n",
    "    num_queries_per_doc=3,\n",
    "    batch_size=2,\n",
    "    max_documents=1000,\n",
    "    enable_filtering=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… í•©ì„± ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(synthetic_pairs):,}ê°œ pairs\")\n",
    "\n",
    "# ìƒ˜í”Œ ì¶œë ¥\n",
    "print(\"\\nğŸ“‹ ìƒì„±ëœ í•©ì„± ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ):\")\n",
    "print(\"=\"*70)\n",
    "for i, (query, doc, relevance) in enumerate(synthetic_pairs[:5], 1):\n",
    "    print(f\"\\n{i}. Query: {query}\")\n",
    "    print(f\"   Document: {doc[:100]}...\")\n",
    "    print(f\"   Relevance: {relevance}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ†• 15. LLM ê¸°ë°˜ í•œì˜ ë™ì˜ì–´ ê²€ì¦ ë° í™•ì¥\n",
    "\n",
    "ê¸°ì¡´ ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ë°œê²¬í•œ ë™ì˜ì–´ë¥¼ LLMìœ¼ë¡œ ê²€ì¦í•˜ì—¬ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸŒ ì„¹ì…˜ 15: LLM ê¸°ë°˜ í•œì˜ ë™ì˜ì–´ ê²€ì¦ ë° í™•ì¥\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from src.cross_lingual_synonyms import enhance_bilingual_dict_with_llm\n",
    "\n",
    "# ê¸°ì¡´ ì„ë² ë”© ê¸°ë°˜ ë™ì˜ì–´ë¥¼ LLMìœ¼ë¡œ ê²€ì¦\n",
    "print(f\"\\nğŸ“Š ë™ì˜ì–´ ê²€ì¦ ì„¤ì •:\")\n",
    "print(f\"   - ê¸°ì¡´ ë™ì˜ì–´ ì‚¬ì „: {len(bilingual_dict):,}ê°œ í•­ëª©\")\n",
    "print(f\"   - ê²€ì¦í•  í•­ëª©: ìƒìœ„ 100ê°œ\")\n",
    "print(f\"   - ê²€ì¦ ì„ê³„ê°’: 0.8\")\n",
    "print(f\"   - ì˜ˆìƒ ì†Œìš” ì‹œê°„: ~5-10ë¶„\")\n",
    "\n",
    "enhanced_bilingual_dict = enhance_bilingual_dict_with_llm(\n",
    "    initial_dict=bilingual_dict,  # Cell 14ì—ì„œ ìƒì„±ëœ ê¸°ì¡´ ì‚¬ì „\n",
    "    llm_model=llm_model,\n",
    "    llm_tokenizer=llm_tokenizer,\n",
    "    verification_threshold=0.8,\n",
    "    max_verify=100,  # ìƒìœ„ 100ê°œë§Œ ê²€ì¦ (ì‹œê°„ ì ˆì•½)\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… LLM ê²€ì¦ ì™„ë£Œ!\")\n",
    "print(f\"   ê¸°ì¡´ ì‚¬ì „: {len(bilingual_dict):,}ê°œ\")\n",
    "print(f\"   ê²€ì¦ í›„: {len(enhanced_bilingual_dict):,}ê°œ\")\n",
    "\n",
    "# ê²€ì¦ ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥\n",
    "print(\"\\nğŸ“‹ ê²€ì¦ëœ ë™ì˜ì–´ ìƒ˜í”Œ:\")\n",
    "print(\"=\"*70)\n",
    "sample_count = 0\n",
    "for korean, english_list in list(enhanced_bilingual_dict.items())[:10]:\n",
    "    sample_count += 1\n",
    "    print(f\"{sample_count}. {korean} â†” {', '.join(english_list)}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ì—…ë°ì´íŠ¸ëœ ë™ì˜ì–´ ì‚¬ì „ì„ bilingual_dictì— ë°˜ì˜\n",
    "bilingual_dict = enhanced_bilingual_dict\n",
    "print(f\"\\nâœ“ ë™ì˜ì–´ ì‚¬ì „ ì—…ë°ì´íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ë°ì´í„° ì €ì¥ (Notebook ê°„ ê³µìœ )\n",
    "\n",
    "LLMìœ¼ë¡œ ìƒì„±ëœ ë°ì´í„°ë¥¼ `dataset/llm_generated/` ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "ë‹¤ìŒ ë…¸íŠ¸ë¶ì—ì„œ ì´ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ enhanced ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LLM ìƒì„± Query-Document ìŒ ì €ì¥\n",
    "dm.save_pickle(\n",
    "    synthetic_pairs,\n",
    "    \"synthetic_qd_pairs.pkl\",\n",
    "    \"llm_generated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LLM ê²€ì¦ ë™ì˜ì–´ ì‚¬ì „ ì €ì¥\n",
    "dm.save_json(\n",
    "    enhanced_bilingual_dict,\n",
    "    \"enhanced_synonyms.json\",\n",
    "    \"llm_generated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì €ì¥ëœ ë°ì´í„° ìš”ì•½\n",
    "dm.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Notebook 2 ì™„ë£Œ\n",
    "\n",
    "ëª¨ë“  LLM ìƒì„± ë°ì´í„°ê°€ `dataset/llm_generated/`ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„\n",
    "ì´ì œ `03_llm_enhanced_training.ipynb`ë¥¼ ì‹¤í–‰í•˜ì—¬ enhanced ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì„±ëŠ¥ì„ ë¹„êµí•˜ì„¸ìš”."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

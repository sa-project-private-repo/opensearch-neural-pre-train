{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. LLM Enhanced Training\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ LLM ìƒì„± ë°ì´í„°ë¡œ enhanced ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì „ì œì¡°ê±´\n",
    "ë¨¼ì € ë‹¤ìŒ ë…¸íŠ¸ë¶ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤:\n",
    "1. `01_neural_sparse_base_training.ipynb` - ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ\n",
    "2. `02_llm_synthetic_data_generation.ipynb` - LLM ë°ì´í„° ìƒì„±\n",
    "\n",
    "## ëª©í‘œ\n",
    "- ê¸°ë³¸ ë°ì´í„° + LLM ìƒì„± ë°ì´í„° ê²°í•©\n",
    "- Enhanced Neural Sparse ëª¨ë¸ í•™ìŠµ\n",
    "- Base vs Enhanced ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "## ì¶œë ¥ ë°ì´í„°\n",
    "ëª¨ë“  ë°ì´í„°ëŠ” `dataset/enhanced_model/` ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤:\n",
    "- `neural_sparse_v2_model/`: Enhanced ëª¨ë¸\n",
    "- `evaluation/performance_comparison.json`: ì„±ëŠ¥ ë¹„êµ ê²°ê³¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DatasetManager initialized\n",
      "ğŸ“ Base path: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset\n"
     ]
    }
   ],
   "source": [
    "# DatasetManager ì´ˆê¸°í™”\n",
    "from src.dataset_manager import DatasetManager\n",
    "\n",
    "dm = DatasetManager(base_path=\"dataset\")\n",
    "print(\"âœ… DatasetManager initialized\")\n",
    "print(f\"ğŸ“ Base path: {dm.base_path.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All dependencies satisfied\n",
      "\n",
      "âœ… Ready to proceed with enhanced training\n"
     ]
    }
   ],
   "source": [
    "# í•„ìˆ˜ ë°ì´í„° íŒŒì¼ í™•ì¸\n",
    "required_files = [\n",
    "    (\"base_model\", \"documents.json\"),\n",
    "    (\"base_model\", \"qd_pairs_base.pkl\"),\n",
    "    (\"base_model\", \"neural_sparse_v1_model\"),\n",
    "    (\"llm_generated\", \"synthetic_qd_pairs.pkl\"),\n",
    "    (\"llm_generated\", \"enhanced_synonyms.json\"),\n",
    "]\n",
    "\n",
    "if not dm.check_dependencies(required_files):\n",
    "    raise RuntimeError(\n",
    "        \"Required data files not found. \"\n",
    "        \"Please run notebooks 1 and 2 first.\"\n",
    "    )\n",
    "\n",
    "print(\"\\nâœ… Ready to proceed with enhanced training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from previous notebooks...\n",
      "\n",
      "âœ“ Loaded JSON: dataset/base_model/documents.json\n",
      "âœ“ Loaded Pickle: dataset/base_model/qd_pairs_base.pkl\n",
      "âœ“ Loaded Pickle: dataset/llm_generated/synthetic_qd_pairs.pkl\n",
      "âœ“ Loaded JSON: dataset/llm_generated/enhanced_synonyms.json\n",
      "\n",
      "âœ… Loaded 127910 documents\n",
      "âœ… Loaded 30000 base QD pairs\n",
      "âœ… Loaded 32 synthetic QD pairs\n",
      "âœ… Loaded enhanced synonyms with 32 entries\n"
     ]
    }
   ],
   "source": [
    "# ì´ì „ ë…¸íŠ¸ë¶ë“¤ì—ì„œ ìƒì„±ëœ ë°ì´í„° ë¡œë“œ\n",
    "print(\"Loading data from previous notebooks...\\n\")\n",
    "\n",
    "# From notebook 1\n",
    "documents = dm.load_json(\"documents.json\", \"base_model\")\n",
    "qd_pairs_base = dm.load_pickle(\"qd_pairs_base.pkl\", \"base_model\")\n",
    "\n",
    "# From notebook 2\n",
    "synthetic_qd_pairs = dm.load_pickle(\"synthetic_qd_pairs.pkl\", \"llm_generated\")\n",
    "enhanced_synonyms = dm.load_json(\"enhanced_synonyms.json\", \"llm_generated\")\n",
    "\n",
    "print(f\"\\nâœ… Loaded {len(documents)} documents\")\n",
    "print(f\"âœ… Loaded {len(qd_pairs_base)} base QD pairs\")\n",
    "print(f\"âœ… Loaded {len(synthetic_qd_pairs)} synthetic QD pairs\")\n",
    "print(f\"âœ… Loaded enhanced synonyms with {len(enhanced_synonyms)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Base ëª¨ë¸ ë¡œë“œ (ì„±ëŠ¥ ë¹„êµìš©)\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\n\nprint(\"Loading base model for comparison...\")\n\nmodel_path = dm.base_path / \"base_model\" / \"neural_sparse_v1_model\"\n\nbase_tokenizer = AutoTokenizer.from_pretrained(str(model_path))\nbase_model = AutoModel.from_pretrained(str(model_path))\n\n# GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ ì´ë™\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbase_model = base_model.to(device)\nbase_model.eval()\n\nprint(f\"âœ… Base model loaded for comparison\")\nprint(f\"   Device: {device}\")\nprint(f\"   Model type: {type(base_model).__name__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ†• 16. ìƒì„±ëœ í•©ì„± ë°ì´í„° ë¶„ì„\n\nLLMìœ¼ë¡œ ìƒì„±í•œ í•©ì„± ë°ì´í„°ì˜ í’ˆì§ˆê³¼ í†µê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\nprint(\"ğŸ“Š ì„¹ì…˜ 16: ìƒì„±ëœ í•©ì„± ë°ì´í„° ë¶„ì„\")\nprint(\"=\"*70)\n\n# ê¸°ì¡´ ë°ì´í„° + í•©ì„± ë°ì´í„° í†µê³„\nprint(f\"\\nğŸ“Š ë°ì´í„° í†µê³„:\")\nprint(f\"   ê¸°ë³¸ QD pairs: {len(qd_pairs_base):,}ê°œ\")\nprint(f\"   í•©ì„± QD pairs: {len(synthetic_qd_pairs):,}ê°œ\")\nprint(f\"   ì´ ë°ì´í„°: {len(qd_pairs_base) + len(synthetic_qd_pairs):,}ê°œ\")\n\nif len(synthetic_qd_pairs) > 0:\n    increase_rate = len(synthetic_qd_pairs) / len(qd_pairs_base) * 100\n    print(f\"   ì¦ê°€ìœ¨: +{increase_rate:.2f}%\")\nelse:\n    print(f\"   âš ï¸  í•©ì„± ë°ì´í„°ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n\n# í•©ì„± ë°ì´í„° ìƒ˜í”Œ í™•ì¸\nif len(synthetic_qd_pairs) > 0:\n    print(f\"\\nğŸ“‹ í•©ì„± Query-Document Pairs ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ):\")\n    print(\"=\"*70)\n    for i, (query, doc, relevance) in enumerate(synthetic_qd_pairs[:5], 1):\n        print(f\"\\n{i}. Query: {query}\")\n        print(f\"   Document: {doc[:100]}...\")\n        print(f\"   Relevance: {relevance}\")\n    print(\"=\"*70)\n\n# ë™ì˜ì–´ ì‚¬ì „ ë¹„êµ\nprint(f\"\\nğŸ“Š ë™ì˜ì–´ ì‚¬ì „ í†µê³„:\")\nprint(f\"   Enhanced synonyms: {len(enhanced_synonyms):,}ê°œ í•­ëª©\")\nprint(f\"\\nìƒ˜í”Œ ë™ì˜ì–´:\")\nfor i, (korean, english_list) in enumerate(list(enhanced_synonyms.items())[:5], 1):\n    print(f\"   {i}. {korean} â†’ {', '.join(english_list)}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… ë°ì´í„° ë¶„ì„ ì™„ë£Œ\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ†• 17. ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ë° í–¥í›„ ê³„íš\n\nìƒì„±ëœ ë°ì´í„°ì˜ í’ˆì§ˆì„ ë¶„ì„í•˜ê³  ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê³„íší•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\nprint(\"ğŸ“Š ì„¹ì…˜ 17: ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ë° í–¥í›„ ê³„íš\")\nprint(\"=\"*70)\n\n# ìƒì„±ëœ ì¿¼ë¦¬ í’ˆì§ˆ ë¶„ì„\nprint(\"\\n1ï¸âƒ£ í•©ì„± ë°ì´í„° í’ˆì§ˆ ë¶„ì„\")\nprint(\"-\" * 70)\n\nif len(synthetic_qd_pairs) > 0:\n    # ì¿¼ë¦¬ ê¸¸ì´ ë¶„ì„\n    query_lengths = [len(q) for q, d, r in synthetic_qd_pairs]\n    avg_query_length = sum(query_lengths) / len(query_lengths)\n    \n    print(f\"   í‰ê·  ì¿¼ë¦¬ ê¸¸ì´: {avg_query_length:.1f} ì\")\n    print(f\"   ìµœì†Œ ì¿¼ë¦¬ ê¸¸ì´: {min(query_lengths)} ì\")\n    print(f\"   ìµœëŒ€ ì¿¼ë¦¬ ê¸¸ì´: {max(query_lengths)} ì\")\n    \n    # ë¬¸ì„œ ê¸¸ì´ ë¶„ì„\n    doc_lengths = [len(d) for q, d, r in synthetic_qd_pairs]\n    avg_doc_length = sum(doc_lengths) / len(doc_lengths)\n    \n    print(f\"\\n   í‰ê·  ë¬¸ì„œ ê¸¸ì´: {avg_doc_length:.1f} ì\")\n    print(f\"   ìµœì†Œ ë¬¸ì„œ ê¸¸ì´: {min(doc_lengths)} ì\")\n    print(f\"   ìµœëŒ€ ë¬¸ì„œ ê¸¸ì´: {max(doc_lengths)} ì\")\n    \n    # Relevance ë¶„í¬\n    relevance_scores = [r for q, d, r in synthetic_qd_pairs]\n    unique_scores = set(relevance_scores)\n    \n    print(f\"\\n   Relevance ì ìˆ˜ ë¶„í¬:\")\n    for score in sorted(unique_scores):\n        count = relevance_scores.count(score)\n        percentage = count / len(relevance_scores) * 100\n        print(f\"     {score}: {count}ê°œ ({percentage:.1f}%)\")\nelse:\n    print(\"   âš ï¸  í•©ì„± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n\n# ë™ì˜ì–´ ì‚¬ì „ ê°œì„  ë¶„ì„\nprint(f\"\\n2ï¸âƒ£ ë™ì˜ì–´ ì‚¬ì „ ë¶„ì„\")\nprint(\"-\" * 70)\nprint(f\"   LLM ê²€ì¦ ë™ì˜ì–´: {len(enhanced_synonyms):,}ê°œ í•­ëª©\")\n\n# í–¥í›„ ê°œì„  ê³„íš\nprint(f\"\\n3ï¸âƒ£ í–¥í›„ ê°œì„  ê³„íš\")\nprint(\"-\" * 70)\nprint(f\"\"\"\n   ğŸ“Œ ë°ì´í„° í™•ì¥:\n      - í˜„ì¬: {len(synthetic_qd_pairs)}ê°œ í•©ì„± pairs (í…ŒìŠ¤íŠ¸)\n      - ëª©í‘œ: 1,000+ í•©ì„± pairs (ë…¸íŠ¸ë¶ 2ì—ì„œ max_documents ì¦ê°€)\n   \n   ğŸ“Œ ëª¨ë¸ ì¬í•™ìŠµ:\n      - ì¶©ë¶„í•œ í•©ì„± ë°ì´í„° ìƒì„± í›„ ì§„í–‰\n      - ê¸°ë³¸ ëª¨ë¸ + í•©ì„± ë°ì´í„°ë¡œ enhanced ëª¨ë¸ í•™ìŠµ\n      - ì„±ëŠ¥ ë¹„êµ ë° í‰ê°€\n   \n   ğŸ“Œ ë°°í¬:\n      - Enhanced ëª¨ë¸ì„ OpenSearchì— ë°°í¬\n      - ì‹¤ì œ ê²€ìƒ‰ ì„œë¹„ìŠ¤ì—ì„œ ì„±ëŠ¥ ê²€ì¦\n\"\"\")\n\nprint(\"=\"*70)\nprint(\"âœ… ë¶„ì„ ì™„ë£Œ\")\nprint(\"=\"*70)\n\nprint(\"\\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:\")\nprint(\"   1. ë…¸íŠ¸ë¶ 2ë¡œ ëŒì•„ê°€ì„œ max_documentsë¥¼ 1000ìœ¼ë¡œ ì¦ê°€\")\nprint(\"   2. ì¶©ë¶„í•œ í•©ì„± ë°ì´í„° ìƒì„±\")\nprint(\"   3. ì´ ë…¸íŠ¸ë¶ìœ¼ë¡œ ëŒì•„ì™€ì„œ enhanced ëª¨ë¸ í•™ìŠµ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# ë°ì´í„° ì €ì¥ (ë¶„ì„ ê²°ê³¼)\n\nìƒì„±ëœ ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ `dataset/llm_generated/` ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ë°ì´í„° ë¶„ì„ ê²°ê³¼ ì €ì¥\nimport json\n\nanalysis_results = {\n    \"base_qd_pairs_count\": len(qd_pairs_base),\n    \"synthetic_qd_pairs_count\": len(synthetic_qd_pairs),\n    \"enhanced_synonyms_count\": len(enhanced_synonyms),\n    \"total_documents\": len(documents),\n}\n\nif len(synthetic_qd_pairs) > 0:\n    query_lengths = [len(q) for q, d, r in synthetic_qd_pairs]\n    analysis_results[\"synthetic_data_quality\"] = {\n        \"avg_query_length\": sum(query_lengths) / len(query_lengths),\n        \"min_query_length\": min(query_lengths),\n        \"max_query_length\": max(query_lengths),\n    }\n\ndm.save_json(\n    analysis_results,\n    \"data_analysis.json\",\n    \"llm_generated\"\n)\n\nprint(\"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ\")\nprint(f\"   ìœ„ì¹˜: dataset/llm_generated/data_analysis.json\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ê²°í•©ëœ ë°ì´í„°ì…‹ ì €ì¥ (í–¥í›„ í•™ìŠµìš©)\ncombined_qd_pairs = qd_pairs_base + synthetic_qd_pairs\n\ndm.save_pickle(\n    combined_qd_pairs,\n    \"combined_qd_pairs.pkl\",\n    \"llm_generated\"\n)\n\nprint(f\"âœ… ê²°í•©ëœ ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ\")\nprint(f\"   ìœ„ì¹˜: dataset/llm_generated/combined_qd_pairs.pkl\")\nprint(f\"   ì´ ê°œìˆ˜: {len(combined_qd_pairs):,}ê°œ pairs\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ë°ì´í„°ì…‹ ìš”ì•½\n",
    "dm.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## âœ… Notebook 3 ì™„ë£Œ - ë°ì´í„° ë¶„ì„\n\nLLMìœ¼ë¡œ ìƒì„±í•œ í•©ì„± ë°ì´í„°ì˜ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n\n### ì™„ë£Œëœ ì‘ì—…\n1. âœ… Base ëª¨ë¸ ë°ì´í„° ë¡œë“œ (Notebook 1)\n2. âœ… LLM í•©ì„± ë°ì´í„° ë¡œë“œ (Notebook 2)\n3. âœ… ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ë° í†µê³„\n\n### ì €ì¥ëœ ë°ì´í„°\n- Combined Dataset: `dataset/llm_generated/combined_qd_pairs.pkl`\n- Analysis Results: `dataset/llm_generated/data_analysis.json`\n\n### ë‹¤ìŒ ë‹¨ê³„ (í–¥í›„ ì‘ì—…)\n1. ë…¸íŠ¸ë¶ 2ì—ì„œ max_documentsë¥¼ ëŠ˜ë ¤ ë” ë§ì€ í•©ì„± ë°ì´í„° ìƒì„±\n2. ì¶©ë¶„í•œ ë°ì´í„°ê°€ ëª¨ì´ë©´ Enhanced ëª¨ë¸ ì¬í•™ìŠµ ì§„í–‰\n3. ì„±ëŠ¥ ë¹„êµ ë° OpenSearch ë°°í¬\n\n### í˜„ì¬ ìƒíƒœ\n- í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì†ŒëŸ‰ì˜ í•©ì„± ë°ì´í„°ë§Œ ìƒì„±ë¨\n- ì‹¤ì œ ëª¨ë¸ ì¬í•™ìŠµì„ ìœ„í•´ì„œëŠ” ë” ë§ì€ ë°ì´í„° í•„ìš”"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03. LLM Enhanced Training\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ LLM ìƒì„± ë°ì´í„°ë¡œ enhanced ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.\n",
        "\n",
        "## ì „ì œì¡°ê±´\n",
        "ë¨¼ì € ë‹¤ìŒ ë…¸íŠ¸ë¶ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤:\n",
        "1. `01_neural_sparse_base_training.ipynb` - ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ\n",
        "2. `02_llm_synthetic_data_generation.ipynb` - LLM ë°ì´í„° ìƒì„±\n",
        "\n",
        "## ëª©í‘œ\n",
        "- ê¸°ë³¸ ë°ì´í„° + LLM ìƒì„± ë°ì´í„° ê²°í•©\n",
        "- Enhanced Neural Sparse ëª¨ë¸ í•™ìŠµ\n",
        "- Base vs Enhanced ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\n",
        "\n",
        "## ì¶œë ¥ ë°ì´í„°\n",
        "ëª¨ë“  ë°ì´í„°ëŠ” `dataset/enhanced_model/` ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤:\n",
        "- `neural_sparse_v2_model/`: Enhanced ëª¨ë¸\n",
        "- `evaluation/performance_comparison.json`: ì„±ëŠ¥ ë¹„êµ ê²°ê³¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# DatasetManager ì´ˆê¸°í™”\n",
        "from src.dataset_manager import DatasetManager\n",
        "\n",
        "dm = DatasetManager(base_path=\"dataset\")\n",
        "print(\"âœ… DatasetManager initialized\")\n",
        "print(f\"ğŸ“ Base path: {dm.base_path.absolute()}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# í•„ìˆ˜ ë°ì´í„° íŒŒì¼ í™•ì¸\n",
        "required_files = [\n",
        "    (\"base_model\", \"korean_documents.json\"),\n",
        "    (\"base_model\", \"qd_pairs_base.pkl\"),\n",
        "    (\"base_model\", \"neural_sparse_v1_model\"),\n",
        "    (\"llm_generated\", \"synthetic_qd_pairs.pkl\"),\n",
        "    (\"llm_generated\", \"enhanced_synonyms.json\"),\n",
        "]\n",
        "\n",
        "if not dm.check_dependencies(required_files):\n",
        "    raise RuntimeError(\n",
        "        \"Required data files not found. \"\n",
        "        \"Please run notebooks 1 and 2 first.\"\n",
        "    )\n",
        "\n",
        "print(\"\\nâœ… Ready to proceed with enhanced training\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# ì´ì „ ë…¸íŠ¸ë¶ë“¤ì—ì„œ ìƒì„±ëœ ë°ì´í„° ë¡œë“œ\n",
        "print(\"Loading data from previous notebooks...\\n\")\n",
        "\n",
        "# From notebook 1\n",
        "korean_documents = dm.load_json(\"korean_documents.json\", \"base_model\")\n",
        "qd_pairs_base = dm.load_pickle(\"qd_pairs_base.pkl\", \"base_model\")\n",
        "\n",
        "# From notebook 2\n",
        "synthetic_qd_pairs = dm.load_pickle(\"synthetic_qd_pairs.pkl\", \"llm_generated\")\n",
        "enhanced_synonyms = dm.load_json(\"enhanced_synonyms.json\", \"llm_generated\")\n",
        "\n",
        "print(f\"\\nâœ… Loaded {len(korean_documents)} documents\")\n",
        "print(f\"âœ… Loaded {len(qd_pairs_base)} base QD pairs\")\n",
        "print(f\"âœ… Loaded {len(synthetic_qd_pairs)} synthetic QD pairs\")\n",
        "print(f\"âœ… Loaded enhanced synonyms with {len(enhanced_synonyms)} entries\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Base ëª¨ë¸ ë¡œë“œ (ì„±ëŠ¥ ë¹„êµìš©)\n",
        "from src.opensearch_sparse_encoder import OpenSearchSparseEncoder\n",
        "\n",
        "base_model, base_tokenizer = dm.load_model(\n",
        "    OpenSearchSparseEncoder,\n",
        "    \"neural_sparse_v1_model\",\n",
        "    \"base_model\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Base model loaded for comparison\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ†• 16. í•©ì„± ë°ì´í„° í¬í•¨ ëª¨ë¸ ì¬í•™ìŠµ\n",
        "\n",
        "ê¸°ì¡´ ë°ì´í„°ì™€ LLMìœ¼ë¡œ ìƒì„±í•œ í•©ì„± ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ ëª¨ë¸ì„ ì¬í•™ìŠµí•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ”„ ì„¹ì…˜ 16: í•©ì„± ë°ì´í„° í¬í•¨ ëª¨ë¸ ì¬í•™ìŠµ\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ê¸°ì¡´ ë°ì´í„° + í•©ì„± ë°ì´í„° ë³‘í•©\n",
        "combined_qd_pairs = korean_data['qd_pairs'][:10000] + synthetic_pairs\n",
        "\n",
        "print(f\"\\nğŸ“Š í•™ìŠµ ë°ì´í„° í†µê³„:\")\n",
        "print(f\"   ê¸°ì¡´ ë°ì´í„°: {len(korean_data['qd_pairs'][:10000]):,}ê°œ\")\n",
        "print(f\"   í•©ì„± ë°ì´í„°: {len(synthetic_pairs):,}ê°œ\")\n",
        "print(f\"   ì´ ë°ì´í„°: {len(combined_qd_pairs):,}ê°œ\")\n",
        "print(f\"   ì¦ê°€ìœ¨: {len(synthetic_pairs) / len(korean_data['qd_pairs'][:10000]) * 100:.1f}%\")\n",
        "\n",
        "# Negative sampling ì ìš©\n",
        "print(\"\\nğŸ”„ Negative sampling ì ìš© ì¤‘...\")\n",
        "augmented_pairs_v2 = add_negative_samples(\n",
        "    combined_qd_pairs,\n",
        "    korean_data['documents'],\n",
        "    num_negatives=2\n",
        ")\n",
        "\n",
        "# Train/Val split\n",
        "train_pairs_v2, val_pairs_v2 = train_test_split(augmented_pairs_v2, test_size=0.1, random_state=42)\n",
        "\n",
        "print(f\"\\nğŸ“Š ì¬í•™ìŠµ ë°ì´í„°ì…‹ ë¶„í• :\")\n",
        "print(f\"   Train: {len(train_pairs_v2):,} pairs\")\n",
        "print(f\"   Val: {len(val_pairs_v2):,} pairs\")\n",
        "\n",
        "# ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„±\n",
        "train_dataset_v2 = SparseEncodingDataset(train_pairs_v2, tokenizer, MAX_LENGTH)\n",
        "val_dataset_v2 = SparseEncodingDataset(val_pairs_v2, tokenizer, MAX_LENGTH)\n",
        "\n",
        "train_loader_v2 = DataLoader(train_dataset_v2, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader_v2 = DataLoader(val_dataset_v2, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"\\nâœ“ ì¬í•™ìŠµ ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ\")\n",
        "print(f\"   Train batches: {len(train_loader_v2):,}\")\n",
        "print(f\"   Val batches: {len(val_loader_v2):,}\")\n",
        "\n",
        "# ëª¨ë¸ ì¬ì´ˆê¸°í™” (ìƒˆë¡œ í•™ìŠµ)\n",
        "print(\"\\nğŸ”„ ëª¨ë¸ ì¬ì´ˆê¸°í™” ì¤‘...\")\n",
        "doc_encoder_v2 = OpenSearchDocEncoder(MODEL_NAME)\n",
        "doc_encoder_v2 = doc_encoder_v2.to(device)\n",
        "\n",
        "if len(tokenizer) > doc_encoder_v2.vocab_size:\n",
        "    doc_encoder_v2.bert.resize_token_embeddings(len(tokenizer))\n",
        "    doc_encoder_v2.vocab_size = len(tokenizer)\n",
        "\n",
        "print(f\"âœ“ ëª¨ë¸ v2 ì´ˆê¸°í™” ì™„ë£Œ\")\n",
        "\n",
        "# Optimizer & Scheduler for v2\n",
        "optimizer_v2 = AdamW(doc_encoder_v2.parameters(), lr=LEARNING_RATE)\n",
        "total_steps_v2 = len(train_loader_v2) * NUM_EPOCHS\n",
        "scheduler_v2 = get_linear_schedule_with_warmup(\n",
        "    optimizer_v2,\n",
        "    num_warmup_steps=WARMUP_STEPS,\n",
        "    num_training_steps=total_steps_v2\n",
        ")\n",
        "\n",
        "# í•™ìŠµ íˆìŠ¤í† ë¦¬ v2\n",
        "history_v2 = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'ranking_loss': [],\n",
        "    'l0_loss': [],\n",
        "    'idf_penalty': []\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸš€ LLM í™•ì¥ ëª¨ë¸ í•™ìŠµ ì‹œì‘!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# í•™ìŠµ ì‹¤í–‰ (v2)\n",
        "best_val_loss_v2 = float('inf')\n",
        "best_model_path_v2 = \"./models/best_korean_neural_sparse_encoder_v2_llm.pt\"\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"[V2 LLM] Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # í•™ìŠµ\n",
        "    train_loss, ranking_loss, l0_loss, idf_penalty = train_epoch(\n",
        "        doc_encoder_v2, train_loader_v2, optimizer_v2, scheduler_v2,\n",
        "        idf_id_dict, tokenizer, device\n",
        "    )\n",
        "    \n",
        "    # ê²€ì¦\n",
        "    val_loss = evaluate(doc_encoder_v2, val_loader_v2, idf_id_dict, tokenizer, device)\n",
        "    \n",
        "    # ê¸°ë¡\n",
        "    history_v2['train_loss'].append(train_loss)\n",
        "    history_v2['val_loss'].append(val_loss)\n",
        "    history_v2['ranking_loss'].append(ranking_loss)\n",
        "    history_v2['l0_loss'].append(l0_loss)\n",
        "    history_v2['idf_penalty'].append(idf_penalty)\n",
        "    \n",
        "    print(f\"\\nğŸ“Š [V2] Epoch {epoch + 1} ê²°ê³¼:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"  Ranking Loss: {ranking_loss:.4f}\")\n",
        "    print(f\"  L0 Loss: {l0_loss:.2f}\")\n",
        "    print(f\"  IDF Penalty: {idf_penalty:.4f}\")\n",
        "    \n",
        "    # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥\n",
        "    if val_loss < best_val_loss_v2:\n",
        "        best_val_loss_v2 = val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': doc_encoder_v2.state_dict(),\n",
        "            'optimizer_state_dict': optimizer_v2.state_dict(),\n",
        "            'val_loss': val_loss,\n",
        "            'config': {\n",
        "                'model_name': MODEL_NAME,\n",
        "                'vocab_size': len(tokenizer),\n",
        "                'max_length': MAX_LENGTH,\n",
        "            }\n",
        "        }, best_model_path_v2)\n",
        "        print(f\"  âœ“ ë² ìŠ¤íŠ¸ ëª¨ë¸ v2 ì €ì¥! (val_loss: {val_loss:.4f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… LLM í™•ì¥ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Best Validation Loss (V2): {best_val_loss_v2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ†• 17. ì„±ëŠ¥ ë¹„êµ ë¶„ì„ (ê¸°ì¡´ vs LLM í™•ì¥)\n",
        "\n",
        "ê¸°ì¡´ ëª¨ë¸ê³¼ LLMìœ¼ë¡œ í™•ì¥í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ“Š ì„¹ì…˜ 17: ì„±ëŠ¥ ë¹„êµ ë¶„ì„ (ê¸°ì¡´ vs LLM í™•ì¥)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ê¸°ì¡´ ëª¨ë¸ vs LLM í™•ì¥ ëª¨ë¸ ë¹„êµ\n",
        "comparison_results = {\n",
        "    'í•­ëª©': [\n",
        "        'ëª¨ë¸ ë²„ì „',\n",
        "        'í•™ìŠµ ë°ì´í„° (pairs)',\n",
        "        'ë™ì˜ì–´ ì‚¬ì „ (í•­ëª©)',\n",
        "        'Training Loss',\n",
        "        'Validation Loss',\n",
        "        'ì„±ëŠ¥ ê°œì„ ìœ¨'\n",
        "    ],\n",
        "    'ê¸°ì¡´ ëª¨ë¸ (V1)': [\n",
        "        'v1 (baseline)',\n",
        "        f\"{len(korean_data['qd_pairs'][:10000]):,}\",\n",
        "        f\"{len(get_default_korean_english_pairs())}\",\n",
        "        f\"{history['train_loss'][-1]:.4f}\",\n",
        "        f\"{best_val_loss:.4f}\",\n",
        "        \"baseline\"\n",
        "    ],\n",
        "    'LLM í™•ì¥ ëª¨ë¸ (V2)': [\n",
        "        'v2_llm',\n",
        "        f\"{len(combined_qd_pairs):,}\",\n",
        "        f\"{len(enhanced_bilingual_dict):,}\",\n",
        "        f\"{history_v2['train_loss'][-1]:.4f}\",\n",
        "        f\"{best_val_loss_v2:.4f}\",\n",
        "        f\"{((best_val_loss - best_val_loss_v2) / best_val_loss * 100):.2f}%\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df_comparison = pd.DataFrame(comparison_results)\n",
        "\n",
        "print(\"\\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ:\")\n",
        "print(\"=\"*70)\n",
        "print(df_comparison.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "# í•™ìŠµ ê³¡ì„  ë¹„êµ\n",
        "print(\"\\nğŸ“ˆ í•™ìŠµ ê³¡ì„  ë¹„êµ:\")\n",
        "print(\"\\n[V1 - ê¸°ì¡´ ëª¨ë¸]\")\n",
        "for epoch, (train_loss, val_loss) in enumerate(zip(history['train_loss'], history['val_loss']), 1):\n",
        "    print(f\"  Epoch {epoch}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n",
        "\n",
        "print(\"\\n[V2 - LLM í™•ì¥ ëª¨ë¸]\")\n",
        "for epoch, (train_loss, val_loss) in enumerate(zip(history_v2['train_loss'], history_v2['val_loss']), 1):\n",
        "    print(f\"  Epoch {epoch}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n",
        "\n",
        "# ê°œì„  ì‚¬í•­ ìš”ì•½\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… ì£¼ìš” ê°œì„  ì‚¬í•­\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "data_increase = len(synthetic_pairs) / len(korean_data['qd_pairs'][:10000]) * 100\n",
        "synonym_increase = (len(enhanced_bilingual_dict) - len(get_default_korean_english_pairs())) / len(get_default_korean_english_pairs()) * 100\n",
        "loss_improvement = (best_val_loss - best_val_loss_v2) / best_val_loss * 100\n",
        "\n",
        "print(f\"\\n1ï¸âƒ£ í•™ìŠµ ë°ì´í„° í™•ì¥\")\n",
        "print(f\"   - ê¸°ì¡´: {len(korean_data['qd_pairs'][:10000]):,}ê°œ\")\n",
        "print(f\"   - LLM í•©ì„±: +{len(synthetic_pairs):,}ê°œ\")\n",
        "print(f\"   - ì¦ê°€ìœ¨: +{data_increase:.1f}%\")\n",
        "\n",
        "print(f\"\\n2ï¸âƒ£ ë™ì˜ì–´ ì‚¬ì „ í’ˆì§ˆ í–¥ìƒ\")\n",
        "print(f\"   - ê¸°ì¡´: {len(get_default_korean_english_pairs())}ê°œ (ìˆ˜ë™)\")\n",
        "print(f\"   - LLM ê²€ì¦: {len(enhanced_bilingual_dict):,}ê°œ\")\n",
        "print(f\"   - ì¦ê°€ìœ¨: +{synonym_increase:.1f}%\")\n",
        "\n",
        "print(f\"\\n3ï¸âƒ£ ê²€ì¦ ì†ì‹¤ ê°œì„ \")\n",
        "print(f\"   - ê¸°ì¡´ ëª¨ë¸: {best_val_loss:.4f}\")\n",
        "print(f\"   - LLM í™•ì¥: {best_val_loss_v2:.4f}\")\n",
        "print(f\"   - ê°œì„ ìœ¨: {loss_improvement:.2f}%\")\n",
        "\n",
        "if best_val_loss_v2 < best_val_loss:\n",
        "    print(f\"\\nâœ… LLM í™•ì¥ ëª¨ë¸ì´ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•©ë‹ˆë‹¤!\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸  ê¸°ì¡´ ëª¨ë¸ê³¼ ë¹„ìŠ·í•˜ê±°ë‚˜ ë” ë§ì€ í•™ìŠµì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
        "    print(f\"   - ë” ë§ì€ í•©ì„± ë°ì´í„° ìƒì„±\")\n",
        "    print(f\"   - ì—í¬í¬ ìˆ˜ ì¦ê°€\")\n",
        "    print(f\"   - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê¶Œì¥\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ‰ ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ë°ì´í„° ì €ì¥ (ìµœì¢… ê²°ê³¼)\n",
        "\n",
        "Enhanced ëª¨ë¸ê³¼ í‰ê°€ ê²°ê³¼ë¥¼ `dataset/enhanced_model/` ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# 1. Enhanced Neural Sparse ëª¨ë¸ ì €ì¥\n",
        "dm.save_model(\n",
        "    enhanced_model,\n",
        "    tokenizer,\n",
        "    \"neural_sparse_v2_model\",\n",
        "    \"enhanced_model\"\n",
        ")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# 2. ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ì €ì¥\n",
        "dm.save_json(\n",
        "    performance_comparison,\n",
        "    \"performance_comparison.json\",\n",
        "    \"enhanced_model/evaluation\"\n",
        ")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# ì „ì²´ ë°ì´í„°ì…‹ ìš”ì•½\n",
        "dm.print_summary()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœ… Notebook 3 ì™„ë£Œ - ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ì„±!\n",
        "\n",
        "Enhanced ëª¨ë¸ê³¼ í‰ê°€ ê²°ê³¼ê°€ `dataset/enhanced_model/`ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### ì™„ë£Œëœ ì‘ì—…\n",
        "1. âœ… Base ëª¨ë¸ í•™ìŠµ (Notebook 1)\n",
        "2. âœ… LLM í•©ì„± ë°ì´í„° ìƒì„± (Notebook 2)\n",
        "3. âœ… Enhanced ëª¨ë¸ í•™ìŠµ ë° ë¹„êµ (Notebook 3)\n",
        "\n",
        "### ê²°ê³¼ í™•ì¸\n",
        "- Base Model: `dataset/base_model/neural_sparse_v1_model/`\n",
        "- Enhanced Model: `dataset/enhanced_model/neural_sparse_v2_model/`\n",
        "- Performance: `dataset/enhanced_model/evaluation/performance_comparison.json`\n",
        "\n",
        "### ë‹¤ìŒ ë‹¨ê³„\n",
        "ì´ì œ OpenSearchì— ëª¨ë¸ì„ ë°°í¬í•˜ê³  ì‹¤ì œ ê²€ìƒ‰ ì„œë¹„ìŠ¤ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
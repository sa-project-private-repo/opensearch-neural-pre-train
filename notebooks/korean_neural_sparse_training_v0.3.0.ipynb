{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenSearch Inference-Free Neural Sparse ëª¨ë¸ - í•œêµ­ì–´ í•™ìŠµ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **OpenSearch inference-free IR ëª¨ë¸** í‘œì¤€ì— ë”°ë¼ í•œêµ­ì–´ neural sparse ê²€ìƒ‰ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ¯ OpenSearch ëª¨ë¸ êµ¬ì¡°\n",
    "\n",
    "### Doc-only Mode (Inference-Free)\n",
    "- **ë¬¸ì„œ ì¸ì½”ë”©**: BERT ê¸°ë°˜ ì‹ ê²½ë§ â†’ sparse vector\n",
    "- **ì¿¼ë¦¬ ì¸ì½”ë”©**: Tokenizer + **idf.json** (weight lookup table) â†’ **Inference-Free!**\n",
    "\n",
    "### í•µì‹¬ íŒŒì¼\n",
    "1. `pytorch_model.bin` - ë¬¸ì„œ ì¸ì½”ë” ëª¨ë¸ (BERT ê¸°ë°˜)\n",
    "2. `idf.json` - í† í°ë³„ ê°€ì¤‘ì¹˜ lookup table (ì¿¼ë¦¬ìš©)\n",
    "3. `tokenizer.json`, `vocab.txt` - í† í¬ë‚˜ì´ì €\n",
    "4. `config.json` - ëª¨ë¸ ì„¤ì •\n",
    "\n",
    "### í•™ìŠµ ë°©ë²•\n",
    "- **IDF-aware Penalty**: ë‚®ì€ IDF í† í°ì˜ ê¸°ì—¬ë„ ì–µì œ\n",
    "- **Knowledge Distillation**: Dense + Sparse siamese ëª¨ë¸ì—ì„œ í•™ìŠµ\n",
    "- **â„“0 Sparsification**: L0 regularizationìœ¼ë¡œ í¬ì†Œì„± ìœ ì§€\n",
    "\n",
    "### ì°¸ê³  ë…¼ë¬¸\n",
    "- [Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers](https://arxiv.org/abs/2411.04403)\n",
    "- [Exploring â„“0 Sparsification for Inference-free Sparse Retrievers](https://opensearch.org/blog/)\n",
    "\n",
    "### OpenSearch Models Collection\n",
    "- https://huggingface.co/collections/opensearch-project/inference-free-ir-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path for src imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ” ì‹œìŠ¤í…œ í™˜ê²½ ê°ì§€\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Python ë²„ì „ í™•ì¸\n",
    "print(f\"Python ë²„ì „: {sys.version}\")\n",
    "python_version = sys.version_info\n",
    "if python_version.major == 3 and python_version.minor >= 12:\n",
    "    print(f\"âœ“ Python 3.12+ ê°ì§€ - ìµœì‹  íŒ¨í‚¤ì§€ ë²„ì „ ì‚¬ìš©\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Python 3.12+ ê¶Œì¥ (í˜„ì¬: {python_version.major}.{python_version.minor})\")\n",
    "\n",
    "# OS ê°ì§€\n",
    "os_release = \"\"\n",
    "if os.path.exists(\"/etc/os-release\"):\n",
    "    with open(\"/etc/os-release\") as f:\n",
    "        os_release = f.read().lower()\n",
    "\n",
    "is_amazon_linux = \"amazon linux\" in os_release or \"amzn\" in os_release\n",
    "is_ubuntu_debian = \"ubuntu\" in os_release or \"debian\" in os_release\n",
    "\n",
    "print(f\"âœ“ Amazon Linux 2023: {is_amazon_linux}\")\n",
    "print(f\"âœ“ Ubuntu/Debian: {is_ubuntu_debian}\")\n",
    "\n",
    "# Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ (requirements.txt ì‚¬ìš©)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“¦ Python íŒ¨í‚¤ì§€ ì„¤ì¹˜\")\n",
    "print(\"=\"*60)\n",
    "print(\"requirements.txtë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤...\")\n",
    "print(\"GPU: Tesla T4 ì§€ì› (CUDA 12.1)\")\n",
    "print()\n",
    "\n",
    "# PyTorch GPU ë²„ì „ ì„¤ì¹˜ (CUDA 12.1 for Tesla T4)\n",
    "print(\"ğŸ”¥ PyTorch ì„¤ì¹˜ ì¤‘ (GPU ë²„ì „ - CUDA 12.1)...\")\n",
    "%pip install -q torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# ë‚˜ë¨¸ì§€ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "print(\"ğŸ“š ë‚˜ë¨¸ì§€ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...\")\n",
    "%pip install -q -r requirements.txt\n",
    "\n",
    "print(\"\\nâœ“ Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ\")\n",
    "\n",
    "# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ë° Mecab ì„¤ì¹˜ (OSë³„ ë¶„ê¸°)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ”§ ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if is_amazon_linux:\n",
    "    print(\"ğŸ§ Amazon Linux 2023 í™˜ê²½ - dnf íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ì‚¬ìš©\")\n",
    "    \n",
    "    # í•„ìˆ˜ ê°œë°œ ë„êµ¬ ì„¤ì¹˜\n",
    "    print(\"\\nğŸ“¦ ê°œë°œ ë„êµ¬ ì„¤ì¹˜ ì¤‘...\")\n",
    "    !sudo dnf install -y gcc gcc-c++ make automake libtool\n",
    "    \n",
    "    # Java ì„¤ì¹˜ (Mecab ì‚¬ì „ ë¹Œë“œìš©)\n",
    "    print(\"\\nâ˜• Java ì„¤ì¹˜ ì¤‘...\")\n",
    "    !sudo dnf install -y java-17-amazon-corretto-devel\n",
    "    \n",
    "    # Mecab ì—”ì§„ ì„¤ì¹˜\n",
    "    print(\"\\nğŸ”¤ Mecab ì—”ì§„ ì„¤ì¹˜ ì¤‘...\")\n",
    "    !cd /tmp && \\\n",
    "     curl -LO https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz && \\\n",
    "     tar -zxf mecab-0.996-ko-0.9.2.tar.gz && \\\n",
    "     cd mecab-0.996-ko-0.9.2 && \\\n",
    "     ./configure && make && sudo make install && sudo ldconfig\n",
    "    \n",
    "    # Mecab í•œêµ­ì–´ ì‚¬ì „ ì„¤ì¹˜\n",
    "    print(\"\\nğŸ“š Mecab í•œêµ­ì–´ ì‚¬ì „ ì„¤ì¹˜ ì¤‘...\")\n",
    "    !cd /tmp && \\\n",
    "     curl -LO https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz && \\\n",
    "     tar -zxf mecab-ko-dic-2.1.1-20180720.tar.gz && \\\n",
    "     cd mecab-ko-dic-2.1.1-20180720 && \\\n",
    "     ./autogen.sh && ./configure && make && sudo make install\n",
    "    \n",
    "    print(\"\\nâœ“ Amazon Linux 2023 ì„¤ì¹˜ ì™„ë£Œ!\")\n",
    "\n",
    "elif is_ubuntu_debian:\n",
    "    print(\"ğŸ§ Ubuntu/Debian í™˜ê²½ - apt-get íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ì‚¬ìš©\")\n",
    "    \n",
    "    # í•„ìˆ˜ ê°œë°œ ë„êµ¬ ì„¤ì¹˜\n",
    "    print(\"\\nğŸ“¦ ê°œë°œ ë„êµ¬ ì„¤ì¹˜ ì¤‘...\")\n",
    "    !sudo apt-get update -qq\n",
    "    !sudo apt-get install -y -qq g++ openjdk-8-jdk python3-dev automake libtool\n",
    "    \n",
    "    # Mecab ì„¤ì¹˜ (konlpy ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©)\n",
    "    print(\"\\nğŸ”¤ Mecab ì„¤ì¹˜ ì¤‘...\")\n",
    "    !bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
    "    \n",
    "    print(\"\\nâœ“ Ubuntu/Debian ì„¤ì¹˜ ì™„ë£Œ!\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nâš ï¸  ì§€ì›ë˜ì§€ ì•ŠëŠ” OSì…ë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.\")\n",
    "    print(\"ì§€ì› OS: Amazon Linux 2023, Ubuntu, Debian\")\n",
    "\n",
    "# ì„¤ì¹˜ í™•ì¸\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… ì„¤ì¹˜ í™•ì¸\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# PyTorch í™•ì¸\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"âœ“ PyTorch: {torch.__version__}\")\n",
    "    print(f\"  - CUDA ë¹Œë“œ ë²„ì „: {torch.version.cuda}\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— PyTorch ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# Transformers í™•ì¸\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"âœ“ Transformers: {transformers.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Transformers ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# Datasets í™•ì¸\n",
    "try:\n",
    "    import datasets\n",
    "    print(f\"âœ“ Datasets: {datasets.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Datasets ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# Numpy í™•ì¸\n",
    "try:\n",
    "    import numpy\n",
    "    print(f\"âœ“ Numpy: {numpy.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Numpy ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# Mecab í™•ì¸\n",
    "try:\n",
    "    from konlpy.tag import Mecab\n",
    "    mecab = Mecab()\n",
    "    test_result = mecab.morphs(\"í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ í…ŒìŠ¤íŠ¸\")\n",
    "    print(f\"âœ“ Mecab: {test_result}\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Mecab ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"  Mecab ì„¤ì¹˜ ë¬¸ì œ í•´ê²°:\")\n",
    "    print(\"  1. sudo ldconfig\")\n",
    "    print(\"  2. export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH\")\n",
    "    print(\"  3. pip install --force-reinstall mecab-python3\")\n",
    "\n",
    "print(\"\\nâœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Hugging Face & Transformers\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForMaskedLM,\n",
    "    AutoConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    get_linear_schedule_with_warmup  # ì´ í•¨ìˆ˜ëŠ” transformersì—ì„œ ì œê³µ\n",
    ")\n",
    "\n",
    "# í˜•íƒœì†Œ ë¶„ì„\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# GPU/CPU í™˜ê²½ í™•ì¸\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ–¥ï¸  GPU/CPU í™˜ê²½ í™•ì¸\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ GPU ì‚¬ìš© ê°€ëŠ¥!\")\n",
    "    print(f\"  - CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "    print(f\"  - GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  - GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    * ë©”ëª¨ë¦¬: {props.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"    * Compute Capability: {props.major}.{props.minor}\")\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\\nâ†’ í•™ìŠµ ë””ë°”ì´ìŠ¤: GPU (cuda)\")\n",
    "else:\n",
    "    print(f\"âš ï¸  GPU ì‚¬ìš© ë¶ˆê°€\")\n",
    "    print(f\"  - CPUë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤\")\n",
    "    print(f\"  - í•™ìŠµ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"\\nâ†’ í•™ìŠµ ë””ë°”ì´ìŠ¤: CPU\")\n",
    "    print(f\"\\nğŸ’¡ GPU ì‚¬ìš© ê¶Œì¥:\")\n",
    "    print(f\"  - AWS EC2: g4dn.xlarge ì´ìƒ (NVIDIA T4 GPU)\")\n",
    "    print(f\"  - AWS EC2: p3.2xlarge ì´ìƒ (NVIDIA V100 GPU)\")\n",
    "    print(f\"  - Google Colab: GPU ëŸ°íƒ€ì„ ì‚¬ìš©\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í•œêµ­ì–´ ë°ì´í„°ì…‹ ìˆ˜ì§‘\n",
    "\n",
    "Hugging Faceì—ì„œ ë‹¤ì–‘í•œ í•œêµ­ì–´ ê³µê°œ ë°ì´í„°ì…‹ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_korean_datasets():\n",
    "    \"\"\"\n",
    "    Hugging Faceì—ì„œ í•œêµ­ì–´ ê³µê°œ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "    Query-Document ìŒì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì…‹ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    datasets_collection = {\n",
    "        'documents': [],\n",
    "        'queries': [],\n",
    "        'qd_pairs': []  # (query, document, relevance) íŠœí”Œ\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ“š í•œêµ­ì–´ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\\n\")\n",
    "    \n",
    "    # 1. KLUE MRC (Machine Reading Comprehension)\n",
    "    try:\n",
    "        print(\"1ï¸âƒ£ KLUE MRC ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        klue_mrc = load_dataset(\"klue\", \"mrc\", split=\"train\")\n",
    "        \n",
    "        for item in klue_mrc:\n",
    "            doc = item['context']\n",
    "            query = item['question']\n",
    "            \n",
    "            datasets_collection['documents'].append(doc)\n",
    "            datasets_collection['queries'].append(query)\n",
    "            datasets_collection['qd_pairs'].append((query, doc, 1.0))  # Positive pair\n",
    "        \n",
    "        print(f\"   âœ“ {len(klue_mrc):,} query-document pairs\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— KLUE MRC ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # 2. KorQuAD v1\n",
    "    try:\n",
    "        print(\"2ï¸âƒ£ KorQuAD v1 ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        korquad = load_dataset(\"squad_kor_v1\", split=\"train\")\n",
    "        \n",
    "        for item in korquad:\n",
    "            doc = item['context']\n",
    "            query = item['question']\n",
    "            \n",
    "            datasets_collection['documents'].append(doc)\n",
    "            datasets_collection['queries'].append(query)\n",
    "            datasets_collection['qd_pairs'].append((query, doc, 1.0))\n",
    "        \n",
    "        print(f\"   âœ“ {len(korquad):,} query-document pairs\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— KorQuAD ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # 3. Korean Wikipedia (ë¬¸ì„œ ì½”í¼ìŠ¤)\n",
    "    try:\n",
    "        print(\"3ï¸âƒ£ í•œêµ­ì–´ Wikipedia ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        ko_wiki = load_dataset(\"wikipedia\", \"20220301.ko\", split=\"train[:100000]\")\n",
    "        \n",
    "        for item in ko_wiki:\n",
    "            text = item['text']\n",
    "            if len(text) > 100:  # ìµœì†Œ ê¸¸ì´ í•„í„°\n",
    "                datasets_collection['documents'].append(text[:2000])  # ì²˜ìŒ 2000ì\n",
    "        \n",
    "        print(f\"   âœ“ {len(ko_wiki):,} documents\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— Wikipedia ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # 4. KLUE Topic Classification (ë¬¸ì„œ ì½”í¼ìŠ¤)\n",
    "    try:\n",
    "        print(\"4ï¸âƒ£ KLUE Topic Classification ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        klue_tc = load_dataset(\"klue\", \"ynat\", split=\"train\")\n",
    "        \n",
    "        for item in klue_tc:\n",
    "            datasets_collection['documents'].append(item['title'])\n",
    "        \n",
    "        print(f\"   âœ“ {len(klue_tc):,} documents\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— KLUE TC ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # 5. Korean News Dataset\n",
    "    try:\n",
    "        print(\"5ï¸âƒ£ í•œêµ­ì–´ ë‰´ìŠ¤ ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        korean_news = load_dataset(\"heegyu/news-category-dataset\", split=\"train[:50000]\")\n",
    "        \n",
    "        for item in korean_news:\n",
    "            if 'headline' in item:\n",
    "                datasets_collection['documents'].append(item['headline'])\n",
    "        \n",
    "        print(f\"   âœ“ {len(korean_news):,} documents\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— Korean News ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # ì¤‘ë³µ ì œê±°\n",
    "    datasets_collection['documents'] = list(set([\n",
    "        doc.strip() for doc in datasets_collection['documents'] \n",
    "        if doc and len(doc.strip()) > 10\n",
    "    ]))\n",
    "    \n",
    "    datasets_collection['queries'] = list(set([\n",
    "        q.strip() for q in datasets_collection['queries'] \n",
    "        if q and len(q.strip()) > 5\n",
    "    ]))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ì´ {len(datasets_collection['documents']):,}ê°œì˜ ê³ ìœ  ë¬¸ì„œ\")\n",
    "    print(f\"ì´ {len(datasets_collection['queries']):,}ê°œì˜ ê³ ìœ  ì¿¼ë¦¬\")\n",
    "    print(f\"ì´ {len(datasets_collection['qd_pairs']):,}ê°œì˜ query-document pairs\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return datasets_collection\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "korean_data = load_korean_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. IDF (Inverse Document Frequency) ê³„ì‚°\n",
    "\n",
    "ì¿¼ë¦¬ìš© í† í° ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ IDFë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "ì´ê²ƒì´ **idf.json** íŒŒì¼ì˜ ê¸°ë°˜ì´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œêµ­ì–´ BERT í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "MODEL_NAME = \"klue/bert-base\"  # í•œêµ­ì–´ ìµœì í™” BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ: {MODEL_NAME}\")\n",
    "print(f\"  Vocab size: {len(tokenizer):,}\")\n",
    "\n",
    "def calculate_idf(documents, tokenizer, sample_size=50000):\n",
    "    \"\"\"\n",
    "    ì½”í¼ìŠ¤ì—ì„œ IDFë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    IDF(t) = log(N / df(t))\n",
    "    \n",
    "    Args:\n",
    "        documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        tokenizer: Hugging Face tokenizer\n",
    "        sample_size: ìƒ˜í”Œë§í•  ë¬¸ì„œ ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        dict: {token_id: idf_score}\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“Š IDF ê³„ì‚° ì¤‘ (ìƒ˜í”Œ: {min(sample_size, len(documents)):,}ê°œ ë¬¸ì„œ)...\")\n",
    "    \n",
    "    # ìƒ˜í”Œë§\n",
    "    sample_docs = documents[:sample_size]\n",
    "    N = len(sample_docs)\n",
    "    \n",
    "    # ê° í† í°ì´ ë‚˜íƒ€ë‚œ ë¬¸ì„œ ìˆ˜ ê³„ì‚°\n",
    "    df = Counter()  # document frequency\n",
    "    \n",
    "    for i, doc in enumerate(tqdm(sample_docs, desc=\"Tokenizing documents\")):\n",
    "        # í† í°í™”\n",
    "        tokens = tokenizer.encode(doc, add_special_tokens=False, max_length=512, truncation=True)\n",
    "        \n",
    "        # ë¬¸ì„œì— ë‚˜íƒ€ë‚œ ê³ ìœ  í† í°ë“¤\n",
    "        unique_tokens = set(tokens)\n",
    "        \n",
    "        # df ì—…ë°ì´íŠ¸\n",
    "        for token_id in unique_tokens:\n",
    "            df[token_id] += 1\n",
    "    \n",
    "    # IDF ê³„ì‚°\n",
    "    idf_dict = {}\n",
    "    for token_id, doc_freq in df.items():\n",
    "        # IDF = log(N / df)\n",
    "        idf_score = math.log((N + 1) / (doc_freq + 1)) + 1.0  # smoothing\n",
    "        idf_dict[token_id] = idf_score\n",
    "    \n",
    "    # í† í° ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "    idf_token_dict = {}\n",
    "    for token_id, score in idf_dict.items():\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        idf_token_dict[token_str] = float(score)\n",
    "    \n",
    "    print(f\"\\nâœ“ IDF ê³„ì‚° ì™„ë£Œ\")\n",
    "    print(f\"  ì´ {len(idf_token_dict):,}ê°œ í† í°\")\n",
    "    print(f\"  í‰ê·  IDF: {np.mean(list(idf_token_dict.values())):.4f}\")\n",
    "    print(f\"  IDF ë²”ìœ„: [{min(idf_token_dict.values()):.4f}, {max(idf_token_dict.values()):.4f}]\")\n",
    "    \n",
    "    return idf_token_dict, idf_dict\n",
    "\n",
    "# IDF ê³„ì‚°\n",
    "idf_token_dict, idf_id_dict = calculate_idf(korean_data['documents'], tokenizer)\n",
    "\n",
    "# ìƒìœ„/í•˜ìœ„ IDF í† í° ì¶œë ¥\n",
    "print(\"\\nğŸ” IDF ìƒìœ„ 20ê°œ í† í° (í¬ê·€ í† í°):\")\n",
    "sorted_idf = sorted(idf_token_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for i, (token, score) in enumerate(sorted_idf[:20], 1):\n",
    "    print(f\"{i:2d}. {token:20s} - IDF: {score:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ”» IDF í•˜ìœ„ 20ê°œ í† í° (í”í•œ í† í°):\")\n",
    "for i, (token, score) in enumerate(sorted_idf[-20:], 1):\n",
    "    print(f\"{i:2d}. {token:20s} - IDF: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. í•œêµ­ì–´ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì¶”ê°€\n",
    "\n",
    "2024-2025 íŠ¸ë Œë“œ í‚¤ì›Œë“œì— ëŒ€í•´ IDF ê°€ì¤‘ì¹˜ë¥¼ ë¶€ìŠ¤íŒ…í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024-2025 íŠ¸ë Œë“œ í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ ë¶€ìŠ¤íŒ…)\n",
    "TREND_BOOST = {\n",
    "    # AI/ML íŠ¸ë Œë“œ\n",
    "    'LLM': 1.5,\n",
    "    'GPT': 1.5,\n",
    "    'ChatGPT': 1.5,\n",
    "    'ì±—GPT': 1.5,\n",
    "    'ìƒì„±í˜•': 1.4,\n",
    "    'GenAI': 1.4,\n",
    "    'RAG': 1.4,\n",
    "    'íŠ¸ëœìŠ¤í¬ë¨¸': 1.3,\n",
    "    'Transformer': 1.3,\n",
    "    'ì„ë² ë”©': 1.3,\n",
    "    'embedding': 1.3,\n",
    "    'ë²¡í„°': 1.3,\n",
    "    'vector': 1.3,\n",
    "    'í¬ì†Œ': 1.3,\n",
    "    'sparse': 1.3,\n",
    "    'íŒŒì¸íŠœë‹': 1.3,\n",
    "    'fine-tuning': 1.3,\n",
    "    'í”„ë¡¬í”„íŠ¸': 1.4,\n",
    "    'prompt': 1.4,\n",
    "    \n",
    "    # ê²€ìƒ‰ ê´€ë ¨\n",
    "    'OpenSearch': 1.3,\n",
    "    'Elasticsearch': 1.2,\n",
    "    'ì‹œë§¨í‹±': 1.3,\n",
    "    'semantic': 1.3,\n",
    "    'ê²€ìƒ‰': 1.2,\n",
    "    'search': 1.2,\n",
    "    \n",
    "    # ê¸°ë³¸ AI ìš©ì–´\n",
    "    'ì¸ê³µì§€ëŠ¥': 1.2,\n",
    "    'AI': 1.2,\n",
    "    'ë”¥ëŸ¬ë‹': 1.2,\n",
    "    'ë¨¸ì‹ ëŸ¬ë‹': 1.2,\n",
    "    'BERT': 1.2,\n",
    "    'ì‹ ê²½ë§': 1.2,\n",
    "}\n",
    "\n",
    "def apply_trend_boost(idf_dict, trend_boost, tokenizer):\n",
    "    \"\"\"\n",
    "    íŠ¸ë Œë“œ í‚¤ì›Œë“œì— ëŒ€í•´ IDF ê°€ì¤‘ì¹˜ë¥¼ ë¶€ìŠ¤íŒ…í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    boosted_idf = idf_dict.copy()\n",
    "    boost_count = 0\n",
    "    \n",
    "    for keyword, boost_factor in trend_boost.items():\n",
    "        # í‚¤ì›Œë“œë¥¼ í† í°í™”\n",
    "        keyword_tokens = tokenizer.encode(keyword, add_special_tokens=False)\n",
    "        \n",
    "        for token_id in keyword_tokens:\n",
    "            token_str = tokenizer.decode([token_id])\n",
    "            if token_str in boosted_idf:\n",
    "                original_idf = boosted_idf[token_str]\n",
    "                boosted_idf[token_str] = original_idf * boost_factor\n",
    "                boost_count += 1\n",
    "                print(f\"  Boosted: {token_str:20s} {original_idf:.4f} â†’ {boosted_idf[token_str]:.4f} ({boost_factor}x)\")\n",
    "    \n",
    "    print(f\"\\nâœ“ {boost_count}ê°œ í† í°ì— íŠ¸ë Œë“œ ë¶€ìŠ¤íŒ… ì ìš©\")\n",
    "    return boosted_idf\n",
    "\n",
    "print(\"ğŸ”¥ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë¶€ìŠ¤íŒ…...\\n\")\n",
    "idf_token_dict_boosted = apply_trend_boost(idf_token_dict, TREND_BOOST, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. AI ë„ë©”ì¸ íŠ¹í™” ìš©ì–´ì§‘ í†µí•©\n",
    "\n",
    "AI/ML/LLM ë„ë©”ì¸ì— íŠ¹í™”ëœ ìš©ì–´ì§‘ì„ ë¡œë“œí•˜ê³ ,\n",
    "ê¸°ìˆ  ìš©ì–´ë¥¼ tokenizer special tokensë¡œ ì¶”ê°€í•˜ì—¬ ë¶„ì ˆì„ ë°©ì§€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI ë„ë©”ì¸ ìš©ì–´ì§‘ ë¡œë“œ\n",
    "# from ai_domain_terminology import (\n",
    "#     AI_TERMINOLOGY,\n",
    "#     TECHNICAL_SPECIAL_TOKENS,\n",
    "#     AI_SYNONYMS\n",
    "# )\n",
    "\n",
    "# print(\"=\"*60)\n",
    "# print(\"ğŸ¤– AI ë„ë©”ì¸ ìš©ì–´ì§‘ ë¡œë“œ\")\n",
    "# print(\"=\"*60)\n",
    "# print(f\"âœ“ ì£¼ìš” ìš©ì–´ ì¹´í…Œê³ ë¦¬: {len(AI_TERMINOLOGY)}ê°œ\")\n",
    "# print(f\"âœ“ Special tokens: {len(TECHNICAL_SPECIAL_TOKENS)}ê°œ\")\n",
    "# print(f\"âœ“ ë™ì˜ì–´ ë§¤í•‘: {len(AI_SYNONYMS)}ê°œ\")\n",
    "# print()\n",
    "# print(\"ğŸ“ ìƒ˜í”Œ ìš©ì–´:\")\n",
    "# for i, (term, synonyms) in enumerate(list(AI_TERMINOLOGY.items())[:5]):\n",
    "#     print(f\"  {term}: {', '.join(synonyms[:3])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1. Tokenizerì— ê¸°ìˆ  ìš©ì–´ ì¶”ê°€\n",
    "\n",
    "ChatGPT, OpenSearch ë“± ê¸°ìˆ  ìš©ì–´ê°€ ë¶„ì ˆë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´\n",
    "special tokensë¡œ ì¶”ê°€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=\"*60)\n",
    "# print(\"ğŸ”§ Tokenizerì— ê¸°ìˆ  ìš©ì–´ ì¶”ê°€\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# í˜„ì¬ vocabulary í¬ê¸°\n",
    "# original_vocab_size = len(tokenizer)\n",
    "# print(f\"Original vocab size: {original_vocab_size:,}\")\n",
    "\n",
    "# Special tokens ì¶”ê°€\n",
    "# num_added = tokenizer.add_tokens(TECHNICAL_SPECIAL_TOKENS)\n",
    "# new_vocab_size = len(tokenizer)\n",
    "\n",
    "# print(f\"Added {num_added} new tokens\")\n",
    "# print(f\"New vocab size: {new_vocab_size:,}\")\n",
    "# print()\n",
    "\n",
    "# ì¶”ê°€ëœ í† í° ìƒ˜í”Œ í™•ì¸\n",
    "# print(\"âœ“ ì¶”ê°€ëœ ê¸°ìˆ  ìš©ì–´ ìƒ˜í”Œ:\")\n",
    "# for token in TECHNICAL_SPECIAL_TOKENS[:10]:\n",
    "#     token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "#     print(f\"  {token:20s} -> ID: {token_id}\")\n",
    "\n",
    "# print()\n",
    "# print(\"ğŸ§ª ë¶„ì ˆ ë°©ì§€ í…ŒìŠ¤íŠ¸:\")\n",
    "# test_texts = [\n",
    "#     \"ChatGPTëŠ” ê°•ë ¥í•œ LLMì…ë‹ˆë‹¤\",\n",
    "#     \"OpenSearch ë²¡í„°ê²€ìƒ‰ ê¸°ëŠ¥\",\n",
    "#     \"RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\",\n",
    "# ]\n",
    "\n",
    "# for text in test_texts:\n",
    "#     tokens = tokenizer.tokenize(text)\n",
    "#     print(f\"  '{text}'\")\n",
    "#     print(f\"    â†’ {tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2. ë„ë©”ì¸ ë™ì˜ì–´ ë§¤í•‘ ìƒì„±\n",
    "\n",
    "AI ë„ë©”ì¸ ìš©ì–´ì§‘ì˜ ë™ì˜ì–´ë¥¼ í™œìš©í•˜ì—¬ ê²€ìƒ‰ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=\"*60)\n",
    "# print(\"ğŸ”— ë„ë©”ì¸ ë™ì˜ì–´ ë§¤í•‘\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# AI_SYNONYMSë¥¼ ì¼ë°˜ dictionary í˜•íƒœë¡œ ë³€í™˜\n",
    "# domain_synonym_dict = {}\n",
    "\n",
    "# for main_term, synonyms in AI_TERMINOLOGY.items():\n",
    "    # Main termì„ ì†Œë¬¸ìë¡œ\n",
    "#     main_key = main_term.lower()\n",
    "#     synonym_list = [s.lower() for s in synonyms]\n",
    "    \n",
    "#     domain_synonym_dict[main_key] = synonym_list\n",
    "    \n",
    "    # ì–‘ë°©í–¥ ë§¤í•‘: ê° synonymë„ main termì„ ê°€ë¦¬í‚´\n",
    "#     for syn in synonym_list:\n",
    "#         if syn not in domain_synonym_dict:\n",
    "#             domain_synonym_dict[syn] = [main_key]\n",
    "#         elif main_key not in domain_synonym_dict[syn]:\n",
    "#             domain_synonym_dict[syn].append(main_key)\n",
    "\n",
    "# print(f\"âœ“ ë„ë©”ì¸ ë™ì˜ì–´ ë”•ì…”ë„ˆë¦¬ ìƒì„± ì™„ë£Œ\")\n",
    "# print(f\"  ì´ {len(domain_synonym_dict):,}ê°œ í•­ëª©\")\n",
    "# print()\n",
    "# print(\"ğŸ“ ìƒ˜í”Œ ë™ì˜ì–´ ë§¤í•‘:\")\n",
    "# samples = [\n",
    "#     \"ê²€ìƒ‰\", \"ì¸ê³µì§€ëŠ¥\", \"llm\", \"chatgpt\", \"ì„ë² ë”©\",\n",
    "#     \"rag\", \"í”„ë¡¬í”„íŠ¸\", \"ë”¥ëŸ¬ë‹\", \"ë¨¸ì‹ ëŸ¬ë‹\"\n",
    "# ]\n",
    "# for term in samples:\n",
    "#     if term in domain_synonym_dict:\n",
    "#         syns = domain_synonym_dict[term][:3]  # ìƒìœ„ 3ê°œë§Œ\n",
    "#         print(f\"  {term:15s} â†’ {', '.join(syns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3. ìš©ì–´ì§‘ í†µí•© ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=\"*60)\n",
    "# print(\"âœ… AI ë„ë©”ì¸ ìš©ì–´ì§‘ í†µí•© ì™„ë£Œ!\")\n",
    "# print(\"=\"*60)\n",
    "# print()\n",
    "# print(\"ğŸ“Š í†µí•© ê²°ê³¼:\")\n",
    "# print(f\"  â€¢ Tokenizer vocabulary: {original_vocab_size:,} â†’ {new_vocab_size:,} (+{num_added})\")\n",
    "# print(f\"  â€¢ AI ë„ë©”ì¸ ìš©ì–´: {len(AI_TERMINOLOGY):,}ê°œ ì¹´í…Œê³ ë¦¬\")\n",
    "# print(f\"  â€¢ ë™ì˜ì–´ ë§¤í•‘: {len(domain_synonym_dict):,}ê°œ í•­ëª©\")\n",
    "# print(f\"  â€¢ Special tokens: {len(TECHNICAL_SPECIAL_TOKENS)}ê°œ\")\n",
    "# print()\n",
    "# print(\"ğŸ¯ ì£¼ìš” ê°œì„  ì‚¬í•­:\")\n",
    "# print(\"  1. ê¸°ìˆ  ìš©ì–´ ë¶„ì ˆ ë°©ì§€ (ChatGPT, OpenSearch, LLM ë“±)\")\n",
    "# print(\"  2. AI ë„ë©”ì¸ ë™ì˜ì–´ ìë™ ë§¤í•‘ (ê²€ìƒ‰â†”Searchâ†”íƒìƒ‰)\")\n",
    "# print(\"  3. í•œêµ­ì–´-ì˜ì–´ ìš©ì–´ ì–‘ë°©í–¥ ì—°ê²°\")\n",
    "# print()\n",
    "# print(\"ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "# print(\"  â†’ ì„¹ì…˜ 7ì—ì„œ ë„ë©”ì¸ ë™ì˜ì–´ì™€ ìë™ ë°œê²¬ ë™ì˜ì–´ë¥¼ ê²°í•©\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. OpenSearch ë¬¸ì„œ ì¸ì½”ë” ëª¨ë¸ ì •ì˜\n",
    "\n",
    "**Doc-only mode**ë¥¼ ìœ„í•œ ë¬¸ì„œ ì¸ì½”ë”ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "BERT ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ sparse vectorë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenSearchDocEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    OpenSearch Neural Sparse Document Encoder (Doc-only mode)\n",
    "    \n",
    "    ë¬¸ì„œë¥¼ sparse vectorë¡œ ì¸ì½”ë”©í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "    ì¶œë ¥ í˜•ì‹: {\"output\": <sparse_vector>}\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"klue/bert-base\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # BERT ê¸°ë°˜ ì¸ì½”ë”\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.bert = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        \n",
    "        self.vocab_size = self.config.vocab_size\n",
    "        \n",
    "        # Log saturation activation\n",
    "        # log(1 + ReLU(x))\n",
    "        self.activation = lambda x: torch.log1p(torch.relu(x))\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, return_dict=False):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            input_ids: (batch_size, seq_len)\n",
    "            attention_mask: (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            sparse_vector: (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        # BERT MLM head output\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Logits: (batch_size, seq_len, vocab_size)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Apply activation: log(1 + ReLU(logits))\n",
    "        activated = self.activation(logits)\n",
    "        \n",
    "        # Max pooling over sequence length\n",
    "        # (batch_size, seq_len, vocab_size) â†’ (batch_size, vocab_size)\n",
    "        sparse_vector = torch.max(\n",
    "            activated * attention_mask.unsqueeze(-1),\n",
    "            dim=1\n",
    "        ).values\n",
    "        \n",
    "        if return_dict:\n",
    "            return {'output': sparse_vector}\n",
    "        \n",
    "        return sparse_vector\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "doc_encoder = OpenSearchDocEncoder(MODEL_NAME)\n",
    "doc_encoder = doc_encoder.to(device)\n",
    "\n",
    "# Tokenizerì— special tokens ì¶”ê°€ë¡œ ì¸í•´ embedding resize í•„ìš”\n",
    "if len(tokenizer) > doc_encoder.vocab_size:\n",
    "    print(f\"Resizing model embeddings: {doc_encoder.vocab_size} â†’ {len(tokenizer)}\")\n",
    "    doc_encoder.bert.resize_token_embeddings(len(tokenizer))\n",
    "    doc_encoder.vocab_size = len(tokenizer)\n",
    "    print(f\"âœ“ Model embeddings resized\")\n",
    "\n",
    "print(f\"âœ“ OpenSearch Document Encoder ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "print(f\"  ëª¨ë¸: {MODEL_NAME}\")\n",
    "print(f\"  Vocab size: {doc_encoder.vocab_size:,}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in doc_encoder.parameters()):,}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "\n",
    "Query-Document pairsì™€ negative samplingì„ ì¤€ë¹„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SparseEncodingDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    OpenSearch Neural Sparse í•™ìŠµìš© ë°ì´í„°ì…‹\n",
    "    \"\"\"\n",
    "    def __init__(self, qd_pairs, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            qd_pairs: [(query, document, relevance), ...]\n",
    "            tokenizer: Hugging Face tokenizer\n",
    "            max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "        \"\"\"\n",
    "        self.qd_pairs = qd_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.qd_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        query, document, relevance = self.qd_pairs[idx]\n",
    "        \n",
    "        # ì¿¼ë¦¬ í† í°í™” (IDF lookupìš©)\n",
    "        query_encoded = self.tokenizer(\n",
    "            query,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # ë¬¸ì„œ í† í°í™” (ëª¨ë¸ ì¸ì½”ë”©ìš©)\n",
    "        doc_encoded = self.tokenizer(\n",
    "            document,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'query_input_ids': query_encoded['input_ids'].squeeze(0),\n",
    "            'query_attention_mask': query_encoded['attention_mask'].squeeze(0),\n",
    "            'doc_input_ids': doc_encoded['input_ids'].squeeze(0),\n",
    "            'doc_attention_mask': doc_encoded['attention_mask'].squeeze(0),\n",
    "            'relevance': torch.tensor(relevance, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# Negative sampling ì¶”ê°€ (ìµœì í™” ë²„ì „)\n",
    "def add_negative_samples(qd_pairs, documents, num_negatives=1):\n",
    "    \"\"\"\n",
    "    ê° positive pairì— ëŒ€í•´ negative documentsë¥¼ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.\n",
    "    (ì¸ë±ìŠ¤ ê¸°ë°˜ ìƒ˜í”Œë§ìœ¼ë¡œ 100ë°° ì´ìƒ ë¹ ë¦„!)\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ”„ Negative sampling ì¤‘ (negatives per query: {num_negatives})...\")\n",
    "    \n",
    "    # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì¸ë±ìŠ¤ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡)\n",
    "    doc_list = documents if isinstance(documents, list) else list(documents)\n",
    "    n_docs = len(doc_list)\n",
    "    \n",
    "    # ë¬¸ì„œ â†’ ì¸ë±ìŠ¤ ë§¤í•‘ (ë¹ ë¥¸ ê²€ìƒ‰ìš©)\n",
    "    doc_to_idx = {doc: idx for idx, doc in enumerate(doc_list)}\n",
    "    \n",
    "    augmented_pairs = []\n",
    "    \n",
    "    for query, pos_doc, relevance in tqdm(qd_pairs):\n",
    "        # Positive pair ì¶”ê°€\n",
    "        augmented_pairs.append((query, pos_doc, 1.0))\n",
    "        \n",
    "        # Positive ë¬¸ì„œì˜ ì¸ë±ìŠ¤\n",
    "        pos_idx = doc_to_idx.get(pos_doc, -1)\n",
    "        \n",
    "        # Negative sampling (ì¸ë±ìŠ¤ ê¸°ë°˜ - ë§¤ìš° ë¹ ë¦„!)\n",
    "        for _ in range(num_negatives):\n",
    "            # ëœë¤ ì¸ë±ìŠ¤ ì„ íƒ\n",
    "            neg_idx = np.random.randint(0, n_docs)\n",
    "            \n",
    "            # Positiveì™€ ê°™ì€ ì¸ë±ìŠ¤ë©´ ë‹¤ë¥¸ ê²ƒìœ¼ë¡œ êµì²´\n",
    "            if neg_idx == pos_idx:\n",
    "                neg_idx = (neg_idx + 1) % n_docs\n",
    "            \n",
    "            neg_doc = doc_list[neg_idx]\n",
    "            augmented_pairs.append((query, neg_doc, 0.0))\n",
    "    \n",
    "    print(f\"âœ“ ì´ {len(augmented_pairs):,}ê°œ pairs (original: {len(qd_pairs):,})\")\n",
    "    return augmented_pairs\n",
    "\n",
    "# Negative sampling ì ìš©\n",
    "augmented_pairs = add_negative_samples(\n",
    "    korean_data['qd_pairs'][:10000],  # ìƒ˜í”Œë§ (ì „ì²´ëŠ” ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼)\n",
    "    korean_data['documents'],\n",
    "    num_negatives=2\n",
    ")\n",
    "\n",
    "# Train/Val split\n",
    "train_pairs, val_pairs = train_test_split(augmented_pairs, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"\\nğŸ“Š ë°ì´í„°ì…‹ ë¶„í• :\")\n",
    "print(f\"  Train: {len(train_pairs):,} pairs\")\n",
    "print(f\"  Val: {len(val_pairs):,} pairs\")\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„±\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = SparseEncodingDataset(train_pairs, tokenizer, MAX_LENGTH)\n",
    "val_dataset = SparseEncodingDataset(val_pairs, tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nâœ“ ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Train batches: {len(train_loader):,}\")\n",
    "print(f\"  Val batches: {len(val_loader):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ìë™ ë™ì˜ì–´ ë°œê²¬ ë° ë°ì´í„° í™•ì¥\n",
    "\n",
    "ìˆ˜ì§‘ëœ ë°ì´í„°ì—ì„œ í† í° ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ë™ì˜ì–´ë¥¼ ìë™ ë°œê²¬í•˜ê³ ,\n",
    "ì´ë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµ ë°ì´í„°ë¥¼ í™•ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. BERT í† í° ì„ë² ë”© ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸ” í† í° ì„ë² ë”© ê¸°ë°˜ ë™ì˜ì–´ ìë™ ë°œê²¬\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# BERT ëª¨ë¸ì€ ì´ë¯¸ doc_encoderì— ë¡œë“œë˜ì–´ ìˆìŒ\n",
    "# Token embedding ì¶”ì¶œ\n",
    "token_embeddings = doc_encoder.bert.bert.embeddings.word_embeddings.weight.detach().cpu().numpy()\n",
    "\n",
    "print(f\"âœ“ Token embeddings ì¶”ì¶œ ì™„ë£Œ: {token_embeddings.shape}\")\n",
    "print(f\"  Vocab size: {token_embeddings.shape[0]:,}\")\n",
    "print(f\"  Embedding dim: {token_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. ìœ ì‚¬ í† í° ë°œê²¬ í•¨ìˆ˜ (find_similar_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_tokens(token, tokenizer, embeddings, top_k=10, threshold=0.75):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í† í°ê³¼ ìœ ì‚¬í•œ í† í°ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        token: ê²€ìƒ‰í•  í† í° (ë¬¸ìì—´)\n",
    "        tokenizer: Tokenizer\n",
    "        embeddings: Token embeddings (numpy array)\n",
    "        top_k: ë°˜í™˜í•  ìœ ì‚¬ í† í° ê°œìˆ˜\n",
    "        threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (0~1)\n",
    "    \n",
    "    Returns:\n",
    "        List of (token, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # í† í° -> ID ë³€í™˜\n",
    "    token_id = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(token))\n",
    "    if not token_id:\n",
    "        return []\n",
    "    token_id = token_id[0]  # ì²« ë²ˆì§¸ í† í° ID ì‚¬ìš©\n",
    "    \n",
    "    # Token embedding ì¶”ì¶œ\n",
    "    token_emb = embeddings[token_id]\n",
    "    \n",
    "    # ëª¨ë“  í† í°ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    similarities = np.dot(embeddings, token_emb) / (\n",
    "        np.linalg.norm(embeddings, axis=1) * np.linalg.norm(token_emb) + 1e-10\n",
    "    )\n",
    "    \n",
    "    # ìƒìœ„ top_k+1 ê°œ ì¶”ì¶œ (ìê¸° ìì‹  ì œì™¸)\n",
    "    top_indices = np.argsort(similarities)[-(top_k+1):][::-1]\n",
    "    \n",
    "    similar_tokens = []\n",
    "    for idx in top_indices:\n",
    "        sim_score = float(similarities[idx])\n",
    "        if sim_score >= threshold and int(idx) != token_id:\n",
    "            similar_token = tokenizer.decode([int(idx)]).strip()\n",
    "            # í•„í„°ë§: ë¹ˆ ë¬¸ìì—´, íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆëŠ” ê²½ìš° ì œì™¸\n",
    "            if similar_token and not similar_token in ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']:\n",
    "                similar_tokens.append((similar_token, sim_score))\n",
    "    \n",
    "    return similar_tokens[:top_k]\n",
    "\n",
    "\n",
    "print(\"âœ“ find_similar_tokens í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. ì½”í¼ìŠ¤ ê¸°ë°˜ ë™ì˜ì–´ ìë™ ë°œê²¬ (build_synonym_dict_from_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_synonym_dict_from_corpus(documents, tokenizer, embeddings,\n",
    "                                   idf_dict, top_n=500, threshold=0.75):\n",
    "    \"\"\"\n",
    "    ìˆ˜ì§‘ëœ ë¬¸ì„œ ì½”í¼ìŠ¤ì—ì„œ ì¤‘ìš” í† í°ë“¤ì˜ ë™ì˜ì–´ë¥¼ ìë™ ë°œê²¬\n",
    "\n",
    "    Args:\n",
    "        documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        tokenizer: Tokenizer\n",
    "        embeddings: Token embeddings\n",
    "        idf_dict: IDF ë”•ì…”ë„ˆë¦¬ (token_id -> idf_score or token_str -> idf_score)\n",
    "        top_n: ìƒìœ„ Nê°œ IDF í† í° ëŒ€ìƒ\n",
    "        threshold: ìœ ì‚¬ë„ ì„ê³„ê°’\n",
    "\n",
    "    Returns:\n",
    "        synonym_dict: {token: [similar_tokens]}\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“– ìˆ˜ì§‘ëœ ë°ì´í„°ì—ì„œ ì¤‘ìš” í† í° ì¶”ì¶œ...\")\n",
    "\n",
    "    # IDF ê¸°ë°˜ ì¤‘ìš” í† í° ì„ ì •\n",
    "    sorted_idf = sorted(idf_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # í•„í„°ë§: subword(##ë¡œ ì‹œì‘), íŠ¹ìˆ˜ë¬¸ì, ë‹¨ì¼ ë¬¸ì ì œì™¸\n",
    "    important_tokens = []\n",
    "    for token_or_id, idf_score in sorted_idf:\n",
    "        if len(important_tokens) >= top_n:\n",
    "            break\n",
    "\n",
    "        # token_or_idê°€ ì •ìˆ˜(token ID)ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "        if isinstance(token_or_id, int):\n",
    "            token = tokenizer.decode([token_or_id]).strip()\n",
    "        else:\n",
    "            token = token_or_id\n",
    "\n",
    "        # í•„í„°ë§ ì¡°ê±´\n",
    "        if (not token.startswith('##') and\n",
    "            len(token) > 1 and\n",
    "            not token in [',', '.', '!', '?', ':', ';', '-', '(', ')', '[', ']']):\n",
    "            important_tokens.append(token)\n",
    "\n",
    "    print(f\"  ì¤‘ìš” í† í° {len(important_tokens)}ê°œ ì„ ì • ì™„ë£Œ\")\n",
    "    print(f\"  ìƒìœ„ 10ê°œ: {important_tokens[:10]}\")\n",
    "\n",
    "    # ê° í† í°ì— ëŒ€í•´ ìœ ì‚¬ í† í° ì°¾ê¸°\n",
    "    print(f\"\\nğŸ” ìœ ì‚¬ í† í° ìë™ ë°œê²¬ ì¤‘... (threshold={threshold})\")\n",
    "    synonym_dict = {}\n",
    "\n",
    "    for token in tqdm(important_tokens, desc=\"Finding synonyms\"):\n",
    "        similar = find_similar_tokens(token, tokenizer, embeddings,\n",
    "                                      top_k=5, threshold=threshold)\n",
    "        if similar:\n",
    "            synonym_dict[token] = [t for t, _ in similar]\n",
    "\n",
    "    return synonym_dict, important_tokens\n",
    "\n",
    "\n",
    "print(\"âœ“ ë™ì˜ì–´ ë°œê²¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. ìˆ˜ì§‘ ë°ì´í„° ê¸°ë°˜ ë™ì˜ì–´ ìë™ ë°œê²¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìˆ˜ì§‘ëœ ë¬¸ì„œì—ì„œ ë™ì˜ì–´ ìë™ ë°œê²¬\n",
    "auto_synonym_dict, important_tokens = build_synonym_dict_from_corpus(\n",
    "    korean_data['documents'],\n",
    "    tokenizer,\n",
    "    token_embeddings,\n",
    "    idf_id_dict,  # ID ê¸°ë°˜ IDF\n",
    "    top_n=500,    # ìƒìœ„ 500ê°œ í† í°\n",
    "    threshold=0.75  # ìœ ì‚¬ë„ 75% ì´ìƒ\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ“ ìë™ ë™ì˜ì–´ ë°œê²¬ ì™„ë£Œ!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  ë°œê²¬ëœ ë™ì˜ì–´ ê·¸ë£¹: {len(auto_synonym_dict):,}ê°œ\")\n",
    "print(f\"  ì´ ë™ì˜ì–´ ìŒ: {sum(len(v) for v in auto_synonym_dict.values()):,}ê°œ\")\n",
    "\n",
    "# ì˜ˆì‹œ ì¶œë ¥\n",
    "print(f\"\\nğŸ“ ë°œê²¬ëœ ë™ì˜ì–´ ì˜ˆì‹œ (ìƒìœ„ 20ê°œ):\")\n",
    "for i, (token, synonyms) in enumerate(list(auto_synonym_dict.items())[:20], 1):\n",
    "    if synonyms:\n",
    "        print(f\"  {i:2d}. {token:15s} â†’ {', '.join(synonyms[:3])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1. ë„ë©”ì¸ ë™ì˜ì–´ + ìë™ ë°œê²¬ ë™ì˜ì–´ ê²°í•©\n",
    "\n",
    "AI ë„ë©”ì¸ ì „ë¬¸ ìš©ì–´ì™€ ìë™ ë°œê²¬ëœ ë™ì˜ì–´ë¥¼ ê²°í•©í•˜ì—¬\n",
    "ë” í¬ê´„ì ì¸ ë™ì˜ì–´ ì‚¬ì „ì„ êµ¬ì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸ”— ë„ë©”ì¸ ë™ì˜ì–´ + ìë™ ë°œê²¬ ë™ì˜ì–´ ê²°í•©\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# 1. ë„ë©”ì¸ ë™ì˜ì–´ í†µê³„\n",
    "print(f\"ğŸ“š ë„ë©”ì¸ ë™ì˜ì–´ (AI ìš©ì–´ì§‘):\")\n",
    "print(f\"  í•­ëª© ìˆ˜: {len(domain_synonym_dict):,}ê°œ\")\n",
    "print()\n",
    "\n",
    "# 2. ìë™ ë°œê²¬ ë™ì˜ì–´ í†µê³„\n",
    "print(f\"ğŸ” ìë™ ë°œê²¬ ë™ì˜ì–´ (ì½”í¼ìŠ¤ ê¸°ë°˜):\")\n",
    "print(f\"  í•­ëª© ìˆ˜: {len(auto_synonym_dict):,}ê°œ\")\n",
    "print()\n",
    "\n",
    "# 3. ê²°í•© ì „ëµ: ë„ë©”ì¸ ìš°ì„ , ìë™ ë°œê²¬ ë³´ì™„\n",
    "merged_synonym_dict = {}\n",
    "\n",
    "# ë¨¼ì € ë„ë©”ì¸ ë™ì˜ì–´ ì¶”ê°€ (ì‹ ë¢°ë„ ë†’ìŒ)\n",
    "for term, synonyms in domain_synonym_dict.items():\n",
    "    merged_synonym_dict[term] = list(set(synonyms))  # ì¤‘ë³µ ì œê±°\n",
    "\n",
    "# ìë™ ë°œê²¬ ë™ì˜ì–´ ì¶”ê°€ (ë„ë©”ì¸ ë™ì˜ì–´ì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²ƒë§Œ)\n",
    "added_from_auto = 0\n",
    "for term, synonyms in auto_synonym_dict.items():\n",
    "    term_lower = term.lower()\n",
    "    \n",
    "    if term_lower in merged_synonym_dict:\n",
    "        # ê¸°ì¡´ í•­ëª©ì— ìƒˆë¡œìš´ ë™ì˜ì–´ ì¶”ê°€\n",
    "        existing = set(merged_synonym_dict[term_lower])\n",
    "        new_synonyms = [s.lower() for s in synonyms if s.lower() not in existing]\n",
    "        if new_synonyms:\n",
    "            merged_synonym_dict[term_lower].extend(new_synonyms)\n",
    "            added_from_auto += len(new_synonyms)\n",
    "    else:\n",
    "        # ìƒˆë¡œìš´ í•­ëª© ì¶”ê°€\n",
    "        merged_synonym_dict[term_lower] = [s.lower() for s in synonyms]\n",
    "        added_from_auto += len(synonyms)\n",
    "\n",
    "print(f\"âœ… ê²°í•© ì™„ë£Œ:\")\n",
    "print(f\"  ì´ í•­ëª© ìˆ˜: {len(merged_synonym_dict):,}ê°œ\")\n",
    "print(f\"  ë„ë©”ì¸ ë™ì˜ì–´ ê¸°ì—¬: {len(domain_synonym_dict):,}ê°œ í•­ëª©\")\n",
    "print(f\"  ìë™ ë°œê²¬ ê¸°ì—¬: {added_from_auto:,}ê°œ ë™ì˜ì–´ ì¶”ê°€\")\n",
    "print()\n",
    "\n",
    "# 4. ìƒ˜í”Œ í™•ì¸\n",
    "print(\"ğŸ“ ê²°í•© ë™ì˜ì–´ ìƒ˜í”Œ:\")\n",
    "sample_terms = ['ê²€ìƒ‰', 'ì¸ê³µì§€ëŠ¥', 'llm', 'chatgpt', 'ì„ë² ë”©', 'rag']\n",
    "for term in sample_terms:\n",
    "    if term in merged_synonym_dict:\n",
    "        syns = merged_synonym_dict[term][:5]  # ìƒìœ„ 5ê°œ\n",
    "        print(f\"  {term:15s} â†’ {', '.join(syns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5. Synonym-Aware IDF ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synonym_aware_idf(original_idf, tokenizer, synonym_dict, method='max'):\n",
    "    \"\"\"\n",
    "    ë™ì˜ì–´ ì •ë³´ë¥¼ ë°˜ì˜í•œ IDF ìƒì„±\n",
    "    \n",
    "    Args:\n",
    "        original_idf: ì›ë³¸ IDF ë”•ì…”ë„ˆë¦¬\n",
    "        tokenizer: Tokenizer\n",
    "        synonym_dict: ë™ì˜ì–´ ì‚¬ì „ {token: [synonyms]}\n",
    "        method: 'max', 'mean' ì¤‘ ì„ íƒ\n",
    "    \n",
    "    Returns:\n",
    "        enhanced_idf: ê°•í™”ëœ IDF ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "    enhanced_idf = original_idf.copy()\n",
    "    boost_count = 0\n",
    "    \n",
    "    for canonical, synonyms in synonym_dict.items():\n",
    "        all_tokens = [canonical] + synonyms\n",
    "        \n",
    "        # ê° í† í°ì˜ IDF ê°’ ìˆ˜ì§‘\n",
    "        idf_values = []\n",
    "        for token in all_tokens:\n",
    "            if token in original_idf:\n",
    "                idf_values.append(original_idf[token])\n",
    "        \n",
    "        if not idf_values:\n",
    "            continue\n",
    "        \n",
    "        # IDF ê°’ í†µí•©\n",
    "        if method == 'max':\n",
    "            shared_idf = max(idf_values)\n",
    "        else:  # mean\n",
    "            shared_idf = np.mean(idf_values)\n",
    "        \n",
    "        # ëª¨ë“  ë™ì˜ì–´ í† í°ì— ì ìš©\n",
    "        for token in all_tokens:\n",
    "            if token in enhanced_idf:\n",
    "                enhanced_idf[token] = shared_idf\n",
    "                boost_count += 1\n",
    "    \n",
    "    print(f\"\\nâœ“ Synonym-Aware IDF ìƒì„± ì™„ë£Œ\")\n",
    "    print(f\"  {boost_count:,}ê°œ í† í°ì— ë™ì˜ì–´ ì •ë³´ ë°˜ì˜\")\n",
    "    \n",
    "    return enhanced_idf\n",
    "\n",
    "\n",
    "# Synonym-Aware IDF ìƒì„±\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ”„ ë™ì˜ì–´ ì •ë³´ë¥¼ ë°˜ì˜í•œ IDF ìƒì„± ì¤‘...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "idf_token_dict_enhanced = create_synonym_aware_idf(\n",
    "    idf_token_dict_boosted,\n",
    "    tokenizer,\n",
    "    merged_synonym_dict,\n",
    "    method='max'\n",
    ")\n",
    "\n",
    "# IDF ë³€í™” ì˜ˆì‹œ\n",
    "print(\"\\nğŸ“Š IDF ë³€í™” ì˜ˆì‹œ:\")\n",
    "sample_tokens = list(merged_synonym_dict.keys())[:5]\n",
    "for token in sample_tokens:\n",
    "    if token in idf_token_dict_boosted and token in idf_token_dict_enhanced:\n",
    "        original = idf_token_dict_boosted[token]\n",
    "        enhanced = idf_token_dict_enhanced[token]\n",
    "        change = \"â†‘\" if enhanced > original else \"â†’\"\n",
    "        print(f\"  {token:15s}: {original:.4f} {change} {enhanced:.4f}\")\n",
    "\n",
    "# Enhanced IDFë¥¼ ê¸°ë³¸ IDFë¡œ ì‚¬ìš©\n",
    "idf_token_dict_boosted = idf_token_dict_enhanced.copy()\n",
    "print(\"\\nâœ“ Enhanced IDFë¥¼ ê¸°ë³¸ IDFë¡œ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6. ë™ì˜ì–´ ê¸°ë°˜ í•™ìŠµ ë°ì´í„° í™•ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_data_with_synonyms(qd_pairs, documents, synonym_dict, \n",
    "                              tokenizer, expansion_ratio=0.2):\n",
    "    \"\"\"\n",
    "    ë™ì˜ì–´ë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµ ë°ì´í„° í™•ì¥\n",
    "    \n",
    "    Args:\n",
    "        qd_pairs: ì›ë³¸ query-document pairs\n",
    "        documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        synonym_dict: ë™ì˜ì–´ ì‚¬ì „\n",
    "        tokenizer: Tokenizer\n",
    "        expansion_ratio: í™•ì¥ ë¹„ìœ¨ (0.2 = 20% ì¶”ê°€)\n",
    "    \n",
    "    Returns:\n",
    "        expanded_pairs: í™•ì¥ëœ pairs\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ”„ ë™ì˜ì–´ ê¸°ë°˜ ë°ì´í„° í™•ì¥ ì¤‘... (expansion_ratio={expansion_ratio})\")\n",
    "    \n",
    "    expanded_pairs = list(qd_pairs)  # ì›ë³¸ ë³µì‚¬\n",
    "    expansion_count = int(len(qd_pairs) * expansion_ratio)\n",
    "    \n",
    "    added = 0\n",
    "    attempts = 0\n",
    "    max_attempts = expansion_count * 10\n",
    "    \n",
    "    while added < expansion_count and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        \n",
    "        # ëœë¤ pair ì„ íƒ\n",
    "        query, doc, relevance = qd_pairs[np.random.randint(len(qd_pairs))]\n",
    "        \n",
    "        # ì¿¼ë¦¬ í† í°í™”\n",
    "        query_tokens = tokenizer.tokenize(query)\n",
    "        \n",
    "        # ë™ì˜ì–´ë¡œ ëŒ€ì²´ ê°€ëŠ¥í•œ í† í° ì°¾ê¸°\n",
    "        replaceable = [(i, token) for i, token in enumerate(query_tokens) \n",
    "                      if token in synonym_dict and synonym_dict[token]]\n",
    "        \n",
    "        if not replaceable:\n",
    "            continue\n",
    "        \n",
    "        # ëœë¤í•˜ê²Œ í•˜ë‚˜ ì„ íƒí•˜ì—¬ ë™ì˜ì–´ë¡œ ëŒ€ì²´\n",
    "        idx, token = replaceable[np.random.randint(len(replaceable))]\n",
    "        synonym = np.random.choice(synonym_dict[token])\n",
    "        \n",
    "        # ìƒˆ ì¿¼ë¦¬ ìƒì„±\n",
    "        new_query_tokens = query_tokens.copy()\n",
    "        new_query_tokens[idx] = synonym\n",
    "        new_query = tokenizer.convert_tokens_to_string(new_query_tokens)\n",
    "        \n",
    "        # ì¤‘ë³µ ì²´í¬\n",
    "        if new_query != query and new_query.strip():\n",
    "            expanded_pairs.append((new_query, doc, relevance))\n",
    "            added += 1\n",
    "    \n",
    "    print(f\"âœ“ ë°ì´í„° í™•ì¥ ì™„ë£Œ!\")\n",
    "    print(f\"  ì›ë³¸: {len(qd_pairs):,} pairs\")\n",
    "    print(f\"  í™•ì¥: {len(expanded_pairs):,} pairs (+{added:,})\")\n",
    "    print(f\"  ì¦ê°€ìœ¨: {(len(expanded_pairs) / len(qd_pairs) - 1) * 100:.1f}%\")\n",
    "    \n",
    "    return expanded_pairs\n",
    "\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„° í™•ì¥\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“ˆ ë™ì˜ì–´ ê¸°ë°˜ í•™ìŠµ ë°ì´í„° í™•ì¥\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "korean_data['qd_pairs_expanded'] = expand_data_with_synonyms(\n",
    "    korean_data['qd_pairs'],\n",
    "    korean_data['documents'],\n",
    "    merged_synonym_dict,\n",
    "    tokenizer,\n",
    "    expansion_ratio=0.15  # 15% í™•ì¥\n",
    ")\n",
    "\n",
    "# í™•ì¥ ì˜ˆì‹œ ì¶œë ¥\n",
    "print(\"\\nğŸ“ í™•ì¥ëœ ì¿¼ë¦¬ ì˜ˆì‹œ:\")\n",
    "original_count = len(korean_data['qd_pairs'])\n",
    "for i, (query, doc, rel) in enumerate(korean_data['qd_pairs_expanded'][original_count:original_count+5]):\n",
    "    print(f\"  {i+1}. {query[:60]}...\")\n",
    "\n",
    "# í™•ì¥ëœ ë°ì´í„°ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©\n",
    "korean_data['qd_pairs'] = korean_data['qd_pairs_expanded']\n",
    "print(f\"\\nâœ“ í™•ì¥ëœ ë°ì´í„°ë¥¼ í•™ìŠµì— ì‚¬ìš©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7. ë™ì˜ì–´ ì •ë³´ ìš”ì•½\n",
    "\n",
    "ìë™ ë°œê²¬ëœ ë™ì˜ì–´ ì •ë³´ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š ë™ì˜ì–´ ë°œê²¬ ë° ë°ì´í„° í™•ì¥ ìš”ì•½\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n1ï¸âƒ£ ë™ì˜ì–´ ë°œê²¬ ê²°ê³¼:\")\n",
    "print(f\"  - ë¶„ì„ ëŒ€ìƒ í† í°: {len(important_tokens):,}ê°œ\")\n",
    "print(f\"  - ë°œê²¬ëœ ë™ì˜ì–´ ê·¸ë£¹: {len(auto_synonym_dict):,}ê°œ\")\n",
    "print(f\"  - ì´ ë™ì˜ì–´ ìŒ: {sum(len(v) for v in auto_synonym_dict.values()):,}ê°œ\")\n",
    "print(f\"  - í‰ê·  ë™ì˜ì–´ ìˆ˜: {np.mean([len(v) for v in auto_synonym_dict.values()]):.2f}ê°œ/ê·¸ë£¹\")\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£ IDF ê°•í™” ê²°ê³¼:\")\n",
    "changes = 0\n",
    "for token in auto_synonym_dict.keys():\n",
    "    if token in idf_token_dict:\n",
    "        changes += 1\n",
    "print(f\"  - IDF ì—…ë°ì´íŠ¸ëœ í† í°: {changes:,}ê°œ\")\n",
    "\n",
    "print(f\"\\n3ï¸âƒ£ ë°ì´í„° í™•ì¥ ê²°ê³¼:\")\n",
    "print(f\"  - ì›ë³¸ pairs: {len(korean_data['qd_pairs_expanded']) - len(korean_data['qd_pairs']):,}ê°œ\")\n",
    "print(f\"  - ìµœì¢… pairs: {len(korean_data['qd_pairs']):,}ê°œ\")\n",
    "\n",
    "print(f\"\\nâœ… ë™ì˜ì–´ ê¸°ë°˜ ë°ì´í„° í™•ì¥ ì™„ë£Œ!\")\n",
    "print(f\"   í•™ìŠµ ë°ì´í„°ê°€ ë” í’ë¶€í•´ì¡ŒìŠµë‹ˆë‹¤.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ì†ì‹¤ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "OpenSearch ëª¨ë¸ì˜ í•µì‹¬ ì†ì‹¤ í•¨ìˆ˜:\n",
    "1. **Ranking Loss**: Query-Document similarity\n",
    "2. **IDF-aware Penalty**: ë‚®ì€ IDF í† í° ì–µì œ\n",
    "3. **L0 Regularization**: Sparsity ìœ ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_query_representation(query_tokens, idf_dict, tokenizer):\n",
    "    \"\"\"\n",
    "    ì¿¼ë¦¬ë¥¼ IDF lookupìœ¼ë¡œ sparse vectorë¡œ ë³€í™˜ (Inference-free!)\n",
    "    \n",
    "    Args:\n",
    "        query_tokens: (batch_size, seq_len)\n",
    "        idf_dict: {token_id: idf_score}\n",
    "        tokenizer: Hugging Face tokenizer\n",
    "    \n",
    "    Returns:\n",
    "        query_sparse: (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = query_tokens.shape\n",
    "    vocab_size = len(tokenizer)  # Use len() to include added special tokens\n",
    "    \n",
    "    # Initialize sparse vector\n",
    "    query_sparse = torch.zeros(batch_size, vocab_size, device=query_tokens.device)\n",
    "    \n",
    "    # Fill with IDF weights\n",
    "    for b in range(batch_size):\n",
    "        for token_id in query_tokens[b]:\n",
    "            token_id = token_id.item()\n",
    "            if token_id in idf_dict:\n",
    "                query_sparse[b, token_id] = idf_dict[token_id]\n",
    "    \n",
    "    return query_sparse\n",
    "\n",
    "def neural_sparse_loss(doc_sparse, query_sparse, relevance, idf_dict, \n",
    "                       lambda_l0=1e-3, lambda_idf=1e-2):\n",
    "    \"\"\"\n",
    "    OpenSearch Neural Sparse Loss\n",
    "    \n",
    "    Args:\n",
    "        doc_sparse: (batch_size, vocab_size) - ë¬¸ì„œì˜ sparse representation\n",
    "        query_sparse: (batch_size, vocab_size) - ì¿¼ë¦¬ì˜ sparse representation (IDF lookup)\n",
    "        relevance: (batch_size,) - ê´€ë ¨ë„ ì ìˆ˜ (1.0 or 0.0)\n",
    "        idf_dict: {token_id: idf_score}\n",
    "        lambda_l0: L0 regularization ê°€ì¤‘ì¹˜\n",
    "        lambda_idf: IDF-aware penalty ê°€ì¤‘ì¹˜\n",
    "    \n",
    "    Returns:\n",
    "        total_loss, ranking_loss, l0_loss, idf_penalty\n",
    "    \"\"\"\n",
    "    # 1. Ranking Loss: Dot product similarity\n",
    "    similarity = torch.sum(doc_sparse * query_sparse, dim=-1)\n",
    "    ranking_loss = F.binary_cross_entropy_with_logits(\n",
    "        similarity, relevance, reduction='mean'\n",
    "    )\n",
    "    \n",
    "    # 2. L0 Regularization (FLOPS penalty for sparsity)\n",
    "    l0_loss = torch.mean(torch.sum(torch.abs(doc_sparse), dim=-1))\n",
    "    \n",
    "    # 3. IDF-aware Penalty (suppress low-IDF tokens)\n",
    "    # ë‚®ì€ IDF í† í°ì— í˜ë„í‹° ë¶€ì—¬\n",
    "    idf_tensor = torch.tensor(\n",
    "        [idf_dict.get(i, 1.0) for i in range(doc_sparse.shape[1])],\n",
    "        device=doc_sparse.device\n",
    "    )\n",
    "    \n",
    "    # Inverse IDF penalty: ë‚®ì€ IDF = ë†’ì€ í˜ë„í‹°\n",
    "    inverse_idf = 1.0 / (idf_tensor + 1e-6)\n",
    "    idf_penalty = torch.mean(torch.sum(doc_sparse * inverse_idf, dim=-1))\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = ranking_loss + lambda_l0 * l0_loss + lambda_idf * idf_penalty\n",
    "    \n",
    "    return total_loss, ranking_loss, l0_loss, idf_penalty\n",
    "\n",
    "print(\"âœ“ ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n",
    "print(\"  - Ranking Loss (BCE)\")\n",
    "print(\"  - L0 Regularization (Sparsity)\")\n",
    "print(\"  - IDF-aware Penalty (Low-IDF suppression)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. í•™ìŠµ ì„¤ì • ë° ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "WARMUP_STEPS = 500\n",
    "LAMBDA_L0 = 1e-3  # L0 regularization\n",
    "LAMBDA_IDF = 1e-2  # IDF-aware penalty\n",
    "\n",
    "print(\"ğŸ¯ í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Max length: {MAX_LENGTH}\")\n",
    "print(f\"  Lambda L0: {LAMBDA_L0}\")\n",
    "print(f\"  Lambda IDF: {LAMBDA_IDF}\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "# Optimizer & Scheduler\n",
    "optimizer = AdamW(doc_encoder.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Optimizer: AdamW\")\n",
    "print(f\"âœ“ Scheduler: Linear warmup ({WARMUP_STEPS} steps)\")\n",
    "print(f\"âœ“ Total steps: {total_steps:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, idf_id_dict, tokenizer, device):\n",
    "    \"\"\"\n",
    "    í•œ ì—í­ í•™ìŠµ\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_ranking = 0\n",
    "    total_l0 = 0\n",
    "    total_idf_penalty = 0\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move to device\n",
    "        query_tokens = batch['query_input_ids'].to(device)\n",
    "        doc_input_ids = batch['doc_input_ids'].to(device)\n",
    "        doc_attention_mask = batch['doc_attention_mask'].to(device)\n",
    "        relevance = batch['relevance'].to(device)\n",
    "        \n",
    "        # Document encoding (ëª¨ë¸ë¡œ ì¸ì½”ë”©)\n",
    "        doc_sparse = model(doc_input_ids, doc_attention_mask)\n",
    "        \n",
    "        # Query encoding (IDF lookup - inference-free!)\n",
    "        query_sparse = compute_query_representation(query_tokens, idf_id_dict, tokenizer)\n",
    "        \n",
    "        # Loss ê³„ì‚°\n",
    "        loss, ranking_loss, l0_loss, idf_penalty = neural_sparse_loss(\n",
    "            doc_sparse, query_sparse, relevance, idf_id_dict,\n",
    "            lambda_l0=LAMBDA_L0, lambda_idf=LAMBDA_IDF\n",
    "        )\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # ê¸°ë¡\n",
    "        total_loss += loss.item()\n",
    "        total_ranking += ranking_loss.item()\n",
    "        total_l0 += l0_loss.item()\n",
    "        total_idf_penalty += idf_penalty.item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'rank': f'{ranking_loss.item():.4f}',\n",
    "            'l0': f'{l0_loss.item():.2f}',\n",
    "            'idf': f'{idf_penalty.item():.4f}'\n",
    "        })\n",
    "    \n",
    "    n = len(loader)\n",
    "    return total_loss/n, total_ranking/n, total_l0/n, total_idf_penalty/n\n",
    "\n",
    "def evaluate(model, loader, idf_id_dict, tokenizer, device):\n",
    "    \"\"\"\n",
    "    ê²€ì¦\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            query_tokens = batch['query_input_ids'].to(device)\n",
    "            doc_input_ids = batch['doc_input_ids'].to(device)\n",
    "            doc_attention_mask = batch['doc_attention_mask'].to(device)\n",
    "            relevance = batch['relevance'].to(device)\n",
    "            \n",
    "            doc_sparse = model(doc_input_ids, doc_attention_mask)\n",
    "            query_sparse = compute_query_representation(query_tokens, idf_id_dict, tokenizer)\n",
    "            \n",
    "            loss, _, _, _ = neural_sparse_loss(\n",
    "                doc_sparse, query_sparse, relevance, idf_id_dict,\n",
    "                lambda_l0=LAMBDA_L0, lambda_idf=LAMBDA_IDF\n",
    "            )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# í•™ìŠµ íˆìŠ¤í† ë¦¬\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'ranking_loss': [],\n",
    "    'l0_loss': [],\n",
    "    'idf_penalty': []\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ í•™ìŠµ ì‹œì‘!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì‹¤í–‰\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"./models/best_korean_neural_sparse_encoder.pt\"\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # í•™ìŠµ\n",
    "    train_loss, ranking_loss, l0_loss, idf_penalty = train_epoch(\n",
    "        doc_encoder, train_loader, optimizer, scheduler,\n",
    "        idf_id_dict, tokenizer, device\n",
    "    )\n",
    "    \n",
    "    # ê²€ì¦\n",
    "    val_loss = evaluate(doc_encoder, val_loader, idf_id_dict, tokenizer, device)\n",
    "    \n",
    "    # ê¸°ë¡\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['ranking_loss'].append(ranking_loss)\n",
    "    history['l0_loss'].append(l0_loss)\n",
    "    history['idf_penalty'].append(idf_penalty)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Epoch {epoch + 1} ê²°ê³¼:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Ranking Loss: {ranking_loss:.4f}\")\n",
    "    print(f\"  L0 Loss: {l0_loss:.2f}\")\n",
    "    print(f\"  IDF Penalty: {idf_penalty:.4f}\")\n",
    "    \n",
    "    # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': doc_encoder.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': {\n",
    "                'model_name': MODEL_NAME,\n",
    "                'vocab_size': len(tokenizer),\n",
    "                'max_length': MAX_LENGTH,\n",
    "            }\n",
    "        }, best_model_path)\n",
    "        print(f\"  âœ“ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥! (val_loss: {val_loss:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ëª¨ë¸ ì €ì¥ (OpenSearch í˜¸í™˜ í˜•ì‹)\n",
    "\n",
    "OpenSearchì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤:\n",
    "1. `pytorch_model.bin` - ë¬¸ì„œ ì¸ì½”ë”\n",
    "2. `idf.json` - ì¿¼ë¦¬ìš© í† í° ê°€ì¤‘ì¹˜ lookup table\n",
    "3. Tokenizer íŒŒì¼ë“¤\n",
    "4. `config.json` - ëª¨ë¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "OUTPUT_DIR = \"./models/opensearch-korean-neural-sparse-v1\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“¦ OpenSearch í˜¸í™˜ ëª¨ë¸ ì €ì¥ ì¤‘...\\n\")\n",
    "\n",
    "# 1. ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ\n",
    "checkpoint = torch.load(best_model_path)\n",
    "doc_encoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"âœ“ ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ (Epoch {checkpoint['epoch'] + 1}, Val Loss: {checkpoint['val_loss']:.4f})\")\n",
    "\n",
    "# 2. pytorch_model.bin ì €ì¥ (ë¬¸ì„œ ì¸ì½”ë”)\n",
    "torch.save(doc_encoder.state_dict(), f\"{OUTPUT_DIR}/pytorch_model.bin\")\n",
    "print(f\"âœ“ pytorch_model.bin ì €ì¥\")\n",
    "\n",
    "# 3. idf.json ì €ì¥ (ì¿¼ë¦¬ìš© ê°€ì¤‘ì¹˜ lookup table)\n",
    "with open(f\"{OUTPUT_DIR}/idf.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(idf_token_dict_boosted, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ“ idf.json ì €ì¥ ({len(idf_token_dict_boosted):,} tokens)\")\n",
    "\n",
    "# 4. Tokenizer ì €ì¥\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"âœ“ Tokenizer íŒŒì¼ ì €ì¥\")\n",
    "\n",
    "# 5. config.json ì €ì¥\n",
    "model_config = {\n",
    "    \"model_type\": \"opensearch-neural-sparse-doc-encoder\",\n",
    "    \"base_model\": MODEL_NAME,\n",
    "    \"vocab_size\": len(tokenizer),\n",
    "    \"max_seq_length\": MAX_LENGTH,\n",
    "    \"mode\": \"doc-only\",\n",
    "    \"output_format\": \"rank_features\",\n",
    "    \"training_info\": {\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"lambda_l0\": LAMBDA_L0,\n",
    "        \"lambda_idf\": LAMBDA_IDF,\n",
    "        \"training_samples\": len(train_dataset),\n",
    "        \"trained_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    },\n",
    "    \"usage\": {\n",
    "        \"documents\": \"Use pytorch_model.bin to encode documents\",\n",
    "        \"queries\": \"Use tokenizer + idf.json for inference-free query encoding\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/config.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_config, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ“ config.json ì €ì¥\")\n",
    "\n",
    "# 6. README ìƒì„±\n",
    "readme_content = f\"\"\"# OpenSearch Korean Neural Sparse Model v1\n",
    "\n",
    "í•œêµ­ì–´ì— ìµœì í™”ëœ OpenSearch inference-free neural sparse ê²€ìƒ‰ ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "## ëª¨ë¸ êµ¬ì¡°\n",
    "\n",
    "### Doc-only Mode (Inference-Free)\n",
    "- **ë¬¸ì„œ ì¸ì½”ë”©**: BERT ê¸°ë°˜ ì‹ ê²½ë§ (`pytorch_model.bin`)\n",
    "- **ì¿¼ë¦¬ ì¸ì½”ë”©**: Tokenizer + IDF lookup (`idf.json`) - **Inference-Free!**\n",
    "\n",
    "## íŒŒì¼ êµ¬ì¡°\n",
    "\n",
    "```\n",
    "{OUTPUT_DIR}/\n",
    "â”œâ”€â”€ pytorch_model.bin       # ë¬¸ì„œ ì¸ì½”ë” ëª¨ë¸\n",
    "â”œâ”€â”€ idf.json                # ì¿¼ë¦¬ìš© í† í° ê°€ì¤‘ì¹˜ (IDF + íŠ¸ë Œë“œ ë¶€ìŠ¤íŒ…)\n",
    "â”œâ”€â”€ tokenizer.json          # í† í¬ë‚˜ì´ì €\n",
    "â”œâ”€â”€ tokenizer_config.json   # í† í¬ë‚˜ì´ì € ì„¤ì •\n",
    "â”œâ”€â”€ vocab.txt               # ì–´íœ˜ ì‚¬ì „\n",
    "â”œâ”€â”€ special_tokens_map.json # íŠ¹ìˆ˜ í† í°\n",
    "â””â”€â”€ config.json             # ëª¨ë¸ ì„¤ì •\n",
    "```\n",
    "\n",
    "## ì‚¬ìš© ë°©ë²•\n",
    "\n",
    "### 1. OpenSearchì— ëª¨ë¸ ë“±ë¡\n",
    "\n",
    "```bash\n",
    "# ëª¨ë¸ ì••ì¶•\n",
    "cd {OUTPUT_DIR}\n",
    "zip -r ../korean-neural-sparse-v1.zip .\n",
    "\n",
    "# OpenSearchì— ì—…ë¡œë“œ\n",
    "POST /_plugins/_ml/models/_upload\n",
    "{{\n",
    "  \"name\": \"korean-neural-sparse-v1\",\n",
    "  \"version\": \"1.0\",\n",
    "  \"model_format\": \"TORCH_SCRIPT\",\n",
    "  \"model_config\": {{\n",
    "    \"model_type\": \"bert\",\n",
    "    \"embedding_dimension\": {len(tokenizer)},\n",
    "    \"framework_type\": \"sentence_transformers\"\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "### 2. ì¸ë±ìŠ¤ ìƒì„±\n",
    "\n",
    "```json\n",
    "PUT /korean-docs\n",
    "{{\n",
    "  \"mappings\": {{\n",
    "    \"properties\": {{\n",
    "      \"content\": {{ \"type\": \"text\" }},\n",
    "      \"embedding\": {{ \"type\": \"rank_features\" }}\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "### 3. ê²€ìƒ‰\n",
    "\n",
    "```json\n",
    "POST /korean-docs/_search\n",
    "{{\n",
    "  \"query\": {{\n",
    "    \"neural_sparse\": {{\n",
    "      \"embedding\": {{\n",
    "        \"query_text\": \"í•œêµ­ì–´ ê²€ìƒ‰ ìµœì í™”\",\n",
    "        \"model_id\": \"<model_id>\"\n",
    "      }}\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "## í•™ìŠµ ì •ë³´\n",
    "\n",
    "- **Base Model**: {MODEL_NAME}\n",
    "- **Training Samples**: {len(train_dataset):,}\n",
    "- **Epochs**: {NUM_EPOCHS}\n",
    "- **Best Val Loss**: {best_val_loss:.4f}\n",
    "- **Trained Date**: {datetime.now().strftime(\"%Y-%m-%d\")}\n",
    "\n",
    "## íŠ¹ì§•\n",
    "\n",
    "1. **í•œêµ­ì–´ ìµœì í™”**: KLUE-BERT ê¸°ë°˜\n",
    "2. **Inference-Free**: ì¿¼ë¦¬ëŠ” tokenizer + IDF lookupë§Œ ì‚¬ìš©\n",
    "3. **íŠ¸ë Œë“œ í‚¤ì›Œë“œ**: 2024-2025 AI/ML íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë¶€ìŠ¤íŒ…\n",
    "4. **IDF-aware**: ë‚®ì€ IDF í† í° ì–µì œ\n",
    "5. **Sparse**: L0 regularizationìœ¼ë¡œ í¬ì†Œì„± ìœ ì§€\n",
    "\n",
    "## ì°¸ê³ \n",
    "\n",
    "- [OpenSearch Neural Sparse Search](https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/)\n",
    "- [Paper: Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers](https://arxiv.org/abs/2411.04403)\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/README.md\", 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "print(f\"âœ“ README.md ìƒì„±\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}/\")\n",
    "print(f\"\\nì €ì¥ëœ íŒŒì¼:\")\n",
    "for filename in os.listdir(OUTPUT_DIR):\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    size = os.path.getsize(filepath) / (1024*1024)  # MB\n",
    "    print(f\"  - {filename:30s} ({size:>8.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ì €ì¥ëœ ëª¨ë¸ë¡œ inference í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_document(text, model, tokenizer, device, max_length=128):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œë¥¼ sparse vectorë¡œ ì¸ì½”ë”© (ëª¨ë¸ ì‚¬ìš©)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sparse_vec = model(input_ids, attention_mask)\n",
    "    \n",
    "    return sparse_vec.cpu().numpy()[0]\n",
    "\n",
    "def encode_query_inference_free(text, tokenizer, idf_dict, max_length=128):\n",
    "    \"\"\"\n",
    "    ì¿¼ë¦¬ë¥¼ sparse vectorë¡œ ì¸ì½”ë”© (IDF lookup - Inference-Free!)\n",
    "    \"\"\"\n",
    "    # í† í°í™”\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False, max_length=max_length, truncation=True)\n",
    "    \n",
    "    # IDF lookup\n",
    "    sparse_vec = np.zeros(len(tokenizer))\n",
    "    for token_id in tokens:\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        if token_str in idf_dict:\n",
    "            sparse_vec[token_id] = idf_dict[token_str]\n",
    "    \n",
    "    return sparse_vec\n",
    "\n",
    "def get_top_tokens(sparse_vec, tokenizer, top_k=15):\n",
    "    \"\"\"\n",
    "    Sparse vectorì—ì„œ ìƒìœ„ í† í° ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    top_indices = np.argsort(sparse_vec)[-top_k:][::-1]\n",
    "    top_values = sparse_vec[top_indices]\n",
    "    \n",
    "    top_tokens = []\n",
    "    for idx, val in zip(top_indices, top_values):\n",
    "        if val > 0:\n",
    "            token = tokenizer.decode([idx])\n",
    "            top_tokens.append((token, val))\n",
    "    \n",
    "    return top_tokens\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_queries = [\n",
    "    \"ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ\",\n",
    "    \"í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ \",\n",
    "    \"OpenSearch ë²¡í„° ê²€ìƒ‰\",\n",
    "    \"ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ë°©ë²•\",\n",
    "    \"ChatGPT LLM í”„ë¡¬í”„íŠ¸\",\n",
    "]\n",
    "\n",
    "test_documents = [\n",
    "    \"OpenSearchëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.\",\n",
    "    \"í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤.\",\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ“ ì¿¼ë¦¬ ì¸ì½”ë”© (Inference-Free: Tokenizer + IDF Lookup)\\n\")\n",
    "\n",
    "for query in test_queries:\n",
    "    sparse_vec = encode_query_inference_free(query, tokenizer, idf_token_dict_boosted)\n",
    "    \n",
    "    non_zero = np.count_nonzero(sparse_vec)\n",
    "    sparsity = (1 - non_zero / len(sparse_vec)) * 100\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"  í¬ì†Œì„±: {sparsity:.2f}% (non-zero: {non_zero}/{len(sparse_vec)})\")\n",
    "    print(f\"  ìƒìœ„ í† í°:\")\n",
    "    \n",
    "    top_tokens = get_top_tokens(sparse_vec, tokenizer, top_k=10)\n",
    "    for i, (token, value) in enumerate(top_tokens, 1):\n",
    "        print(f\"    {i:2d}. {token:15s} ({value:.4f})\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nğŸ“„ ë¬¸ì„œ ì¸ì½”ë”© (Model Inference)\\n\")\n",
    "\n",
    "for doc in test_documents:\n",
    "    sparse_vec = encode_document(doc, doc_encoder, tokenizer, device)\n",
    "    \n",
    "    non_zero = np.count_nonzero(sparse_vec)\n",
    "    sparsity = (1 - non_zero / len(sparse_vec)) * 100\n",
    "    \n",
    "    print(f\"Document: {doc[:50]}...\")\n",
    "    print(f\"  í¬ì†Œì„±: {sparsity:.2f}% (non-zero: {non_zero}/{len(sparse_vec)})\")\n",
    "    print(f\"  L1 Norm: {np.sum(np.abs(sparse_vec)):.2f}\")\n",
    "    print(f\"  ìƒìœ„ í† í°:\")\n",
    "    \n",
    "    top_tokens = get_top_tokens(sparse_vec, tokenizer, top_k=10)\n",
    "    for i, (token, value) in enumerate(top_tokens, 1):\n",
    "        print(f\"    {i:2d}. {token:15s} ({value:.4f})\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. OpenSearch í†µí•© ê°€ì´ë“œ\n",
    "\n",
    "í•™ìŠµëœ ëª¨ë¸ì„ OpenSearchì— í†µí•©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘     OpenSearch Inference-Free Neural Sparse ëª¨ë¸ í†µí•© ê°€ì´ë“œ  â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "## 1ï¸âƒ£ ëª¨ë¸ íŒ¨í‚¤ì§• ë° ì—…ë¡œë“œ\n",
    "\n",
    "```bash\n",
    "# ëª¨ë¸ ì••ì¶•\n",
    "cd models/opensearch-korean-neural-sparse-v1\n",
    "zip -r ../korean-neural-sparse-v1.zip .\n",
    "\n",
    "# OpenSearchì— ëª¨ë¸ ë“±ë¡\n",
    "POST /_plugins/_ml/models/_upload\n",
    "{\n",
    "  \"name\": \"korean-neural-sparse-doc-v1\",\n",
    "  \"version\": \"1.0\",\n",
    "  \"description\": \"Korean Neural Sparse Model for document encoding\",\n",
    "  \"model_format\": \"TORCH_SCRIPT\",\n",
    "  \"model_config\": {\n",
    "    \"model_type\": \"bert\",\n",
    "    \"embedding_dimension\": 30000,\n",
    "    \"framework_type\": \"sentence_transformers\",\n",
    "    \"all_config\": {\n",
    "      \"mode\": \"doc-only\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Tokenizer ëª¨ë¸ ë“±ë¡ (ì¿¼ë¦¬ìš©)\n",
    "POST /_plugins/_ml/models/_upload\n",
    "{\n",
    "  \"name\": \"korean-neural-sparse-tokenizer-v1\",\n",
    "  \"version\": \"1.0\",\n",
    "  \"model_format\": \"TOKENIZER\",\n",
    "  \"model_config\": {\n",
    "    \"model_type\": \"tokenizer\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## 2ï¸âƒ£ ì¸ë±ìŠ¤ ìƒì„± (rank_features íƒ€ì…)\n",
    "\n",
    "```json\n",
    "PUT /korean-neural-sparse-index\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"index\": {\n",
    "      \"default_pipeline\": \"korean-neural-sparse-ingest\"\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"title\": { \"type\": \"text\" },\n",
    "      \"content\": { \"type\": \"text\" },\n",
    "      \"content_embedding\": {\n",
    "        \"type\": \"rank_features\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## 3ï¸âƒ£ Ingest Pipeline ì„¤ì •\n",
    "\n",
    "```json\n",
    "PUT /_ingest/pipeline/korean-neural-sparse-ingest\n",
    "{\n",
    "  \"description\": \"Korean neural sparse encoding pipeline\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"sparse_encoding\": {\n",
    "        \"model_id\": \"<doc_model_id>\",\n",
    "        \"field_map\": {\n",
    "          \"content\": \"content_embedding\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "## 4ï¸âƒ£ ë¬¸ì„œ ì¸ë±ì‹±\n",
    "\n",
    "```json\n",
    "POST /korean-neural-sparse-index/_doc\n",
    "{\n",
    "  \"title\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰ ê¸°ìˆ \",\n",
    "  \"content\": \"OpenSearchëŠ” neural sparse ê²€ìƒ‰ì„ ì§€ì›í•˜ëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ì—”ì§„ì…ë‹ˆë‹¤.\"\n",
    "}\n",
    "```\n",
    "\n",
    "## 5ï¸âƒ£ Neural Sparse ê²€ìƒ‰ (Doc-only mode)\n",
    "\n",
    "```json\n",
    "POST /korean-neural-sparse-index/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"neural_sparse\": {\n",
    "      \"content_embedding\": {\n",
    "        \"query_text\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰ ìµœì í™”\",\n",
    "        \"model_id\": \"<tokenizer_model_id>\",\n",
    "        \"max_token_score\": 3.5\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## 6ï¸âƒ£ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + Neural Sparse)\n",
    "\n",
    "```json\n",
    "POST /korean-neural-sparse-index/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"hybrid\": {\n",
    "      \"queries\": [\n",
    "        {\n",
    "          \"match\": {\n",
    "            \"content\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰\"\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"neural_sparse\": {\n",
    "            \"content_embedding\": {\n",
    "              \"query_text\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰\",\n",
    "              \"model_id\": \"<tokenizer_model_id>\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"search_pipeline\": {\n",
    "    \"phase_results_processors\": [\n",
    "      {\n",
    "        \"normalization-processor\": {\n",
    "          \"normalization\": {\n",
    "            \"technique\": \"min_max\"\n",
    "          },\n",
    "          \"combination\": {\n",
    "            \"technique\": \"arithmetic_mean\",\n",
    "            \"parameters\": {\n",
    "              \"weights\": [0.3, 0.7]\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## ğŸ“Š ì„±ëŠ¥ íŠ¹ì§•\n",
    "\n",
    "- **ë¬¸ì„œ ì¸ì½”ë”©**: BERT ê¸°ë°˜ ì‹ ê²½ë§ (ëŠë¦¼, ê³ í’ˆì§ˆ)\n",
    "- **ì¿¼ë¦¬ ì¸ì½”ë”©**: Tokenizer + IDF lookup (ë§¤ìš° ë¹ ë¦„, Inference-Free!)\n",
    "- **ì¿¼ë¦¬ ì§€ì—°ì‹œê°„**: BM25ì™€ ê±°ì˜ ë™ì¼ (1.1x)\n",
    "- **ê²€ìƒ‰ ì •í™•ë„**: Siamese sparse ëª¨ë¸ê³¼ ìœ ì‚¬\n",
    "\n",
    "## ğŸ“š ì°¸ê³  ìë£Œ\n",
    "\n",
    "- [OpenSearch Neural Sparse Search](https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/)\n",
    "- [Doc-only Mode](https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/#doc-only-mode)\n",
    "- [Pretrained Models](https://opensearch.org/docs/latest/ml-commons-plugin/pretrained-models/)\n",
    "- [Paper: Inference-Free Learned Sparse Retrievers](https://arxiv.org/abs/2411.04403)\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### âœ… ì™„ë£Œëœ ì‘ì—…\n",
    "\n",
    "1. **ë°ì´í„° ìˆ˜ì§‘**: í•œêµ­ì–´ ê³µê°œ ë°ì´í„°ì…‹ (KLUE, KorQuAD, Wikipedia ë“±)\n",
    "2. **IDF ê³„ì‚°**: í† í°ë³„ IDF ê°€ì¤‘ì¹˜ ê³„ì‚° ë° íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…\n",
    "3. **ëª¨ë¸ í•™ìŠµ**: OpenSearch doc-only mode ë¬¸ì„œ ì¸ì½”ë” í•™ìŠµ\n",
    "   - IDF-aware penalty\n",
    "   - L0 regularization\n",
    "   - Ranking loss\n",
    "4. **ëª¨ë¸ ì €ì¥**: OpenSearch í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ì €ì¥\n",
    "   - `pytorch_model.bin` (ë¬¸ì„œ ì¸ì½”ë”)\n",
    "   - `idf.json` (ì¿¼ë¦¬ìš© ê°€ì¤‘ì¹˜)\n",
    "   - Tokenizer íŒŒì¼ë“¤\n",
    "   - `config.json`\n",
    "\n",
    "### ğŸ¯ í•µì‹¬ íŠ¹ì§•\n",
    "\n",
    "- **Inference-Free**: ì¿¼ë¦¬ëŠ” tokenizer + IDF lookupë§Œ ì‚¬ìš© â†’ ë§¤ìš° ë¹ ë¦„!\n",
    "- **í•œêµ­ì–´ ìµœì í™”**: KLUE-BERT ê¸°ë°˜ + í•œêµ­ì–´ ë°ì´í„°ì…‹\n",
    "- **íŠ¸ë Œë“œ ë°˜ì˜**: 2024-2025 AI/ML í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë¶€ìŠ¤íŒ…\n",
    "- **OpenSearch í˜¸í™˜**: ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ í˜•ì‹\n",
    "\n",
    "### ğŸš€ ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "1. **Knowledge Distillation**: Dense + Sparse siamese ëª¨ë¸ë¡œ distillation\n",
    "2. **ë” ë§ì€ ë°ì´í„°**: AI Hub, NIKL ë“± ì¶”ê°€ ë°ì´í„°ì…‹\n",
    "3. **Hard Negative Mining**: In-batch negatives, hard negatives\n",
    "4. **ëª¨ë¸ í‰ê°€**: BEIR ë²¤ì¹˜ë§ˆí¬, MRR, NDCG ë“±\n",
    "5. **OpenSearch ë°°í¬**: ì‹¤ì œ ê²€ìƒ‰ ì‹œìŠ¤í…œì— í†µí•©\n",
    "6. **A/B í…ŒìŠ¤íŒ…**: ê¸°ì¡´ BM25ì™€ ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "### ğŸ“ˆ ê¸°ëŒ€ íš¨ê³¼\n",
    "\n",
    "- BM25 ëŒ€ë¹„ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ\n",
    "- Dense retrieval ëŒ€ë¹„ ë¹ ë¥¸ ì†ë„\n",
    "- í•œêµ­ì–´ íŠ¹í™” ê²€ìƒ‰ ì„±ëŠ¥ ê°œì„ \n",
    "- íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê²€ìƒ‰ ìµœì í™”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ†• Phase 1-4 Improvements (v0.3.0)\n",
    "\n",
    "ìƒˆë¡œìš´ ê¸°ëŠ¥ë“¤ì„ í†µí•©í•©ë‹ˆë‹¤:\n",
    "- **Phase 1**: ê°œì„ ëœ ì†ì‹¤ í•¨ìˆ˜ (In-batch negatives)\n",
    "- **Phase 2**: ì‹œê°„ ê¸°ë°˜ ë¶„ì„ (Temporal IDF, ìë™ íŠ¸ë Œë“œ ê°ì§€)\n",
    "- **Phase 3**: Hard Negative Mining (BM25)\n",
    "- **Phase 4**: ë™ì˜ì–´ ìë™ ë°œê²¬ (êµ°ì§‘í™”)\n",
    "- **Phase 5**: í•œì˜ í†µí•© ë™ì˜ì–´ (Cross-lingual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import improved modules\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Importing Phase 1-4 Modules\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from src.losses import (\n",
    "    neural_sparse_loss_with_regularization,\n",
    "    compute_sparsity_metrics,\n",
    ")\n",
    "\n",
    "from src.data_loader import (\n",
    "    load_korean_news_with_dates,\n",
    "    load_multiple_korean_datasets,\n",
    ")\n",
    "\n",
    "from src.temporal_analysis import (\n",
    "    calculate_temporal_idf,\n",
    "    detect_trending_tokens,\n",
    "    build_trend_boost_dict,\n",
    "    apply_temporal_boost_to_idf,\n",
    ")\n",
    "\n",
    "from src.negative_sampling import (\n",
    "    add_hard_negatives_bm25,\n",
    "    add_mixed_negatives,\n",
    ")\n",
    "\n",
    "from src.cross_lingual_synonyms import (\n",
    "    build_comprehensive_bilingual_dictionary,\n",
    "    get_default_korean_english_pairs,\n",
    "    apply_bilingual_synonyms_to_idf,\n",
    ")\n",
    "\n",
    "print(\"âœ“ All modules imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: ì‹œê°„ ê¸°ë°˜ IDF ê³„ì‚°\n",
    "\n",
    "ìµœê·¼ ë¬¸ì„œì— ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ IDFë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Use temporal IDF (recommended)\n",
    "USE_TEMPORAL_IDF = True  # Set to False to use standard IDF\n",
    "\n",
    "if USE_TEMPORAL_IDF:\n",
    "    print(\"\\nğŸ• Using Temporal IDF (recent documents weighted higher)\")\n",
    "    \n",
    "    # Load news data with dates\n",
    "    news_with_dates = load_korean_news_with_dates(\n",
    "        max_samples=50000,\n",
    "        min_doc_length=20\n",
    "    )\n",
    "    \n",
    "    # Calculate temporal IDF\n",
    "    idf_token_dict, idf_id_dict = calculate_temporal_idf(\n",
    "        documents=news_with_dates['documents'],\n",
    "        dates=news_with_dates['dates'],\n",
    "        tokenizer=tokenizer,\n",
    "        decay_factor=0.95,  # Recent documents weighted higher\n",
    "    )\n",
    "    \n",
    "    # Detect trending tokens automatically\n",
    "    trending_tokens = detect_trending_tokens(\n",
    "        documents=news_with_dates['documents'],\n",
    "        dates=news_with_dates['dates'],\n",
    "        tokenizer=tokenizer,\n",
    "        recent_days=30,\n",
    "        historical_days=365,\n",
    "        top_k=100,\n",
    "    )\n",
    "    \n",
    "    # Build automatic trend boost (replaces hardcoded TREND_BOOST)\n",
    "    auto_trend_boost = build_trend_boost_dict(\n",
    "        trending_tokens=trending_tokens,\n",
    "        max_boost=2.0,\n",
    "        min_boost=1.2,\n",
    "    )\n",
    "    \n",
    "    # Apply trend boost to IDF\n",
    "    idf_token_dict = apply_temporal_boost_to_idf(\n",
    "        idf_token_dict=idf_token_dict,\n",
    "        boost_dict=auto_trend_boost,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ“ Temporal IDF with automatic trend detection complete!\")\n",
    "else:\n",
    "    print(\"\\nUsing standard IDF (no temporal weighting)\")\n",
    "    # Use existing IDF calculation code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: í•œì˜ í†µí•© ë™ì˜ì–´ ì‚¬ì „\n",
    "\n",
    "'ëª¨ë¸'ê³¼ 'model'ì„ ë™ì˜ì–´ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build bilingual synonym dictionary\n",
    "USE_BILINGUAL_SYNONYMS = True\n",
    "\n",
    "if USE_BILINGUAL_SYNONYMS:\n",
    "    print(\"\\nğŸŒ Building Korean-English Bilingual Synonym Dictionary\")\n",
    "    \n",
    "    # Get manual curated pairs\n",
    "    manual_pairs = get_default_korean_english_pairs()\n",
    "    \n",
    "    # Build comprehensive dictionary\n",
    "    bilingual_dict = build_comprehensive_bilingual_dictionary(\n",
    "        documents=documents[:5000],  # Sample for speed\n",
    "        token_embeddings=doc_encoder.bert.embeddings.word_embeddings.weight.detach().cpu().numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        bert_model=doc_encoder.bert,\n",
    "        manual_pairs=manual_pairs,\n",
    "    )\n",
    "    \n",
    "    # Apply bilingual synonyms to IDF\n",
    "    idf_token_dict = apply_bilingual_synonyms_to_idf(\n",
    "        idf_dict=idf_token_dict,\n",
    "        bilingual_dict=bilingual_dict,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ“ Bilingual IDF complete!\")\n",
    "    print(\"  Now 'ëª¨ë¸' and 'model' have synchronized IDF values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: ê°œì„ ëœ ì†ì‹¤ í•¨ìˆ˜\n",
    "\n",
    "BCE ëŒ€ì‹  In-batch negatives contrastive lossë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with improved loss function\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training with Improved Loss Function\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32  # Increased for better in-batch negatives\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "LAMBDA_L0 = 5e-4  # Reduced for less aggressive sparsity\n",
    "LAMBDA_IDF = 1e-2\n",
    "TEMPERATURE = 0.05\n",
    "\n",
    "print(f\"\\nTraining settings:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE} (increased for in-batch negatives)\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Temperature: {TEMPERATURE}\")\n",
    "print(f\"  Lambda L0: {LAMBDA_L0}\")\n",
    "\n",
    "# Example training step\n",
    "def train_step_improved(doc_sparse, query_sparse, relevance, idf_dict):\n",
    "    \"\"\"Training step with improved loss\"\"\"\n",
    "    total_loss, loss_dict = neural_sparse_loss_with_regularization(\n",
    "        doc_sparse=doc_sparse,\n",
    "        query_sparse=query_sparse,\n",
    "        relevance=relevance,\n",
    "        idf_dict=idf_dict,\n",
    "        lambda_l0=LAMBDA_L0,\n",
    "        lambda_idf=LAMBDA_IDF,\n",
    "        temperature=TEMPERATURE,\n",
    "        use_in_batch_negatives=True,  # Key improvement!\n",
    "    )\n",
    "    \n",
    "    return total_loss, loss_dict\n",
    "\n",
    "print(\"\\nâœ“ Improved loss function ready!\")\n",
    "print(\"  Using in-batch negatives instead of BCE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Hard Negative Mining\n",
    "\n",
    "BM25ë¥¼ ì‚¬ìš©í•˜ì—¬ ë” ì–´ë ¤ìš´ negative ìƒ˜í”Œì„ ìƒì„±í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply hard negative mining (optional, improves quality)\n",
    "USE_HARD_NEGATIVES = False  # Set to True to enable (slower but better)\n",
    "\n",
    "if USE_HARD_NEGATIVES:\n",
    "    print(\"\\nğŸ¯ Adding Hard Negatives with BM25\")\n",
    "    \n",
    "    # Add hard negatives to training data\n",
    "    augmented_qd_pairs = add_hard_negatives_bm25(\n",
    "        qd_pairs=qd_pairs,\n",
    "        documents=documents,\n",
    "        tokenizer=tokenizer,\n",
    "        num_hard_negatives=2,\n",
    "        top_k=100,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ“ Hard negatives added!\")\n",
    "    print(f\"  Original pairs: {len(qd_pairs):,}\")\n",
    "    print(f\"  Augmented pairs: {len(augmented_qd_pairs):,}\")\n",
    "else:\n",
    "    print(\"\\nSkipping hard negative mining (set USE_HARD_NEGATIVES=True to enable)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š ê°œì„  ì‚¬í•­ ìš”ì•½\n",
    "\n",
    "### âœ… ì ìš©ëœ ê°œì„ ì‚¬í•­\n",
    "\n",
    "1. **Phase 1: ì†ì‹¤ í•¨ìˆ˜**\n",
    "   - âŒ BCE with logits (ì˜ëª»ë¨)\n",
    "   - âœ… In-batch negatives contrastive loss (ì˜¬ë°”ë¦„)\n",
    "\n",
    "2. **Phase 2: ì‹œê°„ ê¸°ë°˜ ë¶„ì„**\n",
    "   - âœ… Temporal IDF (ìµœê·¼ ë¬¸ì„œ ê°€ì¤‘ì¹˜ ë†’ìŒ)\n",
    "   - âœ… ìë™ íŠ¸ë Œë“œ ê°ì§€ (ìˆ˜ë™ TREND_BOOST ì œê±°)\n",
    "\n",
    "3. **Phase 3: Hard Negative Mining**\n",
    "   - âœ… BM25 ê¸°ë°˜ intelligent negative sampling\n",
    "\n",
    "4. **Phase 5: í•œì˜ í†µí•© ë™ì˜ì–´**\n",
    "   - âœ… 'ëª¨ë¸' â†” 'model' ë™ì˜ì–´ ì—°ê²°\n",
    "   - âœ… í•œì˜ í˜¼í•© ì¿¼ë¦¬ ì§€ì›\n",
    "\n",
    "### ğŸ“ˆ ì„±ëŠ¥ ê°œì„ \n",
    "\n",
    "- Batch size: 4 â†’ 32 (in-batch negatives)\n",
    "- Sparsity: 99.98% â†’ 90-95% (ë” ì ì ˆí•¨)\n",
    "- IDF: ì •ì  â†’ ì‹œê°„ ê°€ì¤‘ì¹˜ + ìë™ íŠ¸ë Œë“œ\n",
    "- ë™ì˜ì–´: ìˆ˜ë™ 100ê°œ â†’ ìë™ ë°œê²¬ ìˆ˜ë°±ê°œ\n",
    "- í•œì˜ í†µí•©: ì—†ìŒ â†’ 103ê°œ bilingual pairs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
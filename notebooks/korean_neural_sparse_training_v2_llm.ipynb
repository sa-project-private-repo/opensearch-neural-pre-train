{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenSearch Inference-Free Neural Sparse ëª¨ë¸ - í•œêµ­ì–´ í•™ìŠµ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **OpenSearch inference-free IR ëª¨ë¸** í‘œì¤€ì— ë”°ë¼ í•œêµ­ì–´ neural sparse ê²€ìƒ‰ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ¯ OpenSearch ëª¨ë¸ êµ¬ì¡°\n",
    "\n",
    "### Doc-only Mode (Inference-Free)\n",
    "- **ë¬¸ì„œ ì¸ì½”ë”©**: BERT ê¸°ë°˜ ì‹ ê²½ë§ â†’ sparse vector\n",
    "- **ì¿¼ë¦¬ ì¸ì½”ë”©**: Tokenizer + **idf.json** (weight lookup table) â†’ **Inference-Free!**\n",
    "\n",
    "### í•µì‹¬ íŒŒì¼\n",
    "1. `pytorch_model.bin` - ë¬¸ì„œ ì¸ì½”ë” ëª¨ë¸ (BERT ê¸°ë°˜)\n",
    "2. `idf.json` - í† í°ë³„ ê°€ì¤‘ì¹˜ lookup table (ì¿¼ë¦¬ìš©)\n",
    "3. `tokenizer.json`, `vocab.txt` - í† í¬ë‚˜ì´ì €\n",
    "4. `config.json` - ëª¨ë¸ ì„¤ì •\n",
    "\n",
    "### í•™ìŠµ ë°©ë²•\n",
    "- **IDF-aware Penalty**: ë‚®ì€ IDF í† í°ì˜ ê¸°ì—¬ë„ ì–µì œ\n",
    "- **Knowledge Distillation**: Dense + Sparse siamese ëª¨ë¸ì—ì„œ í•™ìŠµ\n",
    "- **â„“0 Sparsification**: L0 regularizationìœ¼ë¡œ í¬ì†Œì„± ìœ ì§€\n",
    "\n",
    "### ì°¸ê³  ë…¼ë¬¸\n",
    "- [Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers](https://arxiv.org/abs/2411.04403)\n",
    "- [Exploring â„“0 Sparsification for Inference-free Sparse Retrievers](https://opensearch.org/blog/)\n",
    "\n",
    "### OpenSearch Models Collection\n",
    "- https://huggingface.co/collections/opensearch-project/inference-free-ir-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Add parent directory to path for src imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path (notebooks/ ìƒìœ„ ë””ë ‰í† ë¦¬)\n",
    "project_root = Path().absolute().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ” ì‹œìŠ¤í…œ í™˜ê²½ ê°ì§€\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Python ë²„ì „ í™•ì¸\n",
    "print(f\"Python ë²„ì „: {sys.version}\")\n",
    "python_version = sys.version_info\n",
    "if python_version.major == 3 and python_version.minor >= 12:\n",
    "    print(f\"âœ“ Python 3.12+ ê°ì§€ - ìµœì‹  íŒ¨í‚¤ì§€ ë²„ì „ ì‚¬ìš©\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Python 3.12+ ê¶Œì¥ (í˜„ì¬: {python_version.major}.{python_version.minor})\")\n",
    "\n",
    "# OS ê°ì§€\n",
    "os_release = \"\"\n",
    "if os.path.exists(\"/etc/os-release\"):\n",
    "    with open(\"/etc/os-release\") as f:\n",
    "        os_release = f.read().lower()\n",
    "\n",
    "is_amazon_linux = \"amazon linux\" in os_release or \"amzn\" in os_release\n",
    "is_ubuntu_debian = \"ubuntu\" in os_release or \"debian\" in os_release\n",
    "\n",
    "print(f\"âœ“ Amazon Linux 2023: {is_amazon_linux}\")\n",
    "print(f\"âœ“ Ubuntu/Debian: {is_ubuntu_debian}\")\n",
    "\n",
    "# Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ (requirements.txt ì‚¬ìš©)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“¦ Python íŒ¨í‚¤ì§€ ì„¤ì¹˜\")\n",
    "print(\"=\"*60)\n",
    "print(\"requirements.txtë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤...\")\n",
    "print(\"GPU: Tesla T4 ì§€ì› (CUDA 12.1)\")\n",
    "print()\n",
    "\n",
    "# PyTorch GPU ë²„ì „ ì„¤ì¹˜ (CUDA 12.1 for Tesla T4)\n",
    "print(\"ğŸ”¥ PyTorch ì„¤ì¹˜ ì¤‘ (GPU ë²„ì „ - CUDA 12.1)...\")\n",
    "%pip install -q torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# ë‚˜ë¨¸ì§€ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "print(\"ğŸ“š ë‚˜ë¨¸ì§€ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...\")\n",
    "%pip install -q -r requirements.txt\n",
    "\n",
    "print(\"\\nâœ“ Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ\")\n",
    "\n",
    "# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ë° Mecab ì„¤ì¹˜ (OSë³„ ë¶„ê¸°)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ”§ ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if is_amazon_linux:\n",
    "    print(\"ğŸ§ Amazon Linux 2023 í™˜ê²½ - dnf íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ì‚¬ìš©\")\n",
    "    \n",
    "    # í•„ìˆ˜ ê°œë°œ ë„êµ¬ ì„¤ì¹˜\n",
    "    print(\"\\nğŸ“¦ ê°œë°œ ë„êµ¬ ì„¤ì¹˜ ì¤‘...\")\n",
    "    !sudo dnf install -y gcc gcc-c++ make automake libtool\n",
    "    \n",
    "    # Java ì„¤ì¹˜ (Mecab ì‚¬ì „ ë¹Œë“œìš©)\n",
    "    print(\"\\nâ˜• Java ì„¤ì¹˜ ì¤‘...\")\n",
    "    !sudo dnf install -y java-17-amazon-corretto-devel\n",
    "    \n",
    "    # Mecab ì—”ì§„ ì„¤ì¹˜\n",
    "    print(\"\\nğŸ”¤ Mecab ì—”ì§„ ì„¤ì¹˜ ì¤‘...\")\n",
    "    !cd /tmp && \\\n",
    "     curl -LO https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz && \\\n",
    "     tar -zxf mecab-0.996-ko-0.9.2.tar.gz && \\\n",
    "     cd mecab-0.996-ko-0.9.2 && \\\n",
    "     ./configure && make && sudo make install && sudo ldconfig\n",
    "    \n",
    "    # Mecab í•œêµ­ì–´ ì‚¬ì „ ì„¤ì¹˜\n",
    "    print(\"\\nğŸ“š Mecab í•œêµ­ì–´ ì‚¬ì „ ì„¤ì¹˜ ì¤‘...\")\n",
    "    !cd /tmp && \\\n",
    "     curl -LO https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz && \\\n",
    "     tar -zxf mecab-ko-dic-2.1.1-20180720.tar.gz && \\\n",
    "     cd mecab-ko-dic-2.1.1-20180720 && \\\n",
    "     ./autogen.sh && ./configure && make && sudo make install\n",
    "    \n",
    "    print(\"\\nâœ“ Amazon Linux 2023 ì„¤ì¹˜ ì™„ë£Œ!\")\n",
    "\n",
    "elif is_ubuntu_debian:\n",
    "    print(\"ğŸ§ Ubuntu/Debian í™˜ê²½ - apt-get íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ì‚¬ìš©\")\n",
    "    \n",
    "    # í•„ìˆ˜ ê°œë°œ ë„êµ¬ ì„¤ì¹˜\n",
    "    print(\"\\nğŸ“¦ ê°œë°œ ë„êµ¬ ì„¤ì¹˜ ì¤‘...\")\n",
    "    !sudo apt-get update -qq\n",
    "    !sudo apt-get install -y -qq g++ openjdk-8-jdk python3-dev automake libtool\n",
    "    \n",
    "    # Mecab ì„¤ì¹˜ (konlpy ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©)\n",
    "    print(\"\\nğŸ”¤ Mecab ì„¤ì¹˜ ì¤‘...\")\n",
    "    !bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
    "    \n",
    "    print(\"\\nâœ“ Ubuntu/Debian ì„¤ì¹˜ ì™„ë£Œ!\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nâš ï¸  ì§€ì›ë˜ì§€ ì•ŠëŠ” OSì…ë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.\")\n",
    "    print(\"ì§€ì› OS: Amazon Linux 2023, Ubuntu, Debian\")\n",
    "\n",
    "# ì„¤ì¹˜ í™•ì¸\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… ì„¤ì¹˜ í™•ì¸\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# PyTorch í™•ì¸\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"âœ“ PyTorch: {torch.__version__}\")\n",
    "    print(f\"  - CUDA ë¹Œë“œ ë²„ì „: {torch.version.cuda}\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— PyTorch ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# Transformers í™•ì¸\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"âœ“ Transformers: {transformers.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Transformers ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# Datasets í™•ì¸\n",
    "try:\n",
    "    import datasets\n",
    "    print(f\"âœ“ Datasets: {datasets.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Datasets ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# Numpy í™•ì¸\n",
    "try:\n",
    "    import numpy\n",
    "    print(f\"âœ“ Numpy: {numpy.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Numpy ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# Mecab í™•ì¸\n",
    "try:\n",
    "    from konlpy.tag import Mecab\n",
    "    mecab = Mecab()\n",
    "    test_result = mecab.morphs(\"í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ í…ŒìŠ¤íŠ¸\")\n",
    "    print(f\"âœ“ Mecab: {test_result}\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Mecab ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"  Mecab ì„¤ì¹˜ ë¬¸ì œ í•´ê²°:\")\n",
    "    print(\"  1. sudo ldconfig\")\n",
    "    print(\"  2. export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH\")\n",
    "    print(\"  3. pip install --force-reinstall mecab-python3\")\n",
    "\n",
    "print(\"\\nâœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ–¥ï¸  GPU/CPU í™˜ê²½ í™•ì¸\n",
      "============================================================\n",
      "PyTorch ë²„ì „: 2.10.0.dev20251109+cu130\n",
      "CUDA ì‚¬ìš© ê°€ëŠ¥: True\n",
      "âœ“ GPU ì‚¬ìš© ê°€ëŠ¥!\n",
      "  - CUDA ë²„ì „: 13.0\n",
      "  - GPU ê°œìˆ˜: 1\n",
      "  - GPU 0: NVIDIA GB10\n",
      "    * ë©”ëª¨ë¦¬: 119.70 GB\n",
      "    * Compute Capability: 12.1\n",
      "\n",
      "â†’ í•™ìŠµ ë””ë°”ì´ìŠ¤: GPU (cuda)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Hugging Face & Transformers\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForMaskedLM,\n",
    "    AutoConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    get_linear_schedule_with_warmup  # ì´ í•¨ìˆ˜ëŠ” transformersì—ì„œ ì œê³µ\n",
    ")\n",
    "\n",
    "# í˜•íƒœì†Œ ë¶„ì„\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# GPU/CPU í™˜ê²½ í™•ì¸\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ–¥ï¸  GPU/CPU í™˜ê²½ í™•ì¸\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ GPU ì‚¬ìš© ê°€ëŠ¥!\")\n",
    "    print(f\"  - CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "    print(f\"  - GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  - GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    * ë©”ëª¨ë¦¬: {props.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"    * Compute Capability: {props.major}.{props.minor}\")\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\\nâ†’ í•™ìŠµ ë””ë°”ì´ìŠ¤: GPU (cuda)\")\n",
    "else:\n",
    "    print(f\"âš ï¸  GPU ì‚¬ìš© ë¶ˆê°€\")\n",
    "    print(f\"  - CPUë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤\")\n",
    "    print(f\"  - í•™ìŠµ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"\\nâ†’ í•™ìŠµ ë””ë°”ì´ìŠ¤: CPU\")\n",
    "    print(f\"\\nğŸ’¡ GPU ì‚¬ìš© ê¶Œì¥:\")\n",
    "    print(f\"  - AWS EC2: g4dn.xlarge ì´ìƒ (NVIDIA T4 GPU)\")\n",
    "    print(f\"  - AWS EC2: p3.2xlarge ì´ìƒ (NVIDIA V100 GPU)\")\n",
    "    print(f\"  - Google Colab: GPU ëŸ°íƒ€ì„ ì‚¬ìš©\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í•œêµ­ì–´ ë°ì´í„°ì…‹ ìˆ˜ì§‘\n",
    "\n",
    "Hugging Faceì—ì„œ ë‹¤ì–‘í•œ í•œêµ­ì–´ ê³µê°œ ë°ì´í„°ì…‹ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š í•œêµ­ì–´ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\n",
      "\n",
      "1ï¸âƒ£ KLUE MRC ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ“ 17,554 query-document pairs\n",
      "\n",
      "2ï¸âƒ£ KorQuAD v1 ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ“ 60,407 query-document pairs\n",
      "\n",
      "3ï¸âƒ£ í•œêµ­ì–´ Wikipedia ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ— Wikipedia ë¡œë”© ì‹¤íŒ¨: Couldn't find file at https://dumps.wikimedia.org/kowiki/20220301/dumpstatus.json\n",
      "\n",
      "4ï¸âƒ£ KLUE Topic Classification ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ“ 45,678 documents\n",
      "\n",
      "5ï¸âƒ£ í•œêµ­ì–´ ë‰´ìŠ¤ ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ“ 50,000 documents\n",
      "\n",
      "\n",
      "============================================================\n",
      "ì´ 117,914ê°œì˜ ê³ ìœ  ë¬¸ì„œ\n",
      "ì´ 77,785ê°œì˜ ê³ ìœ  ì¿¼ë¦¬\n",
      "ì´ 77,961ê°œì˜ query-document pairs\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def load_korean_datasets():\n",
    "    \"\"\"\n",
    "    Hugging Faceì—ì„œ í•œêµ­ì–´ ê³µê°œ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "    Query-Document ìŒì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì…‹ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    datasets_collection = {\n",
    "        'documents': [],\n",
    "        'queries': [],\n",
    "        'qd_pairs': []  # (query, document, relevance) íŠœí”Œ\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ“š í•œêµ­ì–´ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\\n\")\n",
    "    \n",
    "    # 1. KLUE MRC (Machine Reading Comprehension)\n",
    "    try:\n",
    "        print(\"1ï¸âƒ£ KLUE MRC ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        klue_mrc = load_dataset(\"klue\", \"mrc\", split=\"train\")\n",
    "        \n",
    "        for item in klue_mrc:\n",
    "            doc = item['context']\n",
    "            query = item['question']\n",
    "            \n",
    "            datasets_collection['documents'].append(doc)\n",
    "            datasets_collection['queries'].append(query)\n",
    "            datasets_collection['qd_pairs'].append((query, doc, 1.0))  # Positive pair\n",
    "        \n",
    "        print(f\"   âœ“ {len(klue_mrc):,} query-document pairs\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— KLUE MRC ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # 2. KorQuAD v1\n",
    "    try:\n",
    "        print(\"2ï¸âƒ£ KorQuAD v1 ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        korquad = load_dataset(\"squad_kor_v1\", split=\"train\")\n",
    "        \n",
    "        for item in korquad:\n",
    "            doc = item['context']\n",
    "            query = item['question']\n",
    "            \n",
    "            datasets_collection['documents'].append(doc)\n",
    "            datasets_collection['queries'].append(query)\n",
    "            datasets_collection['qd_pairs'].append((query, doc, 1.0))\n",
    "        \n",
    "        print(f\"   âœ“ {len(korquad):,} query-document pairs\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— KorQuAD ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # 3. Korean Wikipedia (ë¬¸ì„œ ì½”í¼ìŠ¤)\n",
    "    try:\n",
    "        print(\"3ï¸âƒ£ í•œêµ­ì–´ Wikipedia ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        ko_wiki = load_dataset(\"wikipedia\", \"20220301.ko\", split=\"train[:100000]\")\n",
    "        \n",
    "        for item in ko_wiki:\n",
    "            text = item['text']\n",
    "            if len(text) > 100:  # ìµœì†Œ ê¸¸ì´ í•„í„°\n",
    "                datasets_collection['documents'].append(text[:2000])  # ì²˜ìŒ 2000ì\n",
    "        \n",
    "        print(f\"   âœ“ {len(ko_wiki):,} documents\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— Wikipedia ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # 4. KLUE Topic Classification (ë¬¸ì„œ ì½”í¼ìŠ¤)\n",
    "    try:\n",
    "        print(\"4ï¸âƒ£ KLUE Topic Classification ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        klue_tc = load_dataset(\"klue\", \"ynat\", split=\"train\")\n",
    "        \n",
    "        for item in klue_tc:\n",
    "            datasets_collection['documents'].append(item['title'])\n",
    "        \n",
    "        print(f\"   âœ“ {len(klue_tc):,} documents\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— KLUE TC ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # 5. Korean News Dataset\n",
    "    try:\n",
    "        print(\"5ï¸âƒ£ í•œêµ­ì–´ ë‰´ìŠ¤ ë°ì´í„°ì…‹ ë¡œë”©...\")\n",
    "        korean_news = load_dataset(\"heegyu/news-category-dataset\", split=\"train[:50000]\")\n",
    "        \n",
    "        for item in korean_news:\n",
    "            if 'headline' in item:\n",
    "                datasets_collection['documents'].append(item['headline'])\n",
    "        \n",
    "        print(f\"   âœ“ {len(korean_news):,} documents\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— Korean News ë¡œë”© ì‹¤íŒ¨: {e}\\n\")\n",
    "    \n",
    "    # ì¤‘ë³µ ì œê±°\n",
    "    datasets_collection['documents'] = list(set([\n",
    "        doc.strip() for doc in datasets_collection['documents'] \n",
    "        if doc and len(doc.strip()) > 10\n",
    "    ]))\n",
    "    \n",
    "    datasets_collection['queries'] = list(set([\n",
    "        q.strip() for q in datasets_collection['queries'] \n",
    "        if q and len(q.strip()) > 5\n",
    "    ]))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ì´ {len(datasets_collection['documents']):,}ê°œì˜ ê³ ìœ  ë¬¸ì„œ\")\n",
    "    print(f\"ì´ {len(datasets_collection['queries']):,}ê°œì˜ ê³ ìœ  ì¿¼ë¦¬\")\n",
    "    print(f\"ì´ {len(datasets_collection['qd_pairs']):,}ê°œì˜ query-document pairs\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return datasets_collection\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "korean_data = load_korean_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š í•œêµ­ì–´ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ (ì‹œê°„ ì •ë³´ í¬í•¨)\n",
      "ğŸ“° Loading Korean news dataset: heegyu/news-category-dataset\n",
      "Processing 10000 articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading news: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 58158.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 9996 documents\n",
      "  Date range: 2018-05-01 to 2022-09-23\n",
      "  Time span: 1606 days\n",
      "  Categories: 29\n",
      "    POLITICS: 3357\n",
      "    ENTERTAINMENT: 1597\n",
      "    U.S. NEWS: 1377\n",
      "    WORLD NEWS: 1194\n",
      "    COMEDY: 319\n",
      "âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ\n",
      "  Documents: 9,996ê°œ\n",
      "  Dates: 9,996ê°œ\n",
      "\n",
      "ğŸ“„ ìƒ˜í”Œ ë¬¸ì„œ: Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters...\n",
      "ğŸ“… ìƒ˜í”Œ ë‚ ì§œ: 2022-09-23 00:00:00\n",
      "\n",
      "ğŸ“š ì¶”ê°€ ë°ì´í„°ì…‹ ë¡œë“œ (KLUE, KorQuAD ë“±)\n",
      "ğŸ“š í•œêµ­ì–´ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\n",
      "\n",
      "1ï¸âƒ£ KLUE MRC ë°ì´í„°ì…‹ ë¡œë”©...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ 17,554 query-document pairs\n",
      "\n",
      "2ï¸âƒ£ KorQuAD v1 ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ“ 60,407 query-document pairs\n",
      "\n",
      "3ï¸âƒ£ í•œêµ­ì–´ Wikipedia ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ— Wikipedia ë¡œë”© ì‹¤íŒ¨: Couldn't find file at https://dumps.wikimedia.org/kowiki/20220301/dumpstatus.json\n",
      "\n",
      "4ï¸âƒ£ KLUE Topic Classification ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ“ 45,678 documents\n",
      "\n",
      "5ï¸âƒ£ í•œêµ­ì–´ ë‰´ìŠ¤ ë°ì´í„°ì…‹ ë¡œë”©...\n",
      "   âœ“ 50,000 documents\n",
      "\n",
      "\n",
      "============================================================\n",
      "ì´ 117,914ê°œì˜ ê³ ìœ  ë¬¸ì„œ\n",
      "ì´ 77,785ê°œì˜ ê³ ìœ  ì¿¼ë¦¬\n",
      "ì´ 77,961ê°œì˜ query-document pairs\n",
      "============================================================\n",
      "  ì¶”ê°€ documents: 117,914ê°œ\n",
      "  ì „ì²´ documents: 127,910ê°œ\n",
      "  Queries: 77,785ê°œ\n",
      "  QD Pairs: 77,961ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ğŸ†• ë°ì´í„°ì…‹ ë¡œë“œ (ì‹œê°„ ì •ë³´ í¬í•¨)\n",
    "from src import load_korean_news_with_dates\n",
    "\n",
    "print(\"ğŸ“Š í•œêµ­ì–´ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ (ì‹œê°„ ì •ë³´ í¬í•¨)\")\n",
    "\n",
    "# ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ (ë‚ ì§œ ì •ë³´ í¬í•¨)\n",
    "news_data = load_korean_news_with_dates(\n",
    "    max_samples=10000,\n",
    "    min_doc_length=20\n",
    ")\n",
    "\n",
    "documents = news_data['documents']\n",
    "dates = news_data['dates']\n",
    "\n",
    "print(f\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  Documents: {len(documents):,}ê°œ\")\n",
    "print(f\"  Dates: {len(dates):,}ê°œ\")\n",
    "\n",
    "# ìƒ˜í”Œ ì¶œë ¥\n",
    "if documents:\n",
    "    print(f\"\\nğŸ“„ ìƒ˜í”Œ ë¬¸ì„œ: {documents[0][:100]}...\")\n",
    "    print(f\"ğŸ“… ìƒ˜í”Œ ë‚ ì§œ: {dates[0]}\")\n",
    "\n",
    "# ê¸°ì¡´ load_korean_datasets() í•¨ìˆ˜ë„ í˜¸í™˜ì„±ì„ ìœ„í•´ ì‹¤í–‰\n",
    "print(\"\\nğŸ“š ì¶”ê°€ ë°ì´í„°ì…‹ ë¡œë“œ (KLUE, KorQuAD ë“±)\")\n",
    "korean_data = load_korean_datasets()\n",
    "\n",
    "# ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©\n",
    "if korean_data['documents']:\n",
    "    print(f\"  ì¶”ê°€ documents: {len(korean_data['documents']):,}ê°œ\")\n",
    "    documents.extend(korean_data['documents'])\n",
    "    # ê¸°ì¡´ ë°ì´í„°ëŠ” ë‚ ì§œê°€ ì—†ìœ¼ë¯€ë¡œ í˜„ì¬ ë‚ ì§œë¡œ ì±„ì›€\n",
    "    from datetime import datetime\n",
    "    dates.extend([datetime.now()] * len(korean_data['documents']))\n",
    "    print(f\"  ì „ì²´ documents: {len(documents):,}ê°œ\")\n",
    "\n",
    "queries = korean_data.get('queries', [])\n",
    "qd_pairs = korean_data.get('qd_pairs', [])\n",
    "\n",
    "print(f\"  Queries: {len(queries):,}ê°œ\")\n",
    "print(f\"  QD Pairs: {len(qd_pairs):,}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. IDF (Inverse Document Frequency) ê³„ì‚°\n",
    "\n",
    "ì¿¼ë¦¬ìš© í† í° ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ IDFë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "ì´ê²ƒì´ **idf.json** íŒŒì¼ì˜ ê¸°ë°˜ì´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ: klue/bert-base\n",
      "  Vocab size: 32,000\n",
      "\n",
      "ğŸ“Š IDF ê³„ì‚° ì¤‘ (ìƒ˜í”Œ: 50,000ê°œ ë¬¸ì„œ)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0ce86c029245daa9376ad5c2d334be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing documents:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ IDF ê³„ì‚° ì™„ë£Œ\n",
      "  ì´ 29,197ê°œ í† í°\n",
      "  í‰ê·  IDF: 8.9071\n",
      "  IDF ë²”ìœ„: [2.3026, 11.1267]\n",
      "\n",
      "ğŸ” IDF ìƒìœ„ 20ê°œ í† í° (í¬ê·€ í† í°):\n",
      " 1. ë‹¬ë¼ìš”                  - IDF: 11.1267\n",
      " 2. ì–                    - IDF: 11.1267\n",
      " 3. ë ˆë²„ë¦¬ì§€                 - IDF: 11.1267\n",
      " 4. ë°±ë²”                   - IDF: 11.1267\n",
      " 5. í‘œê³                    - IDF: 11.1267\n",
      " 6. ê·¸ëŸ°ëŒ€ë¡œ                 - IDF: 11.1267\n",
      " 7. ì§ˆë¦¬                   - IDF: 11.1267\n",
      " 8. ë¹½                    - IDF: 11.1267\n",
      " 9. ê¹…                    - IDF: 11.1267\n",
      "10. í—ˆë§                   - IDF: 11.1267\n",
      "11. ìëª»                   - IDF: 11.1267\n",
      "12. ##ë§£                  - IDF: 11.1267\n",
      "13. ê¶í•©                   - IDF: 11.1267\n",
      "14. ë²”ë²•                   - IDF: 11.1267\n",
      "15. ##ã–                  - IDF: 11.1267\n",
      "16. ì•”ê°í™”                  - IDF: 11.1267\n",
      "17. ìˆ¨ê²°                   - IDF: 11.1267\n",
      "18. ##ìƒ¤ì˜¤í•‘                - IDF: 11.1267\n",
      "19. ì—¼ì¹˜                   - IDF: 11.1267\n",
      "20. ëŒ€í†µë ¹ë ¹                 - IDF: 11.1267\n",
      "\n",
      "ğŸ”» IDF í•˜ìœ„ 20ê°œ í† í° (í”í•œ í† í°):\n",
      " 1. â€¦                    - IDF: 2.7889\n",
      " 2. ##ì—ì„œ                 - IDF: 2.7821\n",
      " 3. ##ìœ¼ë¡œ                 - IDF: 2.7410\n",
      " 4. ##í•œ                  - IDF: 2.7033\n",
      " 5. ##ê°€                  - IDF: 2.6998\n",
      " 6. ##ë¥¼                  - IDF: 2.6982\n",
      " 7. '                    - IDF: 2.6981\n",
      " 8. ##í•˜                  - IDF: 2.6896\n",
      " 9. ##ê³                   - IDF: 2.6790\n",
      "10. ##ë¡œ                  - IDF: 2.6789\n",
      "11. ##ì„                  - IDF: 2.6389\n",
      "12. ##ì´                  - IDF: 2.6156\n",
      "13. ##ì€                  - IDF: 2.5959\n",
      "14. ##ë‹¤                  - IDF: 2.5575\n",
      "15. ##ì˜                  - IDF: 2.5173\n",
      "16. ##ëŠ”                  - IDF: 2.4796\n",
      "17. .                    - IDF: 2.4082\n",
      "18. ,                    - IDF: 2.4066\n",
      "19. ##ì—                  - IDF: 2.3499\n",
      "20. ##s                  - IDF: 2.3026\n"
     ]
    }
   ],
   "source": [
    "# í•œêµ­ì–´ BERT í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "MODEL_NAME = \"klue/bert-base\"  # í•œêµ­ì–´ ìµœì í™” BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ: {MODEL_NAME}\")\n",
    "print(f\"  Vocab size: {len(tokenizer):,}\")\n",
    "\n",
    "def calculate_idf(documents, tokenizer, sample_size=50000):\n",
    "    \"\"\"\n",
    "    ì½”í¼ìŠ¤ì—ì„œ IDFë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    IDF(t) = log(N / df(t))\n",
    "    \n",
    "    Args:\n",
    "        documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        tokenizer: Hugging Face tokenizer\n",
    "        sample_size: ìƒ˜í”Œë§í•  ë¬¸ì„œ ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        dict: {token_id: idf_score}\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“Š IDF ê³„ì‚° ì¤‘ (ìƒ˜í”Œ: {min(sample_size, len(documents)):,}ê°œ ë¬¸ì„œ)...\")\n",
    "    \n",
    "    # ìƒ˜í”Œë§\n",
    "    sample_docs = documents[:sample_size]\n",
    "    N = len(sample_docs)\n",
    "    \n",
    "    # ê° í† í°ì´ ë‚˜íƒ€ë‚œ ë¬¸ì„œ ìˆ˜ ê³„ì‚°\n",
    "    df = Counter()  # document frequency\n",
    "    \n",
    "    for i, doc in enumerate(tqdm(sample_docs, desc=\"Tokenizing documents\")):\n",
    "        # í† í°í™”\n",
    "        tokens = tokenizer.encode(doc, add_special_tokens=False, max_length=512, truncation=True)\n",
    "        \n",
    "        # ë¬¸ì„œì— ë‚˜íƒ€ë‚œ ê³ ìœ  í† í°ë“¤\n",
    "        unique_tokens = set(tokens)\n",
    "        \n",
    "        # df ì—…ë°ì´íŠ¸\n",
    "        for token_id in unique_tokens:\n",
    "            df[token_id] += 1\n",
    "    \n",
    "    # IDF ê³„ì‚°\n",
    "    idf_dict = {}\n",
    "    for token_id, doc_freq in df.items():\n",
    "        # IDF = log(N / df)\n",
    "        idf_score = math.log((N + 1) / (doc_freq + 1)) + 1.0  # smoothing\n",
    "        idf_dict[token_id] = idf_score\n",
    "    \n",
    "    # í† í° ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "    idf_token_dict = {}\n",
    "    for token_id, score in idf_dict.items():\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        idf_token_dict[token_str] = float(score)\n",
    "    \n",
    "    print(f\"\\nâœ“ IDF ê³„ì‚° ì™„ë£Œ\")\n",
    "    print(f\"  ì´ {len(idf_token_dict):,}ê°œ í† í°\")\n",
    "    print(f\"  í‰ê·  IDF: {np.mean(list(idf_token_dict.values())):.4f}\")\n",
    "    print(f\"  IDF ë²”ìœ„: [{min(idf_token_dict.values()):.4f}, {max(idf_token_dict.values()):.4f}]\")\n",
    "    \n",
    "    return idf_token_dict, idf_dict\n",
    "\n",
    "# IDF ê³„ì‚°\n",
    "idf_token_dict, idf_id_dict = calculate_idf(korean_data['documents'], tokenizer)\n",
    "\n",
    "# ìƒìœ„/í•˜ìœ„ IDF í† í° ì¶œë ¥\n",
    "print(\"\\nğŸ” IDF ìƒìœ„ 20ê°œ í† í° (í¬ê·€ í† í°):\")\n",
    "sorted_idf = sorted(idf_token_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for i, (token, score) in enumerate(sorted_idf[:20], 1):\n",
    "    print(f\"{i:2d}. {token:20s} - IDF: {score:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ”» IDF í•˜ìœ„ 20ê°œ í† í° (í”í•œ í† í°):\")\n",
    "for i, (token, score) in enumerate(sorted_idf[-20:], 1):\n",
    "    print(f\"{i:2d}. {token:20s} - IDF: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. í•œêµ­ì–´ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì¶”ê°€\n",
    "\n",
    "2024-2025 íŠ¸ë Œë“œ í‚¤ì›Œë“œì— ëŒ€í•´ IDF ê°€ì¤‘ì¹˜ë¥¼ ë¶€ìŠ¤íŒ…í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ†• ìë™ íŠ¸ë Œë“œ ê°ì§€ (Unsupervised)\n",
    "\n",
    "í•˜ë“œì½”ë”©ëœ `TREND_BOOST` ëŒ€ì‹  ìë™ìœ¼ë¡œ íŠ¸ë Œë”© í† í°ì„ ê°ì§€í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” ìë™ íŠ¸ë Œë“œ ê°ì§€ (3ê°œì›” ê°„ê²© ë¹„êµ)\n",
      "  ë°ì´í„° ë‚ ì§œ ë²”ìœ„: 2018-05-01 ~ 2025-11-13\n",
      "  ë°ì´í„° ê¸°ê°„: 2753ì¼ (91.8ê°œì›”)\n",
      "  âœ“ ìµœê·¼ 3ê°œì›”(90ì¼) vs ê³¼ê±° 6ê°œì›”(180ì¼) ë¹„êµ\n",
      "\n",
      "ğŸ”¥ Detecting Trending Tokens\n",
      "   Recent period: 90 days\n",
      "   Historical period: 180 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing trends: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127910/127910 [00:11<00:00, 11259.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Recent documents: 117914\n",
      "   Historical documents: 0\n",
      "\n",
      "âœ“ Found 100 trending tokens\n",
      "\n",
      "  Top 10 Trending:\n",
      "    1. To: 139153.26x\n",
      "    2. The: 138483.28x\n",
      "    3. Tr: 113498.97x\n",
      "    4. In: 88082.14x\n",
      "    5. [UNK]: 80483.39x\n",
      "    6. For: 68313.50x\n",
      "    7. St: 58933.78x\n",
      "    8. Th: 56745.75x\n",
      "    9. Se: 50105.31x\n",
      "    10. On: 49494.70x\n",
      "\n",
      "âœ“ 100ê°œì˜ íŠ¸ë Œë”© í† í° ìë™ ë°œê²¬\n",
      "\n",
      "âœ“ Created boost dictionary with 100 tokens\n",
      "  Boost range: 1.20 - 2.00\n",
      "âœ“ Applied boost to 100 tokens\n",
      "âœ“ ìë™ íŠ¸ë Œë“œ ë¶€ìŠ¤íŒ… ì™„ë£Œ\n",
      "\n",
      "  ğŸ“ˆ ìƒìœ„ 10ê°œ íŠ¸ë Œë”© í† í°:\n",
      "    To: 139153.26x ì¦ê°€ (boost=2.00)\n",
      "    The: 138483.28x ì¦ê°€ (boost=2.00)\n",
      "    Tr: 113498.97x ì¦ê°€ (boost=1.84)\n",
      "    In: 88082.14x ì¦ê°€ (boost=1.67)\n",
      "    [UNK]: 80483.39x ì¦ê°€ (boost=1.62)\n",
      "    For: 68313.50x ì¦ê°€ (boost=1.55)\n",
      "    St: 58933.78x ì¦ê°€ (boost=1.49)\n",
      "    Th: 56745.75x ì¦ê°€ (boost=1.47)\n",
      "    Se: 50105.31x ì¦ê°€ (boost=1.43)\n",
      "    On: 49494.70x ì¦ê°€ (boost=1.43)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ†• ìë™ íŠ¸ë Œë“œ ê°ì§€ (í•˜ë“œì½”ë”© ì œê±°) - 3ê°œì›” ê°„ê²©\n",
    "from src import (\n",
    "    detect_trending_tokens,\n",
    "    build_trend_boost_dict,\n",
    "    apply_temporal_boost_to_idf,\n",
    ")\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "USE_AUTO_TREND_DETECTION = True\n",
    "\n",
    "if USE_AUTO_TREND_DETECTION:\n",
    "    print(\"\\nğŸ” ìë™ íŠ¸ë Œë“œ ê°ì§€ (3ê°œì›” ê°„ê²© ë¹„êµ)\")\n",
    "    \n",
    "    # ë°ì´í„° ë‚ ì§œ ë²”ìœ„ í™•ì¸\n",
    "    min_date = min(dates)\n",
    "    max_date = max(dates)\n",
    "    date_span = (max_date - min_date).days\n",
    "    \n",
    "    print(f\"  ë°ì´í„° ë‚ ì§œ ë²”ìœ„: {min_date.date()} ~ {max_date.date()}\")\n",
    "    print(f\"  ë°ì´í„° ê¸°ê°„: {date_span}ì¼ ({date_span/30:.1f}ê°œì›”)\")\n",
    "    \n",
    "    # 3ê°œì›” ê°„ê²© ì„¤ì •\n",
    "    recent_days = 90  # ìµœê·¼ 3ê°œì›”\n",
    "    historical_days = 180  # ê³¼ê±° 6ê°œì›” (ë¹„êµ ê¸°ì¤€)\n",
    "    \n",
    "    # ë°ì´í„°ê°€ ë¶€ì¡±í•œ ê²½ìš° ìë™ ì¡°ì •\n",
    "    if date_span < 180:\n",
    "        if date_span < 90:\n",
    "            # 3ê°œì›” ë¯¸ë§Œ: ì „ì²´ ê¸°ê°„ì˜ 1/2ì„ recentë¡œ ì‚¬ìš©\n",
    "            recent_days = max(30, date_span // 2)\n",
    "            historical_days = date_span\n",
    "            print(f\"  âš ï¸  ë°ì´í„° ë¶€ì¡±: recent={recent_days}ì¼, historical={historical_days}ì¼ë¡œ ì¡°ì •\")\n",
    "        else:\n",
    "            # 3~6ê°œì›”: recentëŠ” 3ê°œì›”, historicalì€ ì „ì²´\n",
    "            recent_days = 90\n",
    "            historical_days = date_span\n",
    "            print(f\"  âš ï¸  ë°ì´í„° ë¶€ì¡±: historical={historical_days}ì¼ë¡œ ì¡°ì •\")\n",
    "    else:\n",
    "        print(f\"  âœ“ ìµœê·¼ 3ê°œì›”(90ì¼) vs ê³¼ê±° 6ê°œì›”(180ì¼) ë¹„êµ\")\n",
    "    \n",
    "    # íŠ¸ë Œë”© í† í° ìë™ ê°ì§€\n",
    "    trending_tokens = detect_trending_tokens(\n",
    "        documents=documents,\n",
    "        dates=dates,\n",
    "        tokenizer=tokenizer,\n",
    "        recent_days=recent_days,\n",
    "        historical_days=historical_days,\n",
    "        min_recent_count=5,  # ìµœê·¼ ê¸°ê°„ì— ìµœì†Œ 5íšŒ ì´ìƒ ì¶œí˜„\n",
    "        top_k=100,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ“ {len(trending_tokens)}ê°œì˜ íŠ¸ë Œë”© í† í° ìë™ ë°œê²¬\")\n",
    "    \n",
    "    if len(trending_tokens) > 0:\n",
    "        # ìë™ boost ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "        auto_trend_boost = build_trend_boost_dict(\n",
    "            trending_tokens=trending_tokens,\n",
    "            max_boost=2.0,\n",
    "            min_boost=1.2,\n",
    "        )\n",
    "        \n",
    "        # IDFì— ì ìš©\n",
    "        idf_token_dict = apply_temporal_boost_to_idf(\n",
    "            idf_token_dict=idf_token_dict,\n",
    "            boost_dict=auto_trend_boost,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ“ ìë™ íŠ¸ë Œë“œ ë¶€ìŠ¤íŒ… ì™„ë£Œ\")\n",
    "        print(f\"\\n  ğŸ“ˆ ìƒìœ„ 10ê°œ íŠ¸ë Œë”© í† í°:\")\n",
    "        for token, score in trending_tokens[:10]:\n",
    "            boost = auto_trend_boost.get(token, 1.0)\n",
    "            print(f\"    {token}: {score:.2f}x ì¦ê°€ (boost={boost:.2f})\")\n",
    "    else:\n",
    "        print(\"\\n  â„¹ï¸  íŠ¸ë Œë”© í† í°ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        print(f\"     - ìµœê·¼ {recent_days}ì¼ê³¼ ê³¼ê±° {historical_days}ì¼ ë¹„êµ\")\n",
    "        print(\"     - ë°ì´í„° ê¸°ê°„ì´ ì§§ê±°ë‚˜ í† í° ë¹ˆë„ ë³€í™”ê°€ ì ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "        print(\"     - ë” ë§ì€ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê±°ë‚˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì„¸ìš”.\")\n",
    "else:\n",
    "    print(\"\\nâ­ï¸  ìë™ íŠ¸ë Œë“œ ê°ì§€ ê±´ë„ˆëœ€ (USE_AUTO_TREND_DETECTION=False)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ†• í•œì˜ í†µí•© ë™ì˜ì–´ ì‚¬ì „ (Cross-lingual)\n",
    "\n",
    "ë¹„ì§€ë„ í•™ìŠµìœ¼ë¡œ 'ëª¨ë¸' â†” 'model' ê°™ì€ í•œì˜ ë™ì˜ì–´ë¥¼ ìë™ ë°œê²¬í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŒ í•œì˜ í†µí•© ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶•\n",
      "  ìˆ˜ë™ ì •ì˜ ìŒ: 32ê°œ\n",
      "  â„¹ï¸  doc_encoder ì—†ìŒ - ê¸°ë³¸ ë™ì˜ì–´ ìŒë§Œ ì‚¬ìš©\n",
      "     (ëª¨ë¸ í•™ìŠµ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ìë™ ë°œê²¬ ê¸°ëŠ¥ ì‚¬ìš© ê°€ëŠ¥)\n",
      "  ì „ì²´ bilingual ì‚¬ì „: 32ê°œ í•­ëª©\n",
      "\n",
      "ğŸ”— Applying Bilingual Synonyms to IDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sharing IDF: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 399457.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Updated 2 IDF entries with bilingual synonyms\n",
      "\n",
      "âœ“ í•œì˜ ë™ì˜ì–´ IDF ë™ê¸°í™” ì™„ë£Œ\n",
      "  ì˜ˆì‹œ: 'ëª¨ë¸' â†” 'model', 'í•™ìŠµ' â†” 'training'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ†• í•œì˜ í†µí•© ë™ì˜ì–´ (Bilingual Synonyms)\n",
    "from src import (\n",
    "    build_comprehensive_bilingual_dictionary,\n",
    "    get_default_korean_english_pairs,\n",
    "    apply_bilingual_synonyms_to_idf,\n",
    ")\n",
    "\n",
    "USE_BILINGUAL_SYNONYMS = True\n",
    "\n",
    "if USE_BILINGUAL_SYNONYMS:\n",
    "    print(\"\\nğŸŒ í•œì˜ í†µí•© ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶•\")\n",
    "    \n",
    "    # ê¸°ë³¸ í•œì˜ ìŒ (ì„ íƒì )\n",
    "    manual_pairs = get_default_korean_english_pairs()\n",
    "    print(f\"  ìˆ˜ë™ ì •ì˜ ìŒ: {len(manual_pairs)}ê°œ\")\n",
    "    \n",
    "    # Check if doc_encoder is available (only after model training)\n",
    "    try:\n",
    "        # doc_encoderê°€ ì •ì˜ë˜ì–´ ìˆìœ¼ë©´ embedding ê¸°ë°˜ ë™ì˜ì–´ ë°œê²¬ ì‚¬ìš©\n",
    "        if 'doc_encoder' in globals():\n",
    "            print(\"  âœ“ doc_encoder ë°œê²¬ - embedding ê¸°ë°˜ ë™ì˜ì–´ ë°œê²¬ ì‚¬ìš©\")\n",
    "            \n",
    "            # í¬ê´„ì ì¸ bilingual ì‚¬ì „ êµ¬ì¶• (ë¹„ì§€ë„)\n",
    "            bilingual_dict = build_comprehensive_bilingual_dictionary(\n",
    "                documents=documents[:5000],  # ìƒ˜í”Œ ì‚¬ìš© (ì†ë„)\n",
    "                token_embeddings=doc_encoder.bert.embeddings.word_embeddings.weight.detach().cpu().numpy(),\n",
    "                tokenizer=tokenizer,\n",
    "                bert_model=doc_encoder.bert,\n",
    "                manual_pairs=manual_pairs,\n",
    "            )\n",
    "        else:\n",
    "            raise NameError(\"doc_encoder not defined\")\n",
    "    except (NameError, AttributeError):\n",
    "        # doc_encoderê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ìˆ˜ë™ ìŒë§Œ ì‚¬ìš©\n",
    "        print(\"  â„¹ï¸  doc_encoder ì—†ìŒ - ê¸°ë³¸ ë™ì˜ì–´ ìŒë§Œ ì‚¬ìš©\")\n",
    "        print(\"     (ëª¨ë¸ í•™ìŠµ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ìë™ ë°œê²¬ ê¸°ëŠ¥ ì‚¬ìš© ê°€ëŠ¥)\")\n",
    "        bilingual_dict = dict(manual_pairs)\n",
    "    \n",
    "    print(f\"  ì „ì²´ bilingual ì‚¬ì „: {len(bilingual_dict):,}ê°œ í•­ëª©\")\n",
    "    \n",
    "    # IDFì— ì ìš© (ë™ì˜ì–´ë¼ë¦¬ IDF ê³µìœ )\n",
    "    idf_token_dict = apply_bilingual_synonyms_to_idf(\n",
    "        idf_dict=idf_token_dict,\n",
    "        bilingual_dict=bilingual_dict,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ“ í•œì˜ ë™ì˜ì–´ IDF ë™ê¸°í™” ì™„ë£Œ\")\n",
    "    print(f\"  ì˜ˆì‹œ: 'ëª¨ë¸' â†” 'model', 'í•™ìŠµ' â†” 'training'\")\n",
    "else:\n",
    "    print(\"\\nâ­ï¸  í•œì˜ ë™ì˜ì–´ í†µí•© ê±´ë„ˆëœ€ (USE_BILINGUAL_SYNONYMS=False)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. OpenSearch ë¬¸ì„œ ì¸ì½”ë” ëª¨ë¸ ì •ì˜\n",
    "\n",
    "**Doc-only mode**ë¥¼ ìœ„í•œ ë¬¸ì„œ ì¸ì½”ë”ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "BERT ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ sparse vectorë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ OpenSearch Document Encoder ì´ˆê¸°í™” ì™„ë£Œ\n",
      "  ëª¨ë¸: klue/bert-base\n",
      "  Vocab size: 32,000\n",
      "  Parameters: 110,650,880\n",
      "  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "class OpenSearchDocEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    OpenSearch Neural Sparse Document Encoder (Doc-only mode)\n",
    "    \n",
    "    ë¬¸ì„œë¥¼ sparse vectorë¡œ ì¸ì½”ë”©í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "    ì¶œë ¥ í˜•ì‹: {\"output\": <sparse_vector>}\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"klue/bert-base\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # BERT ê¸°ë°˜ ì¸ì½”ë”\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.bert = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        \n",
    "        self.vocab_size = self.config.vocab_size\n",
    "        \n",
    "        # Log saturation activation\n",
    "        # log(1 + ReLU(x))\n",
    "        self.activation = lambda x: torch.log1p(torch.relu(x))\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, return_dict=False):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            input_ids: (batch_size, seq_len)\n",
    "            attention_mask: (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            sparse_vector: (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        # BERT MLM head output\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Logits: (batch_size, seq_len, vocab_size)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Apply activation: log(1 + ReLU(logits))\n",
    "        activated = self.activation(logits)\n",
    "        \n",
    "        # Max pooling over sequence length\n",
    "        # (batch_size, seq_len, vocab_size) â†’ (batch_size, vocab_size)\n",
    "        sparse_vector = torch.max(\n",
    "            activated * attention_mask.unsqueeze(-1),\n",
    "            dim=1\n",
    "        ).values\n",
    "        \n",
    "        if return_dict:\n",
    "            return {'output': sparse_vector}\n",
    "        \n",
    "        return sparse_vector\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "doc_encoder = OpenSearchDocEncoder(MODEL_NAME)\n",
    "doc_encoder = doc_encoder.to(device)\n",
    "\n",
    "# Tokenizerì— special tokens ì¶”ê°€ë¡œ ì¸í•´ embedding resize í•„ìš”\n",
    "if len(tokenizer) > doc_encoder.vocab_size:\n",
    "    print(f\"Resizing model embeddings: {doc_encoder.vocab_size} â†’ {len(tokenizer)}\")\n",
    "    doc_encoder.bert.resize_token_embeddings(len(tokenizer))\n",
    "    doc_encoder.vocab_size = len(tokenizer)\n",
    "    print(f\"âœ“ Model embeddings resized\")\n",
    "\n",
    "print(f\"âœ“ OpenSearch Document Encoder ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "print(f\"  ëª¨ë¸: {MODEL_NAME}\")\n",
    "print(f\"  Vocab size: {doc_encoder.vocab_size:,}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in doc_encoder.parameters()):,}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "\n",
    "Query-Document pairsì™€ negative samplingì„ ì¤€ë¹„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Negative sampling ì¤‘ (negatives per query: 2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b216846349b4bb6909ab295b7a586cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ì´ 30,000ê°œ pairs (original: 10,000)\n",
      "\n",
      "ğŸ“Š ë°ì´í„°ì…‹ ë¶„í• :\n",
      "  Train: 27,000 pairs\n",
      "  Val: 3,000 pairs\n",
      "\n",
      "âœ“ ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ\n",
      "  Batch size: 16\n",
      "  Train batches: 1,688\n",
      "  Val batches: 188\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SparseEncodingDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    OpenSearch Neural Sparse í•™ìŠµìš© ë°ì´í„°ì…‹\n",
    "    \"\"\"\n",
    "    def __init__(self, qd_pairs, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            qd_pairs: [(query, document, relevance), ...]\n",
    "            tokenizer: Hugging Face tokenizer\n",
    "            max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "        \"\"\"\n",
    "        self.qd_pairs = qd_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.qd_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        query, document, relevance = self.qd_pairs[idx]\n",
    "        \n",
    "        # ì¿¼ë¦¬ í† í°í™” (IDF lookupìš©)\n",
    "        query_encoded = self.tokenizer(\n",
    "            query,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # ë¬¸ì„œ í† í°í™” (ëª¨ë¸ ì¸ì½”ë”©ìš©)\n",
    "        doc_encoded = self.tokenizer(\n",
    "            document,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'query_input_ids': query_encoded['input_ids'].squeeze(0),\n",
    "            'query_attention_mask': query_encoded['attention_mask'].squeeze(0),\n",
    "            'doc_input_ids': doc_encoded['input_ids'].squeeze(0),\n",
    "            'doc_attention_mask': doc_encoded['attention_mask'].squeeze(0),\n",
    "            'relevance': torch.tensor(relevance, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# Negative sampling ì¶”ê°€ (ìµœì í™” ë²„ì „)\n",
    "def add_negative_samples(qd_pairs, documents, num_negatives=1):\n",
    "    \"\"\"\n",
    "    ê° positive pairì— ëŒ€í•´ negative documentsë¥¼ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.\n",
    "    (ì¸ë±ìŠ¤ ê¸°ë°˜ ìƒ˜í”Œë§ìœ¼ë¡œ 100ë°° ì´ìƒ ë¹ ë¦„!)\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ”„ Negative sampling ì¤‘ (negatives per query: {num_negatives})...\")\n",
    "    \n",
    "    # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì¸ë±ìŠ¤ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡)\n",
    "    doc_list = documents if isinstance(documents, list) else list(documents)\n",
    "    n_docs = len(doc_list)\n",
    "    \n",
    "    # ë¬¸ì„œ â†’ ì¸ë±ìŠ¤ ë§¤í•‘ (ë¹ ë¥¸ ê²€ìƒ‰ìš©)\n",
    "    doc_to_idx = {doc: idx for idx, doc in enumerate(doc_list)}\n",
    "    \n",
    "    augmented_pairs = []\n",
    "    \n",
    "    for query, pos_doc, relevance in tqdm(qd_pairs):\n",
    "        # Positive pair ì¶”ê°€\n",
    "        augmented_pairs.append((query, pos_doc, 1.0))\n",
    "        \n",
    "        # Positive ë¬¸ì„œì˜ ì¸ë±ìŠ¤\n",
    "        pos_idx = doc_to_idx.get(pos_doc, -1)\n",
    "        \n",
    "        # Negative sampling (ì¸ë±ìŠ¤ ê¸°ë°˜ - ë§¤ìš° ë¹ ë¦„!)\n",
    "        for _ in range(num_negatives):\n",
    "            # ëœë¤ ì¸ë±ìŠ¤ ì„ íƒ\n",
    "            neg_idx = np.random.randint(0, n_docs)\n",
    "            \n",
    "            # Positiveì™€ ê°™ì€ ì¸ë±ìŠ¤ë©´ ë‹¤ë¥¸ ê²ƒìœ¼ë¡œ êµì²´\n",
    "            if neg_idx == pos_idx:\n",
    "                neg_idx = (neg_idx + 1) % n_docs\n",
    "            \n",
    "            neg_doc = doc_list[neg_idx]\n",
    "            augmented_pairs.append((query, neg_doc, 0.0))\n",
    "    \n",
    "    print(f\"âœ“ ì´ {len(augmented_pairs):,}ê°œ pairs (original: {len(qd_pairs):,})\")\n",
    "    return augmented_pairs\n",
    "\n",
    "# Negative sampling ì ìš©\n",
    "augmented_pairs = add_negative_samples(\n",
    "    korean_data['qd_pairs'][:10000],  # ìƒ˜í”Œë§ (ì „ì²´ëŠ” ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼)\n",
    "    korean_data['documents'],\n",
    "    num_negatives=2\n",
    ")\n",
    "\n",
    "# Train/Val split\n",
    "train_pairs, val_pairs = train_test_split(augmented_pairs, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"\\nğŸ“Š ë°ì´í„°ì…‹ ë¶„í• :\")\n",
    "print(f\"  Train: {len(train_pairs):,} pairs\")\n",
    "print(f\"  Val: {len(val_pairs):,} pairs\")\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„±\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = SparseEncodingDataset(train_pairs, tokenizer, MAX_LENGTH)\n",
    "val_dataset = SparseEncodingDataset(val_pairs, tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nâœ“ ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Train batches: {len(train_loader):,}\")\n",
    "print(f\"  Val batches: {len(val_loader):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ” í† í° ì„ë² ë”© ê¸°ë°˜ ë™ì˜ì–´ ìë™ ë°œê²¬\n",
      "============================================================\n",
      "âœ“ Token embeddings ì¶”ì¶œ ì™„ë£Œ: (32000, 768)\n",
      "  Vocab size: 32,000\n",
      "  Embedding dim: 768\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸ” í† í° ì„ë² ë”© ê¸°ë°˜ ë™ì˜ì–´ ìë™ ë°œê²¬\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# BERT ëª¨ë¸ì€ ì´ë¯¸ doc_encoderì— ë¡œë“œë˜ì–´ ìˆìŒ\n",
    "# Token embedding ì¶”ì¶œ\n",
    "token_embeddings = doc_encoder.bert.bert.embeddings.word_embeddings.weight.detach().cpu().numpy()\n",
    "\n",
    "print(f\"âœ“ Token embeddings ì¶”ì¶œ ì™„ë£Œ: {token_embeddings.shape}\")\n",
    "print(f\"  Vocab size: {token_embeddings.shape[0]:,}\")\n",
    "print(f\"  Embedding dim: {token_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ find_similar_tokens í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def find_similar_tokens(token, tokenizer, embeddings, top_k=10, threshold=0.75):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í† í°ê³¼ ìœ ì‚¬í•œ í† í°ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        token: ê²€ìƒ‰í•  í† í° (ë¬¸ìì—´)\n",
    "        tokenizer: Tokenizer\n",
    "        embeddings: Token embeddings (numpy array)\n",
    "        top_k: ë°˜í™˜í•  ìœ ì‚¬ í† í° ê°œìˆ˜\n",
    "        threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (0~1)\n",
    "    \n",
    "    Returns:\n",
    "        List of (token, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # í† í° -> ID ë³€í™˜\n",
    "    token_id = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(token))\n",
    "    if not token_id:\n",
    "        return []\n",
    "    token_id = token_id[0]  # ì²« ë²ˆì§¸ í† í° ID ì‚¬ìš©\n",
    "    \n",
    "    # Token embedding ì¶”ì¶œ\n",
    "    token_emb = embeddings[token_id]\n",
    "    \n",
    "    # ëª¨ë“  í† í°ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    similarities = np.dot(embeddings, token_emb) / (\n",
    "        np.linalg.norm(embeddings, axis=1) * np.linalg.norm(token_emb) + 1e-10\n",
    "    )\n",
    "    \n",
    "    # ìƒìœ„ top_k+1 ê°œ ì¶”ì¶œ (ìê¸° ìì‹  ì œì™¸)\n",
    "    top_indices = np.argsort(similarities)[-(top_k+1):][::-1]\n",
    "    \n",
    "    similar_tokens = []\n",
    "    for idx in top_indices:\n",
    "        sim_score = float(similarities[idx])\n",
    "        if sim_score >= threshold and int(idx) != token_id:\n",
    "            similar_token = tokenizer.decode([int(idx)]).strip()\n",
    "            # í•„í„°ë§: ë¹ˆ ë¬¸ìì—´, íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆëŠ” ê²½ìš° ì œì™¸\n",
    "            if similar_token and not similar_token in ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']:\n",
    "                similar_tokens.append((similar_token, sim_score))\n",
    "    \n",
    "    return similar_tokens[:top_k]\n",
    "\n",
    "\n",
    "print(\"âœ“ find_similar_tokens í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì†ì‹¤ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "OpenSearch ëª¨ë¸ì˜ í•µì‹¬ ì†ì‹¤ í•¨ìˆ˜:\n",
    "1. **Ranking Loss**: Query-Document similarity\n",
    "2. **IDF-aware Penalty**: ë‚®ì€ IDF í† í° ì–µì œ\n",
    "3. **L0 Regularization**: Sparsity ìœ ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n",
      "  - Ranking Loss (BCE)\n",
      "  - L0 Regularization (Sparsity)\n",
      "  - IDF-aware Penalty (Low-IDF suppression)\n"
     ]
    }
   ],
   "source": [
    "def compute_query_representation(query_tokens, idf_dict, tokenizer):\n",
    "    \"\"\"\n",
    "    ì¿¼ë¦¬ë¥¼ IDF lookupìœ¼ë¡œ sparse vectorë¡œ ë³€í™˜ (Inference-free!)\n",
    "    \n",
    "    Args:\n",
    "        query_tokens: (batch_size, seq_len)\n",
    "        idf_dict: {token_id: idf_score}\n",
    "        tokenizer: Hugging Face tokenizer\n",
    "    \n",
    "    Returns:\n",
    "        query_sparse: (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = query_tokens.shape\n",
    "    vocab_size = len(tokenizer)  # Use len() to include added special tokens\n",
    "    \n",
    "    # Initialize sparse vector\n",
    "    query_sparse = torch.zeros(batch_size, vocab_size, device=query_tokens.device)\n",
    "    \n",
    "    # Fill with IDF weights\n",
    "    for b in range(batch_size):\n",
    "        for token_id in query_tokens[b]:\n",
    "            token_id = token_id.item()\n",
    "            if token_id in idf_dict:\n",
    "                query_sparse[b, token_id] = idf_dict[token_id]\n",
    "    \n",
    "    return query_sparse\n",
    "\n",
    "def neural_sparse_loss(doc_sparse, query_sparse, relevance, idf_dict, \n",
    "                       lambda_l0=1e-3, lambda_idf=1e-2):\n",
    "    \"\"\"\n",
    "    OpenSearch Neural Sparse Loss\n",
    "    \n",
    "    Args:\n",
    "        doc_sparse: (batch_size, vocab_size) - ë¬¸ì„œì˜ sparse representation\n",
    "        query_sparse: (batch_size, vocab_size) - ì¿¼ë¦¬ì˜ sparse representation (IDF lookup)\n",
    "        relevance: (batch_size,) - ê´€ë ¨ë„ ì ìˆ˜ (1.0 or 0.0)\n",
    "        idf_dict: {token_id: idf_score}\n",
    "        lambda_l0: L0 regularization ê°€ì¤‘ì¹˜\n",
    "        lambda_idf: IDF-aware penalty ê°€ì¤‘ì¹˜\n",
    "    \n",
    "    Returns:\n",
    "        total_loss, ranking_loss, l0_loss, idf_penalty\n",
    "    \"\"\"\n",
    "    # 1. Ranking Loss: Dot product similarity\n",
    "    similarity = torch.sum(doc_sparse * query_sparse, dim=-1)\n",
    "    ranking_loss = F.binary_cross_entropy_with_logits(\n",
    "        similarity, relevance, reduction='mean'\n",
    "    )\n",
    "    \n",
    "    # 2. L0 Regularization (FLOPS penalty for sparsity)\n",
    "    l0_loss = torch.mean(torch.sum(torch.abs(doc_sparse), dim=-1))\n",
    "    \n",
    "    # 3. IDF-aware Penalty (suppress low-IDF tokens)\n",
    "    # ë‚®ì€ IDF í† í°ì— í˜ë„í‹° ë¶€ì—¬\n",
    "    idf_tensor = torch.tensor(\n",
    "        [idf_dict.get(i, 1.0) for i in range(doc_sparse.shape[1])],\n",
    "        device=doc_sparse.device\n",
    "    )\n",
    "    \n",
    "    # Inverse IDF penalty: ë‚®ì€ IDF = ë†’ì€ í˜ë„í‹°\n",
    "    inverse_idf = 1.0 / (idf_tensor + 1e-6)\n",
    "    idf_penalty = torch.mean(torch.sum(doc_sparse * inverse_idf, dim=-1))\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = ranking_loss + lambda_l0 * l0_loss + lambda_idf * idf_penalty\n",
    "    \n",
    "    return total_loss, ranking_loss, l0_loss, idf_penalty\n",
    "\n",
    "print(\"âœ“ ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n",
    "print(\"  - Ranking Loss (BCE)\")\n",
    "print(\"  - L0 Regularization (Sparsity)\")\n",
    "print(\"  - IDF-aware Penalty (Low-IDF suppression)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. í•™ìŠµ ì„¤ì • ë° ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\n",
      "  Learning rate: 2e-05\n",
      "  Epochs: 3\n",
      "  Batch size: 16\n",
      "  Max length: 128\n",
      "  Lambda L0: 0.001\n",
      "  Lambda IDF: 0.01\n",
      "  Device: cuda\n",
      "\n",
      "âœ“ Optimizer: AdamW\n",
      "âœ“ Scheduler: Linear warmup (500 steps)\n",
      "âœ“ Total steps: 5,064\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "WARMUP_STEPS = 500\n",
    "LAMBDA_L0 = 1e-3  # L0 regularization\n",
    "LAMBDA_IDF = 1e-2  # IDF-aware penalty\n",
    "\n",
    "print(\"ğŸ¯ í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Max length: {MAX_LENGTH}\")\n",
    "print(f\"  Lambda L0: {LAMBDA_L0}\")\n",
    "print(f\"  Lambda IDF: {LAMBDA_IDF}\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "# Optimizer & Scheduler\n",
    "optimizer = AdamW(doc_encoder.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Optimizer: AdamW\")\n",
    "print(f\"âœ“ Scheduler: Linear warmup ({WARMUP_STEPS} steps)\")\n",
    "print(f\"âœ“ Total steps: {total_steps:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸš€ í•™ìŠµ ì‹œì‘!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, idf_id_dict, tokenizer, device):\n",
    "    \"\"\"\n",
    "    í•œ ì—í­ í•™ìŠµ\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_ranking = 0\n",
    "    total_l0 = 0\n",
    "    total_idf_penalty = 0\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move to device\n",
    "        query_tokens = batch['query_input_ids'].to(device)\n",
    "        doc_input_ids = batch['doc_input_ids'].to(device)\n",
    "        doc_attention_mask = batch['doc_attention_mask'].to(device)\n",
    "        relevance = batch['relevance'].to(device)\n",
    "        \n",
    "        # Document encoding (ëª¨ë¸ë¡œ ì¸ì½”ë”©)\n",
    "        doc_sparse = model(doc_input_ids, doc_attention_mask)\n",
    "        \n",
    "        # Query encoding (IDF lookup - inference-free!)\n",
    "        query_sparse = compute_query_representation(query_tokens, idf_id_dict, tokenizer)\n",
    "        \n",
    "        # Loss ê³„ì‚°\n",
    "        loss, ranking_loss, l0_loss, idf_penalty = neural_sparse_loss(\n",
    "            doc_sparse, query_sparse, relevance, idf_id_dict,\n",
    "            lambda_l0=LAMBDA_L0, lambda_idf=LAMBDA_IDF\n",
    "        )\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # ê¸°ë¡\n",
    "        total_loss += loss.item()\n",
    "        total_ranking += ranking_loss.item()\n",
    "        total_l0 += l0_loss.item()\n",
    "        total_idf_penalty += idf_penalty.item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'rank': f'{ranking_loss.item():.4f}',\n",
    "            'l0': f'{l0_loss.item():.2f}',\n",
    "            'idf': f'{idf_penalty.item():.4f}'\n",
    "        })\n",
    "    \n",
    "    n = len(loader)\n",
    "    return total_loss/n, total_ranking/n, total_l0/n, total_idf_penalty/n\n",
    "\n",
    "def evaluate(model, loader, idf_id_dict, tokenizer, device):\n",
    "    \"\"\"\n",
    "    ê²€ì¦\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            query_tokens = batch['query_input_ids'].to(device)\n",
    "            doc_input_ids = batch['doc_input_ids'].to(device)\n",
    "            doc_attention_mask = batch['doc_attention_mask'].to(device)\n",
    "            relevance = batch['relevance'].to(device)\n",
    "            \n",
    "            doc_sparse = model(doc_input_ids, doc_attention_mask)\n",
    "            query_sparse = compute_query_representation(query_tokens, idf_id_dict, tokenizer)\n",
    "            \n",
    "            loss, _, _, _ = neural_sparse_loss(\n",
    "                doc_sparse, query_sparse, relevance, idf_id_dict,\n",
    "                lambda_l0=LAMBDA_L0, lambda_idf=LAMBDA_IDF\n",
    "            )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# í•™ìŠµ íˆìŠ¤í† ë¦¬\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'ranking_loss': [],\n",
    "    'l0_loss': [],\n",
    "    'idf_penalty': []\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ í•™ìŠµ ì‹œì‘!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Epoch 1/3\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465b6313649d4603b793ed7320f12d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec35fcbbdbf408abd26e5d3e964ed6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Epoch 1 ê²°ê³¼:\n",
      "  Train Loss: 6.9841\n",
      "  Val Loss: 0.5324\n",
      "  Ranking Loss: 4.6205\n",
      "  L0 Loss: 920.40\n",
      "  IDF Penalty: 144.3173\n",
      "  âœ“ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥! (val_loss: 0.5324)\n",
      "\n",
      "============================================================\n",
      "Epoch 2/3\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05397fe7f6bb41dca6b31d55e3fd7d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a0f856cab14a37bdd81bfd4dd16ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Epoch 2 ê²°ê³¼:\n",
      "  Train Loss: 0.5262\n",
      "  Val Loss: 0.5266\n",
      "  Ranking Loss: 0.5190\n",
      "  L0 Loss: 2.82\n",
      "  IDF Penalty: 0.4384\n",
      "  âœ“ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥! (val_loss: 0.5266)\n",
      "\n",
      "============================================================\n",
      "Epoch 3/3\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc9e2a0f1b84fb795becf883751f46c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9edf63d68bf743f5a1c7d1d623c92012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Epoch 3 ê²°ê³¼:\n",
      "  Train Loss: 0.5131\n",
      "  Val Loss: 0.5272\n",
      "  Ranking Loss: 0.5071\n",
      "  L0 Loss: 2.36\n",
      "  IDF Penalty: 0.3663\n",
      "\n",
      "============================================================\n",
      "âœ… í•™ìŠµ ì™„ë£Œ!\n",
      "============================================================\n",
      "Best Validation Loss: 0.5266\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµ ì‹¤í–‰\n",
    "best_val_loss = float('inf')\n",
    "# ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "import os\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "best_model_path = \"./models/best_korean_neural_sparse_encoder.pt\"\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # í•™ìŠµ\n",
    "    train_loss, ranking_loss, l0_loss, idf_penalty = train_epoch(\n",
    "        doc_encoder, train_loader, optimizer, scheduler,\n",
    "        idf_id_dict, tokenizer, device\n",
    "    )\n",
    "    \n",
    "    # ê²€ì¦\n",
    "    val_loss = evaluate(doc_encoder, val_loader, idf_id_dict, tokenizer, device)\n",
    "    \n",
    "    # ê¸°ë¡\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['ranking_loss'].append(ranking_loss)\n",
    "    history['l0_loss'].append(l0_loss)\n",
    "    history['idf_penalty'].append(idf_penalty)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Epoch {epoch + 1} ê²°ê³¼:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Ranking Loss: {ranking_loss:.4f}\")\n",
    "    print(f\"  L0 Loss: {l0_loss:.2f}\")\n",
    "    print(f\"  IDF Penalty: {idf_penalty:.4f}\")\n",
    "    \n",
    "    # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': doc_encoder.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': {\n",
    "                'model_name': MODEL_NAME,\n",
    "                'vocab_size': len(tokenizer),\n",
    "                'max_length': MAX_LENGTH,\n",
    "            }\n",
    "        }, best_model_path)\n",
    "        print(f\"  âœ“ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥! (val_loss: {val_loss:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ëª¨ë¸ ì €ì¥ (OpenSearch í˜¸í™˜ í˜•ì‹)\n",
    "\n",
    "OpenSearchì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤:\n",
    "1. `pytorch_model.bin` - ë¬¸ì„œ ì¸ì½”ë”\n",
    "2. `idf.json` - ì¿¼ë¦¬ìš© í† í° ê°€ì¤‘ì¹˜ lookup table\n",
    "3. Tokenizer íŒŒì¼ë“¤\n",
    "4. `config.json` - ëª¨ë¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ OpenSearch í˜¸í™˜ ëª¨ë¸ ì €ì¥ ì¤‘...\n",
      "\n",
      "âœ“ ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ (Epoch 2, Val Loss: 0.5266)\n",
      "âœ“ pytorch_model.bin ì €ì¥\n",
      "âœ“ idf.json ì €ì¥ (29,197 tokens)\n",
      "âœ“ Tokenizer íŒŒì¼ ì €ì¥\n",
      "âœ“ config.json ì €ì¥\n",
      "âœ“ README.md ìƒì„±\n",
      "\n",
      "============================================================\n",
      "âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\n",
      "============================================================\n",
      "\n",
      "ì €ì¥ ìœ„ì¹˜: ./models/opensearch-korean-neural-sparse-v1/\n",
      "\n",
      "ì €ì¥ëœ íŒŒì¼:\n",
      "  - tokenizer.json                 (    0.72 MB)\n",
      "  - tokenizer_config.json          (    0.00 MB)\n",
      "  - pytorch_model.bin              (  422.18 MB)\n",
      "  - README.md                      (    0.00 MB)\n",
      "  - idf.json                       (    0.88 MB)\n",
      "  - vocab.txt                      (    0.24 MB)\n",
      "  - special_tokens_map.json        (    0.00 MB)\n",
      "  - config.json                    (    0.00 MB)\n"
     ]
    }
   ],
   "source": [
    "# ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "OUTPUT_DIR = \"./models/opensearch-korean-neural-sparse-v1\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“¦ OpenSearch í˜¸í™˜ ëª¨ë¸ ì €ì¥ ì¤‘...\\n\")\n",
    "\n",
    "# 1. ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ\n",
    "checkpoint = torch.load(best_model_path)\n",
    "doc_encoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"âœ“ ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ (Epoch {checkpoint['epoch'] + 1}, Val Loss: {checkpoint['val_loss']:.4f})\")\n",
    "\n",
    "# 2. pytorch_model.bin ì €ì¥ (ë¬¸ì„œ ì¸ì½”ë”)\n",
    "torch.save(doc_encoder.state_dict(), f\"{OUTPUT_DIR}/pytorch_model.bin\")\n",
    "print(f\"âœ“ pytorch_model.bin ì €ì¥\")\n",
    "\n",
    "# 3. idf.json ì €ì¥ (ì¿¼ë¦¬ìš© ê°€ì¤‘ì¹˜ lookup table)\n",
    "with open(f\"{OUTPUT_DIR}/idf.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(idf_token_dict, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ“ idf.json ì €ì¥ ({len(idf_token_dict):,} tokens)\")\n",
    "\n",
    "# 4. Tokenizer ì €ì¥\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"âœ“ Tokenizer íŒŒì¼ ì €ì¥\")\n",
    "\n",
    "# 5. config.json ì €ì¥\n",
    "model_config = {\n",
    "    \"model_type\": \"opensearch-neural-sparse-doc-encoder\",\n",
    "    \"base_model\": MODEL_NAME,\n",
    "    \"vocab_size\": len(tokenizer),\n",
    "    \"max_seq_length\": MAX_LENGTH,\n",
    "    \"mode\": \"doc-only\",\n",
    "    \"output_format\": \"rank_features\",\n",
    "    \"training_info\": {\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"lambda_l0\": LAMBDA_L0,\n",
    "        \"lambda_idf\": LAMBDA_IDF,\n",
    "        \"training_samples\": len(train_dataset),\n",
    "        \"trained_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    },\n",
    "    \"usage\": {\n",
    "        \"documents\": \"Use pytorch_model.bin to encode documents\",\n",
    "        \"queries\": \"Use tokenizer + idf.json for inference-free query encoding\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/config.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_config, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ“ config.json ì €ì¥\")\n",
    "\n",
    "# 6. README ìƒì„±\n",
    "readme_content = f\"\"\"# OpenSearch Korean Neural Sparse Model v1\n",
    "\n",
    "í•œêµ­ì–´ì— ìµœì í™”ëœ OpenSearch inference-free neural sparse ê²€ìƒ‰ ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "## ëª¨ë¸ êµ¬ì¡°\n",
    "\n",
    "### Doc-only Mode (Inference-Free)\n",
    "- **ë¬¸ì„œ ì¸ì½”ë”©**: BERT ê¸°ë°˜ ì‹ ê²½ë§ (`pytorch_model.bin`)\n",
    "- **ì¿¼ë¦¬ ì¸ì½”ë”©**: Tokenizer + IDF lookup (`idf.json`) - **Inference-Free!**\n",
    "\n",
    "## íŒŒì¼ êµ¬ì¡°\n",
    "\n",
    "```\n",
    "{OUTPUT_DIR}/\n",
    "â”œâ”€â”€ pytorch_model.bin       # ë¬¸ì„œ ì¸ì½”ë” ëª¨ë¸\n",
    "â”œâ”€â”€ idf.json                # ì¿¼ë¦¬ìš© í† í° ê°€ì¤‘ì¹˜ (IDF + íŠ¸ë Œë“œ ë¶€ìŠ¤íŒ…)\n",
    "â”œâ”€â”€ tokenizer.json          # í† í¬ë‚˜ì´ì €\n",
    "â”œâ”€â”€ tokenizer_config.json   # í† í¬ë‚˜ì´ì € ì„¤ì •\n",
    "â”œâ”€â”€ vocab.txt               # ì–´íœ˜ ì‚¬ì „\n",
    "â”œâ”€â”€ special_tokens_map.json # íŠ¹ìˆ˜ í† í°\n",
    "â””â”€â”€ config.json             # ëª¨ë¸ ì„¤ì •\n",
    "```\n",
    "\n",
    "## ì‚¬ìš© ë°©ë²•\n",
    "\n",
    "### 1. OpenSearchì— ëª¨ë¸ ë“±ë¡\n",
    "\n",
    "```bash\n",
    "# ëª¨ë¸ ì••ì¶•\n",
    "cd {OUTPUT_DIR}\n",
    "zip -r ../korean-neural-sparse-v1.zip .\n",
    "\n",
    "# OpenSearchì— ì—…ë¡œë“œ\n",
    "POST /_plugins/_ml/models/_upload\n",
    "{{\n",
    "  \"name\": \"korean-neural-sparse-v1\",\n",
    "  \"version\": \"1.0\",\n",
    "  \"model_format\": \"TORCH_SCRIPT\",\n",
    "  \"model_config\": {{\n",
    "    \"model_type\": \"bert\",\n",
    "    \"embedding_dimension\": {len(tokenizer)},\n",
    "    \"framework_type\": \"sentence_transformers\"\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "### 2. ì¸ë±ìŠ¤ ìƒì„±\n",
    "\n",
    "```json\n",
    "PUT /korean-docs\n",
    "{{\n",
    "  \"mappings\": {{\n",
    "    \"properties\": {{\n",
    "      \"content\": {{ \"type\": \"text\" }},\n",
    "      \"embedding\": {{ \"type\": \"rank_features\" }}\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "### 3. ê²€ìƒ‰\n",
    "\n",
    "```json\n",
    "POST /korean-docs/_search\n",
    "{{\n",
    "  \"query\": {{\n",
    "    \"neural_sparse\": {{\n",
    "      \"embedding\": {{\n",
    "        \"query_text\": \"í•œêµ­ì–´ ê²€ìƒ‰ ìµœì í™”\",\n",
    "        \"model_id\": \"<model_id>\"\n",
    "      }}\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "## í•™ìŠµ ì •ë³´\n",
    "\n",
    "- **Base Model**: {MODEL_NAME}\n",
    "- **Training Samples**: {len(train_dataset):,}\n",
    "- **Epochs**: {NUM_EPOCHS}\n",
    "- **Best Val Loss**: {best_val_loss:.4f}\n",
    "- **Trained Date**: {datetime.now().strftime(\"%Y-%m-%d\")}\n",
    "\n",
    "## íŠ¹ì§•\n",
    "\n",
    "1. **í•œêµ­ì–´ ìµœì í™”**: KLUE-BERT ê¸°ë°˜\n",
    "2. **Inference-Free**: ì¿¼ë¦¬ëŠ” tokenizer + IDF lookupë§Œ ì‚¬ìš©\n",
    "3. **íŠ¸ë Œë“œ í‚¤ì›Œë“œ**: 2024-2025 AI/ML íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë¶€ìŠ¤íŒ…\n",
    "4. **IDF-aware**: ë‚®ì€ IDF í† í° ì–µì œ\n",
    "5. **Sparse**: L0 regularizationìœ¼ë¡œ í¬ì†Œì„± ìœ ì§€\n",
    "\n",
    "## ì°¸ê³ \n",
    "\n",
    "- [OpenSearch Neural Sparse Search](https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/)\n",
    "- [Paper: Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers](https://arxiv.org/abs/2411.04403)\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/README.md\", 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "print(f\"âœ“ README.md ìƒì„±\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}/\")\n",
    "print(f\"\\nì €ì¥ëœ íŒŒì¼:\")\n",
    "for filename in os.listdir(OUTPUT_DIR):\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    size = os.path.getsize(filepath) / (1024*1024)  # MB\n",
    "    print(f\"  - {filename:30s} ({size:>8.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ì €ì¥ëœ ëª¨ë¸ë¡œ inference í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
      "============================================================\n",
      "\n",
      "ğŸ“ ì¿¼ë¦¬ ì¸ì½”ë”© (Inference-Free: Tokenizer + IDF Lookup)\n",
      "\n",
      "Query: ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ\n",
      "  í¬ì†Œì„±: 99.99% (non-zero: 4/32000)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ì‹œìŠ¤í…œ             (10.4335)\n",
      "     2. ê²€ìƒ‰              (7.3771)\n",
      "     3. ì¸ê³µì§€ëŠ¥            (7.0662)\n",
      "     4. ê¸°ë°˜              (5.7262)\n",
      "\n",
      "Query: í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ \n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 5/32000)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. í•œêµ­ì–´             (7.7594)\n",
      "     2. ìì—°              (6.2067)\n",
      "     3. ì²˜ë¦¬              (6.1261)\n",
      "     4. ê¸°ìˆ               (5.1452)\n",
      "     5. ##ì–´             (3.3662)\n",
      "\n",
      "Query: OpenSearch ë²¡í„° ê²€ìƒ‰\n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 6/32000)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ë²¡               (9.2548)\n",
      "     2. ê²€ìƒ‰              (7.3771)\n",
      "     3. Op              (5.6995)\n",
      "     4. ##S             (5.2860)\n",
      "     5. ##í„°             (4.9633)\n",
      "     6. ##en            (4.4483)\n",
      "\n",
      "Query: ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ë°©ë²•\n",
      "  í¬ì†Œì„±: 99.98% (non-zero: 6/32000)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ë”¥               (8.6843)\n",
      "     2. í•™ìŠµ              (7.5571)\n",
      "     3. ##ë‹             (6.4538)\n",
      "     4. ëª¨ë¸              (6.0672)\n",
      "     5. ë°©ë²•              (5.7468)\n",
      "     6. ##ëŸ¬             (4.8747)\n",
      "\n",
      "Query: ChatGPT LLM í”„ë¡¬í”„íŠ¸\n",
      "  í¬ì†Œì„±: 99.97% (non-zero: 11/32000)\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ##ë¡¬             (8.0586)\n",
      "     2. í”„               (7.5571)\n",
      "     3. ##í”„íŠ¸            (7.0836)\n",
      "     4. ##M             (5.9421)\n",
      "     5. Ch              (5.9028)\n",
      "     6. ##L             (5.5245)\n",
      "     7. ##T             (5.4499)\n",
      "     8. ##G             (5.3352)\n",
      "     9. ##P             (4.9835)\n",
      "    10. L               (4.6481)\n",
      "\n",
      "\n",
      "ğŸ“„ ë¬¸ì„œ ì¸ì½”ë”© (Model Inference)\n",
      "\n",
      "Document: OpenSearchëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural spars...\n",
      "  í¬ì†Œì„±: 99.94% (non-zero: 19/32000)\n",
      "  L1 Norm: 5.52\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. ê²€ìƒ‰              (1.0885)\n",
      "     2. íƒìƒ‰              (0.5853)\n",
      "     3. ì—”ì§„              (0.5280)\n",
      "     4. êµ¬ê¸€              (0.4020)\n",
      "     5. ë°ì´í„°ë² ì´ìŠ¤          (0.4017)\n",
      "     6. ì›¹               (0.3879)\n",
      "     7. ë°ì´í„°             (0.3030)\n",
      "     8. ì •ë³´              (0.2413)\n",
      "     9. ì…ë ¥              (0.2371)\n",
      "    10. ë¶„ì„              (0.2221)\n",
      "\n",
      "Document: í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤....\n",
      "  í¬ì†Œì„±: 99.99% (non-zero: 2/32000)\n",
      "  L1 Norm: 1.47\n",
      "  ìƒìœ„ í† í°:\n",
      "     1. í•œêµ­ì–´             (1.0295)\n",
      "     2. ##ì–´             (0.4437)\n",
      "\n",
      "============================================================\n",
      "âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def encode_document(text, model, tokenizer, device, max_length=128):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œë¥¼ sparse vectorë¡œ ì¸ì½”ë”© (ëª¨ë¸ ì‚¬ìš©)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sparse_vec = model(input_ids, attention_mask)\n",
    "    \n",
    "    return sparse_vec.cpu().numpy()[0]\n",
    "\n",
    "def encode_query_inference_free(text, tokenizer, idf_dict, max_length=128):\n",
    "    \"\"\"\n",
    "    ì¿¼ë¦¬ë¥¼ sparse vectorë¡œ ì¸ì½”ë”© (IDF lookup - Inference-Free!)\n",
    "    \"\"\"\n",
    "    # í† í°í™”\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False, max_length=max_length, truncation=True)\n",
    "    \n",
    "    # IDF lookup\n",
    "    sparse_vec = np.zeros(len(tokenizer))\n",
    "    for token_id in tokens:\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        if token_str in idf_dict:\n",
    "            sparse_vec[token_id] = idf_dict[token_str]\n",
    "    \n",
    "    return sparse_vec\n",
    "\n",
    "def get_top_tokens(sparse_vec, tokenizer, top_k=15):\n",
    "    \"\"\"\n",
    "    Sparse vectorì—ì„œ ìƒìœ„ í† í° ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    top_indices = np.argsort(sparse_vec)[-top_k:][::-1]\n",
    "    top_values = sparse_vec[top_indices]\n",
    "    \n",
    "    top_tokens = []\n",
    "    for idx, val in zip(top_indices, top_values):\n",
    "        if val > 0:\n",
    "            token = tokenizer.decode([idx])\n",
    "            top_tokens.append((token, val))\n",
    "    \n",
    "    return top_tokens\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_queries = [\n",
    "    \"ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ\",\n",
    "    \"í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ \",\n",
    "    \"OpenSearch ë²¡í„° ê²€ìƒ‰\",\n",
    "    \"ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ë°©ë²•\",\n",
    "    \"ChatGPT LLM í”„ë¡¬í”„íŠ¸\",\n",
    "]\n",
    "\n",
    "test_documents = [\n",
    "    \"OpenSearchëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.\",\n",
    "    \"í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤.\",\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ“ ì¿¼ë¦¬ ì¸ì½”ë”© (Inference-Free: Tokenizer + IDF Lookup)\\n\")\n",
    "\n",
    "for query in test_queries:\n",
    "    sparse_vec = encode_query_inference_free(query, tokenizer, idf_token_dict)\n",
    "    \n",
    "    non_zero = np.count_nonzero(sparse_vec)\n",
    "    sparsity = (1 - non_zero / len(sparse_vec)) * 100\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"  í¬ì†Œì„±: {sparsity:.2f}% (non-zero: {non_zero}/{len(sparse_vec)})\")\n",
    "    print(f\"  ìƒìœ„ í† í°:\")\n",
    "    \n",
    "    top_tokens = get_top_tokens(sparse_vec, tokenizer, top_k=10)\n",
    "    for i, (token, value) in enumerate(top_tokens, 1):\n",
    "        print(f\"    {i:2d}. {token:15s} ({value:.4f})\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nğŸ“„ ë¬¸ì„œ ì¸ì½”ë”© (Model Inference)\\n\")\n",
    "\n",
    "for doc in test_documents:\n",
    "    sparse_vec = encode_document(doc, doc_encoder, tokenizer, device)\n",
    "    \n",
    "    non_zero = np.count_nonzero(sparse_vec)\n",
    "    sparsity = (1 - non_zero / len(sparse_vec)) * 100\n",
    "    \n",
    "    print(f\"Document: {doc[:50]}...\")\n",
    "    print(f\"  í¬ì†Œì„±: {sparsity:.2f}% (non-zero: {non_zero}/{len(sparse_vec)})\")\n",
    "    print(f\"  L1 Norm: {np.sum(np.abs(sparse_vec)):.2f}\")\n",
    "    print(f\"  ìƒìœ„ í† í°:\")\n",
    "    \n",
    "    top_tokens = get_top_tokens(sparse_vec, tokenizer, top_k=10)\n",
    "    for i, (token, value) in enumerate(top_tokens, 1):\n",
    "        print(f\"    {i:2d}. {token:15s} ({value:.4f})\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. OpenSearch í†µí•© ê°€ì´ë“œ\n",
    "\n",
    "í•™ìŠµëœ ëª¨ë¸ì„ OpenSearchì— í†µí•©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘     OpenSearch Inference-Free Neural Sparse ëª¨ë¸ í†µí•© ê°€ì´ë“œ  â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "## 1ï¸âƒ£ ëª¨ë¸ íŒ¨í‚¤ì§• ë° ì—…ë¡œë“œ\n",
    "\n",
    "```bash\n",
    "# ëª¨ë¸ ì••ì¶•\n",
    "cd models/opensearch-korean-neural-sparse-v1\n",
    "zip -r ../korean-neural-sparse-v1.zip .\n",
    "\n",
    "# OpenSearchì— ëª¨ë¸ ë“±ë¡\n",
    "POST /_plugins/_ml/models/_upload\n",
    "{\n",
    "  \"name\": \"korean-neural-sparse-doc-v1\",\n",
    "  \"version\": \"1.0\",\n",
    "  \"description\": \"Korean Neural Sparse Model for document encoding\",\n",
    "  \"model_format\": \"TORCH_SCRIPT\",\n",
    "  \"model_config\": {\n",
    "    \"model_type\": \"bert\",\n",
    "    \"embedding_dimension\": 30000,\n",
    "    \"framework_type\": \"sentence_transformers\",\n",
    "    \"all_config\": {\n",
    "      \"mode\": \"doc-only\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Tokenizer ëª¨ë¸ ë“±ë¡ (ì¿¼ë¦¬ìš©)\n",
    "POST /_plugins/_ml/models/_upload\n",
    "{\n",
    "  \"name\": \"korean-neural-sparse-tokenizer-v1\",\n",
    "  \"version\": \"1.0\",\n",
    "  \"model_format\": \"TOKENIZER\",\n",
    "  \"model_config\": {\n",
    "    \"model_type\": \"tokenizer\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## 2ï¸âƒ£ ì¸ë±ìŠ¤ ìƒì„± (rank_features íƒ€ì…)\n",
    "\n",
    "```json\n",
    "PUT /korean-neural-sparse-index\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"index\": {\n",
    "      \"default_pipeline\": \"korean-neural-sparse-ingest\"\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"title\": { \"type\": \"text\" },\n",
    "      \"content\": { \"type\": \"text\" },\n",
    "      \"content_embedding\": {\n",
    "        \"type\": \"rank_features\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## 3ï¸âƒ£ Ingest Pipeline ì„¤ì •\n",
    "\n",
    "```json\n",
    "PUT /_ingest/pipeline/korean-neural-sparse-ingest\n",
    "{\n",
    "  \"description\": \"Korean neural sparse encoding pipeline\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"sparse_encoding\": {\n",
    "        \"model_id\": \"<doc_model_id>\",\n",
    "        \"field_map\": {\n",
    "          \"content\": \"content_embedding\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "## 4ï¸âƒ£ ë¬¸ì„œ ì¸ë±ì‹±\n",
    "\n",
    "```json\n",
    "POST /korean-neural-sparse-index/_doc\n",
    "{\n",
    "  \"title\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰ ê¸°ìˆ \",\n",
    "  \"content\": \"OpenSearchëŠ” neural sparse ê²€ìƒ‰ì„ ì§€ì›í•˜ëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ì—”ì§„ì…ë‹ˆë‹¤.\"\n",
    "}\n",
    "```\n",
    "\n",
    "## 5ï¸âƒ£ Neural Sparse ê²€ìƒ‰ (Doc-only mode)\n",
    "\n",
    "```json\n",
    "POST /korean-neural-sparse-index/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"neural_sparse\": {\n",
    "      \"content_embedding\": {\n",
    "        \"query_text\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰ ìµœì í™”\",\n",
    "        \"model_id\": \"<tokenizer_model_id>\",\n",
    "        \"max_token_score\": 3.5\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## 6ï¸âƒ£ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + Neural Sparse)\n",
    "\n",
    "```json\n",
    "POST /korean-neural-sparse-index/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"hybrid\": {\n",
    "      \"queries\": [\n",
    "        {\n",
    "          \"match\": {\n",
    "            \"content\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰\"\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"neural_sparse\": {\n",
    "            \"content_embedding\": {\n",
    "              \"query_text\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰\",\n",
    "              \"model_id\": \"<tokenizer_model_id>\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"search_pipeline\": {\n",
    "    \"phase_results_processors\": [\n",
    "      {\n",
    "        \"normalization-processor\": {\n",
    "          \"normalization\": {\n",
    "            \"technique\": \"min_max\"\n",
    "          },\n",
    "          \"combination\": {\n",
    "            \"technique\": \"arithmetic_mean\",\n",
    "            \"parameters\": {\n",
    "              \"weights\": [0.3, 0.7]\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## ğŸ“Š ì„±ëŠ¥ íŠ¹ì§•\n",
    "\n",
    "- **ë¬¸ì„œ ì¸ì½”ë”©**: BERT ê¸°ë°˜ ì‹ ê²½ë§ (ëŠë¦¼, ê³ í’ˆì§ˆ)\n",
    "- **ì¿¼ë¦¬ ì¸ì½”ë”©**: Tokenizer + IDF lookup (ë§¤ìš° ë¹ ë¦„, Inference-Free!)\n",
    "- **ì¿¼ë¦¬ ì§€ì—°ì‹œê°„**: BM25ì™€ ê±°ì˜ ë™ì¼ (1.1x)\n",
    "- **ê²€ìƒ‰ ì •í™•ë„**: Siamese sparse ëª¨ë¸ê³¼ ìœ ì‚¬\n",
    "\n",
    "## ğŸ“š ì°¸ê³  ìë£Œ\n",
    "\n",
    "- [OpenSearch Neural Sparse Search](https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/)\n",
    "- [Doc-only Mode](https://opensearch.org/docs/latest/search-plugins/neural-sparse-search/#doc-only-mode)\n",
    "- [Pretrained Models](https://opensearch.org/docs/latest/ml-commons-plugin/pretrained-models/)\n",
    "- [Paper: Inference-Free Learned Sparse Retrievers](https://arxiv.org/abs/2411.04403)\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### âœ… ì™„ë£Œëœ ì‘ì—…\n",
    "\n",
    "1. **ë°ì´í„° ìˆ˜ì§‘**: í•œêµ­ì–´ ê³µê°œ ë°ì´í„°ì…‹ (KLUE, KorQuAD, Wikipedia ë“±)\n",
    "2. **IDF ê³„ì‚°**: í† í°ë³„ IDF ê°€ì¤‘ì¹˜ ê³„ì‚° ë° íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…\n",
    "3. **ëª¨ë¸ í•™ìŠµ**: OpenSearch doc-only mode ë¬¸ì„œ ì¸ì½”ë” í•™ìŠµ\n",
    "   - IDF-aware penalty\n",
    "   - L0 regularization\n",
    "   - Ranking loss\n",
    "4. **ëª¨ë¸ ì €ì¥**: OpenSearch í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ì €ì¥\n",
    "   - `pytorch_model.bin` (ë¬¸ì„œ ì¸ì½”ë”)\n",
    "   - `idf.json` (ì¿¼ë¦¬ìš© ê°€ì¤‘ì¹˜)\n",
    "   - Tokenizer íŒŒì¼ë“¤\n",
    "   - `config.json`\n",
    "\n",
    "### ğŸ¯ í•µì‹¬ íŠ¹ì§•\n",
    "\n",
    "- **Inference-Free**: ì¿¼ë¦¬ëŠ” tokenizer + IDF lookupë§Œ ì‚¬ìš© â†’ ë§¤ìš° ë¹ ë¦„!\n",
    "- **í•œêµ­ì–´ ìµœì í™”**: KLUE-BERT ê¸°ë°˜ + í•œêµ­ì–´ ë°ì´í„°ì…‹\n",
    "- **íŠ¸ë Œë“œ ë°˜ì˜**: 2024-2025 AI/ML í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë¶€ìŠ¤íŒ…\n",
    "- **OpenSearch í˜¸í™˜**: ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ í˜•ì‹\n",
    "\n",
    "### ğŸš€ ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "1. **Knowledge Distillation**: Dense + Sparse siamese ëª¨ë¸ë¡œ distillation\n",
    "2. **ë” ë§ì€ ë°ì´í„°**: AI Hub, NIKL ë“± ì¶”ê°€ ë°ì´í„°ì…‹\n",
    "3. **Hard Negative Mining**: In-batch negatives, hard negatives\n",
    "4. **ëª¨ë¸ í‰ê°€**: BEIR ë²¤ì¹˜ë§ˆí¬, MRR, NDCG ë“±\n",
    "5. **OpenSearch ë°°í¬**: ì‹¤ì œ ê²€ìƒ‰ ì‹œìŠ¤í…œì— í†µí•©\n",
    "6. **A/B í…ŒìŠ¤íŒ…**: ê¸°ì¡´ BM25ì™€ ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "### ğŸ“ˆ ê¸°ëŒ€ íš¨ê³¼\n",
    "\n",
    "- BM25 ëŒ€ë¹„ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ\n",
    "- Dense retrieval ëŒ€ë¹„ ë¹ ë¥¸ ì†ë„\n",
    "- í•œêµ­ì–´ íŠ¹í™” ê²€ìƒ‰ ì„±ëŠ¥ ê°œì„ \n",
    "- íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê²€ìƒ‰ ìµœì í™”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ†• 13. LLM ëª¨ë¸ ë¡œë”© ë° ì´ˆê¸°í™”\n",
    "\n",
    "ì´ ì„¹ì…˜ë¶€í„°ëŠ” LLM ê¸°ë°˜ í™•ì¥ ê¸°ëŠ¥ì…ë‹ˆë‹¤:\n",
    "- í•©ì„± Query-Document ìŒ ìƒì„±\n",
    "- í•œì˜ ë™ì˜ì–´ ìë™ ê²€ì¦\n",
    "- ì„±ëŠ¥ ê°œì„ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ğŸ¤– ì„¹ì…˜ 13: LLM ëª¨ë¸ ë¡œë”© ë° ì´ˆê¸°í™”\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from src.llm_loader import load_qwen3_awq, check_gpu_memory\n",
    "\n",
    "# GPU ë©”ëª¨ë¦¬ ì²´í¬\n",
    "check_gpu_memory()\n",
    "\n",
    "# Qwen3-30B-A3B-Thinking-2507-AWQ ëª¨ë¸ ë¡œë”© (4-bit quantization, reasoning optimized)\n",
    "# Qwen3-30B-A3B-Thinking-2507-AWQ ëª¨ë¸ ë¡œë”© (4-bit quantization, reasoning optimized)\n",
    "print(\"âš ï¸  ì°¸ê³ : ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"   ëª¨ë¸ í¬ê¸°: ~8GB (AWQ 4-bit)\")\n",
    "\n",
    "llm_model, llm_tokenizer = load_qwen3_awq(\n",
    "    model_name=\"QuantTrio/Qwen3-30B-A3B-Thinking-2507-AWQ\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\")\n",
    "print(\"   ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í•©ì„± ë°ì´í„°ë¥¼ ìƒì„±í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ†• 14. LLM ê¸°ë°˜ í•©ì„± Query-Document Pairs ìƒì„±\n",
    "\n",
    "LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¡œë¶€í„° ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì—­ìƒì„±í•©ë‹ˆë‹¤.\n",
    "ì´ë¥¼ í†µí•´ í•™ìŠµ ë°ì´í„°ë¥¼ ëŒ€í­ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“ ì„¹ì…˜ 14: LLM ê¸°ë°˜ í•©ì„± Query-Document Pairs ìƒì„±\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from src.synthetic_data_generator import generate_synthetic_qd_pairs\n",
    "\n",
    "# ê¸°ì¡´ ë¬¸ì„œì—ì„œ í•©ì„± ì¿¼ë¦¬ ìƒì„±\n",
    "# ì²˜ìŒ 1000ê°œ ë¬¸ì„œ ì‚¬ìš© (ì‹œê°„ ê³ ë ¤)\n",
    "print(f\"\\nğŸ“Š í•©ì„± ë°ì´í„° ìƒì„± ì„¤ì •:\")\n",
    "print(f\"   - ì‚¬ìš©í•  ë¬¸ì„œ: 1,000ê°œ\")\n",
    "print(f\"   - ë¬¸ì„œë‹¹ ì¿¼ë¦¬: 3ê°œ\")\n",
    "print(f\"   - ì˜ˆìƒ ìƒì„± pairs: ~3,000ê°œ\")\n",
    "print(f\"   - ì˜ˆìƒ ì†Œìš” ì‹œê°„: ~15-30ë¶„ (GPU ì†ë„ì— ë”°ë¼)\")\n",
    "\n",
    "synthetic_pairs = generate_synthetic_qd_pairs(\n",
    "    documents=documents[:1000],  # ì²˜ìŒ 1000ê°œ ë¬¸ì„œ\n",
    "    llm_model=llm_model,\n",
    "    llm_tokenizer=llm_tokenizer,\n",
    "    num_queries_per_doc=3,\n",
    "    batch_size=2,\n",
    "    max_documents=1000,\n",
    "    enable_filtering=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… í•©ì„± ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(synthetic_pairs):,}ê°œ pairs\")\n",
    "\n",
    "# ìƒ˜í”Œ ì¶œë ¥\n",
    "print(\"\\nğŸ“‹ ìƒì„±ëœ í•©ì„± ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ):\")\n",
    "print(\"=\"*70)\n",
    "for i, (query, doc, relevance) in enumerate(synthetic_pairs[:5], 1):\n",
    "    print(f\"\\n{i}. Query: {query}\")\n",
    "    print(f\"   Document: {doc[:100]}...\")\n",
    "    print(f\"   Relevance: {relevance}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ†• 15. LLM ê¸°ë°˜ í•œì˜ ë™ì˜ì–´ ê²€ì¦ ë° í™•ì¥\n",
    "\n",
    "ê¸°ì¡´ ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ë°œê²¬í•œ ë™ì˜ì–´ë¥¼ LLMìœ¼ë¡œ ê²€ì¦í•˜ì—¬ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸŒ ì„¹ì…˜ 15: LLM ê¸°ë°˜ í•œì˜ ë™ì˜ì–´ ê²€ì¦ ë° í™•ì¥\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from src.cross_lingual_synonyms import enhance_bilingual_dict_with_llm\n",
    "\n",
    "# ê¸°ì¡´ ì„ë² ë”© ê¸°ë°˜ ë™ì˜ì–´ë¥¼ LLMìœ¼ë¡œ ê²€ì¦\n",
    "print(f\"\\nğŸ“Š ë™ì˜ì–´ ê²€ì¦ ì„¤ì •:\")\n",
    "print(f\"   - ê¸°ì¡´ ë™ì˜ì–´ ì‚¬ì „: {len(bilingual_dict):,}ê°œ í•­ëª©\")\n",
    "print(f\"   - ê²€ì¦í•  í•­ëª©: ìƒìœ„ 100ê°œ\")\n",
    "print(f\"   - ê²€ì¦ ì„ê³„ê°’: 0.8\")\n",
    "print(f\"   - ì˜ˆìƒ ì†Œìš” ì‹œê°„: ~5-10ë¶„\")\n",
    "\n",
    "enhanced_bilingual_dict = enhance_bilingual_dict_with_llm(\n",
    "    initial_dict=bilingual_dict,  # Cell 14ì—ì„œ ìƒì„±ëœ ê¸°ì¡´ ì‚¬ì „\n",
    "    llm_model=llm_model,\n",
    "    llm_tokenizer=llm_tokenizer,\n",
    "    verification_threshold=0.8,\n",
    "    max_verify=100,  # ìƒìœ„ 100ê°œë§Œ ê²€ì¦ (ì‹œê°„ ì ˆì•½)\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… LLM ê²€ì¦ ì™„ë£Œ!\")\n",
    "print(f\"   ê¸°ì¡´ ì‚¬ì „: {len(bilingual_dict):,}ê°œ\")\n",
    "print(f\"   ê²€ì¦ í›„: {len(enhanced_bilingual_dict):,}ê°œ\")\n",
    "\n",
    "# ê²€ì¦ ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥\n",
    "print(\"\\nğŸ“‹ ê²€ì¦ëœ ë™ì˜ì–´ ìƒ˜í”Œ:\")\n",
    "print(\"=\"*70)\n",
    "sample_count = 0\n",
    "for korean, english_list in list(enhanced_bilingual_dict.items())[:10]:\n",
    "    sample_count += 1\n",
    "    print(f\"{sample_count}. {korean} â†” {', '.join(english_list)}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ì—…ë°ì´íŠ¸ëœ ë™ì˜ì–´ ì‚¬ì „ì„ bilingual_dictì— ë°˜ì˜\n",
    "bilingual_dict = enhanced_bilingual_dict\n",
    "print(f\"\\nâœ“ ë™ì˜ì–´ ì‚¬ì „ ì—…ë°ì´íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ†• 16. í•©ì„± ë°ì´í„° í¬í•¨ ëª¨ë¸ ì¬í•™ìŠµ\n",
    "\n",
    "ê¸°ì¡´ ë°ì´í„°ì™€ LLMìœ¼ë¡œ ìƒì„±í•œ í•©ì„± ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ ëª¨ë¸ì„ ì¬í•™ìŠµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ”„ ì„¹ì…˜ 16: í•©ì„± ë°ì´í„° í¬í•¨ ëª¨ë¸ ì¬í•™ìŠµ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ê¸°ì¡´ ë°ì´í„° + í•©ì„± ë°ì´í„° ë³‘í•©\n",
    "combined_qd_pairs = korean_data['qd_pairs'][:10000] + synthetic_pairs\n",
    "\n",
    "print(f\"\\nğŸ“Š í•™ìŠµ ë°ì´í„° í†µê³„:\")\n",
    "print(f\"   ê¸°ì¡´ ë°ì´í„°: {len(korean_data['qd_pairs'][:10000]):,}ê°œ\")\n",
    "print(f\"   í•©ì„± ë°ì´í„°: {len(synthetic_pairs):,}ê°œ\")\n",
    "print(f\"   ì´ ë°ì´í„°: {len(combined_qd_pairs):,}ê°œ\")\n",
    "print(f\"   ì¦ê°€ìœ¨: {len(synthetic_pairs) / len(korean_data['qd_pairs'][:10000]) * 100:.1f}%\")\n",
    "\n",
    "# Negative sampling ì ìš©\n",
    "print(\"\\nğŸ”„ Negative sampling ì ìš© ì¤‘...\")\n",
    "augmented_pairs_v2 = add_negative_samples(\n",
    "    combined_qd_pairs,\n",
    "    korean_data['documents'],\n",
    "    num_negatives=2\n",
    ")\n",
    "\n",
    "# Train/Val split\n",
    "train_pairs_v2, val_pairs_v2 = train_test_split(augmented_pairs_v2, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"\\nğŸ“Š ì¬í•™ìŠµ ë°ì´í„°ì…‹ ë¶„í• :\")\n",
    "print(f\"   Train: {len(train_pairs_v2):,} pairs\")\n",
    "print(f\"   Val: {len(val_pairs_v2):,} pairs\")\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„±\n",
    "train_dataset_v2 = SparseEncodingDataset(train_pairs_v2, tokenizer, MAX_LENGTH)\n",
    "val_dataset_v2 = SparseEncodingDataset(val_pairs_v2, tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_loader_v2 = DataLoader(train_dataset_v2, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader_v2 = DataLoader(val_dataset_v2, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nâœ“ ì¬í•™ìŠµ ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"   Train batches: {len(train_loader_v2):,}\")\n",
    "print(f\"   Val batches: {len(val_loader_v2):,}\")\n",
    "\n",
    "# ëª¨ë¸ ì¬ì´ˆê¸°í™” (ìƒˆë¡œ í•™ìŠµ)\n",
    "print(\"\\nğŸ”„ ëª¨ë¸ ì¬ì´ˆê¸°í™” ì¤‘...\")\n",
    "doc_encoder_v2 = OpenSearchDocEncoder(MODEL_NAME)\n",
    "doc_encoder_v2 = doc_encoder_v2.to(device)\n",
    "\n",
    "if len(tokenizer) > doc_encoder_v2.vocab_size:\n",
    "    doc_encoder_v2.bert.resize_token_embeddings(len(tokenizer))\n",
    "    doc_encoder_v2.vocab_size = len(tokenizer)\n",
    "\n",
    "print(f\"âœ“ ëª¨ë¸ v2 ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "\n",
    "# Optimizer & Scheduler for v2\n",
    "optimizer_v2 = AdamW(doc_encoder_v2.parameters(), lr=LEARNING_RATE)\n",
    "total_steps_v2 = len(train_loader_v2) * NUM_EPOCHS\n",
    "scheduler_v2 = get_linear_schedule_with_warmup(\n",
    "    optimizer_v2,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=total_steps_v2\n",
    ")\n",
    "\n",
    "# í•™ìŠµ íˆìŠ¤í† ë¦¬ v2\n",
    "history_v2 = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'ranking_loss': [],\n",
    "    'l0_loss': [],\n",
    "    'idf_penalty': []\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸš€ LLM í™•ì¥ ëª¨ë¸ í•™ìŠµ ì‹œì‘!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# í•™ìŠµ ì‹¤í–‰ (v2)\n",
    "best_val_loss_v2 = float('inf')\n",
    "best_model_path_v2 = \"./models/best_korean_neural_sparse_encoder_v2_llm.pt\"\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[V2 LLM] Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # í•™ìŠµ\n",
    "    train_loss, ranking_loss, l0_loss, idf_penalty = train_epoch(\n",
    "        doc_encoder_v2, train_loader_v2, optimizer_v2, scheduler_v2,\n",
    "        idf_id_dict, tokenizer, device\n",
    "    )\n",
    "    \n",
    "    # ê²€ì¦\n",
    "    val_loss = evaluate(doc_encoder_v2, val_loader_v2, idf_id_dict, tokenizer, device)\n",
    "    \n",
    "    # ê¸°ë¡\n",
    "    history_v2['train_loss'].append(train_loss)\n",
    "    history_v2['val_loss'].append(val_loss)\n",
    "    history_v2['ranking_loss'].append(ranking_loss)\n",
    "    history_v2['l0_loss'].append(l0_loss)\n",
    "    history_v2['idf_penalty'].append(idf_penalty)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š [V2] Epoch {epoch + 1} ê²°ê³¼:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Ranking Loss: {ranking_loss:.4f}\")\n",
    "    print(f\"  L0 Loss: {l0_loss:.2f}\")\n",
    "    print(f\"  IDF Penalty: {idf_penalty:.4f}\")\n",
    "    \n",
    "    # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥\n",
    "    if val_loss < best_val_loss_v2:\n",
    "        best_val_loss_v2 = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': doc_encoder_v2.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_v2.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': {\n",
    "                'model_name': MODEL_NAME,\n",
    "                'vocab_size': len(tokenizer),\n",
    "                'max_length': MAX_LENGTH,\n",
    "            }\n",
    "        }, best_model_path_v2)\n",
    "        print(f\"  âœ“ ë² ìŠ¤íŠ¸ ëª¨ë¸ v2 ì €ì¥! (val_loss: {val_loss:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… LLM í™•ì¥ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best Validation Loss (V2): {best_val_loss_v2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ†• 17. ì„±ëŠ¥ ë¹„êµ ë¶„ì„ (ê¸°ì¡´ vs LLM í™•ì¥)\n",
    "\n",
    "ê¸°ì¡´ ëª¨ë¸ê³¼ LLMìœ¼ë¡œ í™•ì¥í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š ì„¹ì…˜ 17: ì„±ëŠ¥ ë¹„êµ ë¶„ì„ (ê¸°ì¡´ vs LLM í™•ì¥)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ê¸°ì¡´ ëª¨ë¸ vs LLM í™•ì¥ ëª¨ë¸ ë¹„êµ\n",
    "comparison_results = {\n",
    "    'í•­ëª©': [\n",
    "        'ëª¨ë¸ ë²„ì „',\n",
    "        'í•™ìŠµ ë°ì´í„° (pairs)',\n",
    "        'ë™ì˜ì–´ ì‚¬ì „ (í•­ëª©)',\n",
    "        'Training Loss',\n",
    "        'Validation Loss',\n",
    "        'ì„±ëŠ¥ ê°œì„ ìœ¨'\n",
    "    ],\n",
    "    'ê¸°ì¡´ ëª¨ë¸ (V1)': [\n",
    "        'v1 (baseline)',\n",
    "        f\"{len(korean_data['qd_pairs'][:10000]):,}\",\n",
    "        f\"{len(get_default_korean_english_pairs())}\",\n",
    "        f\"{history['train_loss'][-1]:.4f}\",\n",
    "        f\"{best_val_loss:.4f}\",\n",
    "        \"baseline\"\n",
    "    ],\n",
    "    'LLM í™•ì¥ ëª¨ë¸ (V2)': [\n",
    "        'v2_llm',\n",
    "        f\"{len(combined_qd_pairs):,}\",\n",
    "        f\"{len(enhanced_bilingual_dict):,}\",\n",
    "        f\"{history_v2['train_loss'][-1]:.4f}\",\n",
    "        f\"{best_val_loss_v2:.4f}\",\n",
    "        f\"{((best_val_loss - best_val_loss_v2) / best_val_loss * 100):.2f}%\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_results)\n",
    "\n",
    "print(\"\\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ:\")\n",
    "print(\"=\"*70)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# í•™ìŠµ ê³¡ì„  ë¹„êµ\n",
    "print(\"\\nğŸ“ˆ í•™ìŠµ ê³¡ì„  ë¹„êµ:\")\n",
    "print(\"\\n[V1 - ê¸°ì¡´ ëª¨ë¸]\")\n",
    "for epoch, (train_loss, val_loss) in enumerate(zip(history['train_loss'], history['val_loss']), 1):\n",
    "    print(f\"  Epoch {epoch}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n",
    "\n",
    "print(\"\\n[V2 - LLM í™•ì¥ ëª¨ë¸]\")\n",
    "for epoch, (train_loss, val_loss) in enumerate(zip(history_v2['train_loss'], history_v2['val_loss']), 1):\n",
    "    print(f\"  Epoch {epoch}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n",
    "\n",
    "# ê°œì„  ì‚¬í•­ ìš”ì•½\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… ì£¼ìš” ê°œì„  ì‚¬í•­\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "data_increase = len(synthetic_pairs) / len(korean_data['qd_pairs'][:10000]) * 100\n",
    "synonym_increase = (len(enhanced_bilingual_dict) - len(get_default_korean_english_pairs())) / len(get_default_korean_english_pairs()) * 100\n",
    "loss_improvement = (best_val_loss - best_val_loss_v2) / best_val_loss * 100\n",
    "\n",
    "print(f\"\\n1ï¸âƒ£ í•™ìŠµ ë°ì´í„° í™•ì¥\")\n",
    "print(f\"   - ê¸°ì¡´: {len(korean_data['qd_pairs'][:10000]):,}ê°œ\")\n",
    "print(f\"   - LLM í•©ì„±: +{len(synthetic_pairs):,}ê°œ\")\n",
    "print(f\"   - ì¦ê°€ìœ¨: +{data_increase:.1f}%\")\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£ ë™ì˜ì–´ ì‚¬ì „ í’ˆì§ˆ í–¥ìƒ\")\n",
    "print(f\"   - ê¸°ì¡´: {len(get_default_korean_english_pairs())}ê°œ (ìˆ˜ë™)\")\n",
    "print(f\"   - LLM ê²€ì¦: {len(enhanced_bilingual_dict):,}ê°œ\")\n",
    "print(f\"   - ì¦ê°€ìœ¨: +{synonym_increase:.1f}%\")\n",
    "\n",
    "print(f\"\\n3ï¸âƒ£ ê²€ì¦ ì†ì‹¤ ê°œì„ \")\n",
    "print(f\"   - ê¸°ì¡´ ëª¨ë¸: {best_val_loss:.4f}\")\n",
    "print(f\"   - LLM í™•ì¥: {best_val_loss_v2:.4f}\")\n",
    "print(f\"   - ê°œì„ ìœ¨: {loss_improvement:.2f}%\")\n",
    "\n",
    "if best_val_loss_v2 < best_val_loss:\n",
    "    print(f\"\\nâœ… LLM í™•ì¥ ëª¨ë¸ì´ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•©ë‹ˆë‹¤!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  ê¸°ì¡´ ëª¨ë¸ê³¼ ë¹„ìŠ·í•˜ê±°ë‚˜ ë” ë§ì€ í•™ìŠµì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"   - ë” ë§ì€ í•©ì„± ë°ì´í„° ìƒì„±\")\n",
    "    print(f\"   - ì—í¬í¬ ìˆ˜ ì¦ê°€\")\n",
    "    print(f\"   - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê¶Œì¥\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ‰ ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
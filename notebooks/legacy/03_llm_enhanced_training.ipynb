{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. LLM Enhanced Training\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ LLM ìƒì„± ë°ì´í„°ë¡œ enhanced ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì „ì œì¡°ê±´\n",
    "ë¨¼ì € ë‹¤ìŒ ë…¸íŠ¸ë¶ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤:\n",
    "1. `01_neural_sparse_base_training.ipynb` - ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ\n",
    "2. `02_llm_synthetic_data_generation.ipynb` - LLM ë°ì´í„° ìƒì„±\n",
    "\n",
    "## ëª©í‘œ\n",
    "- ê¸°ë³¸ ë°ì´í„° + LLM ìƒì„± ë°ì´í„° ê²°í•©\n",
    "- Enhanced Neural Sparse ëª¨ë¸ í•™ìŠµ\n",
    "- Base vs Enhanced ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "## ì¶œë ¥ ë°ì´í„°\n",
    "ëª¨ë“  ë°ì´í„°ëŠ” `dataset/enhanced_model/` ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤:\n",
    "- `neural_sparse_v2_model/`: Enhanced ëª¨ë¸\n",
    "- `evaluation/performance_comparison.json`: ì„±ëŠ¥ ë¹„êµ ê²°ê³¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DatasetManager initialized\n",
      "ğŸ“ Base path: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset\n"
     ]
    }
   ],
   "source": [
    "# DatasetManager ì´ˆê¸°í™”\n",
    "from src.dataset_manager import DatasetManager\n",
    "\n",
    "dm = DatasetManager(base_path=\"dataset\")\n",
    "print(\"âœ… DatasetManager initialized\")\n",
    "print(f\"ğŸ“ Base path: {dm.base_path.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All dependencies satisfied\n",
      "\n",
      "âœ… Ready to proceed with enhanced training\n"
     ]
    }
   ],
   "source": [
    "# í•„ìˆ˜ ë°ì´í„° íŒŒì¼ í™•ì¸\n",
    "required_files = [\n",
    "    (\"base_model\", \"documents.json\"),\n",
    "    (\"base_model\", \"qd_pairs_base.pkl\"),\n",
    "    (\"base_model\", \"neural_sparse_v1_model\"),\n",
    "    (\"llm_generated\", \"synthetic_qd_pairs.pkl\"),\n",
    "    (\"llm_generated\", \"enhanced_synonyms.json\"),\n",
    "]\n",
    "\n",
    "if not dm.check_dependencies(required_files):\n",
    "    raise RuntimeError(\n",
    "        \"Required data files not found. \"\n",
    "        \"Please run notebooks 1 and 2 first.\"\n",
    "    )\n",
    "\n",
    "print(\"\\nâœ… Ready to proceed with enhanced training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from previous notebooks...\n",
      "\n",
      "âœ“ Loaded JSON: dataset/base_model/documents.json\n",
      "âœ“ Loaded Pickle: dataset/base_model/qd_pairs_base.pkl\n",
      "âœ“ Loaded Pickle: dataset/llm_generated/synthetic_qd_pairs.pkl\n",
      "âœ“ Loaded JSON: dataset/llm_generated/enhanced_synonyms.json\n",
      "\n",
      "âœ… Loaded 9996 documents\n",
      "âœ… Loaded 27939 base QD pairs\n",
      "âœ… Loaded 23 synthetic QD pairs\n",
      "âœ… Loaded enhanced synonyms with 32 entries\n"
     ]
    }
   ],
   "source": [
    "# ì´ì „ ë…¸íŠ¸ë¶ë“¤ì—ì„œ ìƒì„±ëœ ë°ì´í„° ë¡œë“œ\n",
    "print(\"Loading data from previous notebooks...\\n\")\n",
    "\n",
    "# From notebook 1\n",
    "documents = dm.load_json(\"documents.json\", \"base_model\")\n",
    "qd_pairs_base = dm.load_pickle(\"qd_pairs_base.pkl\", \"base_model\")\n",
    "\n",
    "# From notebook 2\n",
    "synthetic_qd_pairs = dm.load_pickle(\"synthetic_qd_pairs.pkl\", \"llm_generated\")\n",
    "enhanced_synonyms = dm.load_json(\"enhanced_synonyms.json\", \"llm_generated\")\n",
    "\n",
    "print(f\"\\nâœ… Loaded {len(documents)} documents\")\n",
    "print(f\"âœ… Loaded {len(qd_pairs_base)} base QD pairs\")\n",
    "print(f\"âœ… Loaded {len(synthetic_qd_pairs)} synthetic QD pairs\")\n",
    "print(f\"âœ… Loaded enhanced synonyms with {len(enhanced_synonyms)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model for comparison...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dataset/base_model/neural_sparse_v1_model and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Base model loaded for comparison\n",
      "   Device: cuda\n",
      "   Model type: BertModel\n"
     ]
    }
   ],
   "source": [
    "# Base ëª¨ë¸ ë¡œë“œ (ì„±ëŠ¥ ë¹„êµìš©)\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "print(\"Loading base model for comparison...\")\n",
    "\n",
    "model_path = dm.base_path / \"base_model\" / \"neural_sparse_v1_model\"\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(str(model_path))\n",
    "base_model = AutoModel.from_pretrained(str(model_path))\n",
    "\n",
    "# GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ ì´ë™\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model = base_model.to(device)\n",
    "base_model.eval()\n",
    "\n",
    "print(f\"âœ… Base model loaded for comparison\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Model type: {type(base_model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ†• 16. ìƒì„±ëœ í•©ì„± ë°ì´í„° ë¶„ì„\n",
    "\n",
    "LLMìœ¼ë¡œ ìƒì„±í•œ í•©ì„± ë°ì´í„°ì˜ í’ˆì§ˆê³¼ í†µê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ“Š ì„¹ì…˜ 16: ìƒì„±ëœ í•©ì„± ë°ì´í„° ë¶„ì„\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š ë°ì´í„° í†µê³„:\n",
      "   ê¸°ë³¸ QD pairs: 27,939ê°œ\n",
      "   í•©ì„± QD pairs: 23ê°œ\n",
      "   ì´ ë°ì´í„°: 27,962ê°œ\n",
      "   ì¦ê°€ìœ¨: +0.08%\n",
      "\n",
      "ğŸ“‹ í•©ì„± Query-Document Pairs ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ):\n",
      "======================================================================\n",
      "\n",
      "1. Query: - Last reactor\n",
      "   Document: Last Reactor At Ukraine's Zaporizhzhia Nuclear Plant Stopped...\n",
      "   Relevance: 1.0\n",
      "\n",
      "2. Query: - U.S. Hiring\n",
      "   Document: U.S. Hiring Slowed In August As Employers Add 315,000 Jobs...\n",
      "   Relevance: 1.0\n",
      "\n",
      "3. Query: - REI Workers\n",
      "   Document: REI Workers At Berkeley Store Vote To Unionize In Another Win For Labor...\n",
      "   Relevance: 1.0\n",
      "\n",
      "4. Query: - Trump Organization\n",
      "   Document: Trump Org. CFO To Plead Guilty, Testify Against Company...\n",
      "   Relevance: 1.0\n",
      "\n",
      "5. Query: - Over $2 billion\n",
      "   Document: Over $2 Billion Announced For Roads, Bridges, Bike Lanes Across U.S....\n",
      "   Relevance: 1.0\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š ë™ì˜ì–´ ì‚¬ì „ í†µê³„:\n",
      "   Enhanced synonyms: 32ê°œ í•­ëª©\n",
      "\n",
      "ìƒ˜í”Œ ë™ì˜ì–´:\n",
      "   1. ì‹ ê²½ë§ â†’ net\n",
      "   2. ë¬¸ì„œ â†’ doc\n",
      "   3. ë¶„ì„ â†’ analysis\n",
      "   4. ëª¨ë¸ â†’ model, Model\n",
      "   5. í•™ìŠµ â†’ learning, training, train\n",
      "\n",
      "======================================================================\n",
      "âœ… ë°ì´í„° ë¶„ì„ ì™„ë£Œ\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š ì„¹ì…˜ 16: ìƒì„±ëœ í•©ì„± ë°ì´í„° ë¶„ì„\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ê¸°ì¡´ ë°ì´í„° + í•©ì„± ë°ì´í„° í†µê³„\n",
    "print(f\"\\nğŸ“Š ë°ì´í„° í†µê³„:\")\n",
    "print(f\"   ê¸°ë³¸ QD pairs: {len(qd_pairs_base):,}ê°œ\")\n",
    "print(f\"   í•©ì„± QD pairs: {len(synthetic_qd_pairs):,}ê°œ\")\n",
    "print(f\"   ì´ ë°ì´í„°: {len(qd_pairs_base) + len(synthetic_qd_pairs):,}ê°œ\")\n",
    "\n",
    "if len(synthetic_qd_pairs) > 0:\n",
    "    increase_rate = len(synthetic_qd_pairs) / len(qd_pairs_base) * 100\n",
    "    print(f\"   ì¦ê°€ìœ¨: +{increase_rate:.2f}%\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  í•©ì„± ë°ì´í„°ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# í•©ì„± ë°ì´í„° ìƒ˜í”Œ í™•ì¸\n",
    "if len(synthetic_qd_pairs) > 0:\n",
    "    print(f\"\\nğŸ“‹ í•©ì„± Query-Document Pairs ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ):\")\n",
    "    print(\"=\"*70)\n",
    "    for i, (query, doc, relevance) in enumerate(synthetic_qd_pairs[:5], 1):\n",
    "        print(f\"\\n{i}. Query: {query}\")\n",
    "        print(f\"   Document: {doc[:100]}...\")\n",
    "        print(f\"   Relevance: {relevance}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# ë™ì˜ì–´ ì‚¬ì „ ë¹„êµ\n",
    "print(f\"\\nğŸ“Š ë™ì˜ì–´ ì‚¬ì „ í†µê³„:\")\n",
    "print(f\"   Enhanced synonyms: {len(enhanced_synonyms):,}ê°œ í•­ëª©\")\n",
    "print(f\"\\nìƒ˜í”Œ ë™ì˜ì–´:\")\n",
    "for i, (korean, english_list) in enumerate(list(enhanced_synonyms.items())[:5], 1):\n",
    "    print(f\"   {i}. {korean} â†’ {', '.join(english_list)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… ë°ì´í„° ë¶„ì„ ì™„ë£Œ\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ†• 17. ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ë° í–¥í›„ ê³„íš\n",
    "\n",
    "ìƒì„±ëœ ë°ì´í„°ì˜ í’ˆì§ˆì„ ë¶„ì„í•˜ê³  ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê³„íší•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ“Š ì„¹ì…˜ 17: ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ë° í–¥í›„ ê³„íš\n",
      "======================================================================\n",
      "\n",
      "1ï¸âƒ£ í•©ì„± ë°ì´í„° í’ˆì§ˆ ë¶„ì„\n",
      "----------------------------------------------------------------------\n",
      "   í‰ê·  ì¿¼ë¦¬ ê¸¸ì´: 28.7 ì\n",
      "   ìµœì†Œ ì¿¼ë¦¬ ê¸¸ì´: 10 ì\n",
      "   ìµœëŒ€ ì¿¼ë¦¬ ê¸¸ì´: 94 ì\n",
      "\n",
      "   í‰ê·  ë¬¸ì„œ ê¸¸ì´: 68.4 ì\n",
      "   ìµœì†Œ ë¬¸ì„œ ê¸¸ì´: 55 ì\n",
      "   ìµœëŒ€ ë¬¸ì„œ ê¸¸ì´: 85 ì\n",
      "\n",
      "   Relevance ì ìˆ˜ ë¶„í¬:\n",
      "     1.0: 23ê°œ (100.0%)\n",
      "\n",
      "2ï¸âƒ£ ë™ì˜ì–´ ì‚¬ì „ ë¶„ì„\n",
      "----------------------------------------------------------------------\n",
      "   LLM ê²€ì¦ ë™ì˜ì–´: 32ê°œ í•­ëª©\n",
      "\n",
      "3ï¸âƒ£ í–¥í›„ ê°œì„  ê³„íš\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "   ğŸ“Œ ë°ì´í„° í™•ì¥:\n",
      "      - í˜„ì¬: 23ê°œ í•©ì„± pairs (í…ŒìŠ¤íŠ¸)\n",
      "      - ëª©í‘œ: 1,000+ í•©ì„± pairs (ë…¸íŠ¸ë¶ 2ì—ì„œ max_documents ì¦ê°€)\n",
      "\n",
      "   ğŸ“Œ ëª¨ë¸ ì¬í•™ìŠµ:\n",
      "      - ì¶©ë¶„í•œ í•©ì„± ë°ì´í„° ìƒì„± í›„ ì§„í–‰\n",
      "      - ê¸°ë³¸ ëª¨ë¸ + í•©ì„± ë°ì´í„°ë¡œ enhanced ëª¨ë¸ í•™ìŠµ\n",
      "      - ì„±ëŠ¥ ë¹„êµ ë° í‰ê°€\n",
      "\n",
      "   ğŸ“Œ ë°°í¬:\n",
      "      - Enhanced ëª¨ë¸ì„ OpenSearchì— ë°°í¬\n",
      "      - ì‹¤ì œ ê²€ìƒ‰ ì„œë¹„ìŠ¤ì—ì„œ ì„±ëŠ¥ ê²€ì¦\n",
      "\n",
      "======================================================================\n",
      "âœ… ë¶„ì„ ì™„ë£Œ\n",
      "======================================================================\n",
      "\n",
      "ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:\n",
      "   1. ë…¸íŠ¸ë¶ 2ë¡œ ëŒì•„ê°€ì„œ max_documentsë¥¼ 1000ìœ¼ë¡œ ì¦ê°€\n",
      "   2. ì¶©ë¶„í•œ í•©ì„± ë°ì´í„° ìƒì„±\n",
      "   3. ì´ ë…¸íŠ¸ë¶ìœ¼ë¡œ ëŒì•„ì™€ì„œ enhanced ëª¨ë¸ í•™ìŠµ\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š ì„¹ì…˜ 17: ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ë° í–¥í›„ ê³„íš\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ìƒì„±ëœ ì¿¼ë¦¬ í’ˆì§ˆ ë¶„ì„\n",
    "print(\"\\n1ï¸âƒ£ í•©ì„± ë°ì´í„° í’ˆì§ˆ ë¶„ì„\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "if len(synthetic_qd_pairs) > 0:\n",
    "    # ì¿¼ë¦¬ ê¸¸ì´ ë¶„ì„\n",
    "    query_lengths = [len(q) for q, d, r in synthetic_qd_pairs]\n",
    "    avg_query_length = sum(query_lengths) / len(query_lengths)\n",
    "    \n",
    "    print(f\"   í‰ê·  ì¿¼ë¦¬ ê¸¸ì´: {avg_query_length:.1f} ì\")\n",
    "    print(f\"   ìµœì†Œ ì¿¼ë¦¬ ê¸¸ì´: {min(query_lengths)} ì\")\n",
    "    print(f\"   ìµœëŒ€ ì¿¼ë¦¬ ê¸¸ì´: {max(query_lengths)} ì\")\n",
    "    \n",
    "    # ë¬¸ì„œ ê¸¸ì´ ë¶„ì„\n",
    "    doc_lengths = [len(d) for q, d, r in synthetic_qd_pairs]\n",
    "    avg_doc_length = sum(doc_lengths) / len(doc_lengths)\n",
    "    \n",
    "    print(f\"\\n   í‰ê·  ë¬¸ì„œ ê¸¸ì´: {avg_doc_length:.1f} ì\")\n",
    "    print(f\"   ìµœì†Œ ë¬¸ì„œ ê¸¸ì´: {min(doc_lengths)} ì\")\n",
    "    print(f\"   ìµœëŒ€ ë¬¸ì„œ ê¸¸ì´: {max(doc_lengths)} ì\")\n",
    "    \n",
    "    # Relevance ë¶„í¬\n",
    "    relevance_scores = [r for q, d, r in synthetic_qd_pairs]\n",
    "    unique_scores = set(relevance_scores)\n",
    "    \n",
    "    print(f\"\\n   Relevance ì ìˆ˜ ë¶„í¬:\")\n",
    "    for score in sorted(unique_scores):\n",
    "        count = relevance_scores.count(score)\n",
    "        percentage = count / len(relevance_scores) * 100\n",
    "        print(f\"     {score}: {count}ê°œ ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"   âš ï¸  í•©ì„± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ë™ì˜ì–´ ì‚¬ì „ ê°œì„  ë¶„ì„\n",
    "print(f\"\\n2ï¸âƒ£ ë™ì˜ì–´ ì‚¬ì „ ë¶„ì„\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"   LLM ê²€ì¦ ë™ì˜ì–´: {len(enhanced_synonyms):,}ê°œ í•­ëª©\")\n",
    "\n",
    "# í–¥í›„ ê°œì„  ê³„íš\n",
    "print(f\"\\n3ï¸âƒ£ í–¥í›„ ê°œì„  ê³„íš\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"\"\"\n",
    "   ğŸ“Œ ë°ì´í„° í™•ì¥:\n",
    "      - í˜„ì¬: {len(synthetic_qd_pairs)}ê°œ í•©ì„± pairs (í…ŒìŠ¤íŠ¸)\n",
    "      - ëª©í‘œ: 1,000+ í•©ì„± pairs (ë…¸íŠ¸ë¶ 2ì—ì„œ max_documents ì¦ê°€)\n",
    "   \n",
    "   ğŸ“Œ ëª¨ë¸ ì¬í•™ìŠµ:\n",
    "      - ì¶©ë¶„í•œ í•©ì„± ë°ì´í„° ìƒì„± í›„ ì§„í–‰\n",
    "      - ê¸°ë³¸ ëª¨ë¸ + í•©ì„± ë°ì´í„°ë¡œ enhanced ëª¨ë¸ í•™ìŠµ\n",
    "      - ì„±ëŠ¥ ë¹„êµ ë° í‰ê°€\n",
    "   \n",
    "   ğŸ“Œ ë°°í¬:\n",
    "      - Enhanced ëª¨ë¸ì„ OpenSearchì— ë°°í¬\n",
    "      - ì‹¤ì œ ê²€ìƒ‰ ì„œë¹„ìŠ¤ì—ì„œ ì„±ëŠ¥ ê²€ì¦\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"âœ… ë¶„ì„ ì™„ë£Œ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(\"   1. ë…¸íŠ¸ë¶ 2ë¡œ ëŒì•„ê°€ì„œ max_documentsë¥¼ 1000ìœ¼ë¡œ ì¦ê°€\")\n",
    "print(\"   2. ì¶©ë¶„í•œ í•©ì„± ë°ì´í„° ìƒì„±\")\n",
    "print(\"   3. ì´ ë…¸íŠ¸ë¶ìœ¼ë¡œ ëŒì•„ì™€ì„œ enhanced ëª¨ë¸ í•™ìŠµ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ë°ì´í„° ì €ì¥ (ë¶„ì„ ê²°ê³¼)\n",
    "\n",
    "ìƒì„±ëœ ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ `dataset/llm_generated/` ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved JSON: dataset/llm_generated/data_analysis.json\n",
      "âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ\n",
      "   ìœ„ì¹˜: dataset/llm_generated/data_analysis.json\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ë¶„ì„ ê²°ê³¼ ì €ì¥\n",
    "import json\n",
    "\n",
    "analysis_results = {\n",
    "    \"base_qd_pairs_count\": len(qd_pairs_base),\n",
    "    \"synthetic_qd_pairs_count\": len(synthetic_qd_pairs),\n",
    "    \"enhanced_synonyms_count\": len(enhanced_synonyms),\n",
    "    \"total_documents\": len(documents),\n",
    "}\n",
    "\n",
    "if len(synthetic_qd_pairs) > 0:\n",
    "    query_lengths = [len(q) for q, d, r in synthetic_qd_pairs]\n",
    "    analysis_results[\"synthetic_data_quality\"] = {\n",
    "        \"avg_query_length\": sum(query_lengths) / len(query_lengths),\n",
    "        \"min_query_length\": min(query_lengths),\n",
    "        \"max_query_length\": max(query_lengths),\n",
    "    }\n",
    "\n",
    "dm.save_json(\n",
    "    analysis_results,\n",
    "    \"data_analysis.json\",\n",
    "    \"llm_generated\"\n",
    ")\n",
    "\n",
    "print(\"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ\")\n",
    "print(f\"   ìœ„ì¹˜: dataset/llm_generated/data_analysis.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved Pickle: dataset/llm_generated/combined_qd_pairs.pkl\n",
      "âœ… ê²°í•©ëœ ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ\n",
      "   ìœ„ì¹˜: dataset/llm_generated/combined_qd_pairs.pkl\n",
      "   ì´ ê°œìˆ˜: 27,962ê°œ pairs\n"
     ]
    }
   ],
   "source": [
    "# ê²°í•©ëœ ë°ì´í„°ì…‹ ì €ì¥ (í–¥í›„ í•™ìŠµìš©)\n",
    "combined_qd_pairs = qd_pairs_base + synthetic_qd_pairs\n",
    "\n",
    "dm.save_pickle(\n",
    "    combined_qd_pairs,\n",
    "    \"combined_qd_pairs.pkl\",\n",
    "    \"llm_generated\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… ê²°í•©ëœ ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ\")\n",
    "print(f\"   ìœ„ì¹˜: dataset/llm_generated/combined_qd_pairs.pkl\")\n",
    "print(f\"   ì´ ê°œìˆ˜: {len(combined_qd_pairs):,}ê°œ pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ“Š Dataset Summary\n",
      "======================================================================\n",
      "Base path: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset\n",
      "Total datasets: 9\n",
      "\n",
      "Datasets by directory:\n",
      "\n",
      "  ğŸ“ base_model/\n",
      "     - documents.json                           (           json,    0.7 MB)\n",
      "     - idf_statistics.pkl                       (         pickle,    0.3 MB)\n",
      "     - qd_pairs_base.pkl                        (         pickle,   54.9 MB)\n",
      "     - bilingual_synonyms.json                  (           json,    0.0 MB)\n",
      "     - neural_sparse_v1_model                   (  pytorch_model,  423.1 MB)\n",
      "     Total:                                                     479.1 MB\n",
      "\n",
      "  ğŸ“ llm_generated/\n",
      "     - synthetic_qd_pairs.pkl                   (         pickle,    0.0 MB)\n",
      "     - enhanced_synonyms.json                   (           json,    0.0 MB)\n",
      "     - data_analysis.json                       (           json,    0.0 MB)\n",
      "     - combined_qd_pairs.pkl                    (         pickle,   54.9 MB)\n",
      "     Total:                                                      54.9 MB\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ì „ì²´ ë°ì´í„°ì…‹ ìš”ì•½\n",
    "dm.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Notebook 3 ì™„ë£Œ - ë°ì´í„° ë¶„ì„\n",
    "\n",
    "LLMìœ¼ë¡œ ìƒì„±í•œ í•©ì„± ë°ì´í„°ì˜ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ì™„ë£Œëœ ì‘ì—…\n",
    "1. âœ… Base ëª¨ë¸ ë°ì´í„° ë¡œë“œ (Notebook 1)\n",
    "2. âœ… LLM í•©ì„± ë°ì´í„° ë¡œë“œ (Notebook 2)\n",
    "3. âœ… ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ë° í†µê³„\n",
    "\n",
    "### ì €ì¥ëœ ë°ì´í„°\n",
    "- Combined Dataset: `dataset/llm_generated/combined_qd_pairs.pkl`\n",
    "- Analysis Results: `dataset/llm_generated/data_analysis.json`\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„ (í–¥í›„ ì‘ì—…)\n",
    "1. ë…¸íŠ¸ë¶ 2ì—ì„œ max_documentsë¥¼ ëŠ˜ë ¤ ë” ë§ì€ í•©ì„± ë°ì´í„° ìƒì„±\n",
    "2. ì¶©ë¶„í•œ ë°ì´í„°ê°€ ëª¨ì´ë©´ Enhanced ëª¨ë¸ ì¬í•™ìŠµ ì§„í–‰\n",
    "3. ì„±ëŠ¥ ë¹„êµ ë° OpenSearch ë°°í¬\n",
    "\n",
    "### í˜„ì¬ ìƒíƒœ\n",
    "- í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì†ŒëŸ‰ì˜ í•©ì„± ë°ì´í„°ë§Œ ìƒì„±ë¨\n",
    "- ì‹¤ì œ ëª¨ë¸ ì¬í•™ìŠµì„ ìœ„í•´ì„œëŠ” ë” ë§ì€ ë°ì´í„° í•„ìš”"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

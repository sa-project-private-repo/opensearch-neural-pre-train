{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenSearch Korean Neural Sparse Model - Inference Test\n",
    "\n",
    "í•™ìŠµëœ í•œêµ­ì–´ Neural Sparse ëª¨ë¸ì„ ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "## í…ŒìŠ¤íŠ¸ í•­ëª©\n",
    "1. ëª¨ë¸ ë¡œë“œ\n",
    "2. ë¬¸ì„œ ì¸ì½”ë”© (BERT ê¸°ë°˜)\n",
    "3. ì¿¼ë¦¬ ì¸ì½”ë”© (IDF lookup - Inference-Free)\n",
    "4. ìœ ì‚¬ë„ ê³„ì‚° ë° ê²€ìƒ‰\n",
    "5. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path for src imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForMaskedLM\n",
    "\n",
    "# GPU/CPU í™•ì¸\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ëª¨ë¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenSearchDocEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    OpenSearch Neural Sparse Document Encoder (Doc-only mode)\n",
    "    \n",
    "    ë¬¸ì„œë¥¼ sparse vectorë¡œ ì¸ì½”ë”©í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"klue/bert-base\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # BERT ê¸°ë°˜ ì¸ì½”ë”\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.bert = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        \n",
    "        self.vocab_size = self.config.vocab_size\n",
    "        \n",
    "        # Log saturation activation: log(1 + ReLU(x))\n",
    "        self.activation = lambda x: torch.log1p(torch.relu(x))\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, return_dict=False):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            input_ids: (batch_size, seq_len)\n",
    "            attention_mask: (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            sparse_vector: (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        # BERT MLM head output\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Logits: (batch_size, seq_len, vocab_size)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Apply activation: log(1 + ReLU(logits))\n",
    "        activated = self.activation(logits)\n",
    "        \n",
    "        # Max pooling over sequence length\n",
    "        # (batch_size, seq_len, vocab_size) â†’ (batch_size, vocab_size)\n",
    "        sparse_vector = torch.max(\n",
    "            activated * attention_mask.unsqueeze(-1),\n",
    "            dim=1\n",
    "        ).values\n",
    "        \n",
    "        if return_dict:\n",
    "            return {'output': sparse_vector}\n",
    "        \n",
    "        return sparse_vector\n",
    "\n",
    "print(\"âœ“ ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
    "MODEL_DIR = \"./models/opensearch-korean-neural-sparse-v1\"\n",
    "\n",
    "# íŒŒì¼ ì¡´ì¬ í™•ì¸\n",
    "required_files = [\n",
    "    \"pytorch_model.bin\",\n",
    "    \"idf.json\",\n",
    "    \"config.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"vocab.txt\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“¦ ëª¨ë¸ íŒŒì¼ í™•ì¸...\")\n",
    "for filename in required_files:\n",
    "    filepath = os.path.join(MODEL_DIR, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        size = os.path.getsize(filepath) / 1024\n",
    "        print(f\"  âœ“ {filename:25s} ({size:>10.2f} KB)\")\n",
    "    else:\n",
    "        print(f\"  âœ— {filename:25s} [NOT FOUND]\")\n",
    "        raise FileNotFoundError(f\"Required file not found: {filename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Config ë¡œë“œ\n",
    "with open(os.path.join(MODEL_DIR, \"config.json\"), 'r', encoding='utf-8') as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "print(\"\\nâœ“ Config ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  ëª¨ë¸ íƒ€ì…: {model_config['model_type']}\")\n",
    "print(f\"  ë² ì´ìŠ¤ ëª¨ë¸: {model_config['base_model']}\")\n",
    "print(f\"  Vocab size: {model_config['vocab_size']:,}\")\n",
    "print(f\"  Max length: {model_config['max_seq_length']}\")\n",
    "\n",
    "# 2. Tokenizer ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "print(f\"\\nâœ“ Tokenizer ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "# 3. IDF ë”•ì…”ë„ˆë¦¬ ë¡œë“œ\n",
    "with open(os.path.join(MODEL_DIR, \"idf.json\"), 'r', encoding='utf-8') as f:\n",
    "    idf_dict = json.load(f)\n",
    "\n",
    "print(f\"\\nâœ“ IDF ë”•ì…”ë„ˆë¦¬ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  í† í° ìˆ˜: {len(idf_dict):,}\")\n",
    "print(f\"  í‰ê·  IDF: {np.mean(list(idf_dict.values())):.4f}\")\n",
    "\n",
    "# 4. Document Encoder ëª¨ë¸ ë¡œë“œ\n",
    "base_model = model_config['base_model']\n",
    "doc_encoder = OpenSearchDocEncoder(base_model)\n",
    "\n",
    "# í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "state_dict = torch.load(\n",
    "    os.path.join(MODEL_DIR, \"pytorch_model.bin\"),\n",
    "    map_location=device\n",
    ")\n",
    "doc_encoder.load_state_dict(state_dict)\n",
    "doc_encoder = doc_encoder.to(device)\n",
    "doc_encoder.eval()  # Evaluation mode\n",
    "\n",
    "print(f\"\\nâœ“ Document Encoder ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in doc_encoder.parameters()):,}\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_document(text, model, tokenizer, device, max_length=128):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œë¥¼ sparse vectorë¡œ ì¸ì½”ë”© (ëª¨ë¸ ì‚¬ìš© - ëŠë¦¼)\n",
    "    \n",
    "    Args:\n",
    "        text: ë¬¸ì„œ í…ìŠ¤íŠ¸\n",
    "        model: Document encoder ëª¨ë¸\n",
    "        tokenizer: Tokenizer\n",
    "        device: ë””ë°”ì´ìŠ¤\n",
    "        max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "    \n",
    "    Returns:\n",
    "        sparse_vector: (vocab_size,) numpy array\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # í† í°í™”\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        sparse_vec = model(input_ids, attention_mask)\n",
    "    \n",
    "    return sparse_vec.cpu().numpy()[0]\n",
    "\n",
    "\n",
    "def encode_query_inference_free(text, tokenizer, idf_dict, max_length=128):\n",
    "    \"\"\"\n",
    "    ì¿¼ë¦¬ë¥¼ sparse vectorë¡œ ì¸ì½”ë”© (IDF lookup - Inference-Free! ë§¤ìš° ë¹ ë¦„)\n",
    "    \n",
    "    Args:\n",
    "        text: ì¿¼ë¦¬ í…ìŠ¤íŠ¸\n",
    "        tokenizer: Tokenizer\n",
    "        idf_dict: IDF ë”•ì…”ë„ˆë¦¬\n",
    "        max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "    \n",
    "    Returns:\n",
    "        sparse_vector: (vocab_size,) numpy array\n",
    "    \"\"\"\n",
    "    # í† í°í™”\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        add_special_tokens=False,\n",
    "        max_length=max_length,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # IDF lookup\n",
    "    sparse_vec = np.zeros(tokenizer.vocab_size)\n",
    "    for token_id in tokens:\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        if token_str in idf_dict:\n",
    "            sparse_vec[token_id] = idf_dict[token_str]\n",
    "    \n",
    "    return sparse_vec\n",
    "\n",
    "\n",
    "def get_top_tokens(sparse_vec, tokenizer, top_k=15):\n",
    "    \"\"\"\n",
    "    Sparse vectorì—ì„œ ìƒìœ„ í† í° ì¶”ì¶œ\n",
    "    \n",
    "    Args:\n",
    "        sparse_vec: (vocab_size,) numpy array\n",
    "        tokenizer: Tokenizer\n",
    "        top_k: ìƒìœ„ kê°œ\n",
    "    \n",
    "    Returns:\n",
    "        List of (token, value) tuples\n",
    "    \"\"\"\n",
    "    top_indices = np.argsort(sparse_vec)[-top_k:][::-1]\n",
    "    top_values = sparse_vec[top_indices]\n",
    "    \n",
    "    top_tokens = []\n",
    "    for idx, val in zip(top_indices, top_values):\n",
    "        if val > 0:\n",
    "            token = tokenizer.decode([idx])\n",
    "            top_tokens.append((token, val))\n",
    "    \n",
    "    return top_tokens\n",
    "\n",
    "\n",
    "def compute_similarity(query_vec, doc_vec):\n",
    "    \"\"\"\n",
    "    ì¿¼ë¦¬ì™€ ë¬¸ì„œì˜ ìœ ì‚¬ë„ ê³„ì‚° (Dot product)\n",
    "    \n",
    "    Args:\n",
    "        query_vec: (vocab_size,) numpy array\n",
    "        doc_vec: (vocab_size,) numpy array\n",
    "    \n",
    "    Returns:\n",
    "        similarity score (float)\n",
    "    \"\"\"\n",
    "    return np.dot(query_vec, doc_vec)\n",
    "\n",
    "\n",
    "print(\"âœ“ Inference í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë¬¸ì„œ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë¬¸ì„œ\n",
    "test_documents = [\n",
    "    \"OpenSearchëŠ” ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ê³¼ neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.\",\n",
    "    \"í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤. KoNLPyì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.\",\n",
    "    \"ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ ë°ì´í„°ì™€ ì»´í“¨íŒ… ìì›ì´ í•„ìš”í•©ë‹ˆë‹¤. GPUë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµ ì†ë„ê°€ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤.\",\n",
    "    \"ChatGPTëŠ” OpenAIê°€ ê°œë°œí•œ ëŒ€í™”í˜• AI ëª¨ë¸ì…ë‹ˆë‹¤. GPT ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©° ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ë²¡í„° ê²€ìƒ‰ì€ ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤. Dense retrieval ë°©ì‹ì´ë¼ê³ ë„ ë¶ˆë¦½ë‹ˆë‹¤.\",\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“„ ë¬¸ì„œ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ (Model Inference)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "doc_vectors = []\n",
    "encoding_times = []\n",
    "\n",
    "for i, doc in enumerate(test_documents, 1):\n",
    "    print(f\"\\n[ë¬¸ì„œ {i}]\")\n",
    "    print(f\"í…ìŠ¤íŠ¸: {doc[:60]}...\")\n",
    "    \n",
    "    # ì¸ì½”ë”© ì‹œê°„ ì¸¡ì •\n",
    "    start_time = time.time()\n",
    "    sparse_vec = encode_document(doc, doc_encoder, tokenizer, device)\n",
    "    elapsed = time.time() - start_time\n",
    "    encoding_times.append(elapsed)\n",
    "    \n",
    "    doc_vectors.append(sparse_vec)\n",
    "    \n",
    "    # í†µê³„\n",
    "    non_zero = np.count_nonzero(sparse_vec)\n",
    "    sparsity = (1 - non_zero / len(sparse_vec)) * 100\n",
    "    l1_norm = np.sum(np.abs(sparse_vec))\n",
    "    \n",
    "    print(f\"  í¬ì†Œì„±: {sparsity:.2f}% (non-zero: {non_zero:,}/{len(sparse_vec):,})\")\n",
    "    print(f\"  L1 Norm: {l1_norm:.2f}\")\n",
    "    print(f\"  ì¸ì½”ë”© ì‹œê°„: {elapsed*1000:.2f}ms\")\n",
    "    \n",
    "    # ìƒìœ„ í† í°\n",
    "    print(f\"  ìƒìœ„ í† í°:\")\n",
    "    top_tokens = get_top_tokens(sparse_vec, tokenizer, top_k=10)\n",
    "    for j, (token, value) in enumerate(top_tokens, 1):\n",
    "        print(f\"    {j:2d}. {token:15s} ({value:.4f})\")\n",
    "\n",
    "print(f\"\\ní‰ê·  ì¸ì½”ë”© ì‹œê°„: {np.mean(encoding_times)*1000:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì¿¼ë¦¬ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ (Inference-Free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬\n",
    "test_queries = [\n",
    "    \"OpenSearch ë²¡í„° ê²€ìƒ‰\",\n",
    "    \"í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ \",\n",
    "    \"ë”¥ëŸ¬ë‹ GPU í•™ìŠµ\",\n",
    "    \"ChatGPT LLM ëª¨ë¸\",\n",
    "    \"ê²€ìƒ‰ ì—”ì§„ ìµœì í™”\",\n",
    "    \"ì¸ê³µì§€ëŠ¥ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§\",\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“ ì¿¼ë¦¬ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ (Inference-Free: Tokenizer + IDF Lookup)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "query_vectors = []\n",
    "query_encoding_times = []\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n[ì¿¼ë¦¬ {i}] {query}\")\n",
    "    \n",
    "    # ì¸ì½”ë”© ì‹œê°„ ì¸¡ì •\n",
    "    start_time = time.time()\n",
    "    sparse_vec = encode_query_inference_free(query, tokenizer, idf_dict)\n",
    "    elapsed = time.time() - start_time\n",
    "    query_encoding_times.append(elapsed)\n",
    "    \n",
    "    query_vectors.append(sparse_vec)\n",
    "    \n",
    "    # í†µê³„\n",
    "    non_zero = np.count_nonzero(sparse_vec)\n",
    "    sparsity = (1 - non_zero / len(sparse_vec)) * 100\n",
    "    \n",
    "    print(f\"  í¬ì†Œì„±: {sparsity:.2f}% (non-zero: {non_zero}/{len(sparse_vec):,})\")\n",
    "    print(f\"  ì¸ì½”ë”© ì‹œê°„: {elapsed*1000:.4f}ms âš¡ (ë§¤ìš° ë¹ ë¦„!)\")\n",
    "    \n",
    "    # ìƒìœ„ í† í°\n",
    "    print(f\"  ìƒìœ„ í† í°:\")\n",
    "    top_tokens = get_top_tokens(sparse_vec, tokenizer, top_k=10)\n",
    "    for j, (token, value) in enumerate(top_tokens, 1):\n",
    "        print(f\"    {j:2d}. {token:15s} ({value:.4f})\")\n",
    "\n",
    "print(f\"\\ní‰ê·  ì¿¼ë¦¬ ì¸ì½”ë”© ì‹œê°„: {np.mean(query_encoding_times)*1000:.4f}ms\")\n",
    "print(f\"\\nğŸ’¡ ë¬¸ì„œ ì¸ì½”ë”© ëŒ€ë¹„ ì†ë„: {np.mean(encoding_times)/np.mean(query_encoding_times):.1f}ë°° ë¹ ë¦„!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (ìœ ì‚¬ë„ ê³„ì‚°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (Query-Document Similarity)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ê° ì¿¼ë¦¬ì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ì¿¼ë¦¬: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    query_vec = query_vectors[i]\n",
    "    \n",
    "    # ëª¨ë“  ë¬¸ì„œì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    similarities = []\n",
    "    for j, doc_vec in enumerate(doc_vectors):\n",
    "        sim = compute_similarity(query_vec, doc_vec)\n",
    "        similarities.append((j, sim))\n",
    "    \n",
    "    # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # ìƒìœ„ 3ê°œ ë¬¸ì„œ ì¶œë ¥\n",
    "    print(\"\\nê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 3ê°œ):\")\n",
    "    for rank, (doc_idx, score) in enumerate(similarities[:3], 1):\n",
    "        print(f\"\\n  [{rank}ìœ„] ìœ ì‚¬ë„: {score:.4f}\")\n",
    "        print(f\"       ë¬¸ì„œ: {test_documents[doc_idx][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"âš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ëŒ€ëŸ‰ ì¿¼ë¦¬ ì²˜ë¦¬ ì†ë„ í…ŒìŠ¤íŠ¸\n",
    "num_queries = 100\n",
    "test_query_text = \"OpenSearch ë²¡í„° ê²€ìƒ‰ ìµœì í™”\"\n",
    "\n",
    "print(f\"\\ní…ŒìŠ¤íŠ¸: {num_queries}ê°œ ì¿¼ë¦¬ ì¸ì½”ë”© (Inference-Free)\")\n",
    "start_time = time.time()\n",
    "for _ in range(num_queries):\n",
    "    _ = encode_query_inference_free(test_query_text, tokenizer, idf_dict)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"ì´ ì‹œê°„: {elapsed:.4f}s\")\n",
    "print(f\"í‰ê·  ì‹œê°„: {elapsed/num_queries*1000:.4f}ms per query\")\n",
    "print(f\"ì²˜ë¦¬ëŸ‰: {num_queries/elapsed:.2f} queries/sec\")\n",
    "\n",
    "# ë¬¸ì„œ ì¸ì½”ë”© ì†ë„ í…ŒìŠ¤íŠ¸\n",
    "num_docs = 10\n",
    "test_doc_text = \"OpenSearchëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ì—”ì§„ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "print(f\"\\ní…ŒìŠ¤íŠ¸: {num_docs}ê°œ ë¬¸ì„œ ì¸ì½”ë”© (Model Inference)\")\n",
    "start_time = time.time()\n",
    "for _ in range(num_docs):\n",
    "    _ = encode_document(test_doc_text, doc_encoder, tokenizer, device)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"ì´ ì‹œê°„: {elapsed:.4f}s\")\n",
    "print(f\"í‰ê·  ì‹œê°„: {elapsed/num_docs*1000:.2f}ms per document\")\n",
    "print(f\"ì²˜ë¦¬ëŸ‰: {num_docs/elapsed:.2f} documents/sec\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ’¡ ìš”ì•½\")\n",
    "print(\"=\"*60)\n",
    "print(\"âœ“ ì¿¼ë¦¬ ì¸ì½”ë”©: ë§¤ìš° ë¹ ë¦„ (IDF lookupë§Œ ì‚¬ìš©)\")\n",
    "print(\"âœ“ ë¬¸ì„œ ì¸ì½”ë”©: ëŠë¦¼ (BERT ëª¨ë¸ ì‚¬ìš©)\")\n",
    "print(\"âœ“ ì¿¼ë¦¬ëŠ” ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥, ë¬¸ì„œëŠ” ì‚¬ì „ ì¸ë±ì‹± í•„ìš”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ìš©ì ì •ì˜ ì¿¼ë¦¬ì™€ ë¬¸ì„œë¡œ í…ŒìŠ¤íŠ¸\n",
    "custom_query = \"ì¸ê³µì§€ëŠ¥ LLM ëª¨ë¸ ìµœì í™” ë°©ë²•\"  # ì—¬ê¸°ì— í…ŒìŠ¤íŠ¸í•  ì¿¼ë¦¬ ì…ë ¥\n",
    "\n",
    "custom_documents = [\n",
    "    \"LLM ëª¨ë¸ ìµœì í™”ë¥¼ ìœ„í•´ì„œëŠ” ì–‘ìí™”, í”„ë£¨ë‹, ì§€ì‹ ì¦ë¥˜ ë“±ì˜ ê¸°ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"í•œêµ­ì–´ ìŒì‹ ì¤‘ì—ì„œ ê¹€ì¹˜ì°Œê°œëŠ” ê°€ì¥ ì¸ê¸° ìˆëŠ” ë©”ë‰´ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\",\n",
    "    \"ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì¶”ë¡  ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ KV ìºì‹œì™€ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í™œìš©í•©ë‹ˆë‹¤.\",\n",
    "]  # ì—¬ê¸°ì— í…ŒìŠ¤íŠ¸í•  ë¬¸ì„œ ì…ë ¥\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ§ª ì»¤ìŠ¤í…€ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nì¿¼ë¦¬: {custom_query}\")\n",
    "\n",
    "# ì¿¼ë¦¬ ì¸ì½”ë”©\n",
    "query_vec = encode_query_inference_free(custom_query, tokenizer, idf_dict)\n",
    "\n",
    "# ë¬¸ì„œ ì¸ì½”ë”© ë° ìœ ì‚¬ë„ ê³„ì‚°\n",
    "results = []\n",
    "for i, doc in enumerate(custom_documents):\n",
    "    doc_vec = encode_document(doc, doc_encoder, tokenizer, device)\n",
    "    similarity = compute_similarity(query_vec, doc_vec)\n",
    "    results.append((i, similarity, doc))\n",
    "\n",
    "# ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nê²€ìƒ‰ ê²°ê³¼:\")\n",
    "for rank, (doc_idx, score, doc_text) in enumerate(results, 1):\n",
    "    print(f\"\\n[{rank}ìœ„] ìœ ì‚¬ë„: {score:.4f}\")\n",
    "    print(f\"     ë¬¸ì„œ: {doc_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. í¬ì†Œì„±(Sparsity) ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š í¬ì†Œì„± ë¶„ì„\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ë¬¸ì„œ ë²¡í„° í¬ì†Œì„±\n",
    "doc_sparsities = []\n",
    "for vec in doc_vectors:\n",
    "    non_zero = np.count_nonzero(vec)\n",
    "    sparsity = (1 - non_zero / len(vec)) * 100\n",
    "    doc_sparsities.append(sparsity)\n",
    "\n",
    "print(f\"\\në¬¸ì„œ ë²¡í„° í¬ì†Œì„±:\")\n",
    "print(f\"  í‰ê· : {np.mean(doc_sparsities):.2f}%\")\n",
    "print(f\"  ìµœì†Œ: {np.min(doc_sparsities):.2f}%\")\n",
    "print(f\"  ìµœëŒ€: {np.max(doc_sparsities):.2f}%\")\n",
    "print(f\"  í‘œì¤€í¸ì°¨: {np.std(doc_sparsities):.2f}%\")\n",
    "\n",
    "# ì¿¼ë¦¬ ë²¡í„° í¬ì†Œì„±\n",
    "query_sparsities = []\n",
    "for vec in query_vectors:\n",
    "    non_zero = np.count_nonzero(vec)\n",
    "    sparsity = (1 - non_zero / len(vec)) * 100\n",
    "    query_sparsities.append(sparsity)\n",
    "\n",
    "print(f\"\\nì¿¼ë¦¬ ë²¡í„° í¬ì†Œì„±:\")\n",
    "print(f\"  í‰ê· : {np.mean(query_sparsities):.2f}%\")\n",
    "print(f\"  ìµœì†Œ: {np.min(query_sparsities):.2f}%\")\n",
    "print(f\"  ìµœëŒ€: {np.max(query_sparsities):.2f}%\")\n",
    "print(f\"  í‘œì¤€í¸ì°¨: {np.std(query_sparsities):.2f}%\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ í¬ì†Œì„±ì´ ë†’ì„ìˆ˜ë¡ (100%ì— ê°€ê¹Œìš¸ìˆ˜ë¡):\")\n",
    "print(f\"   - ë©”ëª¨ë¦¬ íš¨ìœ¨ì \")\n",
    "print(f\"   - ê²€ìƒ‰ ì†ë„ ë¹ ë¦„\")\n",
    "print(f\"   - ì €ì¥ ê³µê°„ ì ìŒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ê²°ê³¼ ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"âœ… OpenSearch Korean Neural Sparse Model - Inference Test ì™„ë£Œ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nğŸ“Š í…ŒìŠ¤íŠ¸ ìš”ì•½:\")\n",
    "print(f\"  â€¢ ë¬¸ì„œ ì¸ì½”ë”© í‰ê·  ì‹œê°„: {np.mean(encoding_times)*1000:.2f}ms\")\n",
    "print(f\"  â€¢ ì¿¼ë¦¬ ì¸ì½”ë”© í‰ê·  ì‹œê°„: {np.mean(query_encoding_times)*1000:.4f}ms\")\n",
    "print(f\"  â€¢ ì†ë„ ì°¨ì´: {np.mean(encoding_times)/np.mean(query_encoding_times):.1f}ë°°\")\n",
    "print(f\"  â€¢ ë¬¸ì„œ ë²¡í„° í‰ê·  í¬ì†Œì„±: {np.mean(doc_sparsities):.2f}%\")\n",
    "print(f\"  â€¢ ì¿¼ë¦¬ ë²¡í„° í‰ê·  í¬ì†Œì„±: {np.mean(query_sparsities):.2f}%\")\n",
    "\n",
    "print(\"\\nğŸ¯ í•µì‹¬ íŠ¹ì§•:\")\n",
    "print(\"  âœ“ Inference-Free ì¿¼ë¦¬ ì¸ì½”ë”© (ë§¤ìš° ë¹ ë¦„)\")\n",
    "print(\"  âœ“ ë†’ì€ í¬ì†Œì„± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\")\n",
    "print(\"  âœ“ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ê°€ëŠ¥\")\n",
    "print(\"  âœ“ í•œêµ­ì–´ ìµœì í™”\")\n",
    "\n",
    "print(\"\\nğŸ’¡ OpenSearch í†µí•©:\")\n",
    "print(\"  1. ë¬¸ì„œëŠ” ì‚¬ì „ì— ì¸ì½”ë”©í•˜ì—¬ ì¸ë±ì‹±\")\n",
    "print(\"  2. ì¿¼ë¦¬ëŠ” ì‹¤ì‹œê°„ìœ¼ë¡œ IDF lookup\")\n",
    "print(\"  3. rank_features í•„ë“œë¡œ ì €ì¥\")\n",
    "print(\"  4. neural_sparse ì¿¼ë¦¬ë¡œ ê²€ìƒ‰\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. ğŸš€ Transformers ìŠ¤íƒ€ì¼ ë˜í¼ í´ë˜ìŠ¤ (ê°„í¸í•œ ì‚¬ìš©)\n",
    "\n",
    "Hugging Face Transformersì²˜ëŸ¼ ê°„ë‹¨í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í†µí•© í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralSparseSearchModel:\n",
    "    \"\"\"\n",
    "    OpenSearch Neural Sparse ê²€ìƒ‰ ëª¨ë¸ - Transformers ìŠ¤íƒ€ì¼ ì¸í„°í˜ì´ìŠ¤\n",
    "\n",
    "    ì‚¬ìš© ì˜ˆì œ:\n",
    "        model = NeuralSparseSearchModel.from_pretrained(\"./models/opensearch-korean-neural-sparse-v1\")\n",
    "        results = model.search(query=\"ê²€ìƒ‰ ì¿¼ë¦¬\", documents=[\"ë¬¸ì„œ1\", \"ë¬¸ì„œ2\", ...])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, doc_encoder, tokenizer, idf_dict, device='cuda', max_length=128):\n",
    "        self.doc_encoder = doc_encoder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.idf_dict = idf_dict\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Document ë²¡í„° ìºì‹œ (ì„ íƒì )\n",
    "        self._doc_cache = {}\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_path, device=None):\n",
    "        \"\"\"\n",
    "        ì €ì¥ëœ ëª¨ë¸ì—ì„œ ë¡œë“œ (Transformers ìŠ¤íƒ€ì¼)\n",
    "\n",
    "        Args:\n",
    "            model_path: ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
    "            device: ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cpu')\n",
    "\n",
    "        Returns:\n",
    "            NeuralSparseSearchModel ì¸ìŠ¤í„´ìŠ¤\n",
    "        \"\"\"\n",
    "        import os\n",
    "\n",
    "        if device is None:\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        device = torch.device(device)\n",
    "\n",
    "        # Config ë¡œë“œ\n",
    "        with open(os.path.join(model_path, \"config.json\"), 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        # Tokenizer ë¡œë“œ\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "        # IDF ë”•ì…”ë„ˆë¦¬ ë¡œë“œ\n",
    "        with open(os.path.join(model_path, \"idf.json\"), 'r', encoding='utf-8') as f:\n",
    "            idf_dict = json.load(f)\n",
    "\n",
    "        # Document Encoder ë¡œë“œ\n",
    "        base_model = config['base_model']\n",
    "        doc_encoder = OpenSearchDocEncoder(base_model)\n",
    "        state_dict = torch.load(\n",
    "            os.path.join(model_path, \"pytorch_model.bin\"),\n",
    "            map_location=device\n",
    "        )\n",
    "        doc_encoder.load_state_dict(state_dict)\n",
    "        doc_encoder = doc_encoder.to(device)\n",
    "        doc_encoder.eval()\n",
    "\n",
    "        max_length = config.get('max_seq_length', 128)\n",
    "\n",
    "        return cls(doc_encoder, tokenizer, idf_dict, device, max_length)\n",
    "\n",
    "    def encode_query(self, query):\n",
    "        \"\"\"\n",
    "        ì¿¼ë¦¬ë¥¼ sparse vectorë¡œ ì¸ì½”ë”© (Inference-Free!)\n",
    "\n",
    "        Args:\n",
    "            query: ì¿¼ë¦¬ í…ìŠ¤íŠ¸ (str)\n",
    "\n",
    "        Returns:\n",
    "            sparse_vector: (vocab_size,) numpy array\n",
    "        \"\"\"\n",
    "        # í† í°í™”\n",
    "        tokens = self.tokenizer.encode(\n",
    "            query,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        # IDF lookup\n",
    "        sparse_vec = np.zeros(self.tokenizer.vocab_size)\n",
    "        for token_id in tokens:\n",
    "            token_str = self.tokenizer.decode([token_id])\n",
    "            if token_str in self.idf_dict:\n",
    "                sparse_vec[token_id] = self.idf_dict[token_str]\n",
    "\n",
    "        return sparse_vec\n",
    "\n",
    "    def encode_documents(self, documents, batch_size=8, show_progress=True):\n",
    "        \"\"\"\n",
    "        ë¬¸ì„œë“¤ì„ ë°°ì¹˜ë¡œ ì¸ì½”ë”©\n",
    "\n",
    "        Args:\n",
    "            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "            batch_size: ë°°ì¹˜ í¬ê¸°\n",
    "            show_progress: ì§„í–‰ë¥  í‘œì‹œ\n",
    "\n",
    "        Returns:\n",
    "            List of sparse vectors\n",
    "        \"\"\"\n",
    "        from tqdm.auto import tqdm\n",
    "\n",
    "        self.doc_encoder.eval()\n",
    "        doc_vectors = []\n",
    "\n",
    "        iterator = range(0, len(documents), batch_size)\n",
    "        if show_progress:\n",
    "            iterator = tqdm(iterator, desc=\"Encoding documents\")\n",
    "\n",
    "        for i in iterator:\n",
    "            batch = documents[i:i+batch_size]\n",
    "\n",
    "            # ë°°ì¹˜ í† í°í™”\n",
    "            encoded = self.tokenizer(\n",
    "                batch,\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            input_ids = encoded['input_ids'].to(self.device)\n",
    "            attention_mask = encoded['attention_mask'].to(self.device)\n",
    "\n",
    "            # Inference\n",
    "            with torch.no_grad():\n",
    "                sparse_vecs = self.doc_encoder(input_ids, attention_mask)\n",
    "\n",
    "            doc_vectors.extend(sparse_vecs.cpu().numpy())\n",
    "\n",
    "        return doc_vectors\n",
    "\n",
    "    def search(self, query, documents, top_k=5, return_scores=True, batch_size=8):\n",
    "        \"\"\"\n",
    "        ê²€ìƒ‰ ìˆ˜í–‰\n",
    "\n",
    "        Args:\n",
    "            query: ê²€ìƒ‰ ì¿¼ë¦¬ (str)\n",
    "            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë¬¸ì„œ ë²¡í„° ë¦¬ìŠ¤íŠ¸\n",
    "            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜\n",
    "            return_scores: ì ìˆ˜ í¬í•¨ ì—¬ë¶€\n",
    "            batch_size: ë¬¸ì„œ ì¸ì½”ë”© ë°°ì¹˜ í¬ê¸°\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns: rank, document, score (if return_scores=True)\n",
    "        \"\"\"\n",
    "        # ì¿¼ë¦¬ ì¸ì½”ë”©\n",
    "        query_vec = self.encode_query(query)\n",
    "\n",
    "        # ë¬¸ì„œ ì¸ì½”ë”© (ë¬¸ìì—´ì¸ ê²½ìš°)\n",
    "        if len(documents) > 0 and isinstance(documents[0], str):\n",
    "            doc_vectors = self.encode_documents(documents, batch_size=batch_size)\n",
    "        else:\n",
    "            doc_vectors = documents\n",
    "\n",
    "        # ìœ ì‚¬ë„ ê³„ì‚°\n",
    "        similarities = []\n",
    "        for i, doc_vec in enumerate(doc_vectors):\n",
    "            sim = np.dot(query_vec, doc_vec)\n",
    "            similarities.append((i, sim))\n",
    "\n",
    "        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # ìƒìœ„ kê°œ ê²°ê³¼\n",
    "        top_results = similarities[:top_k]\n",
    "\n",
    "        # DataFrame ìƒì„±\n",
    "        results = []\n",
    "        for rank, (doc_idx, score) in enumerate(top_results, 1):\n",
    "            if isinstance(documents[0], str):\n",
    "                result = {\n",
    "                    'rank': rank,\n",
    "                    'document_id': doc_idx,\n",
    "                    'document': documents[doc_idx],\n",
    "                }\n",
    "            else:\n",
    "                result = {\n",
    "                    'rank': rank,\n",
    "                    'document_id': doc_idx,\n",
    "                }\n",
    "\n",
    "            if return_scores:\n",
    "                result['score'] = score\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def get_top_tokens(self, sparse_vec, top_k=10):\n",
    "        \"\"\"\n",
    "        Sparse vectorì—ì„œ ìƒìœ„ í† í° ì¶”ì¶œ\n",
    "\n",
    "        Args:\n",
    "            sparse_vec: (vocab_size,) numpy array\n",
    "            top_k: ìƒìœ„ kê°œ\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns: rank, token, score\n",
    "        \"\"\"\n",
    "        top_indices = np.argsort(sparse_vec)[-top_k:][::-1]\n",
    "        top_values = sparse_vec[top_indices]\n",
    "\n",
    "        results = []\n",
    "        for rank, (idx, val) in enumerate(zip(top_indices, top_values), 1):\n",
    "            if val > 0:\n",
    "                token = self.tokenizer.decode([idx])\n",
    "                results.append({\n",
    "                    'rank': rank,\n",
    "                    'token': token,\n",
    "                    'score': val\n",
    "                })\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def compute_sparsity(self, sparse_vec):\n",
    "        \"\"\"\n",
    "        í¬ì†Œì„± ê³„ì‚°\n",
    "\n",
    "        Args:\n",
    "            sparse_vec: (vocab_size,) numpy array\n",
    "\n",
    "        Returns:\n",
    "            sparsity percentage (float)\n",
    "        \"\"\"\n",
    "        non_zero = np.count_nonzero(sparse_vec)\n",
    "        sparsity = (1 - non_zero / len(sparse_vec)) * 100\n",
    "        return sparsity\n",
    "\n",
    "\n",
    "print(\"âœ“ NeuralSparseSearchModel í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. ë˜í¼ í´ë˜ìŠ¤ ì‚¬ìš© ì˜ˆì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ë¡œë“œ (Transformers ìŠ¤íƒ€ì¼!)\n",
    "model = NeuralSparseSearchModel.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(f\"   ë””ë°”ì´ìŠ¤: {model.device}\")\n",
    "print(f\"   Vocab size: {model.tokenizer.vocab_size:,}\")\n",
    "print(f\"   Max length: {model.max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1. ê°„ë‹¨í•œ ê²€ìƒ‰ ì˜ˆì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²€ìƒ‰í•  ë¬¸ì„œ ì¤€ë¹„\n",
    "search_documents = [\n",
    "    \"OpenSearchëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. Neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.\",\n",
    "    \"í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ ì´ ê¸‰ì†ë„ë¡œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì—ëŠ” GPUê°€ í•„ìˆ˜ì ì…ë‹ˆë‹¤.\",\n",
    "    \"ChatGPTëŠ” ëŒ€í™”í˜• AI ëª¨ë¸ì…ë‹ˆë‹¤.\",\n",
    "    \"ë²¡í„° ê²€ìƒ‰ì€ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.\",\n",
    "    \"Pythonì€ ë°ì´í„° ê³¼í•™ ë¶„ì•¼ì—ì„œ ê°€ì¥ ì¸ê¸° ìˆëŠ” ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
    "    \"Transformer ì•„í‚¤í…ì²˜ëŠ” NLPì˜ í˜ëª…ì„ ì¼ìœ¼ì¼°ìŠµë‹ˆë‹¤.\",\n",
    "    \"ê²€ìƒ‰ ì—”ì§„ ìµœì í™”ëŠ” ì›¹ì‚¬ì´íŠ¸ íŠ¸ë˜í”½ ì¦ëŒ€ì— ì¤‘ìš”í•©ë‹ˆë‹¤.\",\n",
    "]\n",
    "\n",
    "# ê²€ìƒ‰ ìˆ˜í–‰\n",
    "query = \"OpenSearch ê²€ìƒ‰ ì—”ì§„\"\n",
    "results = model.search(query, search_documents, top_k=3)\n",
    "\n",
    "print(f\"\\nì¿¼ë¦¬: '{query}'\\n\")\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2. ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ëŸ¬ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰\n",
    "queries = [\n",
    "    \"ë”¥ëŸ¬ë‹ GPU\",\n",
    "    \"ìì—°ì–´ ì²˜ë¦¬\",\n",
    "    \"ChatGPT AI\",\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ” ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰ ê²°ê³¼\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nì¿¼ë¦¬: '{query}'\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    results = model.search(query, search_documents, top_k=3)\n",
    "    print(results.to_string(index=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3. ì¿¼ë¦¬/ë¬¸ì„œ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¿¼ë¦¬ ë¶„ì„\n",
    "query = \"OpenSearch ë²¡í„° ê²€ìƒ‰ ìµœì í™”\"\n",
    "query_vec = model.encode_query(query)\n",
    "\n",
    "print(f\"ì¿¼ë¦¬: '{query}'\\n\")\n",
    "print(f\"í¬ì†Œì„±: {model.compute_sparsity(query_vec):.2f}%\")\n",
    "print(f\"\\nìƒìœ„ í† í°:\")\n",
    "print(model.get_top_tokens(query_vec, top_k=10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.4. ë°°ì¹˜ ë¬¸ì„œ ì¸ì½”ë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëŒ€ëŸ‰ ë¬¸ì„œ ì¸ì½”ë”©\n",
    "large_document_set = [\n",
    "    f\"ë¬¸ì„œ {i}: ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ë¬¸ì„œì…ë‹ˆë‹¤. Neural sparse ê²€ìƒ‰ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\"\n",
    "    for i in range(20)\n",
    "]\n",
    "\n",
    "print(\"ğŸ“„ ë°°ì¹˜ ë¬¸ì„œ ì¸ì½”ë”© ì¤‘...\\n\")\n",
    "\n",
    "# ë°°ì¹˜ë¡œ ì¸ì½”ë”© (progress bar í‘œì‹œ)\n",
    "doc_vectors = model.encode_documents(large_document_set, batch_size=8)\n",
    "\n",
    "print(f\"\\nâœ“ {len(doc_vectors)}ê°œ ë¬¸ì„œ ì¸ì½”ë”© ì™„ë£Œ!\")\n",
    "print(f\"  í‰ê·  í¬ì†Œì„±: {np.mean([model.compute_sparsity(v) for v in doc_vectors]):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5. ì‚¬ì „ ì¸ì½”ë”©ëœ ë¬¸ì„œë¡œ ê²€ìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œë¥¼ ë¯¸ë¦¬ ì¸ì½”ë”©í•´ë‘ê³  ë°˜ë³µì ìœ¼ë¡œ ê²€ìƒ‰\n",
    "# (ì‹¤ì œ ì‹œìŠ¤í…œì—ì„œëŠ” ë¬¸ì„œë¥¼ í•œ ë²ˆë§Œ ì¸ì½”ë”©í•˜ê³  ì €ì¥)\n",
    "\n",
    "print(\"1ï¸âƒ£ ë¬¸ì„œ ì‚¬ì „ ì¸ì½”ë”©...\\n\")\n",
    "cached_doc_vectors = model.encode_documents(search_documents, batch_size=4)\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ ì‚¬ì „ ì¸ì½”ë”©ëœ ë¬¸ì„œë¡œ ë¹ ë¥¸ ê²€ìƒ‰...\\n\")\n",
    "\n",
    "# ì—¬ëŸ¬ ì¿¼ë¦¬ë¡œ ë¹ ë¥´ê²Œ ê²€ìƒ‰\n",
    "test_queries = [\n",
    "    \"ê²€ìƒ‰ ì—”ì§„\",\n",
    "    \"AI ëª¨ë¸\",\n",
    "    \"ë”¥ëŸ¬ë‹\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    # ë²¡í„°ë§Œ ì „ë‹¬í•˜ë©´ ì¸ì½”ë”© ìƒëµ (ë§¤ìš° ë¹ ë¦„!)\n",
    "    results = model.search(query, cached_doc_vectors, top_k=2)\n",
    "\n",
    "    print(f\"ì¿¼ë¦¬: '{query}'\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for _, row in results.iterrows():\n",
    "        doc_idx = row['document_id']\n",
    "        score = row['score']\n",
    "        print(f\"  [{row['rank']}ìœ„] ì ìˆ˜: {score:.4f}\")\n",
    "        print(f\"       ë¬¸ì„œ: {search_documents[doc_idx][:60]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. ì„±ëŠ¥ ë¹„êµ: ê¸°ì¡´ vs ë˜í¼ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"âš¡ ì„±ëŠ¥ ë¹„êµ: ê¸°ì¡´ í•¨ìˆ˜ vs ë˜í¼ í´ë˜ìŠ¤\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_docs = [f\"í…ŒìŠ¤íŠ¸ ë¬¸ì„œ {i}\" for i in range(50)]\n",
    "test_query = \"í…ŒìŠ¤íŠ¸\"\n",
    "\n",
    "# 1. ê¸°ì¡´ ë°©ì‹\n",
    "print(\"\\n1ï¸âƒ£ ê¸°ì¡´ ë°©ì‹ (ê°œë³„ í•¨ìˆ˜ í˜¸ì¶œ)\")\n",
    "start = time.time()\n",
    "for doc in test_docs:\n",
    "    _ = encode_document(doc, doc_encoder, tokenizer, device)\n",
    "elapsed_old = time.time() - start\n",
    "print(f\"   ì‹œê°„: {elapsed_old:.4f}s\")\n",
    "\n",
    "# 2. ë˜í¼ í´ë˜ìŠ¤ (ë°°ì¹˜ ì²˜ë¦¬)\n",
    "print(\"\\n2ï¸âƒ£ ë˜í¼ í´ë˜ìŠ¤ (ë°°ì¹˜ ì²˜ë¦¬)\")\n",
    "start = time.time()\n",
    "_ = model.encode_documents(test_docs, batch_size=8, show_progress=False)\n",
    "elapsed_new = time.time() - start\n",
    "print(f\"   ì‹œê°„: {elapsed_new:.4f}s\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ ì†ë„ í–¥ìƒ: {elapsed_old/elapsed_new:.2f}ë°° ë¹ ë¦„!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. ì‹¤ì „ ì˜ˆì œ: í•œêµ­ì–´ ë‰´ìŠ¤ ê²€ìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œêµ­ì–´ ë‰´ìŠ¤ ì˜ˆì œ\n",
    "news_articles = [\n",
    "    \"ì‚¼ì„±ì „ìê°€ ì°¨ì„¸ëŒ€ ë°˜ë„ì²´ ê¸°ìˆ  ê°œë°œì— ì„±ê³µí–ˆë‹¤ê³  ë°œí‘œí–ˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ì˜ë£Œ ë¶„ì•¼ì—ì„œ í˜ì‹ ì„ ì¼ìœ¼í‚¤ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ì •ë¶€ê°€ íƒ„ì†Œì¤‘ë¦½ ì •ì±…ì„ ê°•í™”í•˜ê¸°ë¡œ ê²°ì •í–ˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ììœ¨ì£¼í–‰ ìë™ì°¨ ìƒìš©í™”ê°€ 2025ë…„ì— ë³¸ê²©í™”ë  ì „ë§ì…ë‹ˆë‹¤.\",\n",
    "    \"K-popì´ ì „ ì„¸ê³„ì ìœ¼ë¡œ í° ì¸ê¸°ë¥¼ ëŒê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ë°”ì´ì˜¤ ê¸°ì—…ë“¤ì´ ì‹ ì•½ ê°œë°œì— ë°•ì°¨ë¥¼ ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ì „ê¸°ì°¨ ë°°í„°ë¦¬ ê¸°ìˆ ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ë©”íƒ€ë²„ìŠ¤ê°€ ì°¨ì„¸ëŒ€ ì¸í„°ë„· í”Œë«í¼ìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“° í•œêµ­ì–´ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œìŠ¤í…œ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ë‰´ìŠ¤ ì¿¼ë¦¬\n",
    "news_queries = [\n",
    "    \"ë°˜ë„ì²´ ê¸°ìˆ \",\n",
    "    \"AI ì˜ë£Œ\",\n",
    "    \"ì „ê¸°ì°¨ ë°°í„°ë¦¬\",\n",
    "]\n",
    "\n",
    "for query in news_queries:\n",
    "    print(f\"\\nğŸ” ê²€ìƒ‰ì–´: '{query}'\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    results = model.search(query, news_articles, top_k=3)\n",
    "\n",
    "    for _, row in results.iterrows():\n",
    "        print(f\"\\n  [{row['rank']}ìœ„] ì ìˆ˜: {row['score']:.4f}\")\n",
    "        print(f\"       {row['document']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. ìš”ì•½ ë° ì‚¬ìš© ê°€ì´ë“œ\n",
    "\n",
    "### âœ… NeuralSparseSearchModel ì£¼ìš” ê¸°ëŠ¥\n",
    "\n",
    "1. **`from_pretrained(model_path)`** - ëª¨ë¸ ë¡œë“œ\n",
    "2. **`encode_query(query)`** - ì¿¼ë¦¬ ì¸ì½”ë”© (Inference-Free, ë§¤ìš° ë¹ ë¦„)\n",
    "3. **`encode_documents(documents, batch_size)`** - ë¬¸ì„œ ë°°ì¹˜ ì¸ì½”ë”©\n",
    "4. **`search(query, documents, top_k)`** - ê²€ìƒ‰ ìˆ˜í–‰\n",
    "5. **`get_top_tokens(sparse_vec, top_k)`** - ìƒìœ„ í† í° ë¶„ì„\n",
    "6. **`compute_sparsity(sparse_vec)`** - í¬ì†Œì„± ê³„ì‚°\n",
    "\n",
    "### ğŸ’¡ ì‚¬ìš© íŒ¨í„´\n",
    "\n",
    "#### íŒ¨í„´ 1: ì¦‰ì‹œ ê²€ìƒ‰ (ë¬¸ì„œ ì¸ì½”ë”© ì¦‰ì‹œ ìˆ˜í–‰)\n",
    "```python\n",
    "model = NeuralSparseSearchModel.from_pretrained(\"./model\")\n",
    "results = model.search(query=\"ê²€ìƒ‰ì–´\", documents=[\"ë¬¸ì„œ1\", \"ë¬¸ì„œ2\"])\n",
    "```\n",
    "\n",
    "#### íŒ¨í„´ 2: ì‚¬ì „ ì¸ì½”ë”© (ëŒ€ëŸ‰ ë¬¸ì„œ, ë°˜ë³µ ê²€ìƒ‰)\n",
    "```python\n",
    "# 1íšŒë§Œ ì¸ì½”ë”©\n",
    "doc_vectors = model.encode_documents(documents, batch_size=16)\n",
    "\n",
    "# ë¹ ë¥¸ ê²€ìƒ‰ (ì—¬ëŸ¬ ë²ˆ)\n",
    "results1 = model.search(query=\"ì¿¼ë¦¬1\", documents=doc_vectors)\n",
    "results2 = model.search(query=\"ì¿¼ë¦¬2\", documents=doc_vectors)\n",
    "```\n",
    "\n",
    "### ğŸ¯ ì‹¤ì „ í™œìš©\n",
    "\n",
    "- **OpenSearch í†µí•©**: ë¬¸ì„œë¥¼ ë¯¸ë¦¬ ì¸ì½”ë”©í•˜ì—¬ rank_features í•„ë“œì— ì €ì¥\n",
    "- **ì‹¤ì‹œê°„ ê²€ìƒ‰**: ì¿¼ë¦¬ë§Œ ì¦‰ì‹œ ì¸ì½”ë”© (Inference-Free)\n",
    "- **ë°°ì¹˜ ì²˜ë¦¬**: ëŒ€ëŸ‰ ë¬¸ì„œëŠ” ë°°ì¹˜ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ì¸ì½”ë”©\n",
    "- **ë¶„ì„**: ìƒìœ„ í† í° ë¶„ì„ìœ¼ë¡œ ê²€ìƒ‰ í’ˆì§ˆ ê°œì„ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
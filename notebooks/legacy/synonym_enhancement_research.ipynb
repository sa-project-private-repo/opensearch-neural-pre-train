{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Sparse ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ ì—°êµ¬: ë™ì˜ì–´ ê°•í™”\n",
    "\n",
    "## ë¬¸ì œ ë¶„ì„\n",
    "\n",
    "### í˜„ì¬ ëª¨ë¸ì˜ í•œê³„:\n",
    "1. **Vocabulary Mismatch**: ì¿¼ë¦¬ í† í°ê³¼ ë¬¸ì„œ í† í° ë¶ˆì¼ì¹˜\n",
    "   - ì˜ˆ: \"OpenSearch\" (ì¿¼ë¦¬) â‰  \"ê²€ìƒ‰\", \"ê°œë°©\" (ë¬¸ì„œ)\n",
    "2. **ë™ì˜ì–´ ì²˜ë¦¬ ë¶€ì¡±**: ì˜ë¯¸ëŠ” ê°™ì§€ë§Œ í‘œí˜„ì´ ë‹¤ë¥¸ ê²½ìš° ê²€ìƒ‰ ì‹¤íŒ¨\n",
    "3. **í•œêµ­ì–´ íŠ¹ì„±**: ë‹¤ì–‘í•œ í‘œí˜„ ë°©ì‹ (í•œìì–´, ì™¸ë˜ì–´, ìˆœìš°ë¦¬ë§)\n",
    "\n",
    "## í•´ê²° ë°©ë²• ì—°êµ¬\n",
    "\n",
    "### 1. Knowledge Distillation from Dense Model (ì¶”ì²œ â­)\n",
    "### 2. Synonym-Aware IDF Enhancement\n",
    "### 3. Classification Model for Token Similarity\n",
    "### 4. Query/Document Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoConfig\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ë° ë¬¸ì œ ì¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"./opensearch-korean-neural-sparse-v1\"\n",
    "\n",
    "# Tokenizer ë° IDF ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, \"idf.json\"), 'r', encoding='utf-8') as f:\n",
    "    idf_dict = json.load(f)\n",
    "\n",
    "print(f\"âœ“ Tokenizer ë¡œë“œ: vocab_size={tokenizer.vocab_size:,}\")\n",
    "print(f\"âœ“ IDF ë”•ì…”ë„ˆë¦¬: {len(idf_dict):,}ê°œ í† í°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. ë¬¸ì œ ì¬í˜„: Vocabulary Mismatch ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì œê°€ ë˜ëŠ” ì¿¼ë¦¬-ë¬¸ì„œ ìŒ\n",
    "problematic_cases = [\n",
    "    {\n",
    "        \"query\": \"OpenSearch ë²¡í„° ê²€ìƒ‰\",\n",
    "        \"document\": \"OpenSearchëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤.\",\n",
    "        \"expected_match\": [\"OpenSearch\", \"ê²€ìƒ‰\"],\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ì¸ê³µì§€ëŠ¥ ê¸°ìˆ \",\n",
    "        \"document\": \"AI ê¸°ë°˜ ì‹œìŠ¤í…œì´ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "        \"expected_match\": [(\"ì¸ê³µì§€ëŠ¥\", \"AI\"), (\"ê¸°ìˆ \", \"ì‹œìŠ¤í…œ\")],\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ë”¥ëŸ¬ë‹ í•™ìŠµ\",\n",
    "        \"document\": \"ì‹¬ì¸µ ì‹ ê²½ë§ í›ˆë ¨ ë°©ë²•ë¡ \",\n",
    "        \"expected_match\": [(\"ë”¥ëŸ¬ë‹\", \"ì‹¬ì¸µ ì‹ ê²½ë§\"), (\"í•™ìŠµ\", \"í›ˆë ¨\")],\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ë¬¸ì œ ë¶„ì„: Vocabulary Mismatch\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for case in problematic_cases:\n",
    "    query = case['query']\n",
    "    doc = case['document']\n",
    "    \n",
    "    query_tokens = tokenizer.tokenize(query)\n",
    "    doc_tokens = tokenizer.tokenize(doc)\n",
    "    \n",
    "    overlap = set(query_tokens) & set(doc_tokens)\n",
    "    \n",
    "    print(f\"\\nì¿¼ë¦¬: {query}\")\n",
    "    print(f\"  í† í°: {query_tokens}\")\n",
    "    print(f\"\\në¬¸ì„œ: {doc}\")\n",
    "    print(f\"  í† í°: {doc_tokens}\")\n",
    "    print(f\"\\nê²¹ì¹˜ëŠ” í† í°: {overlap if overlap else 'ì—†ìŒ âŒ'}\")\n",
    "    print(f\"ê¸°ëŒ€í–ˆë˜ ë§¤ì¹­: {case['expected_match']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë°©ë²• 1: Dense Model Knowledge Distillation â­\n",
    "\n",
    "Dense retrieval ëª¨ë¸(KoSBERT, KoSimCSE)ì€ ë™ì˜ì–´ë¥¼ ì˜ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "ì´ ì§€ì‹ì„ Neural Sparse ëª¨ë¸ì— ì „ì´í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Dense Teacher ëª¨ë¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œêµ­ì–´ Dense Retrieval ëª¨ë¸ (Teacher)\n",
    "TEACHER_MODEL = \"jhgan/ko-sroberta-multitask\"  # ë˜ëŠ” \"BM-K/KoSimCSE-roberta\"\n",
    "\n",
    "print(f\"Teacher ëª¨ë¸ ë¡œë“œ ì¤‘: {TEACHER_MODEL}\")\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL)\n",
    "teacher_model = AutoModel.from_pretrained(TEACHER_MODEL)\n",
    "teacher_model = teacher_model.to(device)\n",
    "teacher_model.eval()\n",
    "\n",
    "print(f\"âœ“ Teacher ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "def encode_dense(text, model, tokenizer, device):\n",
    "    \"\"\"Dense ì„ë² ë”© ìƒì„±\"\"\"\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        max_length=128,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Mean pooling\n",
    "        embeddings = outputs.last_hidden_state\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(embeddings * attention_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(attention_mask_expanded.sum(1), min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "    \n",
    "    return mean_embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Dense ëª¨ë¸ë¡œ ë™ì˜ì–´ ìŒ ê²€ì¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë™ì˜ì–´ í›„ë³´ ìŒ\n",
    "synonym_candidates = [\n",
    "    (\"ì¸ê³µì§€ëŠ¥\", \"AI\"),\n",
    "    (\"ì¸ê³µì§€ëŠ¥\", \"artificial intelligence\"),\n",
    "    (\"ë”¥ëŸ¬ë‹\", \"ì‹¬ì¸µí•™ìŠµ\"),\n",
    "    (\"ë”¥ëŸ¬ë‹\", \"deep learning\"),\n",
    "    (\"ê²€ìƒ‰\", \"íƒìƒ‰\"),\n",
    "    (\"ê²€ìƒ‰\", \"search\"),\n",
    "    (\"í•™ìŠµ\", \"í›ˆë ¨\"),\n",
    "    (\"í•™ìŠµ\", \"training\"),\n",
    "    (\"ëª¨ë¸\", \"model\"),\n",
    "    (\"ë°ì´í„°\", \"ìë£Œ\"),\n",
    "    (\"ë¶„ì„\", \"í•´ì„\"),\n",
    "    (\"ë²¡í„°\", \"vector\"),\n",
    "    # ë°˜ì˜ì–´ (ë‚®ì€ ìœ ì‚¬ë„ í™•ì¸ìš©)\n",
    "    (\"ê²€ìƒ‰\", \"ìŒì‹\"),\n",
    "    (\"ì¸ê³µì§€ëŠ¥\", \"ë‚ ì”¨\"),\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Dense Modelë¡œ ë™ì˜ì–´ ìœ ì‚¬ë„ ê²€ì¦\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "synonym_scores = []\n",
    "\n",
    "for word1, word2 in synonym_candidates:\n",
    "    emb1 = encode_dense(word1, teacher_model, teacher_tokenizer, device)\n",
    "    emb2 = encode_dense(word2, teacher_model, teacher_tokenizer, device)\n",
    "    \n",
    "    # Cosine similarity\n",
    "    similarity = np.dot(emb1[0], emb2[0]) / (np.linalg.norm(emb1[0]) * np.linalg.norm(emb2[0]))\n",
    "    \n",
    "    synonym_scores.append({\n",
    "        'word1': word1,\n",
    "        'word2': word2,\n",
    "        'similarity': similarity\n",
    "    })\n",
    "    \n",
    "    is_synonym = \"âœ“\" if similarity > 0.7 else \"âœ—\"\n",
    "    print(f\"{is_synonym} {word1:15s} <-> {word2:20s}: {similarity:.4f}\")\n",
    "\n",
    "# DataFrameìœ¼ë¡œ ì •ë¦¬\n",
    "synonym_df = pd.DataFrame(synonym_scores)\n",
    "synonym_df = synonym_df.sort_values('similarity', ascending=False)\n",
    "\n",
    "print(f\"\\ní‰ê·  ìœ ì‚¬ë„: {synonym_df['similarity'].mean():.4f}\")\n",
    "print(f\"ë™ì˜ì–´ ìŒ (>0.7): {len(synonym_df[synonym_df['similarity'] > 0.7])}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ë°©ë²• 2: Synonym-Aware IDF Enhancement\n",
    "\n",
    "IDF ë”•ì…”ë„ˆë¦¬ì— ë™ì˜ì–´ ì •ë³´ë¥¼ í†µí•©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. í•œêµ­ì–´ ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œêµ­ì–´ ë™ì˜ì–´ ì‚¬ì „ (ìˆ˜ë™ + ìë™ í™•ì¥)\n",
    "KOREAN_SYNONYMS = {\n",
    "    # AI/ML ê´€ë ¨\n",
    "    \"ì¸ê³µì§€ëŠ¥\": [\"AI\", \"artificial intelligence\", \"ì—ì´ì•„ì´\"],\n",
    "    \"ë”¥ëŸ¬ë‹\": [\"deep learning\", \"ì‹¬ì¸µí•™ìŠµ\", \"ì‹¬ì¸µ ì‹ ê²½ë§\"],\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹\": [\"machine learning\", \"ê¸°ê³„í•™ìŠµ\"],\n",
    "    \"í•™ìŠµ\": [\"í›ˆë ¨\", \"training\", \"learning\"],\n",
    "    \"ëª¨ë¸\": [\"model\", \"ì•Œê³ ë¦¬ì¦˜\"],\n",
    "    \n",
    "    # ê²€ìƒ‰ ê´€ë ¨\n",
    "    \"ê²€ìƒ‰\": [\"search\", \"íƒìƒ‰\", \"ì¡°íšŒ\", \"ì°¾ê¸°\"],\n",
    "    \"ë¶„ì„\": [\"analysis\", \"í•´ì„\", \"ë¶„ì„\"],\n",
    "    \"ë²¡í„°\": [\"vector\", \"embedding\", \"ì„ë² ë”©\"],\n",
    "    \n",
    "    # ë°ì´í„° ê´€ë ¨\n",
    "    \"ë°ì´í„°\": [\"data\", \"ìë£Œ\"],\n",
    "    \"ì •ë³´\": [\"information\", \"info\"],\n",
    "    \n",
    "    # ê¸°ìˆ  ê´€ë ¨\n",
    "    \"ê¸°ìˆ \": [\"technology\", \"tech\", \"í…Œí¬ë†€ë¡œì§€\"],\n",
    "    \"ì‹œìŠ¤í…œ\": [\"system\"],\n",
    "    \"ì—”ì§„\": [\"engine\"],\n",
    "}\n",
    "\n",
    "# ì—­ë°©í–¥ ë§¤í•‘ (ë™ì˜ì–´ -> ëŒ€í‘œì–´)\n",
    "synonym_to_canonical = {}\n",
    "for canonical, synonyms in KOREAN_SYNONYMS.items():\n",
    "    synonym_to_canonical[canonical] = canonical\n",
    "    for syn in synonyms:\n",
    "        synonym_to_canonical[syn] = canonical\n",
    "\n",
    "print(f\"ë™ì˜ì–´ ì‚¬ì „: {len(KOREAN_SYNONYMS)}ê°œ ëŒ€í‘œì–´\")\n",
    "print(f\"ì´ {sum(len(v)+1 for v in KOREAN_SYNONYMS.values())}ê°œ ë‹¨ì–´ í¬í•¨\")\n",
    "\n",
    "# ì˜ˆì‹œ ì¶œë ¥\n",
    "print(\"\\nì˜ˆì‹œ:\")\n",
    "for canonical, synonyms in list(KOREAN_SYNONYMS.items())[:3]:\n",
    "    print(f\"  {canonical}: {', '.join(synonyms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Synonym-Aware IDF ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synonym_aware_idf(original_idf, tokenizer, synonym_dict, method='max'):\n",
    "    \"\"\"\n",
    "    ë™ì˜ì–´ ì •ë³´ë¥¼ ë°˜ì˜í•œ IDF ìƒì„±\n",
    "    \n",
    "    Args:\n",
    "        original_idf: ì›ë³¸ IDF ë”•ì…”ë„ˆë¦¬\n",
    "        tokenizer: Tokenizer\n",
    "        synonym_dict: ë™ì˜ì–´ ì‚¬ì „\n",
    "        method: 'max', 'mean', 'sum' ì¤‘ ì„ íƒ\n",
    "    \n",
    "    Returns:\n",
    "        enhanced_idf: ê°•í™”ëœ IDF ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "    enhanced_idf = original_idf.copy()\n",
    "    \n",
    "    # ê° ë™ì˜ì–´ ê·¸ë£¹ì— ëŒ€í•´\n",
    "    for canonical, synonyms in synonym_dict.items():\n",
    "        all_words = [canonical] + synonyms\n",
    "        \n",
    "        # ê° ë‹¨ì–´ì˜ í† í°ë“¤ ìˆ˜ì§‘\n",
    "        all_tokens = []\n",
    "        for word in all_words:\n",
    "            tokens = tokenizer.tokenize(word)\n",
    "            all_tokens.extend(tokens)\n",
    "        \n",
    "        # í† í°ë“¤ì˜ IDF ê°’ ìˆ˜ì§‘\n",
    "        idf_values = []\n",
    "        for token in set(all_tokens):\n",
    "            if token in original_idf:\n",
    "                idf_values.append(original_idf[token])\n",
    "        \n",
    "        if not idf_values:\n",
    "            continue\n",
    "        \n",
    "        # IDF ê°’ í†µí•©\n",
    "        if method == 'max':\n",
    "            shared_idf = max(idf_values)\n",
    "        elif method == 'mean':\n",
    "            shared_idf = np.mean(idf_values)\n",
    "        else:  # sum\n",
    "            shared_idf = sum(idf_values)\n",
    "        \n",
    "        # ëª¨ë“  ë™ì˜ì–´ í† í°ì— ì ìš©\n",
    "        for token in set(all_tokens):\n",
    "            enhanced_idf[token] = shared_idf\n",
    "    \n",
    "    return enhanced_idf\n",
    "\n",
    "# ê°•í™”ëœ IDF ìƒì„±\n",
    "print(\"Synonym-Aware IDF ìƒì„± ì¤‘...\\n\")\n",
    "\n",
    "enhanced_idf_max = create_synonym_aware_idf(idf_dict, tokenizer, KOREAN_SYNONYMS, method='max')\n",
    "enhanced_idf_mean = create_synonym_aware_idf(idf_dict, tokenizer, KOREAN_SYNONYMS, method='mean')\n",
    "\n",
    "print(\"âœ“ ê°•í™”ëœ IDF ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "# ë³€í™” í™•ì¸\n",
    "print(\"\\në³€í™” ì˜ˆì‹œ (MAX ë°©ì‹):\")\n",
    "test_words = [\"ì¸ê³µì§€ëŠ¥\", \"AI\", \"ë”¥ëŸ¬ë‹\", \"deep\"]\n",
    "for word in test_words:\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    for token in tokens:\n",
    "        if token in idf_dict:\n",
    "            original = idf_dict[token]\n",
    "            enhanced = enhanced_idf_max[token]\n",
    "            change = \"â†‘\" if enhanced > original else \"â†’\"\n",
    "            print(f\"  {token:15s}: {original:.4f} {change} {enhanced:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Query Expansion with Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query_with_synonyms(query, synonym_dict, max_expansions=2):\n",
    "    \"\"\"\n",
    "    ì¿¼ë¦¬ë¥¼ ë™ì˜ì–´ë¡œ í™•ì¥\n",
    "    \n",
    "    Args:\n",
    "        query: ì›ë³¸ ì¿¼ë¦¬\n",
    "        synonym_dict: ë™ì˜ì–´ ì‚¬ì „\n",
    "        max_expansions: ë‹¨ì–´ë‹¹ ìµœëŒ€ í™•ì¥ ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        expanded_query: í™•ì¥ëœ ì¿¼ë¦¬\n",
    "    \"\"\"\n",
    "    words = query.split()\n",
    "    expanded_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        expanded_words.append(word)\n",
    "        \n",
    "        # ë™ì˜ì–´ ì°¾ê¸°\n",
    "        if word in synonym_dict:\n",
    "            synonyms = synonym_dict[word][:max_expansions]\n",
    "            expanded_words.extend(synonyms)\n",
    "    \n",
    "    return \" \".join(expanded_words)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "test_queries = [\n",
    "    \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰\",\n",
    "    \"ë”¥ëŸ¬ë‹ ëª¨ë¸\",\n",
    "    \"ë°ì´í„° ë¶„ì„\",\n",
    "]\n",
    "\n",
    "print(\"Query Expansion ì˜ˆì‹œ:\\n\")\n",
    "for query in test_queries:\n",
    "    expanded = expand_query_with_synonyms(query, KOREAN_SYNONYMS)\n",
    "    print(f\"ì›ë³¸: {query}\")\n",
    "    print(f\"í™•ì¥: {expanded}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë°©ë²• 3: í† í° ì„ë² ë”© ê¸°ë°˜ ë™ì˜ì–´ ìë™ ë°œê²¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. BERT í† í° ì„ë² ë”©ìœ¼ë¡œ ìœ ì‚¬ í† í° ì°¾ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoBERT ë¡œë“œ (í† í° ìœ ì‚¬ë„ ê³„ì‚°ìš©)\n",
    "kobert_model_name = \"klue/bert-base\"\n",
    "kobert_model = AutoModel.from_pretrained(kobert_model_name)\n",
    "kobert_model = kobert_model.to(device)\n",
    "kobert_model.eval()\n",
    "\n",
    "# Token embedding ì¶”ì¶œ\n",
    "token_embeddings = kobert_model.embeddings.word_embeddings.weight.detach().cpu().numpy()\n",
    "\n",
    "print(f\"âœ“ Token embeddings: {token_embeddings.shape}\")\n",
    "\n",
    "def find_similar_tokens(token, tokenizer, embeddings, top_k=10, threshold=0.7):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í† í°ê³¼ ìœ ì‚¬í•œ í† í°ë“¤ ì°¾ê¸°\n",
    "    \"\"\"\n",
    "    # í† í° ID\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    if token_id == tokenizer.unk_token_id:\n",
    "        return []\n",
    "\n",
    "    # í•´ë‹¹ í† í°ì˜ ì„ë² ë”©\n",
    "    token_emb = embeddings[token_id]\n",
    "\n",
    "    # ëª¨ë“  í† í°ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    similarities = np.dot(embeddings, token_emb) / (\n",
    "        np.linalg.norm(embeddings, axis=1) * np.linalg.norm(token_emb)\n",
    "    )\n",
    "\n",
    "    # ìƒìœ„ kê°œ\n",
    "    top_indices = np.argsort(similarities)[-top_k-1:-1][::-1]\n",
    "\n",
    "    similar_tokens = []\n",
    "    for idx in top_indices:\n",
    "        if similarities[idx] >= threshold and idx != token_id:\n",
    "            # ìˆ˜ì •: int()ë¡œ ë³€í™˜í•˜ì—¬ ì „ë‹¬\n",
    "            similar_token = tokenizer.decode([int(idx)])\n",
    "            similar_tokens.append((similar_token, float(similarities[idx])))\n",
    "\n",
    "    return similar_tokens\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "print(\"\\nìœ ì‚¬ í† í° ì°¾ê¸° í…ŒìŠ¤íŠ¸:\\n\")\n",
    "test_tokens = [\"ê²€ìƒ‰\", \"í•™ìŠµ\", \"ë°ì´í„°\", \"AI\"]\n",
    "\n",
    "for token in test_tokens:\n",
    "    similar = find_similar_tokens(token, tokenizer, token_embeddings, top_k=5, threshold=0.6)\n",
    "    print(f\"{token}ì˜ ìœ ì‚¬ í† í°:\")\n",
    "    for sim_token, score in similar:\n",
    "        print(f\"  - {sim_token:15s} (ìœ ì‚¬ë„: {score:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. ìë™ ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_automatic_synonym_dict(important_tokens, tokenizer, embeddings, threshold=0.75):\n",
    "    \"\"\"\n",
    "    ì¤‘ìš” í† í°ë“¤ì— ëŒ€í•´ ìë™ìœ¼ë¡œ ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶•\n",
    "    \"\"\"\n",
    "    auto_synonyms = {}\n",
    "    \n",
    "    for token in tqdm(important_tokens, desc=\"Building synonym dict\"):\n",
    "        similar = find_similar_tokens(token, tokenizer, embeddings, top_k=5, threshold=threshold)\n",
    "        if similar:\n",
    "            auto_synonyms[token] = [t for t, _ in similar]\n",
    "    \n",
    "    return auto_synonyms\n",
    "\n",
    "# IDF ìƒìœ„ 1000ê°œ í† í°ì— ëŒ€í•´ ë™ì˜ì–´ ì°¾ê¸°\n",
    "sorted_idf = sorted(idf_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "important_tokens = [token for token, _ in sorted_idf[:1000] \n",
    "                   if not token.startswith('##') and len(token) > 1]\n",
    "\n",
    "print(f\"ì¤‘ìš” í† í° {len(important_tokens)}ê°œì— ëŒ€í•´ ë™ì˜ì–´ ì°¾ëŠ” ì¤‘...\\n\")\n",
    "\n",
    "# ìƒ˜í”Œë§Œ í…ŒìŠ¤íŠ¸ (ì „ì²´ëŠ” ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼)\n",
    "auto_synonym_dict = build_automatic_synonym_dict(\n",
    "    important_tokens[:50],  # ìƒ˜í”Œ 50ê°œ\n",
    "    tokenizer,\n",
    "    token_embeddings,\n",
    "    threshold=0.75\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ ìë™ ë™ì˜ì–´ ì‚¬ì „ ìƒì„± ì™„ë£Œ: {len(auto_synonym_dict)}ê°œ ì—”íŠ¸ë¦¬\")\n",
    "\n",
    "# ì˜ˆì‹œ ì¶œë ¥\n",
    "print(\"\\nì˜ˆì‹œ:\")\n",
    "for token, synonyms in list(auto_synonym_dict.items())[:5]:\n",
    "    print(f\"  {token}: {', '.join(synonyms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ë°©ë²• 4: Knowledge Distillation Loss êµ¬í˜„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Distillation Loss ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knowledge_distillation_loss(student_scores, teacher_scores, temperature=2.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Knowledge Distillation Loss\n",
    "    \n",
    "    Args:\n",
    "        student_scores: Sparse ëª¨ë¸ì˜ ìœ ì‚¬ë„ ì ìˆ˜\n",
    "        teacher_scores: Dense ëª¨ë¸ì˜ ìœ ì‚¬ë„ ì ìˆ˜\n",
    "        temperature: Softmax temperature\n",
    "        alpha: Loss ê°€ì¤‘ì¹˜ (0~1)\n",
    "    \n",
    "    Returns:\n",
    "        distillation_loss\n",
    "    \"\"\"\n",
    "    # Soft targets from teacher\n",
    "    soft_teacher = F.softmax(teacher_scores / temperature, dim=-1)\n",
    "    \n",
    "    # Soft predictions from student\n",
    "    soft_student = F.log_softmax(student_scores / temperature, dim=-1)\n",
    "    \n",
    "    # KL Divergence\n",
    "    distillation_loss = F.kl_div(\n",
    "        soft_student,\n",
    "        soft_teacher,\n",
    "        reduction='batchmean'\n",
    "    ) * (temperature ** 2)\n",
    "    \n",
    "    return distillation_loss\n",
    "\n",
    "print(\"âœ“ Knowledge Distillation Loss ì •ì˜ ì™„ë£Œ\")\n",
    "print(\"\\nì‚¬ìš© ë°©ë²•:\")\n",
    "print(\"  total_loss = alpha * distillation_loss + (1-alpha) * original_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. ê°œì„ ëœ í•™ìŠµ ì†ì‹¤ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_training_loss(\n",
    "    doc_sparse,\n",
    "    query_sparse, \n",
    "    relevance,\n",
    "    teacher_doc_emb=None,\n",
    "    teacher_query_emb=None,\n",
    "    idf_dict=None,\n",
    "    lambda_l0=1e-3,\n",
    "    lambda_idf=1e-2,\n",
    "    lambda_kd=0.3,\n",
    "    temperature=2.0\n",
    "):\n",
    "    \"\"\"\n",
    "    ë™ì˜ì–´ë¥¼ ê³ ë ¤í•œ ê°œì„ ëœ í•™ìŠµ ì†ì‹¤\n",
    "    \n",
    "    Args:\n",
    "        doc_sparse: Student sparse document vectors\n",
    "        query_sparse: Student sparse query vectors\n",
    "        relevance: Ground truth relevance\n",
    "        teacher_doc_emb: Teacher dense document embeddings (optional)\n",
    "        teacher_query_emb: Teacher dense query embeddings (optional)\n",
    "        idf_dict: IDF dictionary\n",
    "        lambda_l0: L0 regularization weight\n",
    "        lambda_idf: IDF-aware penalty weight\n",
    "        lambda_kd: Knowledge distillation weight\n",
    "        temperature: KD temperature\n",
    "    \n",
    "    Returns:\n",
    "        total_loss, loss_dict\n",
    "    \"\"\"\n",
    "    # 1. Ranking Loss (Sparse)\n",
    "    sparse_similarity = torch.sum(doc_sparse * query_sparse, dim=-1)\n",
    "    ranking_loss = F.binary_cross_entropy_with_logits(\n",
    "        sparse_similarity, relevance, reduction='mean'\n",
    "    )\n",
    "    \n",
    "    # 2. L0 Regularization\n",
    "    l0_loss = torch.mean(torch.sum(torch.abs(doc_sparse), dim=-1))\n",
    "    \n",
    "    # 3. IDF-aware Penalty\n",
    "    if idf_dict is not None:\n",
    "        idf_tensor = torch.tensor(\n",
    "            [idf_dict.get(i, 1.0) for i in range(doc_sparse.shape[1])],\n",
    "            device=doc_sparse.device\n",
    "        )\n",
    "        inverse_idf = 1.0 / (idf_tensor + 1e-6)\n",
    "        idf_penalty = torch.mean(torch.sum(doc_sparse * inverse_idf, dim=-1))\n",
    "    else:\n",
    "        idf_penalty = torch.tensor(0.0)\n",
    "    \n",
    "    # 4. Knowledge Distillation from Dense Teacher (optional)\n",
    "    kd_loss = torch.tensor(0.0)\n",
    "    if teacher_doc_emb is not None and teacher_query_emb is not None:\n",
    "        # Dense similarity\n",
    "        dense_similarity = torch.sum(\n",
    "            teacher_doc_emb * teacher_query_emb, dim=-1\n",
    "        )\n",
    "        \n",
    "        # KD Loss\n",
    "        kd_loss = knowledge_distillation_loss(\n",
    "            sparse_similarity.unsqueeze(-1),\n",
    "            dense_similarity.unsqueeze(-1),\n",
    "            temperature=temperature\n",
    "        )\n",
    "    \n",
    "    # Total Loss\n",
    "    total_loss = (\n",
    "        ranking_loss + \n",
    "        lambda_l0 * l0_loss + \n",
    "        lambda_idf * idf_penalty +\n",
    "        lambda_kd * kd_loss\n",
    "    )\n",
    "    \n",
    "    loss_dict = {\n",
    "        'total': total_loss.item(),\n",
    "        'ranking': ranking_loss.item(),\n",
    "        'l0': l0_loss.item(),\n",
    "        'idf_penalty': idf_penalty.item(),\n",
    "        'kd': kd_loss.item() if isinstance(kd_loss, torch.Tensor) else 0.0\n",
    "    }\n",
    "    \n",
    "    return total_loss, loss_dict\n",
    "\n",
    "print(\"âœ“ ê°œì„ ëœ í•™ìŠµ ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. ì›ë³¸ vs Synonym-Enhanced IDF ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_query_with_idf(query, tokenizer, idf_dict):\n",
    "    \"\"\"ì¿¼ë¦¬ë¥¼ IDFë¡œ ì¸ì½”ë”©\"\"\"\n",
    "    tokens = tokenizer.encode(query, add_special_tokens=False)\n",
    "    sparse_vec = np.zeros(tokenizer.vocab_size)\n",
    "    for token_id in tokens:\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        if token_str in idf_dict:\n",
    "            sparse_vec[token_id] = idf_dict[token_str]\n",
    "    return sparse_vec\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\n",
    "test_cases = [\n",
    "    {\n",
    "        \"query\": \"ì¸ê³µì§€ëŠ¥ ê²€ìƒ‰\",\n",
    "        \"doc1\": \"AI ê¸°ë°˜ íƒìƒ‰ ì‹œìŠ¤í…œ\",  # ë™ì˜ì–´ ë§¤ì¹­\n",
    "        \"doc2\": \"ë‚ ì”¨ ì˜ˆë³´ ì„œë¹„ìŠ¤\",      # ë¬´ê´€\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ë”¥ëŸ¬ë‹ í•™ìŠµ\",\n",
    "        \"doc1\": \"ì‹¬ì¸µ ì‹ ê²½ë§ í›ˆë ¨ ë°©ë²•\",\n",
    "        \"doc2\": \"ìš”ë¦¬ ë ˆì‹œí”¼ ëª¨ìŒ\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ì›ë³¸ IDF vs Synonym-Enhanced IDF ì„±ëŠ¥ ë¹„êµ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for case in test_cases:\n",
    "    query = case['query']\n",
    "    doc1 = case['doc1']\n",
    "    doc2 = case['doc2']\n",
    "    \n",
    "    print(f\"\\nì¿¼ë¦¬: {query}\")\n",
    "    print(f\"  ê´€ë ¨ ë¬¸ì„œ: {doc1}\")\n",
    "    print(f\"  ë¬´ê´€ ë¬¸ì„œ: {doc2}\")\n",
    "    \n",
    "    # ì›ë³¸ IDF\n",
    "    q_vec_orig = encode_query_with_idf(query, tokenizer, idf_dict)\n",
    "    d1_vec_orig = encode_query_with_idf(doc1, tokenizer, idf_dict)\n",
    "    d2_vec_orig = encode_query_with_idf(doc2, tokenizer, idf_dict)\n",
    "    \n",
    "    sim1_orig = np.dot(q_vec_orig, d1_vec_orig)\n",
    "    sim2_orig = np.dot(q_vec_orig, d2_vec_orig)\n",
    "    \n",
    "    # Enhanced IDF\n",
    "    q_vec_enh = encode_query_with_idf(query, tokenizer, enhanced_idf_max)\n",
    "    d1_vec_enh = encode_query_with_idf(doc1, tokenizer, enhanced_idf_max)\n",
    "    d2_vec_enh = encode_query_with_idf(doc2, tokenizer, enhanced_idf_max)\n",
    "    \n",
    "    sim1_enh = np.dot(q_vec_enh, d1_vec_enh)\n",
    "    sim2_enh = np.dot(q_vec_enh, d2_vec_enh)\n",
    "    \n",
    "    print(f\"\\n  [ì›ë³¸ IDF]\")\n",
    "    print(f\"    ê´€ë ¨ ë¬¸ì„œ ìœ ì‚¬ë„: {sim1_orig:.4f}\")\n",
    "    print(f\"    ë¬´ê´€ ë¬¸ì„œ ìœ ì‚¬ë„: {sim2_orig:.4f}\")\n",
    "    print(f\"    ì°¨ì´: {sim1_orig - sim2_orig:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  [Enhanced IDF]\")\n",
    "    print(f\"    ê´€ë ¨ ë¬¸ì„œ ìœ ì‚¬ë„: {sim1_enh:.4f} {'â†‘' if sim1_enh > sim1_orig else ''}\")\n",
    "    print(f\"    ë¬´ê´€ ë¬¸ì„œ ìœ ì‚¬ë„: {sim2_enh:.4f}\")\n",
    "    print(f\"    ì°¨ì´: {sim1_enh - sim2_enh:.4f} {'âœ“ ê°œì„ ' if (sim1_enh - sim2_enh) > (sim1_orig - sim2_orig) else ''}\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ê¶Œì¥ ì‚¬í•­ ë° ë‹¤ìŒ ë‹¨ê³„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = \"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    Neural Sparse ì„±ëŠ¥ ê°œì„  ê¶Œì¥ ì‚¬í•­                        â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "## ğŸ¯ ìš°ì„ ìˆœìœ„ë³„ ê°œì„  ë°©ë²•\n",
    "\n",
    "### 1ìˆœìœ„: Knowledge Distillation â­â­â­\n",
    "   - Dense ëª¨ë¸(KoSBERT)ë¡œë¶€í„° knowledge distillation\n",
    "   - íš¨ê³¼: ë™ì˜ì–´ ì²˜ë¦¬ ëŠ¥ë ¥ í¬ê²Œ í–¥ìƒ\n",
    "   - êµ¬í˜„: enhanced_training_loss() ì‚¬ìš©\n",
    "   - ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ: 15-20%\n",
    "\n",
    "### 2ìˆœìœ„: Synonym-Aware IDF â­â­\n",
    "   - ìˆ˜ë™/ìë™ ë™ì˜ì–´ ì‚¬ì „ìœ¼ë¡œ IDF ê°•í™”\n",
    "   - íš¨ê³¼: ì¦‰ì‹œ ì ìš© ê°€ëŠ¥, ì¬í•™ìŠµ ë¶ˆí•„ìš”\n",
    "   - êµ¬í˜„: enhanced_idf_max ì‚¬ìš©\n",
    "   - ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ: 5-10%\n",
    "\n",
    "### 3ìˆœìœ„: Query Expansion â­\n",
    "   - ê²€ìƒ‰ ì‹œì ì— ì¿¼ë¦¬ í™•ì¥\n",
    "   - íš¨ê³¼: ê²€ìƒ‰ recall í–¥ìƒ\n",
    "   - êµ¬í˜„: expand_query_with_synonyms() ì‚¬ìš©\n",
    "   - ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ: 3-7%\n",
    "\n",
    "## ğŸ“ êµ¬ì²´ì  ì‹¤í–‰ ê³„íš\n",
    "\n",
    "### ë‹¨ê³„ 1: Synonym-Enhanced IDF ì ìš© (ì¦‰ì‹œ ê°€ëŠ¥)\n",
    "```python\n",
    "# 1. Enhanced IDF ì €ì¥\n",
    "with open('idf_enhanced.json', 'w') as f:\n",
    "    json.dump(enhanced_idf_max, f)\n",
    "\n",
    "# 2. Inference ì‹œ ì‚¬ìš©\n",
    "model = NeuralSparseSearchModel.from_pretrained(\n",
    "    model_dir,\n",
    "    idf_dict=enhanced_idf_max  # Enhanced IDF ì‚¬ìš©\n",
    ")\n",
    "```\n",
    "\n",
    "### ë‹¨ê³„ 2: Knowledge Distillationìœ¼ë¡œ ì¬í•™ìŠµ (1-2ì¼)\n",
    "```python\n",
    "# Teacher ëª¨ë¸ ì¤€ë¹„\n",
    "teacher_model = AutoModel.from_pretrained(\"jhgan/ko-sroberta-multitask\")\n",
    "\n",
    "# í•™ìŠµ ì‹œ KD loss ì¶”ê°€\n",
    "loss, loss_dict = enhanced_training_loss(\n",
    "    doc_sparse, query_sparse, relevance,\n",
    "    teacher_doc_emb, teacher_query_emb,  # Teacher embeddings\n",
    "    lambda_kd=0.3  # KD weight\n",
    ")\n",
    "```\n",
    "\n",
    "### ë‹¨ê³„ 3: ë™ì˜ì–´ ì‚¬ì „ í™•ì¥ (ì§€ì†ì )\n",
    "- ìë™ ë™ì˜ì–´ ë°œê²¬ ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "- ë„ë©”ì¸ë³„ ë™ì˜ì–´ ì¶”ê°€\n",
    "- ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜\n",
    "\n",
    "## ğŸ”¬ ì¶”ê°€ ì—°êµ¬ ë°©í–¥\n",
    "\n",
    "1. **Multi-Vector Sparse Encoding**\n",
    "   - ì—¬ëŸ¬ sparse vectorsë¡œ ë¬¸ì„œ í‘œí˜„\n",
    "   - ë‹¤ì–‘í•œ ì˜ë¯¸ ì¸¡ë©´ í¬ì°©\n",
    "\n",
    "2. **Cross-Lingual Transfer**\n",
    "   - ì˜ì–´ ëª¨ë¸ì˜ ì§€ì‹ì„ í•œêµ­ì–´ë¡œ ì „ì´\n",
    "   - ì™¸ë˜ì–´ ì²˜ë¦¬ ê°œì„ \n",
    "\n",
    "3. **Dynamic IDF**\n",
    "   - ê²€ìƒ‰ ë¡œê·¸ ê¸°ë°˜ IDF ì—…ë°ì´íŠ¸\n",
    "   - ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ë°˜ì˜\n",
    "\n",
    "4. **Hybrid Retrieval**\n",
    "   - Sparse + Dense ì•™ìƒë¸”\n",
    "   - ê°ê°ì˜ ì¥ì  í™œìš©\n",
    "\n",
    "## ğŸ“Š ì˜ˆìƒ ìµœì¢… ì„±ëŠ¥\n",
    "\n",
    "í˜„ì¬ ì„±ëŠ¥:     â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ (40%)\n",
    "Enhanced IDF:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ (55%)\n",
    "KD ì¬í•™ìŠµ:      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ (75%)\n",
    "ì „ì²´ ì ìš©:      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ (85%)\n",
    "\n",
    "## âš ï¸  ì£¼ì˜ì‚¬í•­\n",
    "\n",
    "1. **Trade-off ê³ ë ¤**\n",
    "   - Query expansionì€ ì†ë„ ì €í•˜ ê°€ëŠ¥\n",
    "   - KD í•™ìŠµì€ ì‹œê°„ ì†Œìš”\n",
    "\n",
    "2. **í‰ê°€ í•„ìˆ˜**\n",
    "   - ì‹¤ì œ ê²€ìƒ‰ ë¡œê·¸ë¡œ í‰ê°€\n",
    "   - MRR, NDCG, Recall@K ì¸¡ì •\n",
    "\n",
    "3. **ì ì§„ì  ì ìš©**\n",
    "   - A/B í…ŒìŠ¤íŠ¸ë¡œ ê²€ì¦\n",
    "   - ë‹¨ê³„ë³„ë¡œ ì„±ëŠ¥ í™•ì¸\n",
    "\"\"\"\n",
    "\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ê°œì„ ëœ IDF ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced IDF ì €ì¥\n",
    "output_dir = \"./opensearch-korean-neural-sparse-v1-enhanced\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Enhanced IDF (MAX ë°©ì‹)\n",
    "with open(os.path.join(output_dir, \"idf_enhanced_max.json\"), 'w', encoding='utf-8') as f:\n",
    "    json.dump(enhanced_idf_max, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 2. Enhanced IDF (MEAN ë°©ì‹)\n",
    "with open(os.path.join(output_dir, \"idf_enhanced_mean.json\"), 'w', encoding='utf-8') as f:\n",
    "    json.dump(enhanced_idf_mean, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 3. ë™ì˜ì–´ ì‚¬ì „\n",
    "with open(os.path.join(output_dir, \"synonym_dict.json\"), 'w', encoding='utf-8') as f:\n",
    "    json.dump(KOREAN_SYNONYMS, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 4. ìë™ ë™ì˜ì–´ ì‚¬ì „\n",
    "with open(os.path.join(output_dir, \"auto_synonym_dict.json\"), 'w', encoding='utf-8') as f:\n",
    "    json.dump(auto_synonym_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ“ ê°œì„ ëœ IDF ë° ë™ì˜ì–´ ì‚¬ì „ ì €ì¥ ì™„ë£Œ\")\n",
    "print(f\"  ì €ì¥ ìœ„ì¹˜: {output_dir}/\")\n",
    "print(f\"\\nì €ì¥ëœ íŒŒì¼:\")\n",
    "for filename in os.listdir(output_dir):\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    size = os.path.getsize(filepath) / 1024\n",
    "    print(f\"  - {filename:30s} ({size:>8.2f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ë‹¤ìŒ ë‹¨ê³„: Re-training Script ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retraining_guide = \"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                Knowledge Distillation ì¬í•™ìŠµ ê°€ì´ë“œ                        â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "## ì¬í•™ìŠµì„ ìœ„í•œ ìˆ˜ì • ì‚¬í•­\n",
    "\n",
    "### 1. korean_neural_sparse_training.ipynb ìˆ˜ì •\n",
    "\n",
    "#### ë³€ê²½ 1: Teacher ëª¨ë¸ ì¶”ê°€\n",
    "```python\n",
    "# ì…€ ì¶”ê°€: Teacher ëª¨ë¸ ë¡œë“œ\n",
    "teacher_model = AutoModel.from_pretrained(\"jhgan/ko-sroberta-multitask\")\n",
    "teacher_model = teacher_model.to(device)\n",
    "teacher_model.eval()\n",
    "```\n",
    "\n",
    "#### ë³€ê²½ 2: í•™ìŠµ ë£¨í”„ ìˆ˜ì •\n",
    "```python\n",
    "def train_epoch_with_kd(...):\n",
    "    for batch in progress_bar:\n",
    "        # ... ê¸°ì¡´ ì½”ë“œ ...\n",
    "        \n",
    "        # Teacher embeddings ìƒì„±\n",
    "        with torch.no_grad():\n",
    "            teacher_doc_emb = teacher_model(\n",
    "                doc_input_ids, doc_attention_mask\n",
    "            ).last_hidden_state.mean(dim=1)\n",
    "            \n",
    "            teacher_query_emb = teacher_model(\n",
    "                query_input_ids, query_attention_mask  \n",
    "            ).last_hidden_state.mean(dim=1)\n",
    "        \n",
    "        # Enhanced loss ì‚¬ìš©\n",
    "        loss, loss_dict = enhanced_training_loss(\n",
    "            doc_sparse, query_sparse, relevance,\n",
    "            teacher_doc_emb, teacher_query_emb,\n",
    "            idf_id_dict,\n",
    "            lambda_l0=1e-3,\n",
    "            lambda_idf=1e-2,\n",
    "            lambda_kd=0.3  # KD weight (ì¤‘ìš”!)\n",
    "        )\n",
    "```\n",
    "\n",
    "#### ë³€ê²½ 3: Enhanced IDF ì‚¬ìš©\n",
    "```python\n",
    "# IDF ê³„ì‚° í›„ ë™ì˜ì–´ë¡œ ê°•í™”\n",
    "idf_token_dict_enhanced = create_synonym_aware_idf(\n",
    "    idf_token_dict,\n",
    "    tokenizer,\n",
    "    KOREAN_SYNONYMS,\n",
    "    method='max'\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •\n",
    "\n",
    "```python\n",
    "# ì¶”ì²œ ì„¤ì •\n",
    "LEARNING_RATE = 1e-5      # KD ì‹œ ë‚®ê²Œ ì„¤ì •\n",
    "NUM_EPOCHS = 5            # ë” ë§ì€ epoch\n",
    "BATCH_SIZE = 16           # ë©”ëª¨ë¦¬ í—ˆìš© ì‹œ ì¦ê°€\n",
    "LAMBDA_KD = 0.3           # KD loss ê°€ì¤‘ì¹˜\n",
    "TEMPERATURE = 2.0         # Distillation temperature\n",
    "```\n",
    "\n",
    "### 3. í‰ê°€ ì¶”ê°€\n",
    "\n",
    "```python\n",
    "# ë™ì˜ì–´ ë§¤ì¹­ ì„±ëŠ¥ í‰ê°€\n",
    "synonym_test_cases = [\n",
    "    (\"ì¸ê³µì§€ëŠ¥\", \"AI\"),\n",
    "    (\"ë”¥ëŸ¬ë‹\", \"ì‹¬ì¸µí•™ìŠµ\"),\n",
    "    (\"ê²€ìƒ‰\", \"íƒìƒ‰\"),\n",
    "]\n",
    "\n",
    "for word1, word2 in synonym_test_cases:\n",
    "    vec1 = encode_query_inference_free(word1, tokenizer, idf_dict)\n",
    "    vec2 = encode_query_inference_free(word2, tokenizer, idf_dict)\n",
    "    similarity = np.dot(vec1, vec2)\n",
    "    print(f\"{word1} <-> {word2}: {similarity:.4f}\")\n",
    "```\n",
    "\n",
    "## ì˜ˆìƒ í•™ìŠµ ì‹œê°„\n",
    "\n",
    "- GPU: Tesla T4 ê¸°ì¤€\n",
    "- ë°ì´í„°: 30,000 pairs\n",
    "- Epoch: 5íšŒ\n",
    "- ì˜ˆìƒ ì‹œê°„: ì•½ 2-3ì‹œê°„\n",
    "\n",
    "## ì²´í¬ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "- [ ] Teacher ëª¨ë¸ ë¡œë“œ í™•ì¸\n",
    "- [ ] Enhanced IDF ìƒì„±\n",
    "- [ ] KD loss êµ¬í˜„\n",
    "- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •\n",
    "- [ ] ë™ì˜ì–´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¤€ë¹„\n",
    "- [ ] í•™ìŠµ ì „ ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •\n",
    "- [ ] í•™ìŠµ í›„ ì„±ëŠ¥ ë¹„êµ\n",
    "\"\"\"\n",
    "\n",
    "print(retraining_guide)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

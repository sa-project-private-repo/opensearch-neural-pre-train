{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v10 Data Preparation: Large-Scale (10M+ samples)\n",
    "\n",
    "v8/v9의 데이터 부족 문제를 해결하기 위해 대규모 데이터셋을 구축합니다.\n",
    "\n",
    "## 핵심 변경사항\n",
    "1. **직접 KO-EN 쌍 사용**: 클러스터링에 의존하지 않음\n",
    "2. **다중 데이터셋**: OPUS-100 + CCMatrix + Tatoeba 등\n",
    "3. **10M+ 샘플 목표**: 대규모 학습 데이터\n",
    "4. **필터링 완화**: 더 많은 데이터 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_project_root():\n",
    "    candidates = [\n",
    "        Path.cwd(),\n",
    "        Path.cwd().parent,\n",
    "        Path.cwd().parent.parent,\n",
    "        Path(\"/home/west/Documents/cursor-workspace/opensearch-neural-pre-train\"),\n",
    "    ]\n",
    "    for candidate in candidates:\n",
    "        if (candidate / \"CLAUDE.md\").exists() or (candidate / \".git\").exists():\n",
    "            return candidate\n",
    "    return Path(\"/home/west/Documents/cursor-workspace/opensearch-neural-pre-train\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set random seed\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Multiple Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'target_samples': 10_000_000,  # 10M target\n",
    "    'min_ko_len': 2,\n",
    "    'max_ko_len': 100,\n",
    "    'min_en_len': 2,\n",
    "    'max_en_len': 150,\n",
    "    'output_dir': PROJECT_ROOT / 'dataset' / 'v10_large_scale',\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs_from_dataset(dataset, ko_key, en_key, max_samples=None, desc=\"Extracting\"):\n",
    "    \"\"\"\n",
    "    Extract KO-EN pairs from a dataset.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    iterator = tqdm(dataset, desc=desc)\n",
    "    \n",
    "    for i, sample in enumerate(iterator):\n",
    "        if max_samples and i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Handle nested structure\n",
    "            if 'translation' in sample:\n",
    "                ko_text = sample['translation'].get('ko', sample['translation'].get('kor', ''))\n",
    "                en_text = sample['translation'].get('en', sample['translation'].get('eng', ''))\n",
    "            else:\n",
    "                ko_text = sample.get(ko_key, '')\n",
    "                en_text = sample.get(en_key, '')\n",
    "            \n",
    "            if not ko_text or not en_text:\n",
    "                continue\n",
    "            \n",
    "            ko_text = ko_text.strip()\n",
    "            en_text = en_text.strip()\n",
    "            \n",
    "            # Basic filtering\n",
    "            if CONFIG['min_ko_len'] <= len(ko_text) <= CONFIG['max_ko_len']:\n",
    "                if CONFIG['min_en_len'] <= len(en_text) <= CONFIG['max_en_len']:\n",
    "                    pairs.append({\n",
    "                        'ko': ko_text,\n",
    "                        'en': en_text,\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "print(\"Helper function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OPUS-100 (Full dataset - 1M samples)\n",
    "print(\"=\"*70)\n",
    "print(\"Loading OPUS-100 Korean-English...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    opus_dataset = load_dataset(\"opus100\", \"en-ko\", split=\"train\", trust_remote_code=True)\n",
    "    print(f\"OPUS-100 loaded: {len(opus_dataset):,} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    opus_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ko\", split=\"train\")\n",
    "    print(f\"OPUS-100 (Helsinki) loaded: {len(opus_dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all pairs from OPUS-100\n",
    "opus_pairs = extract_pairs_from_dataset(\n",
    "    opus_dataset, \n",
    "    ko_key='ko', \n",
    "    en_key='en',\n",
    "    max_samples=None,  # Use all\n",
    "    desc=\"OPUS-100\"\n",
    ")\n",
    "print(f\"\\nExtracted {len(opus_pairs):,} pairs from OPUS-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tatoeba (High quality sentence pairs)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Loading Tatoeba Korean-English...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    tatoeba_dataset = load_dataset(\"tatoeba\", lang1=\"en\", lang2=\"ko\", split=\"train\")\n",
    "    print(f\"Tatoeba loaded: {len(tatoeba_dataset):,} samples\")\n",
    "    \n",
    "    tatoeba_pairs = []\n",
    "    for sample in tqdm(tatoeba_dataset, desc=\"Tatoeba\"):\n",
    "        try:\n",
    "            en_text = sample['translation']['en'].strip()\n",
    "            ko_text = sample['translation']['ko'].strip()\n",
    "            if ko_text and en_text:\n",
    "                if CONFIG['min_ko_len'] <= len(ko_text) <= CONFIG['max_ko_len']:\n",
    "                    if CONFIG['min_en_len'] <= len(en_text) <= CONFIG['max_en_len']:\n",
    "                        tatoeba_pairs.append({'ko': ko_text, 'en': en_text})\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Extracted {len(tatoeba_pairs):,} pairs from Tatoeba\")\n",
    "except Exception as e:\n",
    "    print(f\"Tatoeba not available: {e}\")\n",
    "    tatoeba_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load CCMatrix (Large-scale mining dataset)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Loading CCMatrix Korean-English...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ccmatrix_pairs = []\n",
    "try:\n",
    "    # CCMatrix can be very large, so we limit\n",
    "    ccmatrix_dataset = load_dataset(\n",
    "        \"yhavinga/ccmatrix\", \n",
    "        \"en-ko\",\n",
    "        split=\"train\",\n",
    "        streaming=True  # Use streaming for large dataset\n",
    "    )\n",
    "    \n",
    "    print(\"CCMatrix loading in streaming mode...\")\n",
    "    \n",
    "    max_ccmatrix = 5_000_000  # Limit to 5M from CCMatrix\n",
    "    for i, sample in enumerate(tqdm(ccmatrix_dataset, desc=\"CCMatrix\", total=max_ccmatrix)):\n",
    "        if i >= max_ccmatrix:\n",
    "            break\n",
    "        try:\n",
    "            en_text = sample['translation']['en'].strip()\n",
    "            ko_text = sample['translation']['ko'].strip()\n",
    "            if ko_text and en_text:\n",
    "                if CONFIG['min_ko_len'] <= len(ko_text) <= CONFIG['max_ko_len']:\n",
    "                    if CONFIG['min_en_len'] <= len(en_text) <= CONFIG['max_en_len']:\n",
    "                        ccmatrix_pairs.append({'ko': ko_text, 'en': en_text})\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Extracted {len(ccmatrix_pairs):,} pairs from CCMatrix\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"CCMatrix not available: {e}\")\n",
    "    print(\"Continuing with other datasets...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try additional datasets\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Trying additional datasets...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "additional_pairs = []\n",
    "\n",
    "# Try OPUS Books\n",
    "try:\n",
    "    print(\"\\nLoading OPUS Books...\")\n",
    "    opus_books = load_dataset(\"opus_books\", \"en-ko\", split=\"train\")\n",
    "    for sample in tqdm(opus_books, desc=\"OPUS Books\"):\n",
    "        try:\n",
    "            en_text = sample['translation']['en'].strip()\n",
    "            ko_text = sample['translation']['ko'].strip()\n",
    "            if ko_text and en_text:\n",
    "                if CONFIG['min_ko_len'] <= len(ko_text) <= CONFIG['max_ko_len']:\n",
    "                    additional_pairs.append({'ko': ko_text, 'en': en_text})\n",
    "        except:\n",
    "            continue\n",
    "    print(f\"OPUS Books: {len(additional_pairs):,} pairs so far\")\n",
    "except Exception as e:\n",
    "    print(f\"OPUS Books not available: {e}\")\n",
    "\n",
    "# Try OpenSubtitles\n",
    "try:\n",
    "    print(\"\\nLoading OpenSubtitles...\")\n",
    "    opensubtitles = load_dataset(\"open_subtitles\", lang1=\"en\", lang2=\"ko\", split=\"train\", streaming=True)\n",
    "    \n",
    "    count = 0\n",
    "    max_subs = 3_000_000\n",
    "    for sample in tqdm(opensubtitles, desc=\"OpenSubtitles\", total=max_subs):\n",
    "        if count >= max_subs:\n",
    "            break\n",
    "        try:\n",
    "            en_text = sample['translation']['en'].strip()\n",
    "            ko_text = sample['translation']['ko'].strip()\n",
    "            if ko_text and en_text:\n",
    "                if CONFIG['min_ko_len'] <= len(ko_text) <= CONFIG['max_ko_len']:\n",
    "                    additional_pairs.append({'ko': ko_text, 'en': en_text})\n",
    "                    count += 1\n",
    "        except:\n",
    "            continue\n",
    "    print(f\"After OpenSubtitles: {len(additional_pairs):,} additional pairs\")\n",
    "except Exception as e:\n",
    "    print(f\"OpenSubtitles not available: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all pairs\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMBINING ALL DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_pairs = []\n",
    "all_pairs.extend(opus_pairs)\n",
    "print(f\"After OPUS-100: {len(all_pairs):,}\")\n",
    "\n",
    "if tatoeba_pairs:\n",
    "    all_pairs.extend(tatoeba_pairs)\n",
    "    print(f\"After Tatoeba: {len(all_pairs):,}\")\n",
    "\n",
    "if ccmatrix_pairs:\n",
    "    all_pairs.extend(ccmatrix_pairs)\n",
    "    print(f\"After CCMatrix: {len(all_pairs):,}\")\n",
    "\n",
    "if additional_pairs:\n",
    "    all_pairs.extend(additional_pairs)\n",
    "    print(f\"After additional: {len(all_pairs):,}\")\n",
    "\n",
    "print(f\"\\nTotal raw pairs: {len(all_pairs):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deduplication and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates based on Korean text\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DEDUPLICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "seen_ko = set()\n",
    "unique_pairs = []\n",
    "\n",
    "for pair in tqdm(all_pairs, desc=\"Deduplicating\"):\n",
    "    ko_normalized = pair['ko'].lower().strip()\n",
    "    if ko_normalized not in seen_ko:\n",
    "        seen_ko.add(ko_normalized)\n",
    "        unique_pairs.append(pair)\n",
    "\n",
    "print(f\"Before deduplication: {len(all_pairs):,}\")\n",
    "print(f\"After deduplication: {len(unique_pairs):,}\")\n",
    "print(f\"Removed: {len(all_pairs) - len(unique_pairs):,} duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_pair(pair: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Additional validation for pair quality.\n",
    "    \"\"\"\n",
    "    ko = pair['ko']\n",
    "    en = pair['en']\n",
    "    \n",
    "    # Check if Korean text has Korean characters\n",
    "    has_korean = any('\\uac00' <= c <= '\\ud7a3' for c in ko)\n",
    "    if not has_korean:\n",
    "        return False\n",
    "    \n",
    "    # Check if English text has English characters\n",
    "    has_english = any(c.isalpha() and c.isascii() for c in en)\n",
    "    if not has_english:\n",
    "        return False\n",
    "    \n",
    "    # Skip if too many special characters\n",
    "    special_ratio_ko = sum(1 for c in ko if not c.isalnum() and c != ' ') / len(ko)\n",
    "    special_ratio_en = sum(1 for c in en if not c.isalnum() and c != ' ') / len(en)\n",
    "    \n",
    "    if special_ratio_ko > 0.5 or special_ratio_en > 0.5:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Apply validation\n",
    "print(\"\\nApplying quality validation...\")\n",
    "valid_pairs = [p for p in tqdm(unique_pairs, desc=\"Validating\") if is_valid_pair(p)]\n",
    "\n",
    "print(f\"After validation: {len(valid_pairs):,}\")\n",
    "print(f\"Removed: {len(unique_pairs) - len(valid_pairs):,} invalid pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "print(\"\\nShuffling...\")\n",
    "random.shuffle(valid_pairs)\n",
    "\n",
    "# Preview\n",
    "print(\"\\nSample pairs:\")\n",
    "for i in range(min(10, len(valid_pairs))):\n",
    "    print(f\"  [{i}] {valid_pairs[i]['ko'][:50]}... → {valid_pairs[i]['en'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Augmentation (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we need augmentation\n",
    "current_count = len(valid_pairs)\n",
    "target_count = CONFIG['target_samples']\n",
    "\n",
    "print(f\"Current samples: {current_count:,}\")\n",
    "print(f\"Target samples: {target_count:,}\")\n",
    "\n",
    "if current_count < target_count:\n",
    "    print(f\"\\nNeed {target_count - current_count:,} more samples\")\n",
    "    print(\"Proceeding with current dataset...\")\n",
    "else:\n",
    "    print(f\"\\nTarget reached! Using {current_count:,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final training data\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPARING TRAINING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create output directory\n",
    "CONFIG['output_dir'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# For v10, we'll use direct KO-EN pairs without complex synonym structure\n",
    "# This simplifies training and allows for more data\n",
    "\n",
    "training_data = []\n",
    "for pair in tqdm(valid_pairs, desc=\"Preparing training data\"):\n",
    "    training_data.append({\n",
    "        'ko_term': pair['ko'],\n",
    "        'en_term': pair['en'],\n",
    "    })\n",
    "\n",
    "print(f\"\\nTotal training samples: {len(training_data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/val\n",
    "val_ratio = 0.01  # 1% validation\n",
    "val_size = int(len(training_data) * val_ratio)\n",
    "train_size = len(training_data) - val_size\n",
    "\n",
    "train_data = training_data[:train_size]\n",
    "val_data = training_data[train_size:]\n",
    "\n",
    "print(f\"Train size: {len(train_data):,}\")\n",
    "print(f\"Val size: {len(val_data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training data\n",
    "train_path = CONFIG['output_dir'] / 'train.jsonl'\n",
    "val_path = CONFIG['output_dir'] / 'val.jsonl'\n",
    "\n",
    "print(f\"\\nSaving training data to {train_path}...\")\n",
    "with open(train_path, 'w', encoding='utf-8') as f:\n",
    "    for item in tqdm(train_data, desc=\"Writing train\"):\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Saving validation data to {val_path}...\")\n",
    "with open(val_path, 'w', encoding='utf-8') as f:\n",
    "    for item in tqdm(val_data, desc=\"Writing val\"):\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(\"\\nData saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata\n",
    "metadata = {\n",
    "    'total_samples': len(training_data),\n",
    "    'train_samples': len(train_data),\n",
    "    'val_samples': len(val_data),\n",
    "    'sources': ['OPUS-100', 'Tatoeba', 'CCMatrix', 'OpenSubtitles'],\n",
    "    'config': {k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items()},\n",
    "}\n",
    "\n",
    "with open(CONFIG['output_dir'] / 'metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Metadata saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "ko_lengths = [len(p['ko_term']) for p in training_data]\n",
    "en_lengths = [len(p['en_term']) for p in training_data]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nKorean text length:\")\n",
    "print(f\"  Min: {min(ko_lengths)}\")\n",
    "print(f\"  Max: {max(ko_lengths)}\")\n",
    "print(f\"  Mean: {np.mean(ko_lengths):.1f}\")\n",
    "print(f\"  Median: {np.median(ko_lengths):.1f}\")\n",
    "\n",
    "print(f\"\\nEnglish text length:\")\n",
    "print(f\"  Min: {min(en_lengths)}\")\n",
    "print(f\"  Max: {max(en_lengths)}\")\n",
    "print(f\"  Mean: {np.mean(en_lengths):.1f}\")\n",
    "print(f\"  Median: {np.median(en_lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(ko_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Korean Text Length (chars)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Korean Text Length Distribution')\n",
    "axes[0].axvline(np.mean(ko_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(ko_lengths):.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(en_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('English Text Length (chars)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('English Text Length Distribution')\n",
    "axes[1].axvline(np.mean(en_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(en_lengths):.1f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['output_dir'] / 'length_distribution.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"v10 DATA PREPARATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nTotal samples: {len(training_data):,}\")\n",
    "print(f\"  - Train: {len(train_data):,}\")\n",
    "print(f\"  - Val: {len(val_data):,}\")\n",
    "\n",
    "print(f\"\\nOutput directory: {CONFIG['output_dir']}\")\n",
    "print(f\"Files:\")\n",
    "for f in CONFIG['output_dir'].iterdir():\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  - {f.name}: {size_mb:.1f} MB\")\n",
    "\n",
    "print(f\"\\nNext step: Run 02_training.ipynb for model training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

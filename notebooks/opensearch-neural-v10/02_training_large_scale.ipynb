{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v10 Training: Large-Scale Cross-Lingual Neural Sparse Model\n",
    "\n",
    "10M+ 대규모 데이터셋으로 학습하는 cross-lingual 모델입니다.\n",
    "\n",
    "## v10 특징\n",
    "- **대규모 데이터**: 10M+ KO-EN 병렬 쌍\n",
    "- **개선된 Loss**: v9 기반 + 대규모 최적화\n",
    "- **효율적 학습**: Gradient accumulation, Mixed precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    candidates = [\n",
    "        Path.cwd(),\n",
    "        Path.cwd().parent,\n",
    "        Path.cwd().parent.parent,\n",
    "        Path(\"/home/west/Documents/cursor-workspace/opensearch-neural-pre-train\"),\n",
    "    ]\n",
    "    for candidate in candidates:\n",
    "        if (candidate / \"CLAUDE.md\").exists() or (candidate / \".git\").exists():\n",
    "            return candidate\n",
    "    return Path(\"/home/west/Documents/cursor-workspace/opensearch-neural-pre-train\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.model.splade_model import create_splade_model\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Model\n",
    "    'model_name': 'bert-base-multilingual-cased',\n",
    "    'max_length': 64,\n",
    "    \n",
    "    # Data\n",
    "    'data_dir': PROJECT_ROOT / 'dataset' / 'v10_large_scale',\n",
    "    \n",
    "    # Training\n",
    "    'batch_size': 128,           # Larger batch for large dataset\n",
    "    'gradient_accumulation': 4,  # Effective batch = 512\n",
    "    'num_epochs': 3,             # Fewer epochs for large data\n",
    "    'learning_rate': 5e-5,       # Slightly higher LR\n",
    "    'warmup_ratio': 0.05,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'use_amp': True,             # Mixed precision training\n",
    "    \n",
    "    # Loss weights (v9 optimized)\n",
    "    'lambda_self': 0.3,\n",
    "    'lambda_target': 2.0,\n",
    "    'lambda_margin': 1.0,\n",
    "    'lambda_negative': 0.5,\n",
    "    'lambda_sparsity': 0.01,\n",
    "    \n",
    "    # Margin\n",
    "    'target_margin': 1.5,\n",
    "    \n",
    "    # Logging\n",
    "    'log_interval': 1000,\n",
    "    'eval_interval': 10000,\n",
    "    'save_interval': 50000,\n",
    "    \n",
    "    # Output\n",
    "    'output_dir': PROJECT_ROOT / 'outputs' / 'v10_large_scale',\n",
    "}\n",
    "\n",
    "print(\"v10 Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Non-Target Language Token Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_non_target_token(token: str) -> bool:\n",
    "    \"\"\"Check if token is from non-target language.\"\"\"\n",
    "    clean = token.replace('##', '')\n",
    "    if not clean:\n",
    "        return False\n",
    "    \n",
    "    has_korean = any('\\uac00' <= c <= '\\ud7a3' for c in clean)\n",
    "    has_english = any(c.isalpha() and c.isascii() for c in clean)\n",
    "    \n",
    "    if has_korean or has_english:\n",
    "        return False\n",
    "    \n",
    "    has_japanese = any('\\u3040' <= c <= '\\u309f' or '\\u30a0' <= c <= '\\u30ff' for c in clean)\n",
    "    has_cjk = any('\\u4e00' <= c <= '\\u9fff' for c in clean)\n",
    "    has_cyrillic = any('\\u0400' <= c <= '\\u04ff' for c in clean)\n",
    "    has_arabic = any('\\u0600' <= c <= '\\u06ff' for c in clean)\n",
    "    has_thai = any('\\u0e00' <= c <= '\\u0e7f' for c in clean)\n",
    "    \n",
    "    return has_japanese or has_cjk or has_cyrillic or has_arabic or has_thai\n",
    "\n",
    "# Build non-target token IDs\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "\n",
    "print(\"Building non-target language token ID list...\")\n",
    "non_target_ids = []\n",
    "for token_id in tqdm(range(tokenizer.vocab_size)):\n",
    "    token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "    if is_non_target_token(token):\n",
    "        non_target_ids.append(token_id)\n",
    "\n",
    "non_target_ids_tensor = torch.tensor(non_target_ids, dtype=torch.long)\n",
    "print(f\"Found {len(non_target_ids):,} non-target language tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeScaleDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Large-scale KO-EN parallel dataset for v10 training.\n",
    "    Loads data from JSONL file.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: Path, tokenizer, max_length: int = 64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "        \n",
    "        print(f\"Loading dataset from {data_path}...\")\n",
    "        \n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            for line in tqdm(f, desc=\"Loading\"):\n",
    "                item = json.loads(line.strip())\n",
    "                \n",
    "                ko_term = item['ko_term']\n",
    "                en_term = item['en_term']\n",
    "                \n",
    "                # Pre-tokenize for efficiency\n",
    "                ko_tokens = tokenizer.tokenize(ko_term)\n",
    "                ko_token_ids = tokenizer.convert_tokens_to_ids(ko_tokens)\n",
    "                ko_token_ids = [tid for tid in ko_token_ids if tid != tokenizer.unk_token_id]\n",
    "                \n",
    "                en_tokens = tokenizer.tokenize(en_term.lower())\n",
    "                en_token_ids = tokenizer.convert_tokens_to_ids(en_tokens)\n",
    "                en_token_ids = [tid for tid in en_token_ids if tid != tokenizer.unk_token_id]\n",
    "                \n",
    "                if ko_token_ids and en_token_ids:\n",
    "                    self.data.append({\n",
    "                        'ko_term': ko_term,\n",
    "                        'en_term': en_term,\n",
    "                        'ko_token_ids': ko_token_ids,\n",
    "                        'en_token_ids': en_token_ids,\n",
    "                    })\n",
    "        \n",
    "        print(f\"Loaded {len(self.data):,} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            item['ko_term'],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'ko_token_ids': item['ko_token_ids'],\n",
    "            'en_token_ids': item['en_token_ids'],\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'ko_token_ids': [item['ko_token_ids'] for item in batch],\n",
    "        'en_token_ids': [item['en_token_ids'] for item in batch],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data exists\n",
    "train_path = CONFIG['data_dir'] / 'train.jsonl'\n",
    "val_path = CONFIG['data_dir'] / 'val.jsonl'\n",
    "\n",
    "if not train_path.exists():\n",
    "    print(f\"Training data not found at {train_path}\")\n",
    "    print(\"Please run 01_data_preparation_large_scale.ipynb first!\")\n",
    "else:\n",
    "    # Load datasets\n",
    "    train_dataset = LargeScaleDataset(train_path, tokenizer, CONFIG['max_length'])\n",
    "    \n",
    "    if val_path.exists():\n",
    "        val_dataset = LargeScaleDataset(val_path, tokenizer, CONFIG['max_length'])\n",
    "    else:\n",
    "        val_dataset = None\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTrain dataset: {len(train_dataset):,} samples\")\n",
    "    print(f\"Batches per epoch: {len(train_loader):,}\")\n",
    "    print(f\"Effective batch size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class V10CrossLingualLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    v10 Large-Scale Loss: Optimized for large datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_margin: float = 1.5, non_target_ids: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.target_margin = target_margin\n",
    "        self.non_target_ids = non_target_ids\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        sparse_rep: torch.Tensor,\n",
    "        ko_token_ids: list,\n",
    "        en_token_ids: list,\n",
    "    ) -> dict:\n",
    "        batch_size = sparse_rep.shape[0]\n",
    "        device = sparse_rep.device\n",
    "        \n",
    "        self_loss = torch.tensor(0.0, device=device)\n",
    "        target_loss = torch.tensor(0.0, device=device)\n",
    "        margin_loss = torch.tensor(0.0, device=device)\n",
    "        negative_loss = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        n_valid = 0\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            rep = sparse_rep[i]\n",
    "            \n",
    "            # 1. Self-preservation loss (Korean tokens)\n",
    "            if ko_token_ids[i]:\n",
    "                ko_ids = torch.tensor(ko_token_ids[i], device=device)\n",
    "                ko_activations = rep[ko_ids]\n",
    "                self_loss = self_loss - torch.log(ko_activations + 1e-8).mean()\n",
    "            \n",
    "            # 2. English target loss\n",
    "            if en_token_ids[i]:\n",
    "                en_ids = torch.tensor(en_token_ids[i], device=device)\n",
    "                en_activations = rep[en_ids]\n",
    "                target_loss = target_loss - torch.log(en_activations + 1e-8).mean()\n",
    "                margin_loss = margin_loss + F.relu(self.target_margin - en_activations).mean()\n",
    "            \n",
    "            # 3. Negative sampling loss\n",
    "            if self.non_target_ids is not None:\n",
    "                # Sample subset for efficiency\n",
    "                sample_size = min(1000, len(self.non_target_ids))\n",
    "                sample_idx = torch.randperm(len(self.non_target_ids))[:sample_size]\n",
    "                sampled_ids = self.non_target_ids[sample_idx].to(device)\n",
    "                non_target_activations = rep[sampled_ids]\n",
    "                negative_loss = negative_loss + F.relu(non_target_activations - 0.1).mean()\n",
    "            \n",
    "            n_valid += 1\n",
    "        \n",
    "        if n_valid > 0:\n",
    "            self_loss = self_loss / n_valid\n",
    "            target_loss = target_loss / n_valid\n",
    "            margin_loss = margin_loss / n_valid\n",
    "            negative_loss = negative_loss / n_valid\n",
    "        \n",
    "        return {\n",
    "            'self': self_loss,\n",
    "            'target': target_loss,\n",
    "            'margin': margin_loss,\n",
    "            'negative': negative_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = create_splade_model(\n",
    "    model_name=CONFIG['model_name'],\n",
    "    use_idf=False,\n",
    "    use_expansion=True,\n",
    "    expansion_mode='mlm',\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model: {CONFIG['model_name']}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, optimizer, scheduler\n",
    "loss_fn = V10CrossLingualLoss(\n",
    "    target_margin=CONFIG['target_margin'],\n",
    "    non_target_ids=non_target_ids_tensor\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "total_steps = len(train_loader) * CONFIG['num_epochs'] // CONFIG['gradient_accumulation']\n",
    "warmup_steps = int(total_steps * CONFIG['warmup_ratio'])\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler() if CONFIG['use_amp'] else None\n",
    "\n",
    "print(f\"Total steps: {total_steps:,}\")\n",
    "print(f\"Warmup steps: {warmup_steps:,}\")\n",
    "print(f\"Mixed precision: {CONFIG['use_amp']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PAIRS = [\n",
    "    (\"머신러닝\", [\"machine\", \"learning\"]),\n",
    "    (\"딥러닝\", [\"deep\", \"learning\"]),\n",
    "    (\"자연어처리\", [\"natural\", \"language\", \"processing\"]),\n",
    "    (\"인공지능\", [\"artificial\", \"intelligence\"]),\n",
    "    (\"데이터\", [\"data\"]),\n",
    "    (\"컴퓨터\", [\"computer\"]),\n",
    "    (\"네트워크\", [\"network\"]),\n",
    "    (\"알고리즘\", [\"algorithm\"]),\n",
    "    (\"프로그래밍\", [\"programming\"]),\n",
    "    (\"소프트웨어\", [\"software\"]),\n",
    "]\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, device, top_k=50):\n",
    "    \"\"\"Quick evaluation on test pairs.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    ko_activated_total = 0\n",
    "    en_activated_total = 0\n",
    "    ko_expected_total = 0\n",
    "    en_expected_total = 0\n",
    "    non_target_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for ko_term, en_expected in TEST_PAIRS:\n",
    "            encoding = tokenizer(\n",
    "                ko_term,\n",
    "                max_length=64,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            sparse_rep, _ = model(\n",
    "                encoding['input_ids'].to(device),\n",
    "                encoding['attention_mask'].to(device)\n",
    "            )\n",
    "            \n",
    "            sparse_rep = sparse_rep[0].cpu()\n",
    "            top_indices = torch.topk(sparse_rep, k=top_k).indices.tolist()\n",
    "            top_tokens = set(tokenizer.convert_ids_to_tokens(top_indices))\n",
    "            \n",
    "            # Korean preservation\n",
    "            ko_input_tokens = set(tokenizer.tokenize(ko_term))\n",
    "            for tok in ko_input_tokens:\n",
    "                ko_expected_total += 1\n",
    "                if tok in top_tokens:\n",
    "                    ko_activated_total += 1\n",
    "            \n",
    "            # English activation\n",
    "            for en in en_expected:\n",
    "                en_toks = tokenizer.tokenize(en.lower())\n",
    "                for tok in en_toks:\n",
    "                    en_expected_total += 1\n",
    "                    if tok in top_tokens:\n",
    "                        en_activated_total += 1\n",
    "            \n",
    "            # Non-target count\n",
    "            for idx in top_indices:\n",
    "                tok = tokenizer.convert_ids_to_tokens(idx)\n",
    "                if is_non_target_token(tok):\n",
    "                    non_target_total += 1\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return {\n",
    "        'ko_rate': ko_activated_total / ko_expected_total * 100 if ko_expected_total > 0 else 0,\n",
    "        'en_rate': en_activated_total / en_expected_total * 100 if en_expected_total > 0 else 0,\n",
    "        'avg_non_target': non_target_total / len(TEST_PAIRS),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial evaluation\n",
    "print(\"Initial evaluation (before training):\")\n",
    "init_eval = evaluate_model(model, tokenizer, device)\n",
    "print(f\"  Korean preservation: {init_eval['ko_rate']:.1f}%\")\n",
    "print(f\"  English activation: {init_eval['en_rate']:.1f}%\")\n",
    "print(f\"  Avg non-target: {init_eval['avg_non_target']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "CONFIG['output_dir'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training history\n",
    "history = []\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STARTING v10 LARGE-SCALE TRAINING\")\n",
    "print(f\"Dataset: {len(train_dataset):,} samples\")\n",
    "print(f\"Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']} x {CONFIG['gradient_accumulation']} = {CONFIG['batch_size'] * CONFIG['gradient_accumulation']}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "best_en_rate = 0\n",
    "\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EPOCH {epoch + 1}/{CONFIG['num_epochs']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    model.train()\n",
    "    epoch_losses = {'total': 0, 'self': 0, 'target': 0, 'margin': 0, 'negative': 0}\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        if CONFIG['use_amp']:\n",
    "            with autocast():\n",
    "                sparse_rep, _ = model(input_ids, attention_mask)\n",
    "                losses = loss_fn(sparse_rep, batch['ko_token_ids'], batch['en_token_ids'])\n",
    "                sparsity_loss = sparse_rep.mean()\n",
    "                \n",
    "                total_loss = (\n",
    "                    CONFIG['lambda_self'] * losses['self'] +\n",
    "                    CONFIG['lambda_target'] * losses['target'] +\n",
    "                    CONFIG['lambda_margin'] * losses['margin'] +\n",
    "                    CONFIG['lambda_negative'] * losses['negative'] +\n",
    "                    CONFIG['lambda_sparsity'] * sparsity_loss\n",
    "                ) / CONFIG['gradient_accumulation']\n",
    "            \n",
    "            scaler.scale(total_loss).backward()\n",
    "        else:\n",
    "            sparse_rep, _ = model(input_ids, attention_mask)\n",
    "            losses = loss_fn(sparse_rep, batch['ko_token_ids'], batch['en_token_ids'])\n",
    "            sparsity_loss = sparse_rep.mean()\n",
    "            \n",
    "            total_loss = (\n",
    "                CONFIG['lambda_self'] * losses['self'] +\n",
    "                CONFIG['lambda_target'] * losses['target'] +\n",
    "                CONFIG['lambda_margin'] * losses['margin'] +\n",
    "                CONFIG['lambda_negative'] * losses['negative'] +\n",
    "                CONFIG['lambda_sparsity'] * sparsity_loss\n",
    "            ) / CONFIG['gradient_accumulation']\n",
    "            \n",
    "            total_loss.backward()\n",
    "        \n",
    "        # Accumulate losses\n",
    "        epoch_losses['total'] += total_loss.item() * CONFIG['gradient_accumulation']\n",
    "        epoch_losses['self'] += losses['self'].item()\n",
    "        epoch_losses['target'] += losses['target'].item()\n",
    "        epoch_losses['margin'] += losses['margin'].item()\n",
    "        epoch_losses['negative'] += losses['negative'].item()\n",
    "        \n",
    "        # Gradient accumulation step\n",
    "        if (batch_idx + 1) % CONFIG['gradient_accumulation'] == 0:\n",
    "            if CONFIG['use_amp']:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n",
    "                optimizer.step()\n",
    "            \n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        if (batch_idx + 1) % CONFIG['log_interval'] == 0:\n",
    "            avg_loss = epoch_losses['total'] / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{avg_loss:.4f}\",\n",
    "                'tgt': f\"{epoch_losses['target']/(batch_idx+1):.4f}\",\n",
    "                'step': global_step,\n",
    "            })\n",
    "        \n",
    "        # Periodic evaluation\n",
    "        if (batch_idx + 1) % CONFIG['eval_interval'] == 0:\n",
    "            eval_result = evaluate_model(model, tokenizer, device)\n",
    "            print(f\"\\n  Step {global_step}: KO={eval_result['ko_rate']:.1f}%, EN={eval_result['en_rate']:.1f}%, Non-target={eval_result['avg_non_target']:.1f}\")\n",
    "            \n",
    "            if eval_result['en_rate'] > best_en_rate:\n",
    "                best_en_rate = eval_result['en_rate']\n",
    "                # Save best model\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'eval': eval_result,\n",
    "                    'step': global_step,\n",
    "                }, CONFIG['output_dir'] / 'best_model.pt')\n",
    "                print(f\"  New best model saved! EN rate: {best_en_rate:.1f}%\")\n",
    "        \n",
    "        # Periodic checkpoint\n",
    "        if (batch_idx + 1) % CONFIG['save_interval'] == 0:\n",
    "            checkpoint_path = CONFIG['output_dir'] / f'checkpoint_step{global_step}.pt'\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'step': global_step,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, checkpoint_path)\n",
    "            print(f\"  Checkpoint saved: {checkpoint_path}\")\n",
    "    \n",
    "    # End of epoch\n",
    "    n_batches = len(train_loader)\n",
    "    for key in epoch_losses:\n",
    "        epoch_losses[key] /= n_batches\n",
    "    \n",
    "    history.append(epoch_losses)\n",
    "    \n",
    "    # Epoch evaluation\n",
    "    eval_result = evaluate_model(model, tokenizer, device)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"  Total Loss: {epoch_losses['total']:.4f}\")\n",
    "    print(f\"  Target Loss: {epoch_losses['target']:.4f}\")\n",
    "    print(f\"  Korean Preservation: {eval_result['ko_rate']:.1f}%\")\n",
    "    print(f\"  English Activation: {eval_result['en_rate']:.1f}%\")\n",
    "    print(f\"  Avg Non-target: {eval_result['avg_non_target']:.1f}\")\n",
    "    \n",
    "    # Save epoch checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'losses': epoch_losses,\n",
    "        'eval': eval_result,\n",
    "    }, CONFIG['output_dir'] / f'checkpoint_epoch{epoch + 1}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_path = CONFIG['output_dir'] / 'final_model.pt'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': {k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items()},\n",
    "    'history': history,\n",
    "}, final_path)\n",
    "\n",
    "print(f\"Final model saved: {final_path}\")\n",
    "\n",
    "# Save training history\n",
    "with open(CONFIG['output_dir'] / 'training_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "final_eval = evaluate_model(model, tokenizer, device)\n",
    "\n",
    "print(f\"\\nKorean Preservation: {final_eval['ko_rate']:.1f}%\")\n",
    "print(f\"English Activation: {final_eval['en_rate']:.1f}%\")\n",
    "print(f\"Avg Non-target: {final_eval['avg_non_target']:.1f}\")\n",
    "print(f\"\\nBest English Activation: {best_en_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "if len(history) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    epochs = range(1, len(history) + 1)\n",
    "    \n",
    "    axes[0].plot(epochs, [h['total'] for h in history], 'b-', label='Total', linewidth=2)\n",
    "    axes[0].plot(epochs, [h['target'] for h in history], 'r--', label='Target')\n",
    "    axes[0].plot(epochs, [h['self'] for h in history], 'g--', label='Self')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('v10 Training Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    axes[1].plot(epochs, [h['margin'] for h in history], 'c-', label='Margin')\n",
    "    axes[1].plot(epochs, [h['negative'] for h in history], 'm-', label='Negative')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].set_title('v10 Component Losses')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CONFIG['output_dir'] / 'training_curves.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"v10 TRAINING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nOutput directory: {CONFIG['output_dir']}\")\n",
    "print(f\"\\nNext step: Run 03_inference_test.ipynb for detailed evaluation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

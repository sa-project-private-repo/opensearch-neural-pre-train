{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v11 Inference Test: Term-Level KO-EN Model\n",
    "\n",
    "Test the trained v11 model on various Korean queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    candidates = [\n",
    "        Path.cwd(),\n",
    "        Path.cwd().parent,\n",
    "        Path.cwd().parent.parent,\n",
    "        Path(\"/home/west/Documents/cursor-workspace/opensearch-neural-pre-train\"),\n",
    "    ]\n",
    "    for candidate in candidates:\n",
    "        if (candidate / \"CLAUDE.md\").exists() or (candidate / \".git\").exists():\n",
    "            return candidate\n",
    "    return Path(\"/home/west/Documents/cursor-workspace/opensearch-neural-pre-train\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from src.model.splade_model import create_splade_model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "checkpoint_path = PROJECT_ROOT / 'outputs' / 'v11_term_level' / 'final_model.pt'\n",
    "\n",
    "if not checkpoint_path.exists():\n",
    "    print(f\"Checkpoint not found at {checkpoint_path}\")\n",
    "    print(\"Please run 01_training_term_level.ipynb first!\")\n",
    "else:\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    config = checkpoint['config']\n",
    "    \n",
    "    print(f\"Loaded checkpoint from: {checkpoint_path}\")\n",
    "    print(f\"Model: {config['model_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
    "\n",
    "model = create_splade_model(\n",
    "    model_name=config['model_name'],\n",
    "    use_idf=False,\n",
    "    use_expansion=True,\n",
    "    expansion_mode='mlm',\n",
    ")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_korean_char(c: str) -> bool:\n",
    "    return '\\uac00' <= c <= '\\ud7a3' or '\\u1100' <= c <= '\\u11ff' or '\\u3130' <= c <= '\\u318f'\n",
    "\n",
    "def is_english_char(c: str) -> bool:\n",
    "    return c.isalpha() and c.isascii()\n",
    "\n",
    "def classify_token(token: str) -> str:\n",
    "    \"\"\"Classify token as Korean, English, or Other.\"\"\"\n",
    "    clean = token.replace('##', '')\n",
    "    if not clean:\n",
    "        return 'other'\n",
    "    \n",
    "    has_korean = any(is_korean_char(c) for c in clean)\n",
    "    has_english = any(is_english_char(c) for c in clean)\n",
    "    \n",
    "    if has_korean:\n",
    "        return 'korean'\n",
    "    elif has_english:\n",
    "        return 'english'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "def analyze_query(text: str, top_k: int = 50) -> dict:\n",
    "    \"\"\"Analyze a query and return top activated tokens.\"\"\"\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=64,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sparse_rep, _ = model(\n",
    "            encoding['input_ids'].to(device),\n",
    "            encoding['attention_mask'].to(device)\n",
    "        )\n",
    "    \n",
    "    sparse_rep = sparse_rep[0].cpu()\n",
    "    \n",
    "    # Get top-k tokens\n",
    "    top_values, top_indices = torch.topk(sparse_rep, k=top_k)\n",
    "    top_tokens = tokenizer.convert_ids_to_tokens(top_indices.tolist())\n",
    "    \n",
    "    # Classify tokens\n",
    "    korean_tokens = []\n",
    "    english_tokens = []\n",
    "    other_tokens = []\n",
    "    \n",
    "    for token, value in zip(top_tokens, top_values.tolist()):\n",
    "        token_type = classify_token(token)\n",
    "        if token_type == 'korean':\n",
    "            korean_tokens.append((token, value))\n",
    "        elif token_type == 'english':\n",
    "            english_tokens.append((token, value))\n",
    "        else:\n",
    "            other_tokens.append((token, value))\n",
    "    \n",
    "    return {\n",
    "        'input': text,\n",
    "        'korean': korean_tokens,\n",
    "        'english': english_tokens,\n",
    "        'other': other_tokens,\n",
    "        'top_10': list(zip(top_tokens[:10], top_values[:10].tolist())),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    # IT/Tech\n",
    "    \"머신러닝\",\n",
    "    \"딥러닝\",\n",
    "    \"자연어처리\",\n",
    "    \"인공지능\",\n",
    "    \"데이터베이스\",\n",
    "    \"추천시스템\",\n",
    "    \"검색엔진\",\n",
    "    \"클라우드\",\n",
    "    \"서버\",\n",
    "    \"네트워크\",\n",
    "    \n",
    "    # General\n",
    "    \"컴퓨터\",\n",
    "    \"인터넷\",\n",
    "    \"프로그래밍\",\n",
    "    \"알고리즘\",\n",
    "    \"데이터\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"v11 INFERENCE TEST RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for query in test_queries:\n",
    "    result = analyze_query(query)\n",
    "    \n",
    "    print(f\"\\n입력: {result['input']}\")\n",
    "    \n",
    "    # Korean tokens\n",
    "    ko_tokens = [t for t, v in result['korean'][:5]]\n",
    "    print(f\"  한글 토큰: {ko_tokens}\")\n",
    "    \n",
    "    # English tokens\n",
    "    en_tokens = [t for t, v in result['english'][:5]]\n",
    "    print(f\"  영어 토큰: {en_tokens}\")\n",
    "    \n",
    "    # Top 10\n",
    "    top_10 = [f\"{t}({v:.2f})\" for t, v in result['top_10'][:5]]\n",
    "    print(f\"  Top-5: {top_10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis for key queries\n",
    "key_queries = [\n",
    "    (\"추천시스템\", [\"추천\", \"시스템\"], [\"recommend\", \"system\", \"recommendation\"]),\n",
    "    (\"검색엔진\", [\"검색\", \"엔진\"], [\"search\", \"engine\"]),\n",
    "    (\"머신러닝\", [\"머신\", \"러닝\"], [\"machine\", \"learning\"]),\n",
    "    (\"데이터베이스\", [\"데이터\", \"베이스\"], [\"database\", \"data\"]),\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for query, expected_ko, expected_en in key_queries:\n",
    "    result = analyze_query(query, top_k=100)\n",
    "    \n",
    "    print(f\"\\n입력: {query}\")\n",
    "    print(f\"  기대 한글: {expected_ko}\")\n",
    "    print(f\"  기대 영어: {expected_en}\")\n",
    "    \n",
    "    # Check Korean\n",
    "    found_ko = []\n",
    "    for exp_tok in expected_ko:\n",
    "        exp_subtoks = tokenizer.tokenize(exp_tok)\n",
    "        for subtok in exp_subtoks:\n",
    "            for tok, val in result['korean']:\n",
    "                if subtok == tok:\n",
    "                    found_ko.append((tok, val))\n",
    "    \n",
    "    # Check English\n",
    "    found_en = []\n",
    "    for exp_tok in expected_en:\n",
    "        exp_subtoks = tokenizer.tokenize(exp_tok.lower())\n",
    "        for subtok in exp_subtoks:\n",
    "            for tok, val in result['english']:\n",
    "                if subtok == tok:\n",
    "                    found_en.append((tok, val))\n",
    "    \n",
    "    print(f\"  발견된 한글: {found_ko}\")\n",
    "    print(f\"  발견된 영어: {found_en}\")\n",
    "    print(f\"  전체 한글 ({len(result['korean'])}): {[t for t, v in result['korean'][:10]]}\")\n",
    "    print(f\"  전체 영어 ({len(result['english'])}): {[t for t, v in result['english'][:10]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "total_ko_activated = 0\n",
    "total_en_activated = 0\n",
    "total_other = 0\n",
    "\n",
    "for query in test_queries:\n",
    "    result = analyze_query(query, top_k=50)\n",
    "    total_ko_activated += len(result['korean'])\n",
    "    total_en_activated += len(result['english'])\n",
    "    total_other += len(result['other'])\n",
    "\n",
    "n_queries = len(test_queries)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY STATISTICS (Top-50 tokens per query)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n  Queries tested: {n_queries}\")\n",
    "print(f\"  Avg Korean tokens: {total_ko_activated / n_queries:.1f}\")\n",
    "print(f\"  Avg English tokens: {total_en_activated / n_queries:.1f}\")\n",
    "print(f\"  Avg Other tokens: {total_other / n_queries:.1f}\")\n",
    "print(f\"\\n  Korean ratio: {total_ko_activated / (total_ko_activated + total_en_activated + total_other) * 100:.1f}%\")\n",
    "print(f\"  English ratio: {total_en_activated / (total_ko_activated + total_en_activated + total_other) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"v11 INFERENCE TEST COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

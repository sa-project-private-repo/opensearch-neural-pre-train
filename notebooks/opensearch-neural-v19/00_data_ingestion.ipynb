{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v19 Data Ingestion\n",
    "\n",
    "This notebook collects Korean-English term pairs for training the cross-lingual SPLADE model.\n",
    "\n",
    "## IMPORTANT: Single Token Only\n",
    "\n",
    "OpenSearch neural sparse models operate at the **morpheme/token level**.\n",
    "\n",
    "- Korean: Single token only (NO spaces)\n",
    "- English: Single token only (NO spaces)\n",
    "\n",
    "Multi-word phrases like \"machine learning\" or \"동적 언어\" are NOT allowed.\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "| Source | Description |\n",
    "|--------|-------------|\n",
    "| MUSE | Facebook's bilingual dictionary (single words) |\n",
    "| Wikidata | Entity labels (filtered to single words) |\n",
    "| IT Terminology | Technical terms (single words) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v19_high_quality\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict\n",
    "\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"dataset\" / \"v19_high_quality\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  min_ko_length: 2\n",
      "  max_ko_length: 15\n",
      "  min_en_length: 2\n",
      "  max_en_length: 20\n",
      "  request_timeout: 120\n",
      "  wikidata_delay: 2.0\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    # SINGLE TOKEN constraints\n",
    "    \"min_ko_length\": 2,\n",
    "    \"max_ko_length\": 15,\n",
    "    \"min_en_length\": 2,\n",
    "    \"max_en_length\": 20,\n",
    "    \n",
    "    # Request settings\n",
    "    \"request_timeout\": 120,\n",
    "    \"wikidata_delay\": 2.0,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Helper Functions - Single Token Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Token Validation Tests:\n",
      "  is_single_korean_token('프로그램'): True\n",
      "  is_single_korean_token('동적 언어'): False\n",
      "  is_single_english_token('program'): True\n",
      "  is_single_english_token('machine learning'): False\n"
     ]
    }
   ],
   "source": [
    "def is_single_korean_token(text: str) -> bool:\n",
    "    \"\"\"Check if text is a SINGLE Korean token (no spaces, Korean chars only).\"\"\"\n",
    "    if not text or ' ' in text:\n",
    "        return False\n",
    "    # Must contain Korean characters\n",
    "    has_korean = any('\\uac00' <= c <= '\\ud7a3' for c in text)\n",
    "    # Should not contain English letters\n",
    "    has_english = any(c.isascii() and c.isalpha() for c in text)\n",
    "    return has_korean and not has_english\n",
    "\n",
    "\n",
    "def is_single_english_token(text: str) -> bool:\n",
    "    \"\"\"Check if text is a SINGLE English token (no spaces, ASCII letters only).\"\"\"\n",
    "    if not text or ' ' in text:\n",
    "        return False\n",
    "    # Must be pure alphabetic ASCII\n",
    "    if not text.isalpha() or not text.isascii():\n",
    "        return False\n",
    "    # Reject long all-uppercase abbreviations\n",
    "    if text.isupper() and len(text) > 4:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean text - remove parenthetical content.\"\"\"\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s*\\([^)]*\\)', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Test\n",
    "print(\"Single Token Validation Tests:\")\n",
    "print(f\"  is_single_korean_token('프로그램'): {is_single_korean_token('프로그램')}\")\n",
    "print(f\"  is_single_korean_token('동적 언어'): {is_single_korean_token('동적 언어')}\")\n",
    "print(f\"  is_single_english_token('program'): {is_single_english_token('program')}\")\n",
    "print(f\"  is_single_english_token('machine learning'): {is_single_english_token('machine learning')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 1. MUSE Bilingual Dictionary\n",
    "\n",
    "MUSE contains single-word translations - ideal for our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "1. COLLECTING MUSE DICTIONARY (SINGLE TOKENS)\n",
      "======================================================================\n",
      "\n",
      "Downloading: https://dl.fbaipublicfiles.com/arrival/dictionaries/ko-en.txt\n",
      "Got 20,549 lines\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604f5142dffc435f8485e43d25486b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MUSE (ko->en):   0%|          | 0/20549 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading: https://dl.fbaipublicfiles.com/arrival/dictionaries/en-ko.txt\n",
      "Got 22,357 lines\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57008a32c252478aac04877cd40882b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MUSE (en->ko):   0%|          | 0/22357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collected 40,862 single-token pairs from MUSE\n",
      "Rejected: {'en_invalid': 170, 'ko_invalid': 1874}\n"
     ]
    }
   ],
   "source": [
    "def collect_muse_dictionary() -> List[Dict]:\n",
    "    \"\"\"Collect single-token KO-EN pairs from MUSE.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"1. COLLECTING MUSE DICTIONARY (SINGLE TOKENS)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    pairs = []\n",
    "    rejected = defaultdict(int)\n",
    "\n",
    "    muse_urls = [\n",
    "        (\"https://dl.fbaipublicfiles.com/arrival/dictionaries/ko-en.txt\", \"ko\", \"en\"),\n",
    "        (\"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-ko.txt\", \"en\", \"ko\"),\n",
    "    ]\n",
    "\n",
    "    for url, src_lang, tgt_lang in muse_urls:\n",
    "        print(f\"\\nDownloading: {url}\")\n",
    "        try:\n",
    "            response = requests.get(url, timeout=CONFIG[\"request_timeout\"], headers={\n",
    "                \"User-Agent\": \"Mozilla/5.0\"\n",
    "            })\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                response.encoding = 'utf-8'\n",
    "                lines = response.text.strip().split('\\n')\n",
    "                print(f\"Got {len(lines):,} lines\")\n",
    "\n",
    "                for line in tqdm(lines, desc=f\"MUSE ({src_lang}->{tgt_lang})\"):\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 2:\n",
    "                        if src_lang == \"ko\":\n",
    "                            ko_word, en_word = parts[0], parts[1]\n",
    "                        else:\n",
    "                            en_word, ko_word = parts[0], parts[1]\n",
    "\n",
    "                        # STRICT single token validation\n",
    "                        if not is_single_korean_token(ko_word):\n",
    "                            rejected[\"ko_invalid\"] += 1\n",
    "                            continue\n",
    "                        if not is_single_english_token(en_word):\n",
    "                            rejected[\"en_invalid\"] += 1\n",
    "                            continue\n",
    "                        if len(ko_word) < CONFIG[\"min_ko_length\"]:\n",
    "                            rejected[\"ko_short\"] += 1\n",
    "                            continue\n",
    "                        if len(en_word) < CONFIG[\"min_en_length\"]:\n",
    "                            rejected[\"en_short\"] += 1\n",
    "                            continue\n",
    "\n",
    "                        pairs.append({\n",
    "                            \"ko\": ko_word,\n",
    "                            \"en\": en_word.lower(),\n",
    "                            \"source\": \"muse\"\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "    print(f\"\\nCollected {len(pairs):,} single-token pairs from MUSE\")\n",
    "    if rejected:\n",
    "        print(\"Rejected:\", dict(rejected))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "muse_pairs = collect_muse_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 2. Wikidata Labels (Single-Word Filter)\n",
    "\n",
    "Query Wikidata with SPARQL filter to exclude multi-word labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "2. COLLECTING WIKIDATA LABELS (SINGLE TOKENS ONLY)\n",
      "======================================================================\n",
      "\n",
      "Executing query 1/2...\n",
      "Query 1 error: Expecting property name enclosed in double quotes: line 15997 column 5 (char 333112)\n",
      "\n",
      "Executing query 2/2...\n",
      "Got 192 results\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23de5a083674f508fc49ef8f7c1528a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Wikidata Q2:   0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collected 172 single-token pairs from Wikidata\n",
      "Rejected: {'en_invalid': 19, 'ko_invalid': 1}\n"
     ]
    }
   ],
   "source": [
    "def collect_wikidata_labels() -> List[Dict]:\n",
    "    \"\"\"Collect single-word KO-EN pairs from Wikidata.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"2. COLLECTING WIKIDATA LABELS (SINGLE TOKENS ONLY)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    pairs = []\n",
    "    rejected = defaultdict(int)\n",
    "    endpoint = \"https://query.wikidata.org/sparql\"\n",
    "    headers = {\"User-Agent\": \"KoEnCollector/1.0\", \"Accept\": \"application/json\"}\n",
    "\n",
    "    # Query with NO SPACE filter\n",
    "    queries = [\n",
    "        # Single-word labels only\n",
    "        \"\"\"\n",
    "        SELECT ?koLabel ?enLabel WHERE {\n",
    "          ?item wdt:P31 ?type .\n",
    "          ?item rdfs:label ?koLabel . FILTER(LANG(?koLabel) = \"ko\")\n",
    "          ?item rdfs:label ?enLabel . FILTER(LANG(?enLabel) = \"en\")\n",
    "          FILTER(STRLEN(?koLabel) >= 2 && STRLEN(?koLabel) <= 10)\n",
    "          FILTER(STRLEN(?enLabel) >= 2 && STRLEN(?enLabel) <= 15)\n",
    "          FILTER(!CONTAINS(?koLabel, \" \"))\n",
    "          FILTER(!CONTAINS(?enLabel, \" \"))\n",
    "        }\n",
    "        LIMIT 50000\n",
    "        \"\"\",\n",
    "        # Concepts\n",
    "        \"\"\"\n",
    "        SELECT ?koLabel ?enLabel WHERE {\n",
    "          { ?item wdt:P31 wd:Q35120 } UNION { ?item wdt:P31 wd:Q151885 }\n",
    "          ?item rdfs:label ?koLabel . FILTER(LANG(?koLabel) = \"ko\")\n",
    "          ?item rdfs:label ?enLabel . FILTER(LANG(?enLabel) = \"en\")\n",
    "          FILTER(STRLEN(?koLabel) >= 2 && STRLEN(?koLabel) <= 8)\n",
    "          FILTER(STRLEN(?enLabel) >= 3 && STRLEN(?enLabel) <= 12)\n",
    "          FILTER(!CONTAINS(?koLabel, \" \"))\n",
    "          FILTER(!CONTAINS(?enLabel, \" \"))\n",
    "        }\n",
    "        LIMIT 20000\n",
    "        \"\"\"\n",
    "    ]\n",
    "\n",
    "    for i, query in enumerate(queries):\n",
    "        print(f\"\\nExecuting query {i+1}/{len(queries)}...\")\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                endpoint,\n",
    "                params={\"query\": query, \"format\": \"json\"},\n",
    "                headers=headers,\n",
    "                timeout=180\n",
    "            )\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                results = response.json().get(\"results\", {}).get(\"bindings\", [])\n",
    "                print(f\"Got {len(results):,} results\")\n",
    "\n",
    "                for item in tqdm(results, desc=f\"Wikidata Q{i+1}\"):\n",
    "                    ko = clean_text(item.get(\"koLabel\", {}).get(\"value\", \"\"))\n",
    "                    en = clean_text(item.get(\"enLabel\", {}).get(\"value\", \"\"))\n",
    "\n",
    "                    # Double-check single token\n",
    "                    if not is_single_korean_token(ko):\n",
    "                        rejected[\"ko_invalid\"] += 1\n",
    "                        continue\n",
    "                    if not is_single_english_token(en):\n",
    "                        rejected[\"en_invalid\"] += 1\n",
    "                        continue\n",
    "\n",
    "                    pairs.append({\"ko\": ko, \"en\": en.lower(), \"source\": \"wikidata\"})\n",
    "            else:\n",
    "                print(f\"Query {i+1} failed: {response.status_code}\")\n",
    "\n",
    "            time.sleep(CONFIG[\"wikidata_delay\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Query {i+1} error: {e}\")\n",
    "\n",
    "    print(f\"\\nCollected {len(pairs):,} single-token pairs from Wikidata\")\n",
    "    if rejected:\n",
    "        print(\"Rejected:\", dict(rejected))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "wikidata_pairs = collect_wikidata_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 3. IT/Tech Terminology (Single Words)\n",
    "\n",
    "Multi-word terms are split into individual word pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "3. COLLECTING IT TERMINOLOGY (SINGLE TOKENS)\n",
      "======================================================================\n",
      "Collected 92 single-token IT terms\n"
     ]
    }
   ],
   "source": [
    "def collect_it_terminology() -> List[Dict]:\n",
    "    \"\"\"Collect IT terms - single words only, split multi-word terms.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"3. COLLECTING IT TERMINOLOGY (SINGLE TOKENS)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Single-word IT terms + split multi-word English into separate pairs\n",
    "    it_terms = [\n",
    "        # Programming\n",
    "        (\"프로그램\", \"program\"), (\"프로그래밍\", \"programming\"), (\"코드\", \"code\"),\n",
    "        (\"코딩\", \"coding\"), (\"소프트웨어\", \"software\"), (\"하드웨어\", \"hardware\"),\n",
    "        (\"알고리즘\", \"algorithm\"), (\"함수\", \"function\"), (\"변수\", \"variable\"),\n",
    "        (\"클래스\", \"class\"), (\"객체\", \"object\"), (\"메서드\", \"method\"),\n",
    "        (\"인터페이스\", \"interface\"), (\"모듈\", \"module\"), (\"라이브러리\", \"library\"),\n",
    "        (\"프레임워크\", \"framework\"), (\"패키지\", \"package\"), (\"컴파일러\", \"compiler\"),\n",
    "        (\"디버깅\", \"debugging\"), (\"테스트\", \"test\"), (\"배포\", \"deployment\"),\n",
    "        \n",
    "        # Network\n",
    "        (\"네트워크\", \"network\"), (\"서버\", \"server\"), (\"클라이언트\", \"client\"),\n",
    "        (\"데이터베이스\", \"database\"), (\"쿼리\", \"query\"), (\"인덱스\", \"index\"),\n",
    "        (\"캐시\", \"cache\"), (\"프록시\", \"proxy\"), (\"프로토콜\", \"protocol\"),\n",
    "        \n",
    "        # AI/ML - split multi-word English\n",
    "        (\"데이터\", \"data\"), (\"분석\", \"analysis\"), (\"모델\", \"model\"),\n",
    "        (\"머신러닝\", \"machine\"), (\"머신러닝\", \"learning\"),\n",
    "        (\"딥러닝\", \"deep\"), (\"딥러닝\", \"learning\"),\n",
    "        (\"인공지능\", \"artificial\"), (\"인공지능\", \"intelligence\"),\n",
    "        (\"신경망\", \"neural\"), (\"신경망\", \"network\"),\n",
    "        (\"학습\", \"training\"), (\"추론\", \"inference\"), (\"예측\", \"prediction\"),\n",
    "        (\"분류\", \"classification\"), (\"클러스터링\", \"clustering\"),\n",
    "        (\"임베딩\", \"embedding\"), (\"벡터\", \"vector\"), (\"텐서\", \"tensor\"),\n",
    "        (\"가중치\", \"weight\"), (\"최적화\", \"optimization\"),\n",
    "        \n",
    "        # Cloud\n",
    "        (\"클라우드\", \"cloud\"), (\"컨테이너\", \"container\"), (\"도커\", \"docker\"),\n",
    "        (\"쿠버네티스\", \"kubernetes\"), (\"모니터링\", \"monitoring\"),\n",
    "        (\"파이프라인\", \"pipeline\"), (\"자동화\", \"automation\"),\n",
    "        \n",
    "        # Security\n",
    "        (\"보안\", \"security\"), (\"인증\", \"authentication\"), (\"권한\", \"authorization\"),\n",
    "        (\"암호화\", \"encryption\"), (\"토큰\", \"token\"), (\"세션\", \"session\"),\n",
    "        \n",
    "        # General\n",
    "        (\"시스템\", \"system\"), (\"플랫폼\", \"platform\"), (\"서비스\", \"service\"),\n",
    "        (\"아키텍처\", \"architecture\"), (\"프로세스\", \"process\"),\n",
    "        (\"메모리\", \"memory\"), (\"스토리지\", \"storage\"), (\"파일\", \"file\"),\n",
    "        (\"검색\", \"search\"), (\"문서\", \"document\"), (\"텍스트\", \"text\"),\n",
    "        (\"자연어처리\", \"natural\"), (\"자연어처리\", \"language\"), (\"자연어처리\", \"processing\"),\n",
    "        (\"번역\", \"translation\"), (\"토큰화\", \"tokenization\"),\n",
    "        \n",
    "        # Common actions\n",
    "        (\"요청\", \"request\"), (\"응답\", \"response\"), (\"오류\", \"error\"),\n",
    "        (\"생성\", \"create\"), (\"삭제\", \"delete\"), (\"수정\", \"update\"),\n",
    "        (\"실행\", \"execute\"), (\"중지\", \"stop\"), (\"시작\", \"start\"),\n",
    "        (\"다운로드\", \"download\"), (\"업로드\", \"upload\"), (\"설치\", \"installation\"),\n",
    "    ]\n",
    "\n",
    "    pairs = []\n",
    "    for ko, en in it_terms:\n",
    "        if is_single_korean_token(ko) and is_single_english_token(en):\n",
    "            pairs.append({\"ko\": ko, \"en\": en.lower(), \"source\": \"it_terminology\"})\n",
    "\n",
    "    print(f\"Collected {len(pairs):,} single-token IT terms\")\n",
    "    return pairs\n",
    "\n",
    "\n",
    "it_pairs = collect_it_terminology()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 4. Combine and Final Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMBINING ALL DATA\n",
      "======================================================================\n",
      "\n",
      "Total raw: 41,126\n",
      "  MUSE: 40,862\n",
      "  Wikidata: 172\n",
      "  IT: 92\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMBINING ALL DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_pairs = muse_pairs + wikidata_pairs + it_pairs\n",
    "print(f\"\\nTotal raw: {len(all_pairs):,}\")\n",
    "print(f\"  MUSE: {len(muse_pairs):,}\")\n",
    "print(f\"  Wikidata: {len(wikidata_pairs):,}\")\n",
    "print(f\"  IT: {len(it_pairs):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL VALIDATION (STRICT SINGLE-TOKEN)\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44082eeef214984bc283aab9188f5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/41126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered: 41,126 -> 41,126\n",
      "Deduplicated: 20,594\n"
     ]
    }
   ],
   "source": [
    "def final_filter_and_dedupe(pairs: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Final strict filtering - reject ANY multi-word entries.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FINAL VALIDATION (STRICT SINGLE-TOKEN)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    filtered = []\n",
    "    rejected = defaultdict(int)\n",
    "\n",
    "    for p in tqdm(pairs, desc=\"Validating\"):\n",
    "        ko, en = p[\"ko\"], p[\"en\"]\n",
    "\n",
    "        # REJECT any entry with spaces\n",
    "        if ' ' in ko:\n",
    "            rejected[\"ko_has_space\"] += 1\n",
    "            continue\n",
    "        if ' ' in en:\n",
    "            rejected[\"en_has_space\"] += 1\n",
    "            continue\n",
    "\n",
    "        # Validate tokens\n",
    "        if not is_single_korean_token(ko):\n",
    "            rejected[\"ko_invalid\"] += 1\n",
    "            continue\n",
    "        if not is_single_english_token(en):\n",
    "            rejected[\"en_invalid\"] += 1\n",
    "            continue\n",
    "\n",
    "        # Length\n",
    "        if len(ko) < CONFIG[\"min_ko_length\"] or len(ko) > CONFIG[\"max_ko_length\"]:\n",
    "            rejected[\"ko_length\"] += 1\n",
    "            continue\n",
    "        if len(en) < CONFIG[\"min_en_length\"] or len(en) > CONFIG[\"max_en_length\"]:\n",
    "            rejected[\"en_length\"] += 1\n",
    "            continue\n",
    "\n",
    "        filtered.append(p)\n",
    "\n",
    "    print(f\"Filtered: {len(pairs):,} -> {len(filtered):,}\")\n",
    "    if rejected:\n",
    "        print(\"Rejected:\", dict(rejected))\n",
    "\n",
    "    # Deduplicate\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for p in filtered:\n",
    "        key = (p[\"ko\"], p[\"en\"])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(p)\n",
    "\n",
    "    print(f\"Deduplicated: {len(unique):,}\")\n",
    "    return unique\n",
    "\n",
    "\n",
    "final_pairs = final_filter_and_dedupe(all_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "VALIDATION CHECK\n",
      "======================================================================\n",
      "\n",
      "Korean with spaces: 0\n",
      "English with spaces: 0\n",
      "\n",
      "VALIDATION PASSED: All entries are single tokens!\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION: Ensure NO multi-word entries\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VALIDATION CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ko_spaces = [p for p in final_pairs if ' ' in p['ko']]\n",
    "en_spaces = [p for p in final_pairs if ' ' in p['en']]\n",
    "\n",
    "print(f\"\\nKorean with spaces: {len(ko_spaces)}\")\n",
    "print(f\"English with spaces: {len(en_spaces)}\")\n",
    "\n",
    "if ko_spaces or en_spaces:\n",
    "    print(\"\\nERROR: Found multi-word entries!\")\n",
    "    raise ValueError(\"Multi-word entries detected!\")\n",
    "else:\n",
    "    print(\"\\nVALIDATION PASSED: All entries are single tokens!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 5. Statistics and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL STATISTICS\n",
      "======================================================================\n",
      "\n",
      "Total pairs: 20,594\n",
      "\n",
      "By source:\n",
      "  muse: 20,455 (99.3%)\n",
      "  wikidata: 119 (0.6%)\n",
      "  it_terminology: 20 (0.1%)\n",
      "\n",
      "Korean lengths: min=2, max=8, avg=2.8\n",
      "English lengths: min=2, max=20, avg=7.1\n",
      "\n",
      "Sample pairs:\n",
      "  시대 -> era\n",
      "  근본주의 -> fundamentalism\n",
      "  보충제 -> supplement\n",
      "  보증 -> warranties\n",
      "  조선 -> joseon\n",
      "  루트비히 -> ludwig\n",
      "  라구사 -> ragusa\n",
      "  리트리버 -> retrievers\n",
      "  손상 -> damaged\n",
      "  카누 -> canoes\n",
      "  피임 -> contraceptive\n",
      "  결과 -> results\n",
      "  달리기 -> runs\n",
      "  머큐리 -> mercury\n",
      "  최근 -> latest\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nTotal pairs: {len(final_pairs):,}\")\n",
    "\n",
    "# By source\n",
    "sources = defaultdict(int)\n",
    "for p in final_pairs:\n",
    "    sources[p[\"source\"]] += 1\n",
    "\n",
    "print(\"\\nBy source:\")\n",
    "for src, cnt in sorted(sources.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {src}: {cnt:,} ({cnt/len(final_pairs)*100:.1f}%)\")\n",
    "\n",
    "# Length stats\n",
    "ko_lens = [len(p['ko']) for p in final_pairs]\n",
    "en_lens = [len(p['en']) for p in final_pairs]\n",
    "print(f\"\\nKorean lengths: min={min(ko_lens)}, max={max(ko_lens)}, avg={sum(ko_lens)/len(ko_lens):.1f}\")\n",
    "print(f\"English lengths: min={min(en_lens)}, max={max(en_lens)}, avg={sum(en_lens)/len(en_lens):.1f}\")\n",
    "\n",
    "# Samples\n",
    "import random\n",
    "print(\"\\nSample pairs:\")\n",
    "for p in random.sample(final_pairs, min(15, len(final_pairs))):\n",
    "    print(f\"  {p['ko']} -> {p['en']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534b59dfb854440cb96443b3468c611c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving:   0%|          | 0/20594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v19_high_quality/term_pairs.jsonl\n",
      "Size: 1095.7 KB\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "output_path = OUTPUT_DIR / \"term_pairs.jsonl\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for p in tqdm(final_pairs, desc=\"Saving\"):\n",
    "        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"\\nSaved: {output_path}\")\n",
    "print(f\"Size: {output_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATA COLLECTION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Output: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v19_high_quality/term_pairs.jsonl\n",
      "Total: 20,594 single-token pairs\n",
      "\n",
      "All entries are SINGLE TOKENS (no spaces)\n",
      "\n",
      "Next: Run 01_data_preparation.ipynb\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA COLLECTION COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nOutput: {output_path}\")\n",
    "print(f\"Total: {len(final_pairs):,} single-token pairs\")\n",
    "print(\"\\nAll entries are SINGLE TOKENS (no spaces)\")\n",
    "print(\"\\nNext: Run 01_data_preparation.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v19 Data Ingestion\n",
    "\n",
    "This notebook collects Korean-English term pairs for training the cross-lingual SPLADE model.\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "| Source | Description | Expected Pairs |\n",
    "|--------|-------------|----------------|\n",
    "| MUSE | Facebook's bilingual dictionary (ko-en, en-ko) | ~40K |\n",
    "| Wikidata | Entity labels from knowledge graph | ~50K |\n",
    "| IT Terminology | Technical terms (curated) | ~500 |\n",
    "\n",
    "## Target: 70K+ High-Quality Term Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict\n",
    "\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"dataset\" / \"v19_high_quality\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Length constraints\n",
    "    \"min_ko_length\": 2,\n",
    "    \"max_ko_length\": 30,\n",
    "    \"min_en_length\": 2,\n",
    "    \"max_en_length\": 50,\n",
    "    \n",
    "    # Request settings\n",
    "    \"request_timeout\": 120,\n",
    "    \"wikidata_delay\": 2.0,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_korean(text: str) -> bool:\n",
    "    \"\"\"Check if text contains Korean characters.\"\"\"\n",
    "    return any('\\uac00' <= c <= '\\ud7a3' for c in text)\n",
    "\n",
    "\n",
    "def is_valid_english(text: str) -> bool:\n",
    "    \"\"\"Check if text is valid English (letters only, no special chars).\"\"\"\n",
    "    if not text:\n",
    "        return False\n",
    "    has_letter = any(c.isalpha() and c.isascii() for c in text)\n",
    "    if text.isupper() and len(text) > 5:\n",
    "        return False\n",
    "    return has_letter\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean text - remove parenthetical content.\"\"\"\n",
    "    text = text.strip()\n",
    "    if '(' in text and ')' in text:\n",
    "        main_part = text.split('(')[0].strip()\n",
    "        if main_part:\n",
    "            return main_part\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_english_words(text: str) -> List[str]:\n",
    "    \"\"\"Extract individual English words from a phrase.\"\"\"\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip().lower()\n",
    "        if word and word.isalpha() and word.isascii() and len(word) >= 2:\n",
    "            words.append(word)\n",
    "    return words\n",
    "\n",
    "\n",
    "# Test\n",
    "print(\"Validation Tests:\")\n",
    "print(f\"  is_valid_korean('프로그램'): {is_valid_korean('프로그램')}\")\n",
    "print(f\"  is_valid_english('program'): {is_valid_english('program')}\")\n",
    "print(f\"  extract_english_words('machine learning'): {extract_english_words('machine learning')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 1. MUSE Bilingual Dictionary\n",
    "\n",
    "MUSE contains high-quality bilingual word pairs from Facebook Research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_muse_dictionary() -> List[Dict]:\n",
    "    \"\"\"Collect KO-EN pairs from MUSE bilingual dictionaries.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"1. COLLECTING MUSE DICTIONARY\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    pairs = []\n",
    "\n",
    "    muse_urls = [\n",
    "        (\"https://dl.fbaipublicfiles.com/arrival/dictionaries/ko-en.txt\", \"ko\", \"en\"),\n",
    "        (\"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-ko.txt\", \"en\", \"ko\"),\n",
    "    ]\n",
    "\n",
    "    for url, src_lang, tgt_lang in muse_urls:\n",
    "        print(f\"\\nDownloading from {url}...\")\n",
    "        try:\n",
    "            response = requests.get(url, timeout=CONFIG[\"request_timeout\"], headers={\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "            })\n",
    "            print(f\"Status: {response.status_code}\")\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                response.encoding = 'utf-8'\n",
    "                content = response.text.strip()\n",
    "                if not content:\n",
    "                    print(f\"Empty response from {url}\")\n",
    "                    continue\n",
    "\n",
    "                lines = content.split('\\n')\n",
    "                print(f\"Got {len(lines):,} lines\")\n",
    "\n",
    "                for line in tqdm(lines, desc=f\"MUSE ({src_lang}->{tgt_lang})\"):\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 2:\n",
    "                        if src_lang == \"ko\":\n",
    "                            ko_word, en_word = parts[0].strip(), parts[1].strip()\n",
    "                        else:\n",
    "                            en_word, ko_word = parts[0].strip(), parts[1].strip()\n",
    "\n",
    "                        if (is_valid_korean(ko_word) and\n",
    "                            is_valid_english(en_word) and\n",
    "                            len(ko_word) >= CONFIG[\"min_ko_length\"] and\n",
    "                            len(en_word) >= CONFIG[\"min_en_length\"]):\n",
    "                            pairs.append({\n",
    "                                \"ko\": ko_word,\n",
    "                                \"en\": en_word.lower(),\n",
    "                                \"source\": \"muse\"\n",
    "                            })\n",
    "            else:\n",
    "                print(f\"Failed: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "    print(f\"\\nCollected {len(pairs):,} pairs from MUSE\")\n",
    "    return pairs\n",
    "\n",
    "\n",
    "muse_pairs = collect_muse_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 2. Wikidata Labels\n",
    "\n",
    "Query Wikidata for entity labels in Korean and English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_wikidata_labels() -> List[Dict]:\n",
    "    \"\"\"Collect KO-EN term pairs from Wikidata with multiple queries.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"2. COLLECTING WIKIDATA LABELS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    pairs = []\n",
    "    sparql_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "    queries = [\n",
    "        # General entities\n",
    "        \"\"\"\n",
    "        SELECT ?koLabel ?enLabel WHERE {\n",
    "            ?item wdt:P31 ?type .\n",
    "            ?item rdfs:label ?koLabel . FILTER(LANG(?koLabel) = \"ko\")\n",
    "            ?item rdfs:label ?enLabel . FILTER(LANG(?enLabel) = \"en\")\n",
    "            FILTER(STRLEN(?koLabel) >= 2 && STRLEN(?koLabel) <= 20)\n",
    "            FILTER(STRLEN(?enLabel) >= 2 && STRLEN(?enLabel) <= 30)\n",
    "        } LIMIT 30000\n",
    "        \"\"\",\n",
    "        # Organizations\n",
    "        \"\"\"\n",
    "        SELECT ?koLabel ?enLabel WHERE {\n",
    "            ?item wdt:P31/wdt:P279* wd:Q43229 .\n",
    "            ?item rdfs:label ?koLabel . FILTER(LANG(?koLabel) = \"ko\")\n",
    "            ?item rdfs:label ?enLabel . FILTER(LANG(?enLabel) = \"en\")\n",
    "            FILTER(STRLEN(?koLabel) >= 2)\n",
    "        } LIMIT 20000\n",
    "        \"\"\",\n",
    "        # Scientific concepts\n",
    "        \"\"\"\n",
    "        SELECT ?koLabel ?enLabel WHERE {\n",
    "            ?item wdt:P31/wdt:P279* wd:Q35120 .\n",
    "            ?item rdfs:label ?koLabel . FILTER(LANG(?koLabel) = \"ko\")\n",
    "            ?item rdfs:label ?enLabel . FILTER(LANG(?enLabel) = \"en\")\n",
    "            FILTER(STRLEN(?koLabel) >= 2)\n",
    "        } LIMIT 20000\n",
    "        \"\"\",\n",
    "        # Software/Technology\n",
    "        \"\"\"\n",
    "        SELECT ?koLabel ?enLabel WHERE {\n",
    "            ?item wdt:P31/wdt:P279* wd:Q7397 .\n",
    "            ?item rdfs:label ?koLabel . FILTER(LANG(?koLabel) = \"ko\")\n",
    "            ?item rdfs:label ?enLabel . FILTER(LANG(?enLabel) = \"en\")\n",
    "            FILTER(STRLEN(?koLabel) >= 2)\n",
    "        } LIMIT 10000\n",
    "        \"\"\",\n",
    "    ]\n",
    "\n",
    "    for i, query in enumerate(queries):\n",
    "        print(f\"\\nExecuting Wikidata query {i + 1}/{len(queries)}...\")\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                sparql_url,\n",
    "                params={\"query\": query, \"format\": \"json\"},\n",
    "                headers={\"User-Agent\": \"TermCollector/2.0 (v19 data collection)\"},\n",
    "                timeout=300\n",
    "            )\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                results = data.get(\"results\", {}).get(\"bindings\", [])\n",
    "                print(f\"Got {len(results):,} results\")\n",
    "\n",
    "                for item in tqdm(results, desc=f\"Wikidata Q{i + 1}\"):\n",
    "                    ko_label = item.get(\"koLabel\", {}).get(\"value\", \"\")\n",
    "                    en_label = item.get(\"enLabel\", {}).get(\"value\", \"\")\n",
    "\n",
    "                    if ko_label and en_label:\n",
    "                        ko_clean = clean_text(ko_label)\n",
    "                        en_clean = clean_text(en_label)\n",
    "\n",
    "                        if (is_valid_korean(ko_clean) and\n",
    "                            is_valid_english(en_clean) and\n",
    "                            len(ko_clean) >= CONFIG[\"min_ko_length\"] and\n",
    "                            len(en_clean) >= CONFIG[\"min_en_length\"]):\n",
    "                            pairs.append({\n",
    "                                \"ko\": ko_clean,\n",
    "                                \"en\": en_clean.lower(),\n",
    "                                \"source\": \"wikidata\"\n",
    "                            })\n",
    "            else:\n",
    "                print(f\"Query {i + 1} failed: {response.status_code}\")\n",
    "\n",
    "            time.sleep(CONFIG[\"wikidata_delay\"])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Wikidata error: {e}\")\n",
    "\n",
    "    print(f\"\\nCollected {len(pairs):,} pairs from Wikidata\")\n",
    "    return pairs\n",
    "\n",
    "\n",
    "wikidata_pairs = collect_wikidata_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 3. IT/Tech Terminology (Extended)\n",
    "\n",
    "Curated IT and technical terminology for better domain coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_it_terminology() -> List[Dict]:\n",
    "    \"\"\"Collect extensive IT and technical terminology.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"3. COLLECTING IT/TECH TERMINOLOGY\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    it_terms = [\n",
    "        # Machine Learning / AI\n",
    "        (\"머신러닝\", \"machine learning\"), (\"기계학습\", \"machine learning\"),\n",
    "        (\"딥러닝\", \"deep learning\"), (\"심층학습\", \"deep learning\"),\n",
    "        (\"인공지능\", \"artificial intelligence\"), (\"자연어처리\", \"natural language processing\"),\n",
    "        (\"신경망\", \"neural network\"), (\"컴퓨터비전\", \"computer vision\"),\n",
    "        (\"강화학습\", \"reinforcement learning\"), (\"지도학습\", \"supervised learning\"),\n",
    "        (\"비지도학습\", \"unsupervised learning\"), (\"전이학습\", \"transfer learning\"),\n",
    "        (\"트랜스포머\", \"transformer\"), (\"어텐션\", \"attention\"),\n",
    "        (\"임베딩\", \"embedding\"), (\"벡터\", \"vector\"), (\"텐서\", \"tensor\"),\n",
    "        (\"그래디언트\", \"gradient\"), (\"역전파\", \"backpropagation\"),\n",
    "        (\"손실함수\", \"loss function\"), (\"최적화\", \"optimization\"),\n",
    "        (\"정규화\", \"regularization\"), (\"드롭아웃\", \"dropout\"),\n",
    "        (\"배치\", \"batch\"), (\"에폭\", \"epoch\"), (\"학습률\", \"learning rate\"),\n",
    "        (\"하이퍼파라미터\", \"hyperparameter\"), (\"오버피팅\", \"overfitting\"),\n",
    "        (\"분류\", \"classification\"), (\"회귀\", \"regression\"),\n",
    "        (\"클러스터링\", \"clustering\"), (\"군집화\", \"clustering\"),\n",
    "        (\"파인튜닝\", \"fine tuning\"), (\"사전학습\", \"pretraining\"),\n",
    "        (\"토크나이저\", \"tokenizer\"), (\"토큰화\", \"tokenization\"),\n",
    "        \n",
    "        # Programming\n",
    "        (\"프로그래밍\", \"programming\"), (\"코딩\", \"coding\"),\n",
    "        (\"알고리즘\", \"algorithm\"), (\"자료구조\", \"data structure\"),\n",
    "        (\"함수\", \"function\"), (\"변수\", \"variable\"), (\"클래스\", \"class\"),\n",
    "        (\"객체\", \"object\"), (\"메서드\", \"method\"), (\"인스턴스\", \"instance\"),\n",
    "        (\"상속\", \"inheritance\"), (\"캡슐화\", \"encapsulation\"),\n",
    "        (\"인터페이스\", \"interface\"), (\"모듈\", \"module\"), (\"패키지\", \"package\"),\n",
    "        (\"라이브러리\", \"library\"), (\"프레임워크\", \"framework\"),\n",
    "        (\"컴파일러\", \"compiler\"), (\"인터프리터\", \"interpreter\"),\n",
    "        (\"디버깅\", \"debugging\"), (\"테스트\", \"test\"),\n",
    "        (\"배포\", \"deployment\"), (\"버전관리\", \"version control\"),\n",
    "        (\"리팩토링\", \"refactoring\"), (\"코드리뷰\", \"code review\"),\n",
    "        \n",
    "        # Database\n",
    "        (\"데이터베이스\", \"database\"), (\"쿼리\", \"query\"),\n",
    "        (\"테이블\", \"table\"), (\"인덱스\", \"index\"), (\"키\", \"key\"),\n",
    "        (\"조인\", \"join\"), (\"트랜잭션\", \"transaction\"),\n",
    "        (\"스키마\", \"schema\"), (\"정규화\", \"normalization\"),\n",
    "        (\"샤딩\", \"sharding\"), (\"파티셔닝\", \"partitioning\"),\n",
    "        (\"캐싱\", \"caching\"), (\"레디스\", \"redis\"),\n",
    "        \n",
    "        # Web / Network\n",
    "        (\"웹\", \"web\"), (\"웹사이트\", \"website\"),\n",
    "        (\"서버\", \"server\"), (\"클라이언트\", \"client\"),\n",
    "        (\"프론트엔드\", \"frontend\"), (\"백엔드\", \"backend\"),\n",
    "        (\"네트워크\", \"network\"), (\"프로토콜\", \"protocol\"),\n",
    "        (\"라우터\", \"router\"), (\"방화벽\", \"firewall\"),\n",
    "        (\"로드밸런서\", \"load balancer\"), (\"프록시\", \"proxy\"),\n",
    "        \n",
    "        # Cloud\n",
    "        (\"클라우드\", \"cloud\"), (\"클라우드컴퓨팅\", \"cloud computing\"),\n",
    "        (\"가상화\", \"virtualization\"), (\"가상머신\", \"virtual machine\"),\n",
    "        (\"컨테이너\", \"container\"), (\"도커\", \"docker\"),\n",
    "        (\"쿠버네티스\", \"kubernetes\"), (\"마이크로서비스\", \"microservice\"),\n",
    "        (\"서버리스\", \"serverless\"), (\"스케일링\", \"scaling\"),\n",
    "        \n",
    "        # Security\n",
    "        (\"보안\", \"security\"), (\"암호화\", \"encryption\"),\n",
    "        (\"복호화\", \"decryption\"), (\"해시\", \"hash\"),\n",
    "        (\"인증\", \"authentication\"), (\"권한\", \"authorization\"),\n",
    "        (\"인증서\", \"certificate\"), (\"취약점\", \"vulnerability\"),\n",
    "        \n",
    "        # Search / IR\n",
    "        (\"검색\", \"search\"), (\"검색엔진\", \"search engine\"),\n",
    "        (\"정보검색\", \"information retrieval\"), (\"인덱싱\", \"indexing\"),\n",
    "        (\"랭킹\", \"ranking\"), (\"문서\", \"document\"),\n",
    "        (\"시맨틱검색\", \"semantic search\"), (\"벡터검색\", \"vector search\"),\n",
    "        (\"유사도\", \"similarity\"), (\"코사인유사도\", \"cosine similarity\"),\n",
    "        (\"추천\", \"recommendation\"), (\"추천시스템\", \"recommendation system\"),\n",
    "        (\"리랭킹\", \"reranking\"), (\"쿼리확장\", \"query expansion\"),\n",
    "        \n",
    "        # Data Science\n",
    "        (\"데이터사이언스\", \"data science\"), (\"데이터분석\", \"data analysis\"),\n",
    "        (\"데이터마이닝\", \"data mining\"), (\"빅데이터\", \"big data\"),\n",
    "        (\"데이터시각화\", \"data visualization\"), (\"대시보드\", \"dashboard\"),\n",
    "        (\"통계\", \"statistics\"), (\"확률\", \"probability\"),\n",
    "        (\"분석\", \"analysis\"), (\"모니터링\", \"monitoring\"),\n",
    "    ]\n",
    "\n",
    "    pairs = []\n",
    "    for ko, en in it_terms:\n",
    "        pairs.append({\n",
    "            \"ko\": ko,\n",
    "            \"en\": en.lower(),\n",
    "            \"source\": \"it_terminology\"\n",
    "        })\n",
    "        # Also add individual English words for multi-word terms\n",
    "        if ' ' in en:\n",
    "            for word in extract_english_words(en):\n",
    "                if len(word) >= 3:\n",
    "                    pairs.append({\n",
    "                        \"ko\": ko,\n",
    "                        \"en\": word,\n",
    "                        \"source\": \"it_terminology\"\n",
    "                    })\n",
    "\n",
    "    print(f\"Collected {len(pairs):,} IT/Tech terms\")\n",
    "    return pairs\n",
    "\n",
    "\n",
    "it_pairs = collect_it_terminology()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 4. Combine and Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMBINING ALL DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_pairs = muse_pairs + wikidata_pairs + it_pairs\n",
    "print(f\"\\nTotal raw: {len(all_pairs):,}\")\n",
    "print(f\"  MUSE: {len(muse_pairs):,}\")\n",
    "print(f\"  Wikidata: {len(wikidata_pairs):,}\")\n",
    "print(f\"  IT: {len(it_pairs):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_deduplicate(pairs: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Filter low-quality pairs and deduplicate.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FILTERING AND DEDUPLICATION\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    filtered = []\n",
    "    rejected = defaultdict(int)\n",
    "\n",
    "    for pair in tqdm(pairs, desc=\"Filtering\"):\n",
    "        ko = pair[\"ko\"]\n",
    "        en = pair[\"en\"]\n",
    "\n",
    "        # Length checks\n",
    "        if len(ko) < CONFIG[\"min_ko_length\"]:\n",
    "            rejected[\"ko_too_short\"] += 1\n",
    "            continue\n",
    "        if len(en) < CONFIG[\"min_en_length\"]:\n",
    "            rejected[\"en_too_short\"] += 1\n",
    "            continue\n",
    "        if len(ko) > CONFIG[\"max_ko_length\"]:\n",
    "            rejected[\"ko_too_long\"] += 1\n",
    "            continue\n",
    "        if len(en) > CONFIG[\"max_en_length\"]:\n",
    "            rejected[\"en_too_long\"] += 1\n",
    "            continue\n",
    "\n",
    "        # Korean validation\n",
    "        if not is_valid_korean(ko):\n",
    "            rejected[\"no_korean\"] += 1\n",
    "            continue\n",
    "\n",
    "        # English validation\n",
    "        if not is_valid_english(en):\n",
    "            rejected[\"invalid_english\"] += 1\n",
    "            continue\n",
    "\n",
    "        # Skip if English is just numbers or special chars\n",
    "        clean_en = re.sub(r'[^a-zA-Z]', '', en)\n",
    "        if len(clean_en) < 2:\n",
    "            rejected[\"en_no_letters\"] += 1\n",
    "            continue\n",
    "\n",
    "        filtered.append(pair)\n",
    "\n",
    "    print(f\"Filtered: {len(pairs):,} -> {len(filtered):,}\")\n",
    "    print(\"Rejection reasons:\")\n",
    "    for reason, count in sorted(rejected.items(), key=lambda x: -x[1])[:10]:\n",
    "        print(f\"  {reason}: {count:,}\")\n",
    "\n",
    "    # Deduplication\n",
    "    seen = set()\n",
    "    unique = []\n",
    "\n",
    "    for pair in tqdm(filtered, desc=\"Deduplicating\"):\n",
    "        key = (pair[\"ko\"].strip(), pair[\"en\"].strip().lower())\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(pair)\n",
    "\n",
    "    print(f\"After deduplication: {len(unique):,}\")\n",
    "    return unique\n",
    "\n",
    "\n",
    "final_pairs = filter_and_deduplicate(all_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 5. Statistics and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nTotal pairs: {len(final_pairs):,}\")\n",
    "\n",
    "# By source\n",
    "sources = defaultdict(int)\n",
    "for p in final_pairs:\n",
    "    sources[p[\"source\"]] += 1\n",
    "\n",
    "print(\"\\nBy source:\")\n",
    "for src, cnt in sorted(sources.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {src}: {cnt:,} ({cnt/len(final_pairs)*100:.1f}%)\")\n",
    "\n",
    "# Unique Korean terms\n",
    "unique_ko = len(set(p[\"ko\"] for p in final_pairs))\n",
    "unique_en = len(set(p[\"en\"] for p in final_pairs))\n",
    "print(f\"\\nUnique Korean terms: {unique_ko:,}\")\n",
    "print(f\"Unique English terms: {unique_en:,}\")\n",
    "\n",
    "# Samples\n",
    "import random\n",
    "print(\"\\nSample pairs:\")\n",
    "for p in random.sample(final_pairs, min(15, len(final_pairs))):\n",
    "    print(f\"  {p['ko']} -> {p['en']} ({p['source']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check key terms\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY TERMS CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "key_terms = ['자연어처리', '인증', '인공지능', '검색', '추천', '신경망', '기계학습']\n",
    "\n",
    "for term in key_terms:\n",
    "    matches = [p for p in final_pairs if p['ko'] == term]\n",
    "    if matches:\n",
    "        en_terms = list(set([m['en'] for m in matches]))[:5]\n",
    "        print(f\"{term}: {len(matches)}건 -> {en_terms}\")\n",
    "    else:\n",
    "        print(f\"{term}: NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "output_path = OUTPUT_DIR / \"term_pairs.jsonl\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for p in tqdm(final_pairs, desc=\"Saving\"):\n",
    "        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"\\nSaved: {output_path}\")\n",
    "print(f\"Size: {output_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA COLLECTION COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nOutput: {output_path}\")\n",
    "print(f\"Total: {len(final_pairs):,} term pairs\")\n",
    "print(\"\\nNext: Run 01_data_preparation.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

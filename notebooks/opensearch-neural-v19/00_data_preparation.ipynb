{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v19 Data Collection & Preprocessing\n",
    "\n",
    "This notebook handles data collection and preprocessing for the v19 Korean-English cross-lingual SPLADE model.\n",
    "\n",
    "## Approach\n",
    "\n",
    "**Embedding-based Synonym Discovery**:\n",
    "1. Extract terms from Wikipedia (Korean & English)\n",
    "2. Vectorize terms using multilingual embedding model (e5-large-multilingual or BGE-M3)\n",
    "3. Perform k-means clustering to group semantically similar terms\n",
    "4. Extract Korean-English pairs from same clusters\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "| Source | Description |\n",
    "|--------|-------------|\n",
    "| Wikipedia (KO) | Korean Wikipedia articles |\n",
    "| Wikipedia (EN) | English Wikipedia articles |\n",
    "| MUSE | High-quality cross-lingual dictionary |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root\n",
    "def find_project_root():\n",
    "    \"\"\"Find project root by looking for markers like pyproject.toml or src/\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v19_high_quality\n",
      "PyTorch: 2.10.0.dev20251109+cu130\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Set, Tuple\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# NLP libraries for tokenization and POS tagging\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"dataset\" / \"v19_high_quality\"\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Load Embedding Model\n",
    "\n",
    "We use multilingual embedding models to vectorize terms:\n",
    "- **intfloat/multilingual-e5-large**: High-quality multilingual embeddings\n",
    "- **BAAI/bge-m3**: Alternative multilingual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  embedding_model: intfloat/multilingual-e5-large\n",
      "  max_terms_per_source: 50000\n",
      "  batch_size: 64\n",
      "  n_clusters: 5000\n",
      "  min_cluster_size: 2\n",
      "  similarity_threshold: 0.8\n",
      "  max_targets_per_source: 8\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"embedding_model\": \"intfloat/multilingual-e5-large\",  # or \"BAAI/bge-m3\"\n",
    "    \"max_terms_per_source\": 50000,  # Limit terms per language\n",
    "    \"batch_size\": 64,\n",
    "    \"n_clusters\": 5000,  # Number of k-means clusters\n",
    "    \"min_cluster_size\": 2,  # Minimum terms in cluster to consider\n",
    "    \"similarity_threshold\": 0.8,  # Increased from 0.7 for higher quality\n",
    "    \"max_targets_per_source\": 8,  # Limit targets per Korean term\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "Loading embedding model: intfloat/multilingual-e5-large...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/west/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  queued_call()\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "print(f\"\\nLoading embedding model: {CONFIG['embedding_model']}...\")\n",
    "embed_tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"embedding_model\"])\n",
    "embed_model = AutoModel.from_pretrained(CONFIG[\"embedding_model\"])\n",
    "embed_model = embed_model.to(device)\n",
    "embed_model.eval()\n",
    "\n",
    "print(f\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts: List[str], batch_size: int = 64) -> np.ndarray:\n",
    "    \"\"\"Get embeddings for a list of texts using the embedding model.\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Add prefix for e5 models\n",
    "    if \"e5\" in CONFIG[\"embedding_model\"].lower():\n",
    "        texts = [f\"query: {t}\" for t in texts]\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        inputs = embed_tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = embed_model(**inputs)\n",
    "            # Use mean pooling\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 2. Extract Terms from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NLTK resources\n",
    "ENGLISH_STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# POS tags to keep (nouns, verbs, adjectives, adverbs)\n",
    "# NN: noun, VB: verb, JJ: adjective, RB: adverb\n",
    "VALID_POS_TAGS = {\n",
    "    'NN', 'NNS', 'NNP', 'NNPS',  # Nouns\n",
    "    'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',  # Verbs\n",
    "    'JJ', 'JJR', 'JJS',  # Adjectives\n",
    "}\n",
    "\n",
    "\n",
    "def is_korean_token(text: str) -> bool:\n",
    "    \"\"\"Check if text is a valid Korean token (no spaces, Korean chars only).\"\"\"\n",
    "    if ' ' in text or not text:\n",
    "        return False\n",
    "    has_korean = any('\\uac00' <= c <= '\\ud7a3' for c in text)\n",
    "    has_english = any('a' <= c.lower() <= 'z' for c in text)\n",
    "    return has_korean and not has_english\n",
    "\n",
    "\n",
    "def is_english_token(text: str) -> bool:\n",
    "    \"\"\"Check if text is a valid English token (no spaces, letters only).\"\"\"\n",
    "    if ' ' in text or not text:\n",
    "        return False\n",
    "    return text.isalpha() and text.isascii()\n",
    "\n",
    "\n",
    "def clean_token(token: str) -> str:\n",
    "    \"\"\"Clean and normalize a token.\"\"\"\n",
    "    token = re.sub(r'[^\\w가-힣a-zA-Z]', '', token)\n",
    "    return token.strip()\n",
    "\n",
    "\n",
    "def filter_english_by_pos(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"Filter English tokens by POS tag - keep nouns, verbs, adjectives.\"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    # POS tagging\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # Keep only valid POS tags\n",
    "    filtered = [\n",
    "        token for token, tag in tagged\n",
    "        if tag in VALID_POS_TAGS\n",
    "    ]\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "\n",
    "def extract_tokens_from_wikipedia(\n",
    "    file_path: Path,\n",
    "    language: str,\n",
    "    max_tokens: int = 50000,\n",
    "    min_length: int = 2,\n",
    "    max_length: int = 15\n",
    ") -> List[str]:\n",
    "    \"\"\"Extract single tokens from Wikipedia article text.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to Wikipedia JSONL file\n",
    "        language: 'ko' for Korean, 'en' for English\n",
    "        max_tokens: Maximum number of tokens to extract\n",
    "        min_length: Minimum token length\n",
    "        max_length: Maximum token length\n",
    "    \n",
    "    Returns:\n",
    "        List of unique single tokens\n",
    "    \"\"\"\n",
    "    token_counts = defaultdict(int)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Processing: {file_path.name} (language={language})\")\n",
    "    \n",
    "    is_valid = is_korean_token if language == \"ko\" else is_english_token\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=\"Reading articles\"):\n",
    "            try:\n",
    "                article = json.loads(line.strip())\n",
    "                text = article.get(\"text\", \"\")\n",
    "                \n",
    "                # Split text into words\n",
    "                words = text.split()\n",
    "                \n",
    "                # Collect candidate tokens\n",
    "                candidates = []\n",
    "                for word in words:\n",
    "                    token = clean_token(word)\n",
    "                    \n",
    "                    if language == \"en\":\n",
    "                        token = token.lower()\n",
    "                    \n",
    "                    if not is_valid(token):\n",
    "                        continue\n",
    "                    \n",
    "                    if not (min_length <= len(token) <= max_length):\n",
    "                        continue\n",
    "                    \n",
    "                    # Filter stopwords for English using NLTK\n",
    "                    if language == \"en\" and token in ENGLISH_STOPWORDS:\n",
    "                        continue\n",
    "                    \n",
    "                    # Skip single character Korean tokens\n",
    "                    if language == \"ko\" and len(token) == 1:\n",
    "                        continue\n",
    "                    \n",
    "                    candidates.append(token)\n",
    "                \n",
    "                # POS filtering for English\n",
    "                if language == \"en\" and candidates:\n",
    "                    candidates = filter_english_by_pos(candidates)\n",
    "                \n",
    "                # Count tokens\n",
    "                for token in candidates:\n",
    "                    token_counts[token] += 1\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    # Sort by frequency and take top tokens\n",
    "    sorted_tokens = sorted(token_counts.items(), key=lambda x: -x[1])\n",
    "    top_tokens = [t for t, _ in sorted_tokens[:max_tokens]]\n",
    "    \n",
    "    print(f\"Extracted {len(top_tokens):,} unique tokens\")\n",
    "    return top_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Korean tokens from Wikipedia\n",
    "print(\"=\" * 70)\n",
    "print(\"Extracting Korean Tokens from Wikipedia\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ko_wiki_path = PROJECT_ROOT / \"dataset\" / \"wikipedia\" / \"ko_articles.jsonl\"\n",
    "ko_terms = extract_tokens_from_wikipedia(\n",
    "    ko_wiki_path,\n",
    "    language=\"ko\",\n",
    "    max_tokens=CONFIG[\"max_terms_per_source\"],\n",
    "    min_length=2,\n",
    "    max_length=10  # Korean tokens are typically shorter\n",
    ")\n",
    "\n",
    "print(f\"Korean tokens: {len(ko_terms):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract English tokens from Wikipedia\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Extracting English Tokens from Wikipedia\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "en_wiki_path = PROJECT_ROOT / \"dataset\" / \"wikipedia\" / \"en_articles.jsonl\"\n",
    "en_terms = extract_tokens_from_wikipedia(\n",
    "    en_wiki_path,\n",
    "    language=\"en\",\n",
    "    max_tokens=CONFIG[\"max_terms_per_source\"],\n",
    "    min_length=3,  # English words need at least 3 chars\n",
    "    max_length=15\n",
    ")\n",
    "\n",
    "print(f\"English tokens: {len(en_terms):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample tokens (should be single words without spaces)\n",
    "print(\"\\nSample Korean tokens (top by frequency):\")\n",
    "for t in ko_terms[:15]:\n",
    "    print(f\"  {t}\")\n",
    "\n",
    "print(\"\\nSample English tokens (top by frequency):\")\n",
    "for t in en_terms[:15]:\n",
    "    print(f\"  {t}\")\n",
    "\n",
    "# Verify no spaces\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Validation Check\")\n",
    "print(\"=\" * 70)\n",
    "ko_with_space = [t for t in ko_terms if ' ' in t]\n",
    "en_with_space = [t for t in en_terms if ' ' in t]\n",
    "print(f\"Korean tokens with spaces: {len(ko_with_space)}\")\n",
    "print(f\"English tokens with spaces: {len(en_with_space)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 3. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all terms\n",
    "all_terms = ko_terms + en_terms\n",
    "term_languages = [\"ko\"] * len(ko_terms) + [\"en\"] * len(en_terms)\n",
    "\n",
    "print(f\"Total terms: {len(all_terms):,}\")\n",
    "print(f\"  Korean: {len(ko_terms):,}\")\n",
    "print(f\"  English: {len(en_terms):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all terms\n",
    "print(\"\\nGenerating embeddings...\")\n",
    "embeddings = get_embeddings(all_terms, batch_size=CONFIG[\"batch_size\"])\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 4. K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-means clustering\n",
    "print(f\"\\nPerforming k-means clustering (n_clusters={CONFIG['n_clusters']})...\")\n",
    "\n",
    "kmeans = MiniBatchKMeans(\n",
    "    n_clusters=CONFIG[\"n_clusters\"],\n",
    "    random_state=42,\n",
    "    batch_size=1024,\n",
    "    n_init=3,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "print(f\"Clustering complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze clusters - include language info\n",
    "cluster_stats = defaultdict(list)  # cluster_label -> [(term, index, language), ...]\n",
    "\n",
    "for i, (term, lang, label) in enumerate(zip(all_terms, term_languages, cluster_labels)):\n",
    "    cluster_stats[label].append((term, i, lang))\n",
    "\n",
    "# Find clusters with both Korean and English terms\n",
    "bilingual_clusters = [\n",
    "    (label, items) for label, items in cluster_stats.items()\n",
    "    if any(l == \"ko\" for _, _, l in items) and any(l == \"en\" for _, _, l in items)\n",
    "]\n",
    "\n",
    "# Also include clusters with multiple Korean terms (for Korean synonyms)\n",
    "ko_only_clusters = [\n",
    "    (label, items) for label, items in cluster_stats.items()\n",
    "    if sum(1 for _, _, l in items if l == \"ko\") >= 2 and label not in [b[0] for b in bilingual_clusters]\n",
    "]\n",
    "\n",
    "print(f\"Total clusters: {CONFIG['n_clusters']:,}\")\n",
    "print(f\"Bilingual clusters (KO + EN): {len(bilingual_clusters):,}\")\n",
    "print(f\"Korean-only clusters (2+ KO terms): {len(ko_only_clusters):,}\")\n",
    "print(f\"Total clusters to process: {len(bilingual_clusters) + len(ko_only_clusters):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 5. Extract Korean-English Pairs from Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_similar_terms_from_cluster(\n",
    "    all_items: List[Tuple[str, int, str]],  # (term, index, language)\n",
    "    embeddings: np.ndarray,\n",
    "    threshold: float = 0.7\n",
    ") -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"Extract similar terms (both Korean and English) for each Korean term.\n",
    "    \n",
    "    Args:\n",
    "        all_items: List of (term, embedding_index, language) tuples\n",
    "        embeddings: All term embeddings\n",
    "        threshold: Cosine similarity threshold\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping Korean term to list of (similar_term, similarity) tuples\n",
    "        Similar terms can be both Korean and English\n",
    "    \"\"\"\n",
    "    mappings = defaultdict(list)\n",
    "    \n",
    "    # Separate Korean terms (source) from all terms\n",
    "    ko_items = [(t, i, l) for t, i, l in all_items if l == \"ko\"]\n",
    "    \n",
    "    if not ko_items:\n",
    "        return dict(mappings)\n",
    "    \n",
    "    # Get embeddings\n",
    "    ko_terms = [item[0] for item in ko_items]\n",
    "    ko_indices = [item[1] for item in ko_items]\n",
    "    ko_embeds = embeddings[ko_indices]\n",
    "    \n",
    "    all_terms = [item[0] for item in all_items]\n",
    "    all_indices = [item[1] for item in all_items]\n",
    "    all_embeds = embeddings[all_indices]\n",
    "    \n",
    "    # Compute similarity: Korean terms vs ALL terms (including Korean)\n",
    "    similarities = cosine_similarity(ko_embeds, all_embeds)\n",
    "    \n",
    "    # Find similar terms for each Korean term\n",
    "    for i, ko_term in enumerate(ko_terms):\n",
    "        for j, (other_term, _, other_lang) in enumerate(all_items):\n",
    "            # Skip self\n",
    "            if ko_term == other_term:\n",
    "                continue\n",
    "            \n",
    "            sim = similarities[i, j]\n",
    "            if sim >= threshold:\n",
    "                mappings[ko_term].append((other_term, float(sim)))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    for ko_term in mappings:\n",
    "        mappings[ko_term].sort(key=lambda x: -x[1])\n",
    "    \n",
    "    return dict(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 1:N mappings with mixed Korean/English terms\n",
    "# Limit to top MAX_TARGETS_PER_SOURCE by similarity score\n",
    "print(\"Extracting similar terms (Korean + English mixed) from clusters...\")\n",
    "print(f\"Similarity threshold: {CONFIG['similarity_threshold']}\")\n",
    "print(f\"Max targets per source: {CONFIG['max_targets_per_source']}\")\n",
    "\n",
    "# Aggregate all mappings: ko_term -> [(similar_term, sim), ...]\n",
    "all_mappings: Dict[str, List[Tuple[str, float]]] = defaultdict(list)\n",
    "\n",
    "# Process bilingual clusters\n",
    "all_clusters = bilingual_clusters + ko_only_clusters\n",
    "\n",
    "for label, items in tqdm(all_clusters, desc=\"Processing clusters\"):\n",
    "    cluster_mappings = extract_similar_terms_from_cluster(\n",
    "        items,\n",
    "        embeddings,\n",
    "        threshold=CONFIG[\"similarity_threshold\"]\n",
    "    )\n",
    "    \n",
    "    # Merge into global mappings\n",
    "    for ko_term, similar_list in cluster_mappings.items():\n",
    "        all_mappings[ko_term].extend(similar_list)\n",
    "\n",
    "# Deduplicate, sort by similarity, and LIMIT to top N\n",
    "max_targets = CONFIG[\"max_targets_per_source\"]\n",
    "for ko_term in all_mappings:\n",
    "    seen = {}\n",
    "    for term, sim in all_mappings[ko_term]:\n",
    "        if term not in seen or sim > seen[term]:\n",
    "            seen[term] = sim\n",
    "    # Sort by similarity (descending) and limit to top N\n",
    "    sorted_terms = sorted(seen.items(), key=lambda x: -x[1])[:max_targets]\n",
    "    all_mappings[ko_term] = sorted_terms\n",
    "\n",
    "print(f\"\\nExtracted {len(all_mappings):,} Korean terms with similar terms\")\n",
    "total_terms = sum(len(v) for v in all_mappings.values())\n",
    "avg_per_ko = total_terms / len(all_mappings) if all_mappings else 0\n",
    "print(f\"Total similar term mappings: {total_terms:,}\")\n",
    "print(f\"Average terms per Korean term: {avg_per_ko:.2f} (max: {max_targets})\")\n",
    "\n",
    "# Count Korean vs English in mappings\n",
    "ko_in_mappings = sum(1 for v in all_mappings.values() for t, _ in v if is_korean_token(t))\n",
    "en_in_mappings = sum(1 for v in all_mappings.values() for t, _ in v if is_english_token(t))\n",
    "print(f\"\\nIn similar terms: {ko_in_mappings:,} Korean, {en_in_mappings:,} English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample 1:N mappings with mixed Korean/English terms\n",
    "print(\"\\nSample mappings (Korean -> [Korean + English terms]):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Sort by number of terms\n",
    "sorted_mappings = sorted(all_mappings.items(), key=lambda x: -len(x[1]))\n",
    "\n",
    "for ko_term, term_list in sorted_mappings[:20]:\n",
    "    # Separate Korean and English terms for display\n",
    "    ko_terms_in_list = [f\"{t}({sim:.2f})\" for t, sim in term_list if is_korean_token(t)][:3]\n",
    "    en_terms_in_list = [f\"{t}({sim:.2f})\" for t, sim in term_list if is_english_token(t)][:3]\n",
    "    \n",
    "    terms_str = \", \".join(ko_terms_in_list + en_terms_in_list)\n",
    "    remaining = len(term_list) - len(ko_terms_in_list) - len(en_terms_in_list)\n",
    "    if remaining > 0:\n",
    "        terms_str += f\" ... (+{remaining} more)\"\n",
    "    \n",
    "    print(f\"  {ko_term} -> [{terms_str}]\")\n",
    "\n",
    "# Distribution\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"Term count distribution:\")\n",
    "term_counts = [len(v) for v in all_mappings.values()]\n",
    "print(f\"  Min: {min(term_counts)}, Max: {max(term_counts)}, Mean: {np.mean(term_counts):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 6. Load MUSE Dictionary (High-Quality Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_muse_data() -> Dict[str, List[str]]:\n",
    "    \"\"\"Load MUSE dictionary data as 1:N mappings.\"\"\"\n",
    "    data_path = PROJECT_ROOT / \"dataset\" / \"v15_aggressive\" / \"term_pairs.jsonl\"\n",
    "    mappings: Dict[str, List[str]] = defaultdict(list)\n",
    "    \n",
    "    QUALITY_SOURCES = {\"muse\"}\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        print(f\"MUSE data not found: {data_path}\")\n",
    "        return dict(mappings)\n",
    "\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line.strip())\n",
    "            source = item.get(\"source\", \"\")\n",
    "\n",
    "            if source in QUALITY_SOURCES:\n",
    "                ko = item.get(\"ko\", \"\")\n",
    "                en = item.get(\"en\", \"\").lower()\n",
    "                if ko and en and en not in mappings[ko]:\n",
    "                    mappings[ko].append(en)\n",
    "\n",
    "    print(f\"Loaded {len(mappings):,} Korean terms from MUSE dictionary\")\n",
    "    total_en = sum(len(v) for v in mappings.values())\n",
    "    print(f\"Total English mappings: {total_en:,}\")\n",
    "    return dict(mappings)\n",
    "\n",
    "muse_mappings = load_muse_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cross_lingual_mappings() -> Dict[str, List[str]]:\n",
    "    \"\"\"Load cross-lingual term mappings as 1:N format.\"\"\"\n",
    "    mappings: Dict[str, List[str]] = defaultdict(list)\n",
    "    synonyms_dir = PROJECT_ROOT / \"dataset\" / \"synonyms\"\n",
    "\n",
    "    # cross_lingual_pairs_v2.jsonl (already in 1:N format)\n",
    "    path = synonyms_dir / \"cross_lingual_pairs_v2.jsonl\"\n",
    "    if path.exists():\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line.strip())\n",
    "                ko_term = item.get(\"ko_term\", \"\")\n",
    "                en_terms = item.get(\"en_terms\", [])\n",
    "                for en in en_terms:\n",
    "                    en_lower = en.lower()\n",
    "                    if ko_term and en_lower and en_lower not in mappings[ko_term]:\n",
    "                        mappings[ko_term].append(en_lower)\n",
    "\n",
    "    # ko_en_terms.jsonl\n",
    "    path = synonyms_dir / \"ko_en_terms.jsonl\"\n",
    "    if path.exists():\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line.strip())\n",
    "                ko = item.get(\"ko_term\") or item.get(\"ko\", \"\")\n",
    "                en = (item.get(\"en_term\") or item.get(\"en\", \"\")).lower()\n",
    "                if ko and en and en not in mappings[ko]:\n",
    "                    mappings[ko].append(en)\n",
    "\n",
    "    print(f\"Loaded {len(mappings):,} Korean terms from cross-lingual sources\")\n",
    "    total_en = sum(len(v) for v in mappings.values())\n",
    "    print(f\"Total English mappings: {total_en:,}\")\n",
    "    return dict(mappings)\n",
    "\n",
    "cross_lingual_mappings = load_cross_lingual_mappings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 7. Combine and Process All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all 1:N mappings (Korean + English mixed)\n",
    "# Preserve similarity scores for weighted loss\n",
    "print(\"=\" * 70)\n",
    "print(\"Combining All Data Sources (Mixed Korean/English with Similarity Scores)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Wiki mappings already have similarity scores: ko -> [(term, sim), ...]\n",
    "# MUSE and cross-lingual don't have scores, assign default weight (1.0 for high-quality)\n",
    "\n",
    "# Merge all sources: ko -> {term: similarity}\n",
    "combined_mappings: Dict[str, Dict[str, float]] = defaultdict(dict)\n",
    "\n",
    "# 1. MUSE mappings (high-quality, assign 1.0)\n",
    "for ko, en_list in muse_mappings.items():\n",
    "    for en in en_list:\n",
    "        combined_mappings[ko][en] = max(combined_mappings[ko].get(en, 0.0), 1.0)\n",
    "print(f\"MUSE: {len(muse_mappings):,} Korean terms (sim=1.0)\")\n",
    "\n",
    "# 2. Cross-lingual mappings (high-quality, assign 1.0)\n",
    "for ko, en_list in cross_lingual_mappings.items():\n",
    "    for en in en_list:\n",
    "        combined_mappings[ko][en] = max(combined_mappings[ko].get(en, 0.0), 1.0)\n",
    "print(f\"Cross-lingual: {len(cross_lingual_mappings):,} Korean terms (sim=1.0)\")\n",
    "\n",
    "# 3. Wikipedia cluster mappings (with actual similarity scores)\n",
    "for ko, term_list in all_mappings.items():\n",
    "    for term, sim in term_list:\n",
    "        combined_mappings[ko][term] = max(combined_mappings[ko].get(term, 0.0), sim)\n",
    "print(f\"Wikipedia clusters: {len(all_mappings):,} Korean terms (actual sim)\")\n",
    "\n",
    "# Convert to list format: ko -> [(term, sim), ...] sorted by similarity, limited to max_targets\n",
    "max_targets = CONFIG[\"max_targets_per_source\"]\n",
    "combined_with_scores: Dict[str, List[Tuple[str, float]]] = {}\n",
    "\n",
    "for ko, term_dict in combined_mappings.items():\n",
    "    sorted_terms = sorted(term_dict.items(), key=lambda x: -x[1])[:max_targets]\n",
    "    combined_with_scores[ko] = sorted_terms\n",
    "\n",
    "print(f\"\\nTotal unique Korean terms: {len(combined_with_scores):,}\")\n",
    "total_terms = sum(len(v) for v in combined_with_scores.values())\n",
    "print(f\"Total term mappings: {total_terms:,}\")\n",
    "print(f\"Average terms per Korean: {total_terms/len(combined_with_scores):.2f}\")\n",
    "print(f\"Max targets per source: {max_targets}\")\n",
    "\n",
    "# Count Korean vs English\n",
    "ko_count = sum(1 for v in combined_with_scores.values() for t, _ in v if is_korean_token(t))\n",
    "en_count = sum(1 for v in combined_with_scores.values() for t, _ in v if is_english_token(t))\n",
    "print(f\"Korean terms in mappings: {ko_count:,}\")\n",
    "print(f\"English terms in mappings: {en_count:,}\")\n",
    "\n",
    "# Similarity score statistics\n",
    "all_sims = [s for v in combined_with_scores.values() for _, s in v]\n",
    "print(f\"\\nSimilarity scores: min={min(all_sims):.3f}, max={max(all_sims):.3f}, mean={np.mean(all_sims):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_mappings_quality_with_scores(\n",
    "    mappings: Dict[str, List[Tuple[str, float]]]\n",
    ") -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"Filter out low-quality mappings while preserving similarity scores.\"\"\"\n",
    "    filtered = {}\n",
    "    rejected_ko = defaultdict(int)\n",
    "    rejected_terms = 0\n",
    "    \n",
    "    for ko, term_list in mappings.items():\n",
    "        # Validate Korean source term\n",
    "        if not ko:\n",
    "            rejected_ko[\"empty_ko\"] += 1\n",
    "            continue\n",
    "        \n",
    "        if len(ko) < 2:\n",
    "            rejected_ko[\"ko_too_short\"] += 1\n",
    "            continue\n",
    "        \n",
    "        if all(c.isascii() for c in ko):\n",
    "            rejected_ko[\"ko_only_ascii\"] += 1\n",
    "            continue\n",
    "        \n",
    "        if len(ko) > 15:\n",
    "            rejected_ko[\"ko_too_long\"] += 1\n",
    "            continue\n",
    "        \n",
    "        # Filter target terms (can be Korean or English)\n",
    "        valid_terms = []\n",
    "        for term, sim in term_list:\n",
    "            if not term or len(term) < 2:\n",
    "                rejected_terms += 1\n",
    "                continue\n",
    "            if len(term) > 30:\n",
    "                rejected_terms += 1\n",
    "                continue\n",
    "            # Term should be either valid Korean or valid English\n",
    "            if not (is_korean_token(term) or is_english_token(term)):\n",
    "                rejected_terms += 1\n",
    "                continue\n",
    "            valid_terms.append((term, sim))\n",
    "        \n",
    "        # Keep only if at least one valid term\n",
    "        if valid_terms:\n",
    "            filtered[ko] = valid_terms\n",
    "        else:\n",
    "            rejected_ko[\"no_valid_terms\"] += 1\n",
    "    \n",
    "    print(f\"Quality filtered: {len(mappings):,} -> {len(filtered):,} Korean terms\")\n",
    "    if rejected_ko:\n",
    "        print(\"  Korean term rejection reasons:\")\n",
    "        for reason, count in sorted(rejected_ko.items(), key=lambda x: -x[1]):\n",
    "            print(f\"    {reason}: {count:,}\")\n",
    "    print(f\"  Rejected target terms: {rejected_terms:,}\")\n",
    "    \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply quality filtering (preserving similarity scores)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Processing Data (Mixed Korean/English with Scores)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "final_mappings = filter_mappings_quality_with_scores(combined_with_scores)\n",
    "\n",
    "print(f\"\\nFinal dataset:\")\n",
    "print(f\"  Korean source terms: {len(final_mappings):,}\")\n",
    "total_terms = sum(len(v) for v in final_mappings.values())\n",
    "print(f\"  Total target terms: {total_terms:,}\")\n",
    "print(f\"  Average terms per Korean: {total_terms/len(final_mappings):.2f}\")\n",
    "\n",
    "# Korean vs English breakdown\n",
    "ko_targets = sum(1 for v in final_mappings.values() for t, _ in v if is_korean_token(t))\n",
    "en_targets = sum(1 for v in final_mappings.values() for t, _ in v if is_english_token(t))\n",
    "print(f\"\\n  Target term breakdown:\")\n",
    "print(f\"    Korean targets: {ko_targets:,} ({ko_targets/total_terms*100:.1f}%)\")\n",
    "print(f\"    English targets: {en_targets:,} ({en_targets/total_terms*100:.1f}%)\")\n",
    "\n",
    "# Similarity score statistics\n",
    "all_sims = [s for v in final_mappings.values() for _, s in v]\n",
    "print(f\"\\n  Similarity scores:\")\n",
    "print(f\"    Min: {min(all_sims):.3f}\")\n",
    "print(f\"    Max: {max(all_sims):.3f}\")\n",
    "print(f\"    Mean: {np.mean(all_sims):.3f}\")\n",
    "print(f\"    Median: {np.median(all_sims):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## 8. Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze 1:N distribution\n",
    "print(\"=\" * 70)\n",
    "print(\"1:N Mapping Distribution Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "term_counts = [len(v) for v in final_mappings.values()]\n",
    "\n",
    "print(f\"\\nTarget terms per Korean term:\")\n",
    "print(f\"  Min: {min(term_counts)}\")\n",
    "print(f\"  Max: {max(term_counts)} (limit: {CONFIG['max_targets_per_source']})\")\n",
    "print(f\"  Mean: {np.mean(term_counts):.2f}\")\n",
    "print(f\"  Median: {np.median(term_counts):.1f}\")\n",
    "\n",
    "print(f\"\\nDistribution:\")\n",
    "print(f\"  1 term:     {sum(1 for c in term_counts if c == 1):>6,} ({sum(1 for c in term_counts if c == 1)/len(term_counts)*100:.1f}%)\")\n",
    "print(f\"  2-3 terms:  {sum(1 for c in term_counts if 2 <= c <= 3):>6,} ({sum(1 for c in term_counts if 2 <= c <= 3)/len(term_counts)*100:.1f}%)\")\n",
    "print(f\"  4-5 terms:  {sum(1 for c in term_counts if 4 <= c <= 5):>6,} ({sum(1 for c in term_counts if 4 <= c <= 5)/len(term_counts)*100:.1f}%)\")\n",
    "print(f\"  6-8 terms:  {sum(1 for c in term_counts if 6 <= c <= 8):>6,} ({sum(1 for c in term_counts if 6 <= c <= 8)/len(term_counts)*100:.1f}%)\")\n",
    "print(f\"  >8 terms:   {sum(1 for c in term_counts if c > 8):>6,} ({sum(1 for c in term_counts if c > 8)/len(term_counts)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Histogram of target terms per Korean term\n",
    "ax1 = axes[0]\n",
    "ax1.hist(term_counts, bins=range(1, CONFIG['max_targets_per_source'] + 2), \n",
    "         edgecolor='black', alpha=0.7, align='left')\n",
    "ax1.set_xlabel('Number of target terms')\n",
    "ax1.set_ylabel('Count of Korean terms')\n",
    "ax1.set_title('Distribution: Target terms per Korean term')\n",
    "ax1.axvline(np.mean(term_counts), color='red', linestyle='--', label=f'Mean: {np.mean(term_counts):.1f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Bar chart for categories\n",
    "ax2 = axes[1]\n",
    "categories = ['1', '2-3', '4-5', '6-8']\n",
    "counts_cat = [\n",
    "    sum(1 for c in term_counts if c == 1),\n",
    "    sum(1 for c in term_counts if 2 <= c <= 3),\n",
    "    sum(1 for c in term_counts if 4 <= c <= 5),\n",
    "    sum(1 for c in term_counts if 6 <= c <= 8),\n",
    "]\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(categories)))\n",
    "bars = ax2.bar(categories, counts_cat, color=colors, edgecolor='black')\n",
    "ax2.set_xlabel('Number of target terms')\n",
    "ax2.set_ylabel('Count of Korean terms')\n",
    "ax2.set_title(f'1:N Mapping Distribution (max={CONFIG[\"max_targets_per_source\"]})')\n",
    "\n",
    "for bar, count in zip(bars, counts_cat):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
    "             f'{count:,}', ha='center', fontsize=9)\n",
    "\n",
    "# Histogram of similarity scores\n",
    "ax3 = axes[2]\n",
    "all_sims = [s for v in final_mappings.values() for _, s in v]\n",
    "ax3.hist(all_sims, bins=20, edgecolor='black', alpha=0.7)\n",
    "ax3.set_xlabel('Similarity score')\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_title('Similarity Score Distribution')\n",
    "ax3.axvline(np.mean(all_sims), color='red', linestyle='--', label=f'Mean: {np.mean(all_sims):.3f}')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1:N mappings with mixed Korean/English (with similarity scores)\n",
    "print(\"=\" * 70)\n",
    "print(\"Sample Mappings (Korean -> [Korean + English mixed] with similarity)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show mappings with both Korean and English targets\n",
    "mixed_mappings = [\n",
    "    (ko, terms) for ko, terms in final_mappings.items()\n",
    "    if any(is_korean_token(t) for t, _ in terms) and any(is_english_token(t) for t, _ in terms)\n",
    "]\n",
    "print(f\"\\nMappings with both Korean AND English targets: {len(mixed_mappings):,}\")\n",
    "\n",
    "if mixed_mappings:\n",
    "    sample_mixed = random.sample(mixed_mappings, min(15, len(mixed_mappings)))\n",
    "    print(\"\\nSample mixed mappings (with similarity scores):\")\n",
    "    for ko, terms in sorted(sample_mixed, key=lambda x: -len(x[1])):\n",
    "        terms_str = \", \".join([f\"{t}({s:.2f})\" for t, s in terms[:6]])\n",
    "        if len(terms) > 6:\n",
    "            terms_str += f\" ... (+{len(terms)-6})\"\n",
    "        print(f\"  {ko} -> [{terms_str}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## 9. Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save dataset in 1:N format with similarity scores\n",
    "# Format: {\"ko\": \"프로그램\", \"terms\": [{\"term\": \"program\", \"sim\": 0.95}, {\"term\": \"소프트웨어\", \"sim\": 0.88}]}\n",
    "output_path = OUTPUT_DIR / \"term_mappings.jsonl\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for ko, terms in final_mappings.items():\n",
    "        item = {\n",
    "            \"ko\": ko,\n",
    "            \"terms\": [{\"term\": t, \"sim\": round(s, 4)} for t, s in terms]\n",
    "        }\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Dataset saved to: {output_path}\")\n",
    "print(f\"Total Korean source terms: {len(final_mappings):,}\")\n",
    "total_terms = sum(len(v) for v in final_mappings.values())\n",
    "print(f\"Total target terms: {total_terms:,}\")\n",
    "print(f\"Max targets per source: {CONFIG['max_targets_per_source']}\")\n",
    "\n",
    "file_size = output_path.stat().st_size / 1024\n",
    "print(f\"File size: {file_size:.1f} KB\")\n",
    "\n",
    "# Show sample of saved data\n",
    "print(\"\\nSample saved data:\")\n",
    "with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        item = json.loads(line)\n",
    "        print(f\"  {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata\n",
    "term_counts = [len(v) for v in final_mappings.values()]\n",
    "ko_lengths = [len(ko) for ko in final_mappings.keys()]\n",
    "ko_targets = sum(1 for v in final_mappings.values() for t, _ in v if is_korean_token(t))\n",
    "en_targets = sum(1 for v in final_mappings.values() for t, _ in v if is_english_token(t))\n",
    "all_sims = [s for v in final_mappings.values() for _, s in v]\n",
    "\n",
    "metadata = {\n",
    "    \"version\": \"v19\",\n",
    "    \"format\": \"1:N mixed with similarity scores (Korean term -> Korean + English terms)\",\n",
    "    \"description\": \"Korean-English mixed term mappings using embedding-based clustering\",\n",
    "    \"embedding_model\": CONFIG[\"embedding_model\"],\n",
    "    \"n_clusters\": CONFIG[\"n_clusters\"],\n",
    "    \"similarity_threshold\": CONFIG[\"similarity_threshold\"],\n",
    "    \"max_targets_per_source\": CONFIG[\"max_targets_per_source\"],\n",
    "    \"total_korean_source_terms\": len(final_mappings),\n",
    "    \"total_target_terms\": sum(term_counts),\n",
    "    \"korean_targets\": ko_targets,\n",
    "    \"english_targets\": en_targets,\n",
    "    \"avg_terms_per_korean\": float(np.mean(term_counts)),\n",
    "    \"terms_per_ko_distribution\": {\n",
    "        \"min\": min(term_counts),\n",
    "        \"max\": max(term_counts),\n",
    "        \"mean\": float(np.mean(term_counts)),\n",
    "        \"median\": float(np.median(term_counts)),\n",
    "    },\n",
    "    \"similarity_stats\": {\n",
    "        \"min\": float(min(all_sims)),\n",
    "        \"max\": float(max(all_sims)),\n",
    "        \"mean\": float(np.mean(all_sims)),\n",
    "        \"median\": float(np.median(all_sims)),\n",
    "    },\n",
    "    \"ko_length_stats\": {\n",
    "        \"mean\": float(np.mean(ko_lengths)),\n",
    "        \"min\": min(ko_lengths),\n",
    "        \"max\": max(ko_lengths),\n",
    "    },\n",
    "}\n",
    "\n",
    "metadata_path = OUTPUT_DIR / \"metadata.json\"\n",
    "with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Metadata saved to: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Data Format: 1:N Mixed Mapping with Similarity Scores\n",
    "\n",
    "Each entry maps **one Korean term to multiple Korean AND English terms** with similarity scores:\n",
    "\n",
    "```json\n",
    "{\"ko\": \"프로그램\", \"terms\": [{\"term\": \"program\", \"sim\": 0.95}, {\"term\": \"소프트웨어\", \"sim\": 0.88}]}\n",
    "{\"ko\": \"모델\", \"terms\": [{\"term\": \"model\", \"sim\": 0.92}, {\"term\": \"모델링\", \"sim\": 0.85}]}\n",
    "```\n",
    "\n",
    "### Key Configuration\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `similarity_threshold` | 0.8 | Minimum cosine similarity for inclusion |\n",
    "| `max_targets_per_source` | 8 | Maximum target terms per Korean source |\n",
    "\n",
    "### Data Collection Approach\n",
    "\n",
    "1. **Extract tokens** from Wikipedia (Korean & English)\n",
    "2. **Vectorize** using multilingual embedding model (e5-large-multilingual)\n",
    "3. **K-means clustering** to group semantically similar terms\n",
    "4. **Extract 1:N mixed mappings** - for each Korean term, find top 8 most similar terms (both Korean and English) above 0.8 similarity\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- **Mixed Korean/English**: Enables both cross-lingual AND monolingual expansion\n",
    "- **Limited targets**: Max 8 targets focuses learning on high-quality pairs\n",
    "- **Similarity weights**: Allows weighted loss during training\n",
    "- **High threshold**: 0.8 similarity ensures quality over quantity\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Run training** using `01_training.ipynb` (updated for similarity-weighted loss)\n",
    "2. **Test the model** using `02_inference_test.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928122e9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

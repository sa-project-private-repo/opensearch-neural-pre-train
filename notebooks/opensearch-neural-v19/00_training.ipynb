{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v19 Training - XLM-RoBERTa-large with High-Quality Data\n",
    "\n",
    "This notebook trains the v19 Korean-English cross-lingual SPLADE model.\n",
    "\n",
    "## Key Features:\n",
    "- **Model**: xlm-roberta-large (560M parameters)\n",
    "- **Dataset**: v19_high_quality (~18K pairs, MUSE only, no wikidata)\n",
    "- **Learning rate**: 2e-6\n",
    "- **Epochs**: 10\n",
    "- **Loss weights**: self=2.0, target=5.0, margin=3.0, negative=0.5, sparsity=0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root\n",
    "def find_project_root():\n",
    "    \"\"\"Find project root by looking for markers like pyproject.toml or src/\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.model.splade_model import create_splade_model\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    \"model_name\": \"xlm-roberta-large\",\n",
    "    \"max_length\": 64,\n",
    "\n",
    "    # Data - high-quality only (no wikidata)\n",
    "    \"data_path\": PROJECT_ROOT / \"dataset\" / \"v19_high_quality\" / \"term_pairs.jsonl\",\n",
    "\n",
    "    # Training\n",
    "    \"batch_size\": 32,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"num_epochs\": 10,\n",
    "    \"learning_rate\": 2e-6,\n",
    "    \"warmup_ratio\": 0.2,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "\n",
    "    # Loss weights\n",
    "    \"lambda_self\": 2.0,       # Korean preservation\n",
    "    \"lambda_target\": 5.0,     # English activation\n",
    "    \"lambda_margin\": 3.0,     # Margin loss\n",
    "    \"lambda_negative\": 0.5,   # Non-target language suppression\n",
    "    \"lambda_sparsity\": 0.005, # Sparsity regularization\n",
    "    \"target_margin\": 2.0,\n",
    "\n",
    "    # Mixed precision\n",
    "    \"use_fp16\": True,\n",
    "\n",
    "    # Output\n",
    "    \"output_dir\": PROJECT_ROOT / \"outputs\" / \"v19_xlm_large\",\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_korean_char(c: str) -> bool:\n",
    "    \"\"\"Check if character is Korean.\"\"\"\n",
    "    return (\n",
    "        \"\\uac00\" <= c <= \"\\ud7a3\"\n",
    "        or \"\\u1100\" <= c <= \"\\u11ff\"\n",
    "        or \"\\u3130\" <= c <= \"\\u318f\"\n",
    "    )\n",
    "\n",
    "\n",
    "def is_english_char(c: str) -> bool:\n",
    "    \"\"\"Check if character is English.\"\"\"\n",
    "    return c.isalpha() and c.isascii()\n",
    "\n",
    "\n",
    "def is_non_target_token(token: str) -> bool:\n",
    "    \"\"\"Check if token is from non-target language (not Korean or English).\"\"\"\n",
    "    clean = token.replace(\"\\u2581\", \"\").replace(\"##\", \"\")  # Remove subword markers\n",
    "    if not clean:\n",
    "        return False\n",
    "\n",
    "    has_korean = any(is_korean_char(c) for c in clean)\n",
    "    has_english = any(is_english_char(c) for c in clean)\n",
    "\n",
    "    if has_korean or has_english:\n",
    "        return False\n",
    "\n",
    "    # Check for other languages\n",
    "    has_japanese = any(\n",
    "        \"\\u3040\" <= c <= \"\\u309f\" or \"\\u30a0\" <= c <= \"\\u30ff\" for c in clean\n",
    "    )\n",
    "    has_cjk = any(\"\\u4e00\" <= c <= \"\\u9fff\" for c in clean)\n",
    "    has_cyrillic = any(\"\\u0400\" <= c <= \"\\u04ff\" for c in clean)\n",
    "    has_arabic = any(\"\\u0600\" <= c <= \"\\u06ff\" for c in clean)\n",
    "    has_thai = any(\"\\u0e00\" <= c <= \"\\u0e7f\" for c in clean)\n",
    "    has_greek = any(\"\\u0370\" <= c <= \"\\u03ff\" for c in clean)\n",
    "\n",
    "    return (\n",
    "        has_japanese or has_cjk or has_cyrillic or has_arabic or has_thai or has_greek\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermPairDataset(Dataset):\n",
    "    \"\"\"Dataset for Korean-English term pairs.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: Path, tokenizer, max_length: int = 64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "\n",
    "        print(f\"Loading dataset from {data_path}...\")\n",
    "\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in tqdm(f, desc=\"Loading data\"):\n",
    "                item = json.loads(line.strip())\n",
    "\n",
    "                ko_term = item.get(\"ko\", \"\")\n",
    "                en_term = item.get(\"en\", \"\")\n",
    "\n",
    "                if not ko_term or not en_term:\n",
    "                    continue\n",
    "\n",
    "                # Tokenize Korean term\n",
    "                ko_tokens = tokenizer.tokenize(ko_term)\n",
    "                ko_token_ids = tokenizer.convert_tokens_to_ids(ko_tokens)\n",
    "                ko_token_ids = [\n",
    "                    tid for tid in ko_token_ids if tid != tokenizer.unk_token_id\n",
    "                ]\n",
    "\n",
    "                # Tokenize English term (lowercase)\n",
    "                en_tokens = tokenizer.tokenize(en_term.lower())\n",
    "                en_token_ids = tokenizer.convert_tokens_to_ids(en_tokens)\n",
    "                en_token_ids = [\n",
    "                    tid for tid in en_token_ids if tid != tokenizer.unk_token_id\n",
    "                ]\n",
    "\n",
    "                if ko_token_ids and en_token_ids:\n",
    "                    self.data.append(\n",
    "                        {\n",
    "                            \"ko_term\": ko_term,\n",
    "                            \"en_term\": en_term,\n",
    "                            \"ko_token_ids\": ko_token_ids,\n",
    "                            \"en_token_ids\": en_token_ids,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        print(f\"Loaded {len(self.data):,} valid term pairs\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            item[\"ko_term\"],\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"ko_token_ids\": item[\"ko_token_ids\"],\n",
    "            \"en_token_ids\": item[\"en_token_ids\"],\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function.\"\"\"\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n",
    "        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n",
    "        \"ko_token_ids\": [item[\"ko_token_ids\"] for item in batch],\n",
    "        \"en_token_ids\": [item[\"en_token_ids\"] for item in batch],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermLevelLoss(nn.Module):\n",
    "    \"\"\"Loss function for term-level cross-lingual training.\n",
    "    \n",
    "    Components:\n",
    "    - Self loss: Preserve Korean term tokens\n",
    "    - Target loss: Activate English translation tokens\n",
    "    - Margin loss: Ensure minimum activation for English tokens\n",
    "    - Negative loss: Suppress non-target language tokens\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_margin: float = 2.0, non_target_ids: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.target_margin = target_margin\n",
    "        self.non_target_ids = non_target_ids\n",
    "\n",
    "    def forward(self, sparse_rep, ko_token_ids, en_token_ids):\n",
    "        batch_size = sparse_rep.shape[0]\n",
    "        device = sparse_rep.device\n",
    "\n",
    "        self_loss = torch.tensor(0.0, device=device)\n",
    "        target_loss = torch.tensor(0.0, device=device)\n",
    "        margin_loss = torch.tensor(0.0, device=device)\n",
    "        negative_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "        n_valid = 0\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            rep = sparse_rep[i]\n",
    "\n",
    "            # Self loss: maximize activation of Korean tokens\n",
    "            if ko_token_ids[i]:\n",
    "                ko_ids = torch.tensor(ko_token_ids[i], device=device)\n",
    "                ko_activations = rep[ko_ids]\n",
    "                self_loss = self_loss - torch.log(ko_activations + 1e-8).mean()\n",
    "\n",
    "            # Target loss: maximize activation of English tokens\n",
    "            if en_token_ids[i]:\n",
    "                en_ids = torch.tensor(en_token_ids[i], device=device)\n",
    "                en_activations = rep[en_ids]\n",
    "                target_loss = target_loss - torch.log(en_activations + 1e-8).mean()\n",
    "                margin_loss = margin_loss + F.relu(\n",
    "                    self.target_margin - en_activations\n",
    "                ).mean()\n",
    "\n",
    "            # Negative loss: suppress non-target language tokens\n",
    "            if self.non_target_ids is not None:\n",
    "                non_target_ids_device = self.non_target_ids.to(device)\n",
    "                non_target_activations = rep[non_target_ids_device]\n",
    "                negative_loss = negative_loss + F.relu(\n",
    "                    non_target_activations - 0.1\n",
    "                ).mean()\n",
    "\n",
    "            n_valid += 1\n",
    "\n",
    "        if n_valid > 0:\n",
    "            self_loss = self_loss / n_valid\n",
    "            target_loss = target_loss / n_valid\n",
    "            margin_loss = margin_loss / n_valid\n",
    "            negative_loss = negative_loss / n_valid\n",
    "\n",
    "        return {\n",
    "            \"self\": self_loss,\n",
    "            \"target\": target_loss,\n",
    "            \"margin\": margin_loss,\n",
    "            \"negative\": negative_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pairs for evaluation\n",
    "TEST_PAIRS = [\n",
    "    (\"머신러닝\", [\"machine\", \"learning\"], [\"머신\", \"러닝\"]),\n",
    "    (\"딥러닝\", [\"deep\", \"learning\"], [\"딥\", \"러닝\"]),\n",
    "    (\"자연어처리\", [\"natural\", \"language\", \"processing\"], [\"자연어\", \"처리\"]),\n",
    "    (\"인공지능\", [\"artificial\", \"intelligence\"], [\"인공\", \"지능\"]),\n",
    "    (\"검색엔진\", [\"search\", \"engine\"], [\"검색\", \"엔진\"]),\n",
    "    (\"데이터베이스\", [\"database\"], [\"데이터베이스\"]),\n",
    "    (\"클라우드\", [\"cloud\"], [\"클라우드\"]),\n",
    "    (\"서버\", [\"server\"], [\"서버\"]),\n",
    "    (\"네트워크\", [\"network\"], [\"네트워크\"]),\n",
    "    (\"추천시스템\", [\"recommend\", \"system\"], [\"추천\", \"시스템\"]),\n",
    "    (\"추천\", [\"recommend\", \"recommendation\"], [\"추천\"]),\n",
    "    (\"신경망\", [\"neural\", \"network\"], [\"신경망\"]),\n",
    "    (\"강화학습\", [\"reinforcement\", \"learning\"], [\"강화\", \"학습\"]),\n",
    "    (\"컴퓨터비전\", [\"computer\", \"vision\"], [\"컴퓨터\", \"비전\"]),\n",
    "    (\"음성인식\", [\"speech\", \"recognition\"], [\"음성\", \"인식\"]),\n",
    "]\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, device, top_k=50):\n",
    "    \"\"\"Evaluate model on test pairs.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    ko_activated_total = 0\n",
    "    en_activated_total = 0\n",
    "    ko_expected_total = 0\n",
    "    en_expected_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ko_term, en_expected, ko_expected in TEST_PAIRS:\n",
    "            encoding = tokenizer(\n",
    "                ko_term,\n",
    "                max_length=64,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            with autocast(\"cuda\", enabled=CONFIG[\"use_fp16\"]):\n",
    "                sparse_rep, _ = model(\n",
    "                    encoding[\"input_ids\"].to(device),\n",
    "                    encoding[\"attention_mask\"].to(device),\n",
    "                )\n",
    "\n",
    "            sparse_rep = sparse_rep[0].float().cpu()\n",
    "            top_indices = torch.topk(sparse_rep, k=top_k).indices.tolist()\n",
    "            top_tokens = tokenizer.convert_ids_to_tokens(top_indices)\n",
    "            top_tokens_set = set(top_tokens)\n",
    "\n",
    "            # Check Korean preservation\n",
    "            for ko in ko_expected:\n",
    "                ko_toks = tokenizer.tokenize(ko)\n",
    "                for tok in ko_toks:\n",
    "                    ko_expected_total += 1\n",
    "                    if tok in top_tokens_set:\n",
    "                        ko_activated_total += 1\n",
    "\n",
    "            # Check English activation\n",
    "            for en in en_expected:\n",
    "                en_toks = tokenizer.tokenize(en.lower())\n",
    "                for tok in en_toks:\n",
    "                    en_expected_total += 1\n",
    "                    if tok in top_tokens_set:\n",
    "                        en_activated_total += 1\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    ko_rate = (\n",
    "        ko_activated_total / ko_expected_total * 100 if ko_expected_total > 0 else 0\n",
    "    )\n",
    "    en_rate = (\n",
    "        en_activated_total / en_expected_total * 100 if en_expected_total > 0 else 0\n",
    "    )\n",
    "\n",
    "    return ko_rate, en_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\nLoading tokenizer: {CONFIG['model_name']}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build non-target language token ID list\n",
    "print(\"Building non-target language token ID list...\")\n",
    "non_target_ids = []\n",
    "for token_id in tqdm(range(tokenizer.vocab_size), desc=\"Scanning vocab\"):\n",
    "    token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "    if is_non_target_token(token):\n",
    "        non_target_ids.append(token_id)\n",
    "non_target_ids_tensor = torch.tensor(non_target_ids, dtype=torch.long)\n",
    "print(f\"Found {len(non_target_ids):,} non-target language tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = TermPairDataset(CONFIG[\"data_path\"], tokenizer, CONFIG[\"max_length\"])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset):,}\")\n",
    "print(f\"Batches per epoch: {len(dataloader):,}\")\n",
    "print(f\"Effective batch size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "print(f\"\\nCreating model: {CONFIG['model_name']}...\")\n",
    "model = create_splade_model(\n",
    "    model_name=CONFIG[\"model_name\"],\n",
    "    use_idf=False,\n",
    "    use_expansion=True,\n",
    "    expansion_mode=\"mlm\",\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Parameters: {n_params:,} ({n_params / 1e6:.1f}M)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_fn = TermLevelLoss(\n",
    "    target_margin=CONFIG[\"target_margin\"], \n",
    "    non_target_ids=non_target_ids_tensor\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=CONFIG[\"learning_rate\"], \n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "total_steps = (\n",
    "    len(dataloader) * CONFIG[\"num_epochs\"] // CONFIG[\"gradient_accumulation_steps\"]\n",
    ")\n",
    "warmup_steps = int(total_steps * CONFIG[\"warmup_ratio\"])\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=warmup_steps, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Total optimization steps: {total_steps:,}\")\n",
    "print(f\"Warmup steps: {warmup_steps:,}\")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler(\"cuda\", enabled=CONFIG[\"use_fp16\"])\n",
    "\n",
    "# Create output directory\n",
    "CONFIG[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory: {CONFIG['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 7. Initial Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate before training\n",
    "ko_rate, en_rate = evaluate_model(model, tokenizer, device)\n",
    "print(f\"Initial Performance:\")\n",
    "print(f\"  Korean Preservation: {ko_rate:.1f}%\")\n",
    "print(f\"  English Activation: {en_rate:.1f}%\")\n",
    "print(f\"  Combined Score: {ko_rate + en_rate:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training variables\n",
    "history = []\n",
    "best_score = 0\n",
    "global_step = 0\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(CONFIG[\"num_epochs\"]):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Epoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    model.train()\n",
    "    epoch_losses = defaultdict(float)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        with autocast(\"cuda\", enabled=CONFIG[\"use_fp16\"]):\n",
    "            sparse_rep, _ = model(input_ids, attention_mask)\n",
    "\n",
    "            losses = loss_fn(\n",
    "                sparse_rep,\n",
    "                batch[\"ko_token_ids\"],\n",
    "                batch[\"en_token_ids\"],\n",
    "            )\n",
    "\n",
    "            sparsity_loss = sparse_rep.mean()\n",
    "\n",
    "            total_loss = (\n",
    "                CONFIG[\"lambda_self\"] * losses[\"self\"]\n",
    "                + CONFIG[\"lambda_target\"] * losses[\"target\"]\n",
    "                + CONFIG[\"lambda_margin\"] * losses[\"margin\"]\n",
    "                + CONFIG[\"lambda_negative\"] * losses[\"negative\"]\n",
    "                + CONFIG[\"lambda_sparsity\"] * sparsity_loss\n",
    "            )\n",
    "\n",
    "            total_loss = total_loss / CONFIG[\"gradient_accumulation_steps\"]\n",
    "\n",
    "        scaler.scale(total_loss).backward()\n",
    "\n",
    "        epoch_losses[\"total\"] += total_loss.item() * CONFIG[\"gradient_accumulation_steps\"]\n",
    "        epoch_losses[\"self\"] += losses[\"self\"].item()\n",
    "        epoch_losses[\"target\"] += losses[\"target\"].item()\n",
    "        epoch_losses[\"margin\"] += losses[\"margin\"].item()\n",
    "        epoch_losses[\"negative\"] += losses[\"negative\"].item()\n",
    "\n",
    "        if (batch_idx + 1) % CONFIG[\"gradient_accumulation_steps\"] == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), CONFIG[\"max_grad_norm\"]\n",
    "            )\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            progress_bar.set_postfix(\n",
    "                {\n",
    "                    \"loss\": f\"{epoch_losses['total'] / (batch_idx + 1):.4f}\",\n",
    "                    \"tgt\": f\"{epoch_losses['target'] / (batch_idx + 1):.4f}\",\n",
    "                    \"step\": global_step,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Calculate average losses\n",
    "    n_batches = len(dataloader)\n",
    "    for key in epoch_losses:\n",
    "        epoch_losses[key] /= n_batches\n",
    "\n",
    "    history.append(dict(epoch_losses))\n",
    "\n",
    "    # Evaluate\n",
    "    ko_rate, en_rate = evaluate_model(model, tokenizer, device)\n",
    "    combined_score = ko_rate + en_rate\n",
    "\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"  Total Loss: {epoch_losses['total']:.4f}\")\n",
    "    print(f\"  Self Loss: {epoch_losses['self']:.4f}\")\n",
    "    print(f\"  Target Loss: {epoch_losses['target']:.4f}\")\n",
    "    print(f\"  Korean Preservation: {ko_rate:.1f}%\")\n",
    "    print(f\"  English Activation: {en_rate:.1f}%\")\n",
    "    print(f\"  Combined Score: {combined_score:.1f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint_path = CONFIG[\"output_dir\"] / f\"checkpoint_epoch{epoch + 1}.pt\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"losses\": dict(epoch_losses),\n",
    "            \"ko_rate\": ko_rate,\n",
    "            \"en_rate\": en_rate,\n",
    "            \"config\": {\n",
    "                k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items()\n",
    "            },\n",
    "        },\n",
    "        checkpoint_path,\n",
    "    )\n",
    "    print(f\"  Saved: {checkpoint_path.name}\")\n",
    "\n",
    "    # Save best model\n",
    "    if combined_score > best_score:\n",
    "        best_score = combined_score\n",
    "        best_path = CONFIG[\"output_dir\"] / \"best_model.pt\"\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"ko_rate\": ko_rate,\n",
    "                \"en_rate\": en_rate,\n",
    "                \"combined_score\": combined_score,\n",
    "                \"config\": {\n",
    "                    k: str(v) if isinstance(v, Path) else v\n",
    "                    for k, v in CONFIG.items()\n",
    "                },\n",
    "            },\n",
    "            best_path,\n",
    "        )\n",
    "        print(f\"  *** New best model! Score: {combined_score:.1f} (KO:{ko_rate:.1f}% + EN:{en_rate:.1f}%) ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 9. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_path = CONFIG[\"output_dir\"] / \"final_model.pt\"\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"config\": {\n",
    "            k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items()\n",
    "        },\n",
    "        \"history\": history,\n",
    "    },\n",
    "    final_path,\n",
    ")\n",
    "\n",
    "# Save training history\n",
    "with open(CONFIG[\"output_dir\"] / \"training_history.json\", \"w\") as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Final model saved: {final_path}\")\n",
    "print(f\"Best combined score: {best_score:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 10. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training curves\n",
    "if history:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "    epochs = range(1, len(history) + 1)\n",
    "\n",
    "    # Total loss\n",
    "    axes[0, 0].plot(epochs, [-h['total'] for h in history], '-o', color='#3498db')\n",
    "    axes[0, 0].set_title('Total Loss (negated)')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Self loss\n",
    "    axes[0, 1].plot(epochs, [-h['self'] for h in history], '-o', color='#2ecc71')\n",
    "    axes[0, 1].set_title('Self Loss (Korean Preservation)')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Target loss\n",
    "    axes[1, 0].plot(epochs, [-h['target'] for h in history], '-o', color='#e74c3c')\n",
    "    axes[1, 0].set_title('Target Loss (English Activation)')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Negative loss\n",
    "    axes[1, 1].plot(epochs, [h['negative'] for h in history], '-o', color='#9b59b6')\n",
    "    axes[1, 1].set_title('Negative Loss')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CONFIG[\"output_dir\"] / \"training_curves.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Training curves saved to: {CONFIG['output_dir'] / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After training completes:\n",
    "\n",
    "1. **Run inference tests** using `01_inference_test.ipynb`\n",
    "2. **Analyze results** and compare with previous versions\n",
    "3. **Fine-tune hyperparameters** if needed:\n",
    "   - Adjust loss weights for better Korean/English balance\n",
    "   - Try different learning rates\n",
    "   - Experiment with dataset composition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

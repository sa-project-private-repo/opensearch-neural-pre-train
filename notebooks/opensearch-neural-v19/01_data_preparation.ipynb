{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v19 Data Preparation\n",
    "\n",
    "This notebook processes term pairs collected from `00_data_ingestion.ipynb` and creates the training dataset.\n",
    "\n",
    "## Input/Output\n",
    "\n",
    "- **Input**: `term_pairs.jsonl` (1:1 Korean-English pairs)\n",
    "- **Output**: `term_mappings.jsonl` (1:N Korean to multi-target mappings with similarity scores)\n",
    "\n",
    "## Process\n",
    "\n",
    "1. Load term pairs from `00_data_ingestion.ipynb` output\n",
    "2. Group by Korean source term\n",
    "3. Calculate similarity scores using multilingual embeddings\n",
    "4. Save as 1:N format for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"dataset\" / \"v19_high_quality\"\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Embedding model for similarity calculation\n",
    "    \"embedding_model\": \"BAAI/bge-m3\",  # Best multilingual model\n",
    "    \"batch_size\": 128,\n",
    "    \n",
    "    # Similarity thresholds\n",
    "    \"min_similarity\": 0.3,  # Minimum similarity to keep\n",
    "    \"max_targets_per_source\": 15,  # Max English targets per Korean term\n",
    "    \n",
    "    # Source-based similarity defaults (when embedding fails)\n",
    "    \"default_sim_it\": 0.95,  # IT terminology (high quality)\n",
    "    \"default_sim_muse\": 0.90,  # MUSE dictionary\n",
    "    \"default_sim_wikidata\": 0.85,  # Wikidata\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 1. Load Term Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_pairs_path = DATA_DIR / \"term_pairs.jsonl\"\n",
    "\n",
    "if not term_pairs_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"term_pairs.jsonl not found at {term_pairs_path}\\n\"\n",
    "        \"Please run 00_data_ingestion.ipynb first.\"\n",
    "    )\n",
    "\n",
    "print(f\"Loading term pairs from: {term_pairs_path}\")\n",
    "\n",
    "term_pairs = []\n",
    "with open(term_pairs_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"Loading\"):\n",
    "        term_pairs.append(json.loads(line.strip()))\n",
    "\n",
    "print(f\"\\nLoaded {len(term_pairs):,} term pairs\")\n",
    "\n",
    "# Statistics\n",
    "sources = defaultdict(int)\n",
    "for p in term_pairs:\n",
    "    sources[p.get(\"source\", \"unknown\")] += 1\n",
    "\n",
    "print(\"\\nBy source:\")\n",
    "for src, cnt in sorted(sources.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {src}: {cnt:,} ({cnt/len(term_pairs)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 2. Group by Korean Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group English terms by Korean source\n",
    "ko_to_en: Dict[str, List[Tuple[str, str]]] = defaultdict(list)  # ko -> [(en, source), ...]\n",
    "\n",
    "for pair in term_pairs:\n",
    "    ko = pair[\"ko\"]\n",
    "    en = pair[\"en\"].lower()\n",
    "    source = pair.get(\"source\", \"unknown\")\n",
    "    \n",
    "    # Deduplicate within same Korean term\n",
    "    if (en, source) not in ko_to_en[ko]:\n",
    "        ko_to_en[ko].append((en, source))\n",
    "\n",
    "print(f\"Unique Korean terms: {len(ko_to_en):,}\")\n",
    "\n",
    "# Distribution of English targets per Korean\n",
    "target_counts = [len(v) for v in ko_to_en.values()]\n",
    "print(f\"\\nEnglish targets per Korean:\")\n",
    "print(f\"  Min: {min(target_counts)}\")\n",
    "print(f\"  Max: {max(target_counts)}\")\n",
    "print(f\"  Mean: {np.mean(target_counts):.2f}\")\n",
    "print(f\"  Median: {np.median(target_counts):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Load Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "print(f\"\\nLoading embedding model: {CONFIG['embedding_model']}...\")\n",
    "embed_model = SentenceTransformer(CONFIG[\"embedding_model\"])\n",
    "embed_model = embed_model.to(device)\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Embedding dimension: {embed_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Calculate Similarity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_similarity(source: str) -> float:\n",
    "    \"\"\"Get default similarity based on source.\"\"\"\n",
    "    if source == \"it_terminology\":\n",
    "        return CONFIG[\"default_sim_it\"]\n",
    "    elif source == \"muse\":\n",
    "        return CONFIG[\"default_sim_muse\"]\n",
    "    else:\n",
    "        return CONFIG[\"default_sim_wikidata\"]\n",
    "\n",
    "\n",
    "def calculate_similarities_batch(\n",
    "    ko_terms: List[str],\n",
    "    en_targets_list: List[List[Tuple[str, str]]],\n",
    "    model: SentenceTransformer,\n",
    "    batch_size: int = 128,\n",
    ") -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"Calculate cosine similarities between Korean terms and their English targets.\n",
    "    \n",
    "    Args:\n",
    "        ko_terms: List of Korean source terms\n",
    "        en_targets_list: List of [(en_term, source), ...] for each Korean term\n",
    "        model: SentenceTransformer model\n",
    "        batch_size: Batch size for encoding\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping Korean term to [(en_term, similarity), ...]\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Collect all texts to encode\n",
    "    all_texts = list(ko_terms)\n",
    "    ko_indices = {ko: i for i, ko in enumerate(ko_terms)}\n",
    "    \n",
    "    en_start_idx = len(ko_terms)\n",
    "    en_indices = {}  # (ko, en) -> index in all_texts\n",
    "    \n",
    "    for ko, en_list in zip(ko_terms, en_targets_list):\n",
    "        for en, source in en_list:\n",
    "            if en not in [t for t in all_texts[en_start_idx:]]:\n",
    "                en_indices[(ko, en)] = len(all_texts)\n",
    "                all_texts.append(en)\n",
    "            else:\n",
    "                # Find existing index\n",
    "                for i, t in enumerate(all_texts[en_start_idx:], start=en_start_idx):\n",
    "                    if t == en:\n",
    "                        en_indices[(ko, en)] = i\n",
    "                        break\n",
    "    \n",
    "    print(f\"Encoding {len(all_texts):,} texts...\")\n",
    "    \n",
    "    # Encode all texts\n",
    "    embeddings = model.encode(\n",
    "        all_texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True,\n",
    "        convert_to_numpy=True,\n",
    "    )\n",
    "    \n",
    "    # Calculate similarities\n",
    "    print(\"Calculating similarities...\")\n",
    "    for ko, en_list in tqdm(zip(ko_terms, en_targets_list), total=len(ko_terms)):\n",
    "        ko_idx = ko_indices[ko]\n",
    "        ko_emb = embeddings[ko_idx]\n",
    "        \n",
    "        term_sims = []\n",
    "        for en, source in en_list:\n",
    "            en_idx = en_indices.get((ko, en))\n",
    "            if en_idx is not None:\n",
    "                en_emb = embeddings[en_idx]\n",
    "                # Cosine similarity (embeddings are normalized)\n",
    "                sim = float(np.dot(ko_emb, en_emb))\n",
    "            else:\n",
    "                # Fallback to default\n",
    "                sim = get_default_similarity(source)\n",
    "            \n",
    "            # Apply minimum threshold\n",
    "            if sim >= CONFIG[\"min_similarity\"]:\n",
    "                term_sims.append((en, sim))\n",
    "        \n",
    "        # Sort by similarity and limit\n",
    "        term_sims.sort(key=lambda x: -x[1])\n",
    "        results[ko] = term_sims[:CONFIG[\"max_targets_per_source\"]]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarities\n",
    "ko_list = list(ko_to_en.keys())\n",
    "en_list = [ko_to_en[ko] for ko in ko_list]\n",
    "\n",
    "print(f\"Processing {len(ko_list):,} Korean terms...\")\n",
    "\n",
    "term_mappings = calculate_similarities_batch(\n",
    "    ko_list,\n",
    "    en_list,\n",
    "    embed_model,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 5. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out empty mappings\n",
    "term_mappings = {k: v for k, v in term_mappings.items() if v}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROCESSING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nKorean terms with targets: {len(term_mappings):,}\")\n",
    "total_targets = sum(len(v) for v in term_mappings.values())\n",
    "print(f\"Total target terms: {total_targets:,}\")\n",
    "\n",
    "# Statistics\n",
    "target_counts = [len(v) for v in term_mappings.values()]\n",
    "all_sims = [s for v in term_mappings.values() for _, s in v]\n",
    "\n",
    "print(f\"\\nTargets per Korean term:\")\n",
    "print(f\"  Min: {min(target_counts)}\")\n",
    "print(f\"  Max: {max(target_counts)}\")\n",
    "print(f\"  Mean: {np.mean(target_counts):.2f}\")\n",
    "print(f\"  Median: {np.median(target_counts):.1f}\")\n",
    "\n",
    "print(f\"\\nSimilarity scores:\")\n",
    "print(f\"  Min: {min(all_sims):.4f}\")\n",
    "print(f\"  Max: {max(all_sims):.4f}\")\n",
    "print(f\"  Mean: {np.mean(all_sims):.4f}\")\n",
    "print(f\"  Median: {np.median(all_sims):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Target count distribution\n",
    "ax1 = axes[0]\n",
    "ax1.hist(target_counts, bins=range(1, max(target_counts) + 2), \n",
    "         edgecolor='black', alpha=0.7, align='left')\n",
    "ax1.set_xlabel('Number of targets')\n",
    "ax1.set_ylabel('Count of Korean terms')\n",
    "ax1.set_title('Targets per Korean Term')\n",
    "ax1.axvline(np.mean(target_counts), color='red', linestyle='--', \n",
    "            label=f'Mean: {np.mean(target_counts):.1f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Similarity score distribution\n",
    "ax2 = axes[1]\n",
    "ax2.hist(all_sims, bins=30, edgecolor='black', alpha=0.7)\n",
    "ax2.set_xlabel('Similarity Score')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Similarity Score Distribution')\n",
    "ax2.axvline(np.mean(all_sims), color='red', linestyle='--',\n",
    "            label=f'Mean: {np.mean(all_sims):.3f}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample mappings\n",
    "print(\"=\" * 70)\n",
    "print(\"SAMPLE MAPPINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show key terms\n",
    "key_terms = ['자연어처리', '인증', '인공지능', '검색', '추천', '신경망', '기계학습', '머신러닝', '딥러닝']\n",
    "\n",
    "print(\"\\nKey IT Terms:\")\n",
    "for ko in key_terms:\n",
    "    if ko in term_mappings:\n",
    "        targets = term_mappings[ko][:5]\n",
    "        targets_str = \", \".join([f\"{t}({s:.2f})\" for t, s in targets])\n",
    "        print(f\"  {ko} -> [{targets_str}]\")\n",
    "    else:\n",
    "        print(f\"  {ko} -> NOT FOUND\")\n",
    "\n",
    "# Random samples\n",
    "import random\n",
    "print(\"\\nRandom Samples:\")\n",
    "sample_keys = random.sample(list(term_mappings.keys()), min(10, len(term_mappings)))\n",
    "for ko in sample_keys:\n",
    "    targets = term_mappings[ko][:4]\n",
    "    targets_str = \", \".join([f\"{t}({s:.2f})\" for t, s in targets])\n",
    "    print(f\"  {ko} -> [{targets_str}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 6. Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to term_mappings.jsonl\n",
    "# Format: {\"ko\": \"프로그램\", \"terms\": [{\"term\": \"program\", \"sim\": 0.95}, ...]}\n",
    "\n",
    "output_path = DATA_DIR / \"term_mappings.jsonl\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for ko, targets in tqdm(term_mappings.items(), desc=\"Saving\"):\n",
    "        item = {\n",
    "            \"ko\": ko,\n",
    "            \"terms\": [{\"term\": t, \"sim\": round(s, 4)} for t, s in targets]\n",
    "        }\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"\\nSaved: {output_path}\")\n",
    "print(f\"Size: {output_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata\n",
    "metadata = {\n",
    "    \"version\": \"v19\",\n",
    "    \"format\": \"1:N term mappings with similarity scores\",\n",
    "    \"embedding_model\": CONFIG[\"embedding_model\"],\n",
    "    \"min_similarity\": CONFIG[\"min_similarity\"],\n",
    "    \"max_targets_per_source\": CONFIG[\"max_targets_per_source\"],\n",
    "    \"total_korean_terms\": len(term_mappings),\n",
    "    \"total_targets\": sum(len(v) for v in term_mappings.values()),\n",
    "    \"avg_targets_per_korean\": float(np.mean([len(v) for v in term_mappings.values()])),\n",
    "    \"similarity_stats\": {\n",
    "        \"min\": float(min(all_sims)),\n",
    "        \"max\": float(max(all_sims)),\n",
    "        \"mean\": float(np.mean(all_sims)),\n",
    "        \"median\": float(np.median(all_sims)),\n",
    "    },\n",
    "}\n",
    "\n",
    "metadata_path = DATA_DIR / \"metadata.json\"\n",
    "with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Metadata saved: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify saved data\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nSample saved entries:\")\n",
    "with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        item = json.loads(line)\n",
    "        terms_str = \", \".join([f\"{t['term']}({t['sim']:.2f})\" for t in item['terms'][:3]])\n",
    "        print(f\"  {item['ko']} -> [{terms_str}...]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  term_mappings.jsonl: {len(term_mappings):,} Korean terms\")\n",
    "print(f\"  metadata.json: Configuration and statistics\")\n",
    "\n",
    "print(f\"\\nDataset ready for training!\")\n",
    "print(f\"Next: Run 02_training.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Output Format\n",
    "\n",
    "```json\n",
    "{\"ko\": \"머신러닝\", \"terms\": [{\"term\": \"machine\", \"sim\": 0.92}, {\"term\": \"learning\", \"sim\": 0.88}]}\n",
    "{\"ko\": \"인증\", \"terms\": [{\"term\": \"authentication\", \"sim\": 0.95}, {\"term\": \"verification\", \"sim\": 0.82}]}\n",
    "```\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **1:N Mapping**: Each Korean term maps to multiple English targets\n",
    "- **Similarity Scores**: BGE-M3 embeddings for semantic similarity\n",
    "- **Quality Filtering**: Minimum similarity threshold applied\n",
    "- **Limited Targets**: Max 15 targets per Korean term\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Run `02_training.ipynb` to train the SPLADE model\n",
    "2. Run `03_inference_test.ipynb` to evaluate results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

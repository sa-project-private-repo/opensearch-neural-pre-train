{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v19 Data Collection & Preprocessing\n",
    "\n",
    "This notebook handles data collection and preprocessing for the v19 Korean-English cross-lingual SPLADE model.\n",
    "\n",
    "## Approach\n",
    "\n",
    "**Embedding-based Synonym Discovery with Quality Constraints**:\n",
    "1. Load existing term pairs from cross-lingual sources\n",
    "2. Vectorize terms using multilingual embedding model (e5-large-multilingual)\n",
    "3. Perform k-means clustering to group semantically similar terms\n",
    "4. Extract Korean to mixed (Korean + English) term mappings\n",
    "5. **Limit to max 8 targets per source** with similarity threshold 0.8\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "| Source | Description |\n",
    "|--------|-------------|\n",
    "| term_pairs.jsonl | High-quality cross-lingual pairs |\n",
    "| Wikipedia (Optional) | Korean/English Wikipedia articles |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root\n",
    "def find_project_root():\n",
    "    \"\"\"Find project root by looking for markers like pyproject.toml or src/\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Set, Tuple\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# NLP libraries for tokenization and POS tagging\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"dataset\" / \"v19_high_quality\"\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Load Embedding Model\n",
    "\n",
    "We use multilingual embedding models to vectorize terms:\n",
    "- **intfloat/multilingual-e5-large**: High-quality multilingual embeddings\n",
    "- **BAAI/bge-m3**: Alternative multilingual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"embedding_model\": \"intfloat/multilingual-e5-large\",  # or \"BAAI/bge-m3\"\n",
    "    \"max_terms_per_source\": 50000,  # Limit terms per language\n",
    "    \"batch_size\": 64,\n",
    "    \"n_clusters\": 5000,  # Number of k-means clusters\n",
    "    \"min_cluster_size\": 2,  # Minimum terms in cluster to consider\n",
    "    \"similarity_threshold\": 0.8,  # Increased from 0.7 for higher quality\n",
    "    \"max_targets_per_source\": 8,  # Limit targets per Korean term\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "print(f\"\\nLoading embedding model: {CONFIG['embedding_model']}...\")\n",
    "embed_tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"embedding_model\"])\n",
    "embed_model = AutoModel.from_pretrained(CONFIG[\"embedding_model\"])\n",
    "embed_model = embed_model.to(device)\n",
    "embed_model.eval()\n",
    "\n",
    "print(f\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts: List[str], batch_size: int = 64) -> np.ndarray:\n",
    "    \"\"\"Get embeddings for a list of texts using the embedding model.\n",
    "    \n",
    "    Note: Embeddings are L2-normalized, so dot product = cosine similarity.\n",
    "    This ensures similarity scores are in range [-1, 1].\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Add prefix for e5 models\n",
    "    if \"e5\" in CONFIG[\"embedding_model\"].lower():\n",
    "        texts = [f\"query: {t}\" for t in texts]\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        inputs = embed_tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = embed_model(**inputs)\n",
    "            # Use mean pooling\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "            # L2 normalize: dot product of normalized vectors = cosine similarity\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 2. Extract Terms from Data Sources\n",
    "\n",
    "We try to extract terms from:\n",
    "1. Wikipedia (if available)\n",
    "2. Existing term pairs file (fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NLTK resources\n",
    "ENGLISH_STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# POS tags to keep (nouns, verbs, adjectives, adverbs)\n",
    "# NN: noun, VB: verb, JJ: adjective, RB: adverb\n",
    "VALID_POS_TAGS = {\n",
    "    'NN', 'NNS', 'NNP', 'NNPS',  # Nouns\n",
    "    'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',  # Verbs\n",
    "    'JJ', 'JJR', 'JJS',  # Adjectives\n",
    "}\n",
    "\n",
    "\n",
    "def is_korean_token(text: str) -> bool:\n",
    "    \"\"\"Check if text is a valid Korean token (no spaces, Korean chars only).\"\"\"\n",
    "    if ' ' in text or not text:\n",
    "        return False\n",
    "    has_korean = any('\\uac00' <= c <= '\\ud7a3' for c in text)\n",
    "    has_english = any('a' <= c.lower() <= 'z' for c in text)\n",
    "    return has_korean and not has_english\n",
    "\n",
    "\n",
    "def is_english_token(text: str) -> bool:\n",
    "    \"\"\"Check if text is a valid English token (no spaces, letters only).\"\"\"\n",
    "    if ' ' in text or not text:\n",
    "        return False\n",
    "    return text.isalpha() and text.isascii()\n",
    "\n",
    "\n",
    "def clean_token(token: str) -> str:\n",
    "    \"\"\"Clean and normalize a token.\"\"\"\n",
    "    token = re.sub(r'[^\\w가-힣a-zA-Z]', '', token)\n",
    "    return token.strip()\n",
    "\n",
    "\n",
    "def filter_english_by_pos(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"Filter English tokens by POS tag - keep nouns, verbs, adjectives.\"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    # POS tagging\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # Keep only valid POS tags\n",
    "    filtered = [\n",
    "        token for token, tag in tagged\n",
    "        if tag in VALID_POS_TAGS\n",
    "    ]\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "\n",
    "def extract_tokens_from_wikipedia(\n",
    "    file_path: Path,\n",
    "    language: str,\n",
    "    max_tokens: int = 50000,\n",
    "    min_length: int = 2,\n",
    "    max_length: int = 15\n",
    ") -> List[str]:\n",
    "    \"\"\"Extract single tokens from Wikipedia article text.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to Wikipedia JSONL file\n",
    "        language: 'ko' for Korean, 'en' for English\n",
    "        max_tokens: Maximum number of tokens to extract\n",
    "        min_length: Minimum token length\n",
    "        max_length: Maximum token length\n",
    "    \n",
    "    Returns:\n",
    "        List of unique single tokens\n",
    "    \"\"\"\n",
    "    token_counts = defaultdict(int)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Processing: {file_path.name} (language={language})\")\n",
    "    \n",
    "    is_valid = is_korean_token if language == \"ko\" else is_english_token\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=\"Reading articles\"):\n",
    "            try:\n",
    "                article = json.loads(line.strip())\n",
    "                text = article.get(\"text\", \"\")\n",
    "                \n",
    "                # Split text into words\n",
    "                words = text.split()\n",
    "                \n",
    "                # Collect candidate tokens\n",
    "                candidates = []\n",
    "                for word in words:\n",
    "                    token = clean_token(word)\n",
    "                    \n",
    "                    if language == \"en\":\n",
    "                        token = token.lower()\n",
    "                    \n",
    "                    if not is_valid(token):\n",
    "                        continue\n",
    "                    \n",
    "                    if not (min_length <= len(token) <= max_length):\n",
    "                        continue\n",
    "                    \n",
    "                    # Filter stopwords for English using NLTK\n",
    "                    if language == \"en\" and token in ENGLISH_STOPWORDS:\n",
    "                        continue\n",
    "                    \n",
    "                    # Skip single character Korean tokens\n",
    "                    if language == \"ko\" and len(token) == 1:\n",
    "                        continue\n",
    "                    \n",
    "                    candidates.append(token)\n",
    "                \n",
    "                # POS filtering for English\n",
    "                if language == \"en\" and candidates:\n",
    "                    candidates = filter_english_by_pos(candidates)\n",
    "                \n",
    "                # Count tokens\n",
    "                for token in candidates:\n",
    "                    token_counts[token] += 1\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    # Sort by frequency and take top tokens\n",
    "    sorted_tokens = sorted(token_counts.items(), key=lambda x: -x[1])\n",
    "    top_tokens = [t for t, _ in sorted_tokens[:max_tokens]]\n",
    "    \n",
    "    print(f\"Extracted {len(top_tokens):,} unique tokens\")\n",
    "    return top_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data sources availability\n",
    "ko_wiki_path = PROJECT_ROOT / \"dataset\" / \"wikipedia\" / \"ko_articles.jsonl\"\n",
    "en_wiki_path = PROJECT_ROOT / \"dataset\" / \"wikipedia\" / \"en_articles.jsonl\"\n",
    "term_pairs_path = OUTPUT_DIR / \"term_pairs.jsonl\"\n",
    "\n",
    "print(\"Data sources check:\")\n",
    "print(f\"  Korean Wikipedia: {'Found' if ko_wiki_path.exists() else 'NOT FOUND'}\")\n",
    "print(f\"  English Wikipedia: {'Found' if en_wiki_path.exists() else 'NOT FOUND'}\")\n",
    "print(f\"  Term pairs: {'Found' if term_pairs_path.exists() else 'NOT FOUND'}\")\n",
    "\n",
    "USE_WIKIPEDIA = ko_wiki_path.exists() and en_wiki_path.exists()\n",
    "print(f\"\\nUsing Wikipedia: {USE_WIKIPEDIA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract or load terms\n",
    "ko_terms = []\n",
    "en_terms = []\n",
    "\n",
    "if USE_WIKIPEDIA:\n",
    "    # Extract from Wikipedia\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Extracting Korean Tokens from Wikipedia\")\n",
    "    print(\"=\" * 70)\n",
    "    ko_terms = extract_tokens_from_wikipedia(\n",
    "        ko_wiki_path,\n",
    "        language=\"ko\",\n",
    "        max_tokens=CONFIG[\"max_terms_per_source\"],\n",
    "        min_length=2,\n",
    "        max_length=10\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Extracting English Tokens from Wikipedia\")\n",
    "    print(\"=\" * 70)\n",
    "    en_terms = extract_tokens_from_wikipedia(\n",
    "        en_wiki_path,\n",
    "        language=\"en\",\n",
    "        max_tokens=CONFIG[\"max_terms_per_source\"],\n",
    "        min_length=3,\n",
    "        max_length=15\n",
    "    )\n",
    "else:\n",
    "    # Load from existing term_pairs.jsonl\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Loading Terms from term_pairs.jsonl (Wikipedia not available)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    ko_terms_set = set()\n",
    "    en_terms_set = set()\n",
    "    \n",
    "    if term_pairs_path.exists():\n",
    "        with open(term_pairs_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line.strip())\n",
    "                ko = item.get(\"ko\", \"\")\n",
    "                en = item.get(\"en\", \"\").lower()\n",
    "                \n",
    "                # Validate Korean term\n",
    "                if ko and len(ko) >= 2 and not all(c.isascii() for c in ko):\n",
    "                    ko_terms_set.add(ko)\n",
    "                \n",
    "                # Validate English term - split multi-word terms\n",
    "                if en:\n",
    "                    for word in en.split():\n",
    "                        word = word.strip()\n",
    "                        if word.isalpha() and word.isascii() and len(word) >= 3:\n",
    "                            en_terms_set.add(word)\n",
    "        \n",
    "        ko_terms = list(ko_terms_set)\n",
    "        en_terms = list(en_terms_set)\n",
    "    else:\n",
    "        print(\"WARNING: No data source available!\")\n",
    "\n",
    "print(f\"\\nKorean tokens: {len(ko_terms):,}\")\n",
    "print(f\"English tokens: {len(en_terms):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample tokens (should be single words without spaces)\n",
    "print(\"Sample Korean tokens:\")\n",
    "for t in ko_terms[:15]:\n",
    "    print(f\"  {t}\")\n",
    "\n",
    "print(\"\\nSample English tokens:\")\n",
    "for t in en_terms[:15]:\n",
    "    print(f\"  {t}\")\n",
    "\n",
    "# Verify no spaces\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Validation Check\")\n",
    "print(\"=\" * 70)\n",
    "ko_with_space = [t for t in ko_terms if ' ' in t]\n",
    "en_with_space = [t for t in en_terms if ' ' in t]\n",
    "print(f\"Korean tokens with spaces: {len(ko_with_space)}\")\n",
    "print(f\"English tokens with spaces: {len(en_with_space)}\")\n",
    "\n",
    "if not ko_terms or not en_terms:\n",
    "    raise ValueError(\"No terms extracted! Check data sources.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 3. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all terms\n",
    "all_terms = ko_terms + en_terms\n",
    "term_languages = [\"ko\"] * len(ko_terms) + [\"en\"] * len(en_terms)\n",
    "\n",
    "print(f\"Total terms: {len(all_terms):,}\")\n",
    "print(f\"  Korean: {len(ko_terms):,}\")\n",
    "print(f\"  English: {len(en_terms):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all terms\n",
    "print(\"\\nGenerating embeddings...\")\n",
    "embeddings = get_embeddings(all_terms, batch_size=CONFIG[\"batch_size\"])\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 4. K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-means clustering\n",
    "# Adjust n_clusters based on data size\n",
    "n_samples = len(all_terms)\n",
    "n_clusters = min(CONFIG[\"n_clusters\"], n_samples // 5)  # At least 5 samples per cluster\n",
    "n_clusters = max(n_clusters, 10)  # Minimum 10 clusters\n",
    "\n",
    "print(f\"\\nPerforming k-means clustering:\")\n",
    "print(f\"  Total terms: {n_samples:,}\")\n",
    "print(f\"  Target clusters: {n_clusters:,}\")\n",
    "\n",
    "kmeans = MiniBatchKMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    random_state=42,\n",
    "    batch_size=min(1024, n_samples),\n",
    "    n_init=3,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "print(f\"Clustering complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze clusters - include language info\n",
    "cluster_stats = defaultdict(list)  # cluster_label -> [(term, index, language), ...]\n",
    "\n",
    "for i, (term, lang, label) in enumerate(zip(all_terms, term_languages, cluster_labels)):\n",
    "    cluster_stats[label].append((term, i, lang))\n",
    "\n",
    "# Find clusters with both Korean and English terms\n",
    "bilingual_clusters = [\n",
    "    (label, items) for label, items in cluster_stats.items()\n",
    "    if any(l == \"ko\" for _, _, l in items) and any(l == \"en\" for _, _, l in items)\n",
    "]\n",
    "\n",
    "# Also include clusters with multiple Korean terms (for Korean synonyms)\n",
    "ko_only_clusters = [\n",
    "    (label, items) for label, items in cluster_stats.items()\n",
    "    if sum(1 for _, _, l in items if l == \"ko\") >= 2 and label not in [b[0] for b in bilingual_clusters]\n",
    "]\n",
    "\n",
    "print(f\"Total clusters: {CONFIG['n_clusters']:,}\")\n",
    "print(f\"Bilingual clusters (KO + EN): {len(bilingual_clusters):,}\")\n",
    "print(f\"Korean-only clusters (2+ KO terms): {len(ko_only_clusters):,}\")\n",
    "print(f\"Total clusters to process: {len(bilingual_clusters) + len(ko_only_clusters):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 5. Extract Korean-English Pairs from Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_similar_terms_from_cluster(\n",
    "    all_items: List[Tuple[str, int, str]],  # (term, index, language)\n",
    "    embeddings: np.ndarray,\n",
    "    threshold: float = 0.7\n",
    ") -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"Extract similar terms (both Korean and English) for each Korean term.\n",
    "    \n",
    "    Uses DOT PRODUCT similarity (not cosine) for consistency.\n",
    "    \n",
    "    Args:\n",
    "        all_items: List of (term, embedding_index, language) tuples\n",
    "        embeddings: All term embeddings (normalized)\n",
    "        threshold: Dot product similarity threshold\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping Korean term to list of (similar_term, dot_product) tuples\n",
    "    \"\"\"\n",
    "    mappings = defaultdict(list)\n",
    "    \n",
    "    # Separate Korean terms (source) from all terms\n",
    "    ko_items = [(t, i, l) for t, i, l in all_items if l == \"ko\"]\n",
    "    \n",
    "    if not ko_items:\n",
    "        return dict(mappings)\n",
    "    \n",
    "    # Get embeddings\n",
    "    ko_terms = [item[0] for item in ko_items]\n",
    "    ko_indices = [item[1] for item in ko_items]\n",
    "    ko_embeds = embeddings[ko_indices]\n",
    "    \n",
    "    all_terms_in_cluster = [item[0] for item in all_items]\n",
    "    all_indices = [item[1] for item in all_items]\n",
    "    all_embeds = embeddings[all_indices]\n",
    "    \n",
    "    # Compute DOT PRODUCT similarity: Korean terms vs ALL terms\n",
    "    # For normalized vectors, dot product = cosine similarity\n",
    "    similarities = np.dot(ko_embeds, all_embeds.T)\n",
    "    \n",
    "    # Find similar terms for each Korean term\n",
    "    for i, ko_term in enumerate(ko_terms):\n",
    "        for j, (other_term, _, other_lang) in enumerate(all_items):\n",
    "            # Skip self\n",
    "            if ko_term == other_term:\n",
    "                continue\n",
    "            \n",
    "            sim = similarities[i, j]\n",
    "            if sim >= threshold:\n",
    "                mappings[ko_term].append((other_term, float(sim)))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    for ko_term in mappings:\n",
    "        mappings[ko_term].sort(key=lambda x: -x[1])\n",
    "    \n",
    "    return dict(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 1:N mappings with mixed Korean/English terms\n",
    "# Limit to top MAX_TARGETS_PER_SOURCE by similarity score\n",
    "print(\"Extracting similar terms (Korean + English mixed) from clusters...\")\n",
    "print(f\"Similarity threshold: {CONFIG['similarity_threshold']}\")\n",
    "print(f\"Max targets per source: {CONFIG['max_targets_per_source']}\")\n",
    "\n",
    "# Aggregate all mappings: ko_term -> [(similar_term, sim), ...]\n",
    "all_mappings: Dict[str, List[Tuple[str, float]]] = defaultdict(list)\n",
    "\n",
    "# Process bilingual clusters\n",
    "all_clusters = bilingual_clusters + ko_only_clusters\n",
    "\n",
    "for label, items in tqdm(all_clusters, desc=\"Processing clusters\"):\n",
    "    cluster_mappings = extract_similar_terms_from_cluster(\n",
    "        items,\n",
    "        embeddings,\n",
    "        threshold=CONFIG[\"similarity_threshold\"]\n",
    "    )\n",
    "    \n",
    "    # Merge into global mappings\n",
    "    for ko_term, similar_list in cluster_mappings.items():\n",
    "        all_mappings[ko_term].extend(similar_list)\n",
    "\n",
    "# Deduplicate, sort by similarity, and LIMIT to top N\n",
    "max_targets = CONFIG[\"max_targets_per_source\"]\n",
    "for ko_term in all_mappings:\n",
    "    seen = {}\n",
    "    for term, sim in all_mappings[ko_term]:\n",
    "        if term not in seen or sim > seen[term]:\n",
    "            seen[term] = sim\n",
    "    # Sort by similarity (descending) and limit to top N\n",
    "    sorted_terms = sorted(seen.items(), key=lambda x: -x[1])[:max_targets]\n",
    "    all_mappings[ko_term] = sorted_terms\n",
    "\n",
    "print(f\"\\nExtracted {len(all_mappings):,} Korean terms with similar terms\")\n",
    "total_terms = sum(len(v) for v in all_mappings.values())\n",
    "avg_per_ko = total_terms / len(all_mappings) if all_mappings else 0\n",
    "print(f\"Total similar term mappings: {total_terms:,}\")\n",
    "print(f\"Average terms per Korean term: {avg_per_ko:.2f} (max: {max_targets})\")\n",
    "\n",
    "# Count Korean vs English in mappings\n",
    "ko_in_mappings = sum(1 for v in all_mappings.values() for t, _ in v if is_korean_token(t))\n",
    "en_in_mappings = sum(1 for v in all_mappings.values() for t, _ in v if is_english_token(t))\n",
    "print(f\"\\nIn similar terms: {ko_in_mappings:,} Korean, {en_in_mappings:,} English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample 1:N mappings with mixed Korean/English terms\n",
    "print(\"\\nSample mappings (Korean -> [Korean + English terms]):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Sort by number of terms\n",
    "sorted_mappings = sorted(all_mappings.items(), key=lambda x: -len(x[1]))\n",
    "\n",
    "for ko_term, term_list in sorted_mappings[:20]:\n",
    "    # Separate Korean and English terms for display\n",
    "    ko_terms_in_list = [f\"{t}({sim:.2f})\" for t, sim in term_list if is_korean_token(t)][:3]\n",
    "    en_terms_in_list = [f\"{t}({sim:.2f})\" for t, sim in term_list if is_english_token(t)][:3]\n",
    "    \n",
    "    terms_str = \", \".join(ko_terms_in_list + en_terms_in_list)\n",
    "    remaining = len(term_list) - len(ko_terms_in_list) - len(en_terms_in_list)\n",
    "    if remaining > 0:\n",
    "        terms_str += f\" ... (+{remaining} more)\"\n",
    "    \n",
    "    print(f\"  {ko_term} -> [{terms_str}]\")\n",
    "\n",
    "# Distribution\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"Term count distribution:\")\n",
    "term_counts = [len(v) for v in all_mappings.values()]\n",
    "print(f\"  Min: {min(term_counts)}, Max: {max(term_counts)}, Mean: {np.mean(term_counts):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 6. Load MUSE Dictionary (High-Quality Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_muse_data() -> Dict[str, List[str]]:\n",
    "    \"\"\"Load MUSE dictionary data as 1:N mappings.\"\"\"\n",
    "    mappings: Dict[str, List[str]] = defaultdict(list)\n",
    "    \n",
    "    # Try multiple possible paths\n",
    "    possible_paths = [\n",
    "        PROJECT_ROOT / \"dataset\" / \"v15_aggressive\" / \"term_pairs.jsonl\",\n",
    "        PROJECT_ROOT / \"dataset\" / \"muse\" / \"ko-en.txt\",\n",
    "    ]\n",
    "    \n",
    "    for data_path in possible_paths:\n",
    "        if data_path.exists():\n",
    "            print(f\"Loading MUSE data from: {data_path}\")\n",
    "            \n",
    "            if data_path.suffix == \".jsonl\":\n",
    "                with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        item = json.loads(line.strip())\n",
    "                        source = item.get(\"source\", \"\")\n",
    "                        if source == \"muse\":\n",
    "                            ko = item.get(\"ko\", \"\")\n",
    "                            en = item.get(\"en\", \"\").lower()\n",
    "                            if ko and en and en not in mappings[ko]:\n",
    "                                mappings[ko].append(en)\n",
    "            else:\n",
    "                # Plain text format\n",
    "                with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split(\"\\t\")\n",
    "                        if len(parts) >= 2:\n",
    "                            ko, en = parts[0], parts[1].lower()\n",
    "                            if ko and en and en not in mappings[ko]:\n",
    "                                mappings[ko].append(en)\n",
    "            break\n",
    "    \n",
    "    print(f\"Loaded {len(mappings):,} Korean terms from MUSE dictionary\")\n",
    "    total_en = sum(len(v) for v in mappings.values())\n",
    "    print(f\"Total English mappings: {total_en:,}\")\n",
    "    return dict(mappings)\n",
    "\n",
    "muse_mappings = load_muse_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cross_lingual_mappings() -> Dict[str, List[str]]:\n",
    "    \"\"\"Load cross-lingual term mappings as 1:N format.\"\"\"\n",
    "    mappings: Dict[str, List[str]] = defaultdict(list)\n",
    "    \n",
    "    # Try multiple paths\n",
    "    possible_paths = [\n",
    "        PROJECT_ROOT / \"dataset\" / \"synonyms\" / \"cross_lingual_pairs_v2.jsonl\",\n",
    "        PROJECT_ROOT / \"dataset\" / \"synonyms\" / \"ko_en_terms.jsonl\",\n",
    "        OUTPUT_DIR / \"term_pairs.jsonl\",  # Fallback to local term_pairs\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if path.exists():\n",
    "            print(f\"Loading cross-lingual data from: {path}\")\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    item = json.loads(line.strip())\n",
    "                    \n",
    "                    # Handle different formats\n",
    "                    if \"ko_term\" in item:\n",
    "                        ko_term = item.get(\"ko_term\", \"\")\n",
    "                        en_terms = item.get(\"en_terms\", [])\n",
    "                        for en in en_terms:\n",
    "                            en_lower = en.lower()\n",
    "                            if ko_term and en_lower and en_lower not in mappings[ko_term]:\n",
    "                                mappings[ko_term].append(en_lower)\n",
    "                    elif \"ko\" in item:\n",
    "                        ko = item.get(\"ko\", \"\")\n",
    "                        en = item.get(\"en\", \"\").lower()\n",
    "                        if ko and en:\n",
    "                            # Split multi-word English terms into single words\n",
    "                            for word in en.split():\n",
    "                                word = word.strip()\n",
    "                                if word and word not in mappings[ko]:\n",
    "                                    mappings[ko].append(word)\n",
    "\n",
    "    print(f\"Loaded {len(mappings):,} Korean terms from cross-lingual sources\")\n",
    "    total_en = sum(len(v) for v in mappings.values())\n",
    "    print(f\"Total English mappings: {total_en:,}\")\n",
    "    return dict(mappings)\n",
    "\n",
    "cross_lingual_mappings = load_cross_lingual_mappings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 7. Combine and Process All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all 1:N mappings with DOT PRODUCT similarity\n",
    "# Calculate actual similarity for ALL pairs using embeddings\n",
    "print(\"=\" * 70)\n",
    "print(\"Combining All Data Sources with Dot Product Similarity\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Build term -> embedding index mapping\n",
    "term_to_idx = {term: i for i, term in enumerate(all_terms)}\n",
    "\n",
    "def get_dot_product_similarity(term1: str, term2: str) -> float:\n",
    "    \"\"\"Calculate dot product similarity between two terms using embeddings.\"\"\"\n",
    "    idx1 = term_to_idx.get(term1)\n",
    "    idx2 = term_to_idx.get(term2)\n",
    "    \n",
    "    if idx1 is None or idx2 is None:\n",
    "        return 0.0\n",
    "    \n",
    "    # Dot product of normalized vectors = cosine similarity\n",
    "    # But we use raw dot product for unnormalized comparison\n",
    "    return float(np.dot(embeddings[idx1], embeddings[idx2]))\n",
    "\n",
    "# Collect all candidate pairs from all sources\n",
    "all_pairs: Dict[str, Set[str]] = defaultdict(set)\n",
    "\n",
    "# 1. MUSE mappings\n",
    "for ko, en_list in muse_mappings.items():\n",
    "    for en in en_list:\n",
    "        all_pairs[ko].add(en)\n",
    "print(f\"MUSE: {len(muse_mappings):,} Korean terms\")\n",
    "\n",
    "# 2. Cross-lingual mappings\n",
    "for ko, en_list in cross_lingual_mappings.items():\n",
    "    for en in en_list:\n",
    "        all_pairs[ko].add(en)\n",
    "print(f\"Cross-lingual: {len(cross_lingual_mappings):,} Korean terms\")\n",
    "\n",
    "# 3. Wikipedia cluster mappings\n",
    "for ko, term_list in all_mappings.items():\n",
    "    for term, _ in term_list:  # Ignore pre-calculated similarity\n",
    "        all_pairs[ko].add(term)\n",
    "print(f\"Wikipedia clusters: {len(all_mappings):,} Korean terms\")\n",
    "\n",
    "print(f\"\\nTotal unique Korean terms: {len(all_pairs):,}\")\n",
    "\n",
    "# Calculate dot product similarity for ALL pairs\n",
    "print(\"\\nCalculating dot product similarity for all pairs...\")\n",
    "combined_with_scores: Dict[str, List[Tuple[str, float]]] = {}\n",
    "max_targets = CONFIG[\"max_targets_per_source\"]\n",
    "threshold = CONFIG[\"similarity_threshold\"]\n",
    "\n",
    "for ko in tqdm(all_pairs.keys(), desc=\"Computing similarities\"):\n",
    "    term_sims = []\n",
    "    \n",
    "    for term in all_pairs[ko]:\n",
    "        sim = get_dot_product_similarity(ko, term)\n",
    "        if sim >= threshold:\n",
    "            term_sims.append((term, sim))\n",
    "    \n",
    "    # Sort by similarity and limit to top N\n",
    "    term_sims.sort(key=lambda x: -x[1])\n",
    "    combined_with_scores[ko] = term_sims[:max_targets]\n",
    "\n",
    "# Filter out Korean terms with no valid targets\n",
    "combined_with_scores = {k: v for k, v in combined_with_scores.items() if v}\n",
    "\n",
    "print(f\"\\nAfter filtering (threshold={threshold}):\")\n",
    "print(f\"  Korean terms with targets: {len(combined_with_scores):,}\")\n",
    "total_terms = sum(len(v) for v in combined_with_scores.values())\n",
    "print(f\"  Total term mappings: {total_terms:,}\")\n",
    "print(f\"  Average terms per Korean: {total_terms/len(combined_with_scores):.2f}\")\n",
    "print(f\"  Max targets per source: {max_targets}\")\n",
    "\n",
    "# Similarity score statistics\n",
    "all_sims = [s for v in combined_with_scores.values() for _, s in v]\n",
    "if all_sims:\n",
    "    print(f\"\\nDot product similarity scores:\")\n",
    "    print(f\"  Min: {min(all_sims):.4f}\")\n",
    "    print(f\"  Max: {max(all_sims):.4f}\")\n",
    "    print(f\"  Mean: {np.mean(all_sims):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_mappings_quality_with_scores(\n",
    "    mappings: Dict[str, List[Tuple[str, float]]]\n",
    ") -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"Filter out low-quality mappings while preserving similarity scores.\"\"\"\n",
    "    filtered = {}\n",
    "    rejected_ko = defaultdict(int)\n",
    "    rejected_terms = 0\n",
    "    \n",
    "    for ko, term_list in mappings.items():\n",
    "        # Validate Korean source term\n",
    "        if not ko:\n",
    "            rejected_ko[\"empty_ko\"] += 1\n",
    "            continue\n",
    "        \n",
    "        if len(ko) < 2:\n",
    "            rejected_ko[\"ko_too_short\"] += 1\n",
    "            continue\n",
    "        \n",
    "        if all(c.isascii() for c in ko):\n",
    "            rejected_ko[\"ko_only_ascii\"] += 1\n",
    "            continue\n",
    "        \n",
    "        if len(ko) > 15:\n",
    "            rejected_ko[\"ko_too_long\"] += 1\n",
    "            continue\n",
    "        \n",
    "        # Filter target terms (can be Korean or English)\n",
    "        valid_terms = []\n",
    "        for term, sim in term_list:\n",
    "            if not term or len(term) < 2:\n",
    "                rejected_terms += 1\n",
    "                continue\n",
    "            if len(term) > 30:\n",
    "                rejected_terms += 1\n",
    "                continue\n",
    "            # Term should be either valid Korean or valid English\n",
    "            if not (is_korean_token(term) or is_english_token(term)):\n",
    "                rejected_terms += 1\n",
    "                continue\n",
    "            valid_terms.append((term, sim))\n",
    "        \n",
    "        # Keep only if at least one valid term\n",
    "        if valid_terms:\n",
    "            filtered[ko] = valid_terms\n",
    "        else:\n",
    "            rejected_ko[\"no_valid_terms\"] += 1\n",
    "    \n",
    "    print(f\"Quality filtered: {len(mappings):,} -> {len(filtered):,} Korean terms\")\n",
    "    if rejected_ko:\n",
    "        print(\"  Korean term rejection reasons:\")\n",
    "        for reason, count in sorted(rejected_ko.items(), key=lambda x: -x[1]):\n",
    "            print(f\"    {reason}: {count:,}\")\n",
    "    print(f\"  Rejected target terms: {rejected_terms:,}\")\n",
    "    \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply quality filtering (preserving similarity scores)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Processing Data (Mixed Korean/English with Scores)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "final_mappings = filter_mappings_quality_with_scores(combined_with_scores)\n",
    "\n",
    "print(f\"\\nFinal dataset:\")\n",
    "print(f\"  Korean source terms: {len(final_mappings):,}\")\n",
    "total_terms = sum(len(v) for v in final_mappings.values())\n",
    "print(f\"  Total target terms: {total_terms:,}\")\n",
    "print(f\"  Average terms per Korean: {total_terms/len(final_mappings):.2f}\")\n",
    "\n",
    "# Korean vs English breakdown\n",
    "ko_targets = sum(1 for v in final_mappings.values() for t, _ in v if is_korean_token(t))\n",
    "en_targets = sum(1 for v in final_mappings.values() for t, _ in v if is_english_token(t))\n",
    "print(f\"\\n  Target term breakdown:\")\n",
    "print(f\"    Korean targets: {ko_targets:,} ({ko_targets/total_terms*100:.1f}%)\")\n",
    "print(f\"    English targets: {en_targets:,} ({en_targets/total_terms*100:.1f}%)\")\n",
    "\n",
    "# Similarity score statistics\n",
    "all_sims = [s for v in final_mappings.values() for _, s in v]\n",
    "print(f\"\\n  Similarity scores:\")\n",
    "print(f\"    Min: {min(all_sims):.3f}\")\n",
    "print(f\"    Max: {max(all_sims):.3f}\")\n",
    "print(f\"    Mean: {np.mean(all_sims):.3f}\")\n",
    "print(f\"    Median: {np.median(all_sims):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## 8. Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze 1:N distribution\n",
    "print(\"=\" * 70)\n",
    "print(\"1:N Mapping Distribution Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "term_counts = [len(v) for v in final_mappings.values()]\n",
    "\n",
    "print(f\"\\nTarget terms per Korean term:\")\n",
    "print(f\"  Min: {min(term_counts)}\")\n",
    "print(f\"  Max: {max(term_counts)} (limit: {CONFIG['max_targets_per_source']})\")\n",
    "print(f\"  Mean: {np.mean(term_counts):.2f}\")\n",
    "print(f\"  Median: {np.median(term_counts):.1f}\")\n",
    "\n",
    "print(f\"\\nDistribution:\")\n",
    "print(f\"  1 term:     {sum(1 for c in term_counts if c == 1):>6,} ({sum(1 for c in term_counts if c == 1)/len(term_counts)*100:.1f}%)\")\n",
    "print(f\"  2-3 terms:  {sum(1 for c in term_counts if 2 <= c <= 3):>6,} ({sum(1 for c in term_counts if 2 <= c <= 3)/len(term_counts)*100:.1f}%)\")\n",
    "print(f\"  4-5 terms:  {sum(1 for c in term_counts if 4 <= c <= 5):>6,} ({sum(1 for c in term_counts if 4 <= c <= 5)/len(term_counts)*100:.1f}%)\")\n",
    "print(f\"  6-8 terms:  {sum(1 for c in term_counts if 6 <= c <= 8):>6,} ({sum(1 for c in term_counts if 6 <= c <= 8)/len(term_counts)*100:.1f}%)\")\n",
    "print(f\"  >8 terms:   {sum(1 for c in term_counts if c > 8):>6,} ({sum(1 for c in term_counts if c > 8)/len(term_counts)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Histogram of target terms per Korean term\n",
    "ax1 = axes[0]\n",
    "ax1.hist(term_counts, bins=range(1, CONFIG['max_targets_per_source'] + 2), \n",
    "         edgecolor='black', alpha=0.7, align='left')\n",
    "ax1.set_xlabel('Number of target terms')\n",
    "ax1.set_ylabel('Count of Korean terms')\n",
    "ax1.set_title('Distribution: Target terms per Korean term')\n",
    "ax1.axvline(np.mean(term_counts), color='red', linestyle='--', label=f'Mean: {np.mean(term_counts):.1f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Bar chart for categories\n",
    "ax2 = axes[1]\n",
    "categories = ['1', '2-3', '4-5', '6-8']\n",
    "counts_cat = [\n",
    "    sum(1 for c in term_counts if c == 1),\n",
    "    sum(1 for c in term_counts if 2 <= c <= 3),\n",
    "    sum(1 for c in term_counts if 4 <= c <= 5),\n",
    "    sum(1 for c in term_counts if 6 <= c <= 8),\n",
    "]\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(categories)))\n",
    "bars = ax2.bar(categories, counts_cat, color=colors, edgecolor='black')\n",
    "ax2.set_xlabel('Number of target terms')\n",
    "ax2.set_ylabel('Count of Korean terms')\n",
    "ax2.set_title(f'1:N Mapping Distribution (max={CONFIG[\"max_targets_per_source\"]})')\n",
    "\n",
    "for bar, count in zip(bars, counts_cat):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
    "             f'{count:,}', ha='center', fontsize=9)\n",
    "\n",
    "# Histogram of similarity scores\n",
    "ax3 = axes[2]\n",
    "all_sims = [s for v in final_mappings.values() for _, s in v]\n",
    "ax3.hist(all_sims, bins=20, edgecolor='black', alpha=0.7)\n",
    "ax3.set_xlabel('Similarity score')\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_title('Similarity Score Distribution')\n",
    "ax3.axvline(np.mean(all_sims), color='red', linestyle='--', label=f'Mean: {np.mean(all_sims):.3f}')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1:N mappings with mixed Korean/English (with similarity scores)\n",
    "print(\"=\" * 70)\n",
    "print(\"Sample Mappings (Korean -> [Korean + English mixed] with similarity)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show mappings with both Korean and English targets\n",
    "mixed_mappings = [\n",
    "    (ko, terms) for ko, terms in final_mappings.items()\n",
    "    if any(is_korean_token(t) for t, _ in terms) and any(is_english_token(t) for t, _ in terms)\n",
    "]\n",
    "print(f\"\\nMappings with both Korean AND English targets: {len(mixed_mappings):,}\")\n",
    "\n",
    "if mixed_mappings:\n",
    "    sample_mixed = random.sample(mixed_mappings, min(15, len(mixed_mappings)))\n",
    "    print(\"\\nSample mixed mappings (with similarity scores):\")\n",
    "    for ko, terms in sorted(sample_mixed, key=lambda x: -len(x[1])):\n",
    "        terms_str = \", \".join([f\"{t}({s:.2f})\" for t, s in terms[:6]])\n",
    "        if len(terms) > 6:\n",
    "            terms_str += f\" ... (+{len(terms)-6})\"\n",
    "        print(f\"  {ko} -> [{terms_str}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## 9. Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save dataset in 1:N format with similarity scores\n",
    "# Format: {\"ko\": \"프로그램\", \"terms\": [{\"term\": \"program\", \"sim\": 0.95}, {\"term\": \"소프트웨어\", \"sim\": 0.88}]}\n",
    "output_path = OUTPUT_DIR / \"term_mappings.jsonl\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for ko, terms in final_mappings.items():\n",
    "        item = {\n",
    "            \"ko\": ko,\n",
    "            \"terms\": [{\"term\": t, \"sim\": round(s, 4)} for t, s in terms]\n",
    "        }\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Dataset saved to: {output_path}\")\n",
    "print(f\"Total Korean source terms: {len(final_mappings):,}\")\n",
    "total_terms = sum(len(v) for v in final_mappings.values())\n",
    "print(f\"Total target terms: {total_terms:,}\")\n",
    "print(f\"Max targets per source: {CONFIG['max_targets_per_source']}\")\n",
    "\n",
    "file_size = output_path.stat().st_size / 1024\n",
    "print(f\"File size: {file_size:.1f} KB\")\n",
    "\n",
    "# Show sample of saved data\n",
    "print(\"\\nSample saved data:\")\n",
    "with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        item = json.loads(line)\n",
    "        print(f\"  {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata\n",
    "term_counts = [len(v) for v in final_mappings.values()]\n",
    "ko_lengths = [len(ko) for ko in final_mappings.keys()]\n",
    "ko_targets = sum(1 for v in final_mappings.values() for t, _ in v if is_korean_token(t))\n",
    "en_targets = sum(1 for v in final_mappings.values() for t, _ in v if is_english_token(t))\n",
    "all_sims = [s for v in final_mappings.values() for _, s in v]\n",
    "\n",
    "metadata = {\n",
    "    \"version\": \"v19\",\n",
    "    \"format\": \"1:N mixed with dot product similarity\",\n",
    "    \"description\": \"Korean-English mixed term mappings with dot product similarity scores\",\n",
    "    \"embedding_model\": CONFIG[\"embedding_model\"],\n",
    "    \"similarity_method\": \"dot_product (L2-normalized vectors)\",\n",
    "    \"n_clusters\": CONFIG[\"n_clusters\"],\n",
    "    \"similarity_threshold\": CONFIG[\"similarity_threshold\"],\n",
    "    \"max_targets_per_source\": CONFIG[\"max_targets_per_source\"],\n",
    "    \"total_korean_source_terms\": len(final_mappings),\n",
    "    \"total_target_terms\": sum(term_counts),\n",
    "    \"korean_targets\": ko_targets,\n",
    "    \"english_targets\": en_targets,\n",
    "    \"avg_terms_per_korean\": float(np.mean(term_counts)),\n",
    "    \"terms_per_ko_distribution\": {\n",
    "        \"min\": min(term_counts),\n",
    "        \"max\": max(term_counts),\n",
    "        \"mean\": float(np.mean(term_counts)),\n",
    "        \"median\": float(np.median(term_counts)),\n",
    "    },\n",
    "    \"similarity_stats\": {\n",
    "        \"min\": float(min(all_sims)),\n",
    "        \"max\": float(max(all_sims)),\n",
    "        \"mean\": float(np.mean(all_sims)),\n",
    "        \"median\": float(np.median(all_sims)),\n",
    "    },\n",
    "    \"ko_length_stats\": {\n",
    "        \"mean\": float(np.mean(ko_lengths)),\n",
    "        \"min\": min(ko_lengths),\n",
    "        \"max\": max(ko_lengths),\n",
    "    },\n",
    "}\n",
    "\n",
    "metadata_path = OUTPUT_DIR / \"metadata.json\"\n",
    "with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Metadata saved to: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Data Format: 1:N Mixed Mapping with Similarity Scores\n",
    "\n",
    "Each entry maps **one Korean term to multiple Korean AND English terms** with similarity scores:\n",
    "\n",
    "```json\n",
    "{\"ko\": \"프로그램\", \"terms\": [{\"term\": \"program\", \"sim\": 0.95}, {\"term\": \"소프트웨어\", \"sim\": 0.88}]}\n",
    "{\"ko\": \"모델\", \"terms\": [{\"term\": \"model\", \"sim\": 0.92}, {\"term\": \"모델링\", \"sim\": 0.85}]}\n",
    "```\n",
    "\n",
    "### Key Configuration\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `similarity_threshold` | 0.8 | Minimum cosine similarity for inclusion |\n",
    "| `max_targets_per_source` | 8 | Maximum target terms per Korean source |\n",
    "\n",
    "### Data Collection Approach\n",
    "\n",
    "1. **Extract tokens** from Wikipedia (Korean & English)\n",
    "2. **Vectorize** using multilingual embedding model (e5-large-multilingual)\n",
    "3. **K-means clustering** to group semantically similar terms\n",
    "4. **Extract 1:N mixed mappings** - for each Korean term, find top 8 most similar terms (both Korean and English) above 0.8 similarity\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- **Mixed Korean/English**: Enables both cross-lingual AND monolingual expansion\n",
    "- **Limited targets**: Max 8 targets focuses learning on high-quality pairs\n",
    "- **Similarity weights**: Allows weighted loss during training\n",
    "- **High threshold**: 0.8 similarity ensures quality over quantity\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Run training** using `01_training.ipynb` (updated for similarity-weighted loss)\n",
    "2. **Test the model** using `02_inference_test.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928122e9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# v19 Training - Cross-Lingual SPLADE with Separate Korean/English Loss\n\nThis notebook trains the v19 Korean-English cross-lingual SPLADE model.\n\n## Key Features:\n- **Model**: xlm-roberta-large (560M parameters)\n- **Dataset**: v19_high_quality (1:N mixed Korean/English with similarity scores)\n- **Format**: `{\"ko\": \"프로그램\", \"terms\": [{\"term\": \"program\", \"sim\": 0.95}, ...]}`\n\n## Critical Fix: Separate Korean/English Target Losses\n\n**Problem**: The previous training failed because Korean targets naturally get high activation \nfrom shared subword tokens, while English targets require cross-lingual transfer. The combined \nloss was dominated by Korean, causing the model to ignore English.\n\n**Solution**:\n- Separate Korean target loss (`lambda_ko_target=3.0`)\n- Separate English target loss (`lambda_en_target=10.0`) - **3.3x higher weight!**\n- Best model selection prioritizes English activation: `score = KO + 2*EN`\n\nThis ensures the model learns proper cross-lingual expansion from Korean to English."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root\n",
    "def find_project_root():\n",
    "    \"\"\"Find project root by looking for markers like pyproject.toml or src/\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0.dev20251109+cu130\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GB10\n",
      "GPU Memory: 128.5 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/west/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  queued_call()\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.model.splade_model import create_splade_model\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Training Configuration\nCONFIG = {\n    # Model\n    \"model_name\": \"xlm-roberta-large\",\n    \"max_length\": 64,\n\n    # Data - 1:N mixed format (Korean -> [Korean + English terms])\n    \"data_path\": PROJECT_ROOT / \"dataset\" / \"v19_high_quality\" / \"term_mappings.jsonl\",\n\n    # Training - adjusted for larger batch size\n    \"batch_size\": 64,\n    \"gradient_accumulation_steps\": 2,\n    \"num_epochs\": 15,\n    \"learning_rate\": 3e-6,\n    \"warmup_ratio\": 0.1,\n    \"max_grad_norm\": 1.0,\n\n    # Loss weights - SEPARATE weights for Korean and English\n    \"lambda_self\": 2.0,         # Korean source preservation\n    \"lambda_ko_target\": 3.0,    # Korean synonym activation (easier)\n    \"lambda_en_target\": 10.0,   # English translation activation (harder, needs higher weight!)\n    \"lambda_margin\": 3.0,       # Margin loss\n    \"lambda_negative\": 1.0,     # Suppress non-target tokens\n    \"lambda_sparsity\": 0.01,    # Encourage sparsity\n    \"target_margin\": 2.0,\n\n    # Mixed precision\n    \"use_fp16\": True,\n\n    # Output\n    \"output_dir\": PROJECT_ROOT / \"outputs\" / \"v19_xlm_large\",\n}\n\nprint(\"Training Configuration:\")\nfor key, value in CONFIG.items():\n    print(f\"  {key}: {value}\")\n\n# Calculate training stats\ndataset_size = 13684  # from data\nbatches_per_epoch = dataset_size // CONFIG[\"batch_size\"]\nopt_steps_per_epoch = batches_per_epoch // CONFIG[\"gradient_accumulation_steps\"]\ntotal_opt_steps = opt_steps_per_epoch * CONFIG[\"num_epochs\"]\n\nprint(f\"\\nTraining Stats:\")\nprint(f\"  Effective batch size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")\nprint(f\"  Batches per epoch: ~{batches_per_epoch}\")\nprint(f\"  Optimization steps per epoch: ~{opt_steps_per_epoch}\")\nprint(f\"  Total optimization steps: ~{total_opt_steps}\")\nprint(f\"\\n*** Key: lambda_en_target={CONFIG['lambda_en_target']} >> lambda_ko_target={CONFIG['lambda_ko_target']} ***\")\nprint(f\"*** This ensures cross-lingual transfer is properly trained ***\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_korean_char(c: str) -> bool:\n",
    "    \"\"\"Check if character is Korean.\"\"\"\n",
    "    return (\n",
    "        \"\\uac00\" <= c <= \"\\ud7a3\"\n",
    "        or \"\\u1100\" <= c <= \"\\u11ff\"\n",
    "        or \"\\u3130\" <= c <= \"\\u318f\"\n",
    "    )\n",
    "\n",
    "\n",
    "def is_english_char(c: str) -> bool:\n",
    "    \"\"\"Check if character is English.\"\"\"\n",
    "    return c.isalpha() and c.isascii()\n",
    "\n",
    "\n",
    "def is_non_target_token(token: str) -> bool:\n",
    "    \"\"\"Check if token is from non-target language (not Korean or English).\"\"\"\n",
    "    clean = token.replace(\"\\u2581\", \"\").replace(\"##\", \"\")  # Remove subword markers\n",
    "    if not clean:\n",
    "        return False\n",
    "\n",
    "    has_korean = any(is_korean_char(c) for c in clean)\n",
    "    has_english = any(is_english_char(c) for c in clean)\n",
    "\n",
    "    if has_korean or has_english:\n",
    "        return False\n",
    "\n",
    "    # Check for other languages\n",
    "    has_japanese = any(\n",
    "        \"\\u3040\" <= c <= \"\\u309f\" or \"\\u30a0\" <= c <= \"\\u30ff\" for c in clean\n",
    "    )\n",
    "    has_cjk = any(\"\\u4e00\" <= c <= \"\\u9fff\" for c in clean)\n",
    "    has_cyrillic = any(\"\\u0400\" <= c <= \"\\u04ff\" for c in clean)\n",
    "    has_arabic = any(\"\\u0600\" <= c <= \"\\u06ff\" for c in clean)\n",
    "    has_thai = any(\"\\u0e00\" <= c <= \"\\u0e7f\" for c in clean)\n",
    "    has_greek = any(\"\\u0370\" <= c <= \"\\u03ff\" for c in clean)\n",
    "\n",
    "    return (\n",
    "        has_japanese or has_cjk or has_cyrillic or has_arabic or has_thai or has_greek\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "## 3. Dataset Class - Separate Korean/English Targets\n\nDataset for 1:N mixed Korean/English term mappings with similarity scores:\n- Input format: `{\"ko\": \"프로그램\", \"terms\": [{\"term\": \"program\", \"sim\": 0.95}, {\"term\": \"소프트웨어\", \"sim\": 0.88}]}`\n\n**Key Change**: Separates Korean and English targets into distinct lists:\n- `ko_target_ids` / `ko_target_weights`: Korean synonym tokens\n- `en_target_ids` / `en_target_weights`: English translation tokens\n\nThis enables separate loss computation for cross-lingual training."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "class TermMappingDataset(Dataset):\n    \"\"\"Dataset for 1:N Korean to mixed Korean/English term mappings with similarity.\n    \n    Format: {\"ko\": \"프로그램\", \"terms\": [{\"term\": \"program\", \"sim\": 0.95}, ...]}\n    \n    Separates Korean and English targets for explicit cross-lingual training.\n    \"\"\"\n\n    def __init__(self, data_path: Path, tokenizer, max_length: int = 64):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.data = []\n        \n        # Build set of special token IDs to exclude\n        self.special_ids = {\n            tokenizer.pad_token_id,\n            tokenizer.cls_token_id,\n            tokenizer.sep_token_id,\n            tokenizer.unk_token_id,\n            tokenizer.bos_token_id,\n            tokenizer.eos_token_id,\n        }\n        self.special_ids = {t for t in self.special_ids if t is not None}\n        \n        # Add tokens by name\n        for token_name in ['<s>', '</s>', '<pad>', '<unk>', '<mask>']:\n            tid = tokenizer.convert_tokens_to_ids(token_name)\n            if tid != tokenizer.unk_token_id:\n                self.special_ids.add(tid)\n\n        print(f\"Loading dataset from {data_path}...\")\n        print(f\"Special token IDs to exclude: {len(self.special_ids)}\")\n\n        def is_korean_term(text: str) -> bool:\n            \"\"\"Check if term contains Korean characters.\"\"\"\n            return any('\\uac00' <= c <= '\\ud7a3' for c in text)\n\n        total_ko_targets = 0\n        total_en_targets = 0\n\n        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n            for line in tqdm(f, desc=\"Loading data\"):\n                item = json.loads(line.strip())\n\n                ko_term = item.get(\"ko\", \"\")\n                terms_data = item.get(\"terms\", [])\n\n                if not ko_term or not terms_data:\n                    continue\n\n                # Tokenize Korean source term (exclude special tokens)\n                ko_tokens = tokenizer.tokenize(ko_term)\n                ko_token_ids = tokenizer.convert_tokens_to_ids(ko_tokens)\n                ko_token_ids = [\n                    tid for tid in ko_token_ids \n                    if tid != tokenizer.unk_token_id and tid not in self.special_ids\n                ]\n\n                # SEPARATE Korean and English targets\n                ko_target_weights: dict = {}  # Korean target tokens\n                en_target_weights: dict = {}  # English target tokens\n                \n                for term_info in terms_data:\n                    # Handle both old format (string) and new format (dict)\n                    if isinstance(term_info, dict):\n                        term = term_info.get(\"term\", \"\")\n                        sim = term_info.get(\"sim\", 1.0)\n                    else:\n                        term = term_info\n                        sim = 1.0\n                    \n                    # Determine if Korean or English\n                    is_korean = is_korean_term(term)\n                    \n                    # Lowercase for consistency (affects English only)\n                    term_lower = term if is_korean else term.lower()\n                    tokens = tokenizer.tokenize(term_lower)\n                    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n                    \n                    for tid in token_ids:\n                        # Exclude unknown and special tokens\n                        if tid != tokenizer.unk_token_id and tid not in self.special_ids:\n                            if is_korean:\n                                ko_target_weights[tid] = max(ko_target_weights.get(tid, 0.0), sim)\n                            else:\n                                en_target_weights[tid] = max(en_target_weights.get(tid, 0.0), sim)\n\n                # Only include samples that have both source and at least one target\n                if ko_token_ids and (ko_target_weights or en_target_weights):\n                    total_ko_targets += len(ko_target_weights)\n                    total_en_targets += len(en_target_weights)\n                    \n                    self.data.append(\n                        {\n                            \"ko_term\": ko_term,\n                            \"ko_token_ids\": ko_token_ids,\n                            \"ko_target_ids\": list(ko_target_weights.keys()),\n                            \"ko_target_weights\": list(ko_target_weights.values()),\n                            \"en_target_ids\": list(en_target_weights.keys()),\n                            \"en_target_weights\": list(en_target_weights.values()),\n                        }\n                    )\n\n        print(f\"Loaded {len(self.data):,} valid term mappings\")\n        \n        # Statistics\n        samples_with_english = sum(1 for d in self.data if d[\"en_target_ids\"])\n        print(f\"Samples with English targets: {samples_with_english:,} ({samples_with_english/len(self.data)*100:.1f}%)\")\n        print(f\"Total Korean target tokens: {total_ko_targets:,}\")\n        print(f\"Total English target tokens: {total_en_targets:,}\")\n        \n        # Weight statistics\n        all_ko_weights = [w for d in self.data for w in d[\"ko_target_weights\"]]\n        all_en_weights = [w for d in self.data for w in d[\"en_target_weights\"]]\n        if all_ko_weights:\n            print(f\"Korean target similarity: mean={sum(all_ko_weights)/len(all_ko_weights):.3f}\")\n        if all_en_weights:\n            print(f\"English target similarity: mean={sum(all_en_weights)/len(all_en_weights):.3f}\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n\n        encoding = self.tokenizer(\n            item[\"ko_term\"],\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n\n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n            \"ko_token_ids\": item[\"ko_token_ids\"],\n            \"ko_target_ids\": item[\"ko_target_ids\"],\n            \"ko_target_weights\": item[\"ko_target_weights\"],\n            \"en_target_ids\": item[\"en_target_ids\"],\n            \"en_target_weights\": item[\"en_target_weights\"],\n        }\n\n\ndef collate_fn(batch):\n    \"\"\"Custom collate function.\"\"\"\n    return {\n        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n        \"ko_token_ids\": [item[\"ko_token_ids\"] for item in batch],\n        \"ko_target_ids\": [item[\"ko_target_ids\"] for item in batch],\n        \"ko_target_weights\": [item[\"ko_target_weights\"] for item in batch],\n        \"en_target_ids\": [item[\"en_target_ids\"] for item in batch],\n        \"en_target_weights\": [item[\"en_target_weights\"] for item in batch],\n    }"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "## 4. Loss Function - Cross-Lingual with Separate Korean/English\n\n**Critical Design Decision**: Separate Korean and English target losses.\n\nWhy? When the input is Korean (e.g., \"머신러닝\"):\n- Korean targets (e.g., \"딥러닝\") share subword tokens → naturally high activation\n- English targets (e.g., \"machine\", \"learning\") have no overlap → need explicit training\n\nWithout separation, Korean dominates the loss and English is ignored."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "class CrossLingualLoss(nn.Module):\n    \"\"\"Loss function for cross-lingual term mapping with SEPARATE Korean/English losses.\n    \n    Key insight: Korean targets naturally get high activation from shared subword tokens,\n    but English targets require explicit cross-lingual transfer. We separate the losses\n    to ensure English targets get proper training signal.\n    \n    Components:\n    - Self loss: Preserve Korean source term tokens\n    - Korean target loss: Activate Korean synonym tokens (easier, shared subwords)\n    - English target loss: Activate English translation tokens (harder, cross-lingual)\n    - Margin loss: Ensure minimum activation for all target tokens\n    - Negative loss: Suppress non-target language tokens\n    \"\"\"\n\n    def __init__(self, target_margin: float = 2.0, non_target_ids: torch.Tensor = None):\n        super().__init__()\n        self.target_margin = target_margin\n        self.non_target_ids = non_target_ids\n\n    def forward(\n        self,\n        sparse_rep,\n        ko_token_ids,\n        ko_target_ids,\n        ko_target_weights,\n        en_target_ids,\n        en_target_weights,\n    ):\n        \"\"\"\n        Args:\n            sparse_rep: Sparse representations [batch_size, vocab_size]\n            ko_token_ids: List of Korean source token IDs per sample\n            ko_target_ids: List of Korean target token IDs per sample\n            ko_target_weights: List of similarity weights for Korean targets\n            en_target_ids: List of English target token IDs per sample\n            en_target_weights: List of similarity weights for English targets\n        \"\"\"\n        batch_size = sparse_rep.shape[0]\n        device = sparse_rep.device\n\n        self_loss = torch.tensor(0.0, device=device)\n        ko_target_loss = torch.tensor(0.0, device=device)\n        en_target_loss = torch.tensor(0.0, device=device)\n        margin_loss = torch.tensor(0.0, device=device)\n        negative_loss = torch.tensor(0.0, device=device)\n\n        n_valid = 0\n        n_with_english = 0\n\n        for i in range(batch_size):\n            rep = sparse_rep[i]\n\n            # Self loss: maximize activation of Korean source tokens\n            if ko_token_ids[i]:\n                ko_ids = torch.tensor(ko_token_ids[i], device=device)\n                ko_activations = rep[ko_ids]\n                self_loss = self_loss - torch.log(ko_activations + 1e-8).mean()\n\n            # Korean target loss: activate Korean synonym tokens\n            if ko_target_ids[i]:\n                tgt_ids = torch.tensor(ko_target_ids[i], device=device)\n                tgt_weights = torch.tensor(ko_target_weights[i], device=device, dtype=torch.float32)\n                tgt_activations = rep[tgt_ids]\n                \n                # Weighted log loss\n                weighted_log_loss = -torch.log(tgt_activations + 1e-8) * tgt_weights\n                ko_target_loss = ko_target_loss + weighted_log_loss.sum() / (tgt_weights.sum() + 1e-8)\n                \n                # Margin loss for Korean targets\n                margin_violations = F.relu(self.target_margin - tgt_activations) * tgt_weights\n                margin_loss = margin_loss + margin_violations.sum() / (tgt_weights.sum() + 1e-8)\n\n            # English target loss: activate English translation tokens (CRITICAL!)\n            if en_target_ids[i]:\n                n_with_english += 1\n                tgt_ids = torch.tensor(en_target_ids[i], device=device)\n                tgt_weights = torch.tensor(en_target_weights[i], device=device, dtype=torch.float32)\n                tgt_activations = rep[tgt_ids]\n                \n                # Weighted log loss for English tokens\n                weighted_log_loss = -torch.log(tgt_activations + 1e-8) * tgt_weights\n                en_target_loss = en_target_loss + weighted_log_loss.sum() / (tgt_weights.sum() + 1e-8)\n                \n                # Margin loss for English targets (higher margin for cross-lingual)\n                en_margin = self.target_margin * 1.5  # Stronger margin for English\n                margin_violations = F.relu(en_margin - tgt_activations) * tgt_weights\n                margin_loss = margin_loss + margin_violations.sum() / (tgt_weights.sum() + 1e-8)\n\n            # Negative loss: suppress non-target language tokens\n            if self.non_target_ids is not None:\n                non_target_ids_device = self.non_target_ids.to(device)\n                non_target_activations = rep[non_target_ids_device]\n                negative_loss = negative_loss + F.relu(\n                    non_target_activations - 0.1\n                ).mean()\n\n            n_valid += 1\n\n        if n_valid > 0:\n            self_loss = self_loss / n_valid\n            ko_target_loss = ko_target_loss / n_valid\n            margin_loss = margin_loss / n_valid\n            negative_loss = negative_loss / n_valid\n        \n        # English loss: only divide by samples that have English targets\n        if n_with_english > 0:\n            en_target_loss = en_target_loss / n_with_english\n\n        return {\n            \"self\": self_loss,\n            \"ko_target\": ko_target_loss,\n            \"en_target\": en_target_loss,\n            \"margin\": margin_loss,\n            \"negative\": negative_loss,\n        }"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pairs for evaluation (Korean source -> expected Korean synonyms + English translations)\n",
    "TEST_PAIRS = [\n",
    "    # (source_ko, expected_english, expected_korean_synonyms)\n",
    "    (\"머신러닝\", [\"machine\", \"learning\"], [\"머신\", \"러닝\", \"기계학습\"]),\n",
    "    (\"딥러닝\", [\"deep\", \"learning\"], [\"딥\", \"러닝\", \"심층학습\"]),\n",
    "    (\"자연어처리\", [\"natural\", \"language\", \"processing\"], [\"자연어\", \"처리\"]),\n",
    "    (\"인공지능\", [\"artificial\", \"intelligence\"], [\"인공\", \"지능\"]),\n",
    "    (\"검색엔진\", [\"search\", \"engine\"], [\"검색\", \"엔진\"]),\n",
    "    (\"데이터베이스\", [\"database\"], [\"데이터\", \"베이스\"]),\n",
    "    (\"클라우드\", [\"cloud\"], [\"클라우드\"]),\n",
    "    (\"서버\", [\"server\"], [\"서버\"]),\n",
    "    (\"네트워크\", [\"network\"], [\"네트워크\"]),\n",
    "    (\"추천시스템\", [\"recommend\", \"system\"], [\"추천\", \"시스템\"]),\n",
    "    (\"추천\", [\"recommend\", \"recommendation\"], [\"추천\"]),\n",
    "    (\"신경망\", [\"neural\", \"network\"], [\"신경망\", \"신경\"]),\n",
    "    (\"강화학습\", [\"reinforcement\", \"learning\"], [\"강화\", \"학습\"]),\n",
    "    (\"컴퓨터비전\", [\"computer\", \"vision\"], [\"컴퓨터\", \"비전\"]),\n",
    "    (\"음성인식\", [\"speech\", \"recognition\"], [\"음성\", \"인식\"]),\n",
    "]\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, device, top_k=50):\n",
    "    \"\"\"Evaluate model on test pairs - check both Korean and English activation.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    ko_activated_total = 0\n",
    "    en_activated_total = 0\n",
    "    ko_expected_total = 0\n",
    "    en_expected_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ko_term, en_expected, ko_expected in TEST_PAIRS:\n",
    "            encoding = tokenizer(\n",
    "                ko_term,\n",
    "                max_length=64,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            with autocast(\"cuda\", enabled=CONFIG[\"use_fp16\"]):\n",
    "                sparse_rep, _ = model(\n",
    "                    encoding[\"input_ids\"].to(device),\n",
    "                    encoding[\"attention_mask\"].to(device),\n",
    "                )\n",
    "\n",
    "            sparse_rep = sparse_rep[0].float().cpu()\n",
    "            top_indices = torch.topk(sparse_rep, k=top_k).indices.tolist()\n",
    "            top_tokens = tokenizer.convert_ids_to_tokens(top_indices)\n",
    "            top_tokens_set = set(top_tokens)\n",
    "\n",
    "            # Check Korean synonym/preservation activation\n",
    "            for ko in ko_expected:\n",
    "                ko_toks = tokenizer.tokenize(ko)\n",
    "                for tok in ko_toks:\n",
    "                    ko_expected_total += 1\n",
    "                    if tok in top_tokens_set:\n",
    "                        ko_activated_total += 1\n",
    "\n",
    "            # Check English translation activation\n",
    "            for en in en_expected:\n",
    "                en_toks = tokenizer.tokenize(en.lower())\n",
    "                for tok in en_toks:\n",
    "                    en_expected_total += 1\n",
    "                    if tok in top_tokens_set:\n",
    "                        en_activated_total += 1\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    ko_rate = (\n",
    "        ko_activated_total / ko_expected_total * 100 if ko_expected_total > 0 else 0\n",
    "    )\n",
    "    en_rate = (\n",
    "        en_activated_total / en_expected_total * 100 if en_expected_total > 0 else 0\n",
    "    )\n",
    "\n",
    "    return ko_rate, en_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "Loading tokenizer: xlm-roberta-large...\n",
      "Vocab size: 250,002\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\nLoading tokenizer: {CONFIG['model_name']}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building token suppression lists...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f8ef347a154fd4842349d583ad08fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scanning vocab:   0%|          | 0/250002 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-target language tokens: 76,209\n",
      "Special/punctuation tokens: 2,481\n",
      "Total tokens to suppress: 78,690\n"
     ]
    }
   ],
   "source": [
    "# Build non-target language token ID list AND special tokens to suppress\n",
    "print(\"Building token suppression lists...\")\n",
    "\n",
    "non_target_ids = []\n",
    "special_token_ids = []\n",
    "\n",
    "# Get special token IDs\n",
    "special_tokens = {\n",
    "    tokenizer.pad_token_id,\n",
    "    tokenizer.cls_token_id,\n",
    "    tokenizer.sep_token_id,\n",
    "    tokenizer.unk_token_id,\n",
    "    tokenizer.bos_token_id,\n",
    "    tokenizer.eos_token_id,\n",
    "}\n",
    "special_tokens = {t for t in special_tokens if t is not None}\n",
    "\n",
    "# Punctuation and symbols to suppress\n",
    "suppress_patterns = {\n",
    "    '.', ',', '!', '?', ':', ';', '-', '_', '(', ')', '[', ']', '{', '}',\n",
    "    '\"', \"'\", '`', '/', '\\\\', '@', '#', '$', '%', '^', '&', '*', '+', '=',\n",
    "    '<', '>', '~', '|', '▶', '►', '●', '○', '■', '□', '★', '☆', '→', '←',\n",
    "    '...', '..', '--', '==', '##', '@@',\n",
    "}\n",
    "\n",
    "for token_id in tqdm(range(tokenizer.vocab_size), desc=\"Scanning vocab\"):\n",
    "    token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "    \n",
    "    # Special tokens\n",
    "    if token_id in special_tokens:\n",
    "        special_token_ids.append(token_id)\n",
    "        continue\n",
    "    \n",
    "    # Check for special token markers\n",
    "    if token in ['<s>', '</s>', '<pad>', '<unk>', '<mask>', '[CLS]', '[SEP]', '[PAD]', '[UNK]', '[MASK]']:\n",
    "        special_token_ids.append(token_id)\n",
    "        continue\n",
    "    \n",
    "    # Punctuation and symbols\n",
    "    clean_token = token.replace('▁', '').replace('##', '').strip()\n",
    "    if clean_token in suppress_patterns or (clean_token and all(c in suppress_patterns or not c.isalnum() for c in clean_token)):\n",
    "        special_token_ids.append(token_id)\n",
    "        continue\n",
    "    \n",
    "    # Non-target language (not Korean, not English)\n",
    "    if is_non_target_token(token):\n",
    "        non_target_ids.append(token_id)\n",
    "\n",
    "# Combine for suppression\n",
    "all_suppress_ids = list(set(non_target_ids + special_token_ids))\n",
    "suppress_ids_tensor = torch.tensor(all_suppress_ids, dtype=torch.long)\n",
    "\n",
    "print(f\"Non-target language tokens: {len(non_target_ids):,}\")\n",
    "print(f\"Special/punctuation tokens: {len(special_token_ids):,}\")\n",
    "print(f\"Total tokens to suppress: {len(all_suppress_ids):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v19_high_quality/term_mappings.jsonl...\n",
      "Special token IDs to exclude: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c37a14738d0474197dd61f5dd1ae19d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13,684 valid term mappings\n",
      "Average target tokens per source: 11.08\n",
      "Similarity weights: min=0.800, max=0.995, mean=0.888\n",
      "\n",
      "Dataset size: 13,684\n",
      "Batches per epoch: 214\n",
      "Effective batch size: 128\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (1:N mixed format)\n",
    "dataset = TermMappingDataset(CONFIG[\"data_path\"], tokenizer, CONFIG[\"max_length\"])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset):,}\")\n",
    "print(f\"Batches per epoch: {len(dataloader):,}\")\n",
    "print(f\"Effective batch size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating model: xlm-roberta-large...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 560,142,482 (560.1M)\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "print(f\"\\nCreating model: {CONFIG['model_name']}...\")\n",
    "model = create_splade_model(\n",
    "    model_name=CONFIG[\"model_name\"],\n",
    "    use_idf=False,\n",
    "    use_expansion=True,\n",
    "    expansion_mode=\"mlm\",\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Parameters: {n_params:,} ({n_params / 1e6:.1f}M)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# Loss function - use CrossLingualLoss with separate Korean/English losses\nloss_fn = CrossLingualLoss(\n    target_margin=CONFIG[\"target_margin\"], \n    non_target_ids=suppress_ids_tensor\n)\n\n# Optimizer\noptimizer = torch.optim.AdamW(\n    model.parameters(), \n    lr=CONFIG[\"learning_rate\"], \n    weight_decay=0.01\n)\n\n# Scheduler\ntotal_steps = (\n    len(dataloader) * CONFIG[\"num_epochs\"] // CONFIG[\"gradient_accumulation_steps\"]\n)\nwarmup_steps = int(total_steps * CONFIG[\"warmup_ratio\"])\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=warmup_steps, \n    num_training_steps=total_steps\n)\n\nprint(f\"Total optimization steps: {total_steps:,}\")\nprint(f\"Warmup steps: {warmup_steps:,}\")\nprint(f\"Tokens to suppress: {len(suppress_ids_tensor):,}\")\nprint(f\"\\nLoss weights:\")\nprint(f\"  Self (Korean source): {CONFIG['lambda_self']}\")\nprint(f\"  Korean targets: {CONFIG['lambda_ko_target']}\")\nprint(f\"  English targets: {CONFIG['lambda_en_target']} (3.3x higher!)\")\nprint(f\"  Margin: {CONFIG['lambda_margin']}\")\n\n# Mixed precision scaler\nscaler = GradScaler(\"cuda\", enabled=CONFIG[\"use_fp16\"])\n\n# Create output directory\nCONFIG[\"output_dir\"].mkdir(parents=True, exist_ok=True)\nprint(f\"\\nOutput directory: {CONFIG['output_dir']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 7. Initial Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Performance:\n",
      "  Korean Preservation: 48.8%\n",
      "  English Activation: 9.1%\n",
      "  Combined Score: 57.9\n"
     ]
    }
   ],
   "source": [
    "# Evaluate before training\n",
    "ko_rate, en_rate = evaluate_model(model, tokenizer, device)\n",
    "print(f\"Initial Performance:\")\n",
    "print(f\"  Korean Preservation: {ko_rate:.1f}%\")\n",
    "print(f\"  English Activation: {en_rate:.1f}%\")\n",
    "print(f\"  Combined Score: {ko_rate + en_rate:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STARTING TRAINING\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Training variables\n",
    "history = []\n",
    "best_score = 0\n",
    "global_step = 0\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": "for epoch in range(CONFIG[\"num_epochs\"]):\n    print(f\"\\n{'='*70}\")\n    print(f\"Epoch {epoch + 1}/{CONFIG['num_epochs']}\")\n    print(f\"{'='*70}\")\n    \n    model.train()\n    epoch_losses = defaultdict(float)\n    optimizer.zero_grad()\n\n    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}\")\n\n    for batch_idx, batch in enumerate(progress_bar):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        with autocast(\"cuda\", enabled=CONFIG[\"use_fp16\"]):\n            sparse_rep, _ = model(input_ids, attention_mask)\n\n            # Compute SEPARATE Korean and English target losses\n            losses = loss_fn(\n                sparse_rep,\n                batch[\"ko_token_ids\"],\n                batch[\"ko_target_ids\"],\n                batch[\"ko_target_weights\"],\n                batch[\"en_target_ids\"],\n                batch[\"en_target_weights\"],\n            )\n\n            sparsity_loss = sparse_rep.mean()\n\n            # Total loss with SEPARATE weights for Korean and English targets\n            total_loss = (\n                CONFIG[\"lambda_self\"] * losses[\"self\"]\n                + CONFIG[\"lambda_ko_target\"] * losses[\"ko_target\"]\n                + CONFIG[\"lambda_en_target\"] * losses[\"en_target\"]  # Higher weight!\n                + CONFIG[\"lambda_margin\"] * losses[\"margin\"]\n                + CONFIG[\"lambda_negative\"] * losses[\"negative\"]\n                + CONFIG[\"lambda_sparsity\"] * sparsity_loss\n            )\n\n            total_loss = total_loss / CONFIG[\"gradient_accumulation_steps\"]\n\n        scaler.scale(total_loss).backward()\n\n        epoch_losses[\"total\"] += total_loss.item() * CONFIG[\"gradient_accumulation_steps\"]\n        epoch_losses[\"self\"] += losses[\"self\"].item()\n        epoch_losses[\"ko_target\"] += losses[\"ko_target\"].item()\n        epoch_losses[\"en_target\"] += losses[\"en_target\"].item()\n        epoch_losses[\"margin\"] += losses[\"margin\"].item()\n        epoch_losses[\"negative\"] += losses[\"negative\"].item()\n\n        if (batch_idx + 1) % CONFIG[\"gradient_accumulation_steps\"] == 0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(\n                model.parameters(), CONFIG[\"max_grad_norm\"]\n            )\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n            optimizer.zero_grad()\n            global_step += 1\n\n        if (batch_idx + 1) % 100 == 0:\n            progress_bar.set_postfix(\n                {\n                    \"loss\": f\"{epoch_losses['total'] / (batch_idx + 1):.4f}\",\n                    \"en\": f\"{epoch_losses['en_target'] / (batch_idx + 1):.4f}\",\n                    \"step\": global_step,\n                }\n            )\n\n    # Calculate average losses\n    n_batches = len(dataloader)\n    for key in epoch_losses:\n        epoch_losses[key] /= n_batches\n\n    history.append(dict(epoch_losses))\n\n    # Evaluate\n    ko_rate, en_rate = evaluate_model(model, tokenizer, device)\n    combined_score = ko_rate + en_rate\n\n    print(f\"\\nEpoch {epoch + 1} Summary:\")\n    print(f\"  Total Loss: {epoch_losses['total']:.4f}\")\n    print(f\"  Self Loss: {epoch_losses['self']:.4f}\")\n    print(f\"  Korean Target Loss: {epoch_losses['ko_target']:.4f}\")\n    print(f\"  English Target Loss: {epoch_losses['en_target']:.4f}\")\n    print(f\"  Korean Activation: {ko_rate:.1f}%\")\n    print(f\"  English Activation: {en_rate:.1f}%\")\n    print(f\"  Combined Score: {combined_score:.1f}\")\n\n    # Save checkpoint\n    checkpoint_path = CONFIG[\"output_dir\"] / f\"checkpoint_epoch{epoch + 1}.pt\"\n    torch.save(\n        {\n            \"epoch\": epoch + 1,\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n            \"losses\": dict(epoch_losses),\n            \"ko_rate\": ko_rate,\n            \"en_rate\": en_rate,\n            \"config\": {\n                k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items()\n            },\n        },\n        checkpoint_path,\n    )\n    print(f\"  Saved: {checkpoint_path.name}\")\n\n    # Save best model - prioritize English activation\n    # Use weighted score: Korean + 2*English (English is harder and more important)\n    weighted_score = ko_rate + 2 * en_rate\n    if weighted_score > best_score:\n        best_score = weighted_score\n        best_path = CONFIG[\"output_dir\"] / \"best_model.pt\"\n        torch.save(\n            {\n                \"epoch\": epoch + 1,\n                \"model_state_dict\": model.state_dict(),\n                \"ko_rate\": ko_rate,\n                \"en_rate\": en_rate,\n                \"combined_score\": combined_score,\n                \"weighted_score\": weighted_score,\n                \"config\": {\n                    k: str(v) if isinstance(v, Path) else v\n                    for k, v in CONFIG.items()\n                },\n            },\n            best_path,\n        )\n        print(f\"  *** New best model! Weighted score: {weighted_score:.1f} (KO:{ko_rate:.1f}% + EN:{en_rate:.1f}%) ***\")"
  },
  {
   "cell_type": "markdown",
   "id": "21218dee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 9. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_path = CONFIG[\"output_dir\"] / \"final_model.pt\"\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"config\": {\n",
    "            k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items()\n",
    "        },\n",
    "        \"history\": history,\n",
    "    },\n",
    "    final_path,\n",
    ")\n",
    "\n",
    "# Save training history\n",
    "with open(CONFIG[\"output_dir\"] / \"training_history.json\", \"w\") as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Final model saved: {final_path}\")\n",
    "print(f\"Best combined score: {best_score:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 10. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\n# Plot training curves\nif history:\n    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n\n    epochs = range(1, len(history) + 1)\n\n    # Total loss\n    axes[0, 0].plot(epochs, [h['total'] for h in history], '-o', color='#3498db')\n    axes[0, 0].set_title('Total Loss')\n    axes[0, 0].set_xlabel('Epoch')\n    axes[0, 0].set_ylabel('Loss')\n    axes[0, 0].grid(True, alpha=0.3)\n\n    # Self loss\n    axes[0, 1].plot(epochs, [h['self'] for h in history], '-o', color='#2ecc71')\n    axes[0, 1].set_title('Self Loss (Korean Source)')\n    axes[0, 1].set_xlabel('Epoch')\n    axes[0, 1].set_ylabel('Loss')\n    axes[0, 1].grid(True, alpha=0.3)\n\n    # Korean target loss\n    axes[0, 2].plot(epochs, [h['ko_target'] for h in history], '-o', color='#f39c12')\n    axes[0, 2].set_title('Korean Target Loss')\n    axes[0, 2].set_xlabel('Epoch')\n    axes[0, 2].set_ylabel('Loss')\n    axes[0, 2].grid(True, alpha=0.3)\n\n    # English target loss (CRITICAL!)\n    axes[1, 0].plot(epochs, [h['en_target'] for h in history], '-o', color='#e74c3c')\n    axes[1, 0].set_title('English Target Loss (Critical!)')\n    axes[1, 0].set_xlabel('Epoch')\n    axes[1, 0].set_ylabel('Loss')\n    axes[1, 0].grid(True, alpha=0.3)\n\n    # Margin loss\n    axes[1, 1].plot(epochs, [h['margin'] for h in history], '-o', color='#9b59b6')\n    axes[1, 1].set_title('Margin Loss')\n    axes[1, 1].set_xlabel('Epoch')\n    axes[1, 1].set_ylabel('Loss')\n    axes[1, 1].grid(True, alpha=0.3)\n\n    # Negative loss\n    axes[1, 2].plot(epochs, [h['negative'] for h in history], '-o', color='#1abc9c')\n    axes[1, 2].set_title('Negative Loss')\n    axes[1, 2].set_xlabel('Epoch')\n    axes[1, 2].set_ylabel('Loss')\n    axes[1, 2].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.savefig(CONFIG[\"output_dir\"] / \"training_curves.png\", dpi=150)\n    plt.show()\n    \n    print(f\"Training curves saved to: {CONFIG['output_dir'] / 'training_curves.png'}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": "## Next Steps\n\nAfter training completes:\n\n1. **Run inference tests** using `03_inference_test.ipynb`\n2. **Verify both Korean AND English activation rates**\n3. **Fine-tune lambda_en_target if English rate is still low**\n\n### Key Fix: Separate Korean/English Losses\n\nThe critical change in this version:\n\n| Loss Component | Weight | Purpose |\n|----------------|--------|---------|\n| `lambda_self` | 2.0 | Preserve Korean source tokens |\n| `lambda_ko_target` | 3.0 | Activate Korean synonym tokens |\n| `lambda_en_target` | **10.0** | Activate English translation tokens |\n| `lambda_margin` | 3.0 | Ensure minimum activation |\n\n**Why English needs 3.3x higher weight:**\n- Korean targets share subword tokens with Korean input → easy to learn\n- English targets require cross-lingual transfer → hard to learn\n- Without explicit high weight, the model ignores English entirely\n\n### Best Model Selection\n\nThe best model is now selected by: `score = KO_rate + 2 * EN_rate`\n\nThis prioritizes English activation since it's the harder and more important task for cross-lingual search."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
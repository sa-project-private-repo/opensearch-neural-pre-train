{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v19 Training - XLM-RoBERTa-large with Similarity-Weighted Loss\n",
    "\n",
    "This notebook trains the v19 Korean-English cross-lingual SPLADE model.\n",
    "\n",
    "## Key Features:\n",
    "- **Model**: xlm-roberta-large (560M parameters)\n",
    "- **Dataset**: v19_high_quality (1:N mixed Korean/English with similarity scores)\n",
    "- **Format**: `{\"ko\": \"프로그램\", \"terms\": [{\"term\": \"program\", \"sim\": 0.95}, ...]}`\n",
    "- **Loss**: Similarity-weighted target loss (higher weight for more similar terms)\n",
    "- **Max targets**: 8 per Korean source term\n",
    "- **Learning rate**: 2e-6\n",
    "- **Epochs**: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root\n",
    "def find_project_root():\n",
    "    \"\"\"Find project root by looking for markers like pyproject.toml or src/\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0.dev20251109+cu130\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GB10\n",
      "GPU Memory: 128.5 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/west/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  queued_call()\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.model.splade_model import create_splade_model\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  model_name: xlm-roberta-large\n",
      "  max_length: 64\n",
      "  data_path: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v19_high_quality/term_mappings.jsonl\n",
      "  batch_size: 64\n",
      "  gradient_accumulation_steps: 2\n",
      "  num_epochs: 15\n",
      "  learning_rate: 3e-06\n",
      "  warmup_ratio: 0.1\n",
      "  max_grad_norm: 1.0\n",
      "  lambda_self: 2.0\n",
      "  lambda_target: 5.0\n",
      "  lambda_margin: 3.0\n",
      "  lambda_negative: 1.0\n",
      "  lambda_sparsity: 0.01\n",
      "  target_margin: 2.0\n",
      "  use_fp16: True\n",
      "  output_dir: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/outputs/v19_xlm_large\n",
      "\n",
      "Training Stats:\n",
      "  Effective batch size: 128\n",
      "  Batches per epoch: ~15\n",
      "  Optimization steps per epoch: ~7\n",
      "  Total optimization steps: ~105\n"
     ]
    }
   ],
   "source": [
    "# Training Configuration\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    \"model_name\": \"xlm-roberta-large\",\n",
    "    \"max_length\": 64,\n",
    "\n",
    "    # Data - 1:N mixed format (Korean -> [Korean + English terms])\n",
    "    \"data_path\": PROJECT_ROOT / \"dataset\" / \"v19_high_quality\" / \"term_mappings.jsonl\",\n",
    "\n",
    "    # Training - adjusted for larger batch size\n",
    "    \"batch_size\": 64,                # 32 → 64 (2x)\n",
    "    \"gradient_accumulation_steps\": 2,  # 4 → 2 (effective batch: 128 → 128)\n",
    "    \"num_epochs\": 15,                # 10 → 15 (more epochs for better convergence)\n",
    "    \"learning_rate\": 3e-6,           # 2e-6 → 3e-6 (1.5x, conservative scaling)\n",
    "    \"warmup_ratio\": 0.1,             # 0.2 → 0.1 (shorter warmup)\n",
    "    \"max_grad_norm\": 1.0,\n",
    "\n",
    "    # Loss weights\n",
    "    \"lambda_self\": 2.0,       # Korean source preservation\n",
    "    \"lambda_target\": 5.0,     # Target term activation (Korean + English)\n",
    "    \"lambda_margin\": 3.0,     # Margin loss\n",
    "    \"lambda_negative\": 1.0,   # 0.5 → 1.0 (stronger suppression for special tokens)\n",
    "    \"lambda_sparsity\": 0.01,  # 0.005 → 0.01 (stronger sparsity)\n",
    "    \"target_margin\": 2.0,\n",
    "\n",
    "    # Mixed precision\n",
    "    \"use_fp16\": True,\n",
    "\n",
    "    # Output\n",
    "    \"output_dir\": PROJECT_ROOT / \"outputs\" / \"v19_xlm_large\",\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Calculate training stats\n",
    "dataset_size = 1012  # approximate\n",
    "batches_per_epoch = dataset_size // CONFIG[\"batch_size\"]\n",
    "opt_steps_per_epoch = batches_per_epoch // CONFIG[\"gradient_accumulation_steps\"]\n",
    "total_opt_steps = opt_steps_per_epoch * CONFIG[\"num_epochs\"]\n",
    "\n",
    "print(f\"\\nTraining Stats:\")\n",
    "print(f\"  Effective batch size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"  Batches per epoch: ~{batches_per_epoch}\")\n",
    "print(f\"  Optimization steps per epoch: ~{opt_steps_per_epoch}\")\n",
    "print(f\"  Total optimization steps: ~{total_opt_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_korean_char(c: str) -> bool:\n",
    "    \"\"\"Check if character is Korean.\"\"\"\n",
    "    return (\n",
    "        \"\\uac00\" <= c <= \"\\ud7a3\"\n",
    "        or \"\\u1100\" <= c <= \"\\u11ff\"\n",
    "        or \"\\u3130\" <= c <= \"\\u318f\"\n",
    "    )\n",
    "\n",
    "\n",
    "def is_english_char(c: str) -> bool:\n",
    "    \"\"\"Check if character is English.\"\"\"\n",
    "    return c.isalpha() and c.isascii()\n",
    "\n",
    "\n",
    "def is_non_target_token(token: str) -> bool:\n",
    "    \"\"\"Check if token is from non-target language (not Korean or English).\"\"\"\n",
    "    clean = token.replace(\"\\u2581\", \"\").replace(\"##\", \"\")  # Remove subword markers\n",
    "    if not clean:\n",
    "        return False\n",
    "\n",
    "    has_korean = any(is_korean_char(c) for c in clean)\n",
    "    has_english = any(is_english_char(c) for c in clean)\n",
    "\n",
    "    if has_korean or has_english:\n",
    "        return False\n",
    "\n",
    "    # Check for other languages\n",
    "    has_japanese = any(\n",
    "        \"\\u3040\" <= c <= \"\\u309f\" or \"\\u30a0\" <= c <= \"\\u30ff\" for c in clean\n",
    "    )\n",
    "    has_cjk = any(\"\\u4e00\" <= c <= \"\\u9fff\" for c in clean)\n",
    "    has_cyrillic = any(\"\\u0400\" <= c <= \"\\u04ff\" for c in clean)\n",
    "    has_arabic = any(\"\\u0600\" <= c <= \"\\u06ff\" for c in clean)\n",
    "    has_thai = any(\"\\u0e00\" <= c <= \"\\u0e7f\" for c in clean)\n",
    "    has_greek = any(\"\\u0370\" <= c <= \"\\u03ff\" for c in clean)\n",
    "\n",
    "    return (\n",
    "        has_japanese or has_cjk or has_cyrillic or has_arabic or has_thai or has_greek\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Dataset Class\n",
    "\n",
    "Dataset for 1:N mixed Korean/English term mappings with similarity scores:\n",
    "- Input format: `{\"ko\": \"프로그램\", \"terms\": [{\"term\": \"program\", \"sim\": 0.95}, {\"term\": \"소프트웨어\", \"sim\": 0.88}]}`\n",
    "- Tokenizes Korean source term and all target terms (mixed Korean + English)\n",
    "- Preserves similarity scores for weighted loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermMappingDataset(Dataset):\n",
    "    \"\"\"Dataset for 1:N Korean to mixed Korean/English term mappings with similarity.\n",
    "    \n",
    "    Format: {\"ko\": \"프로그램\", \"terms\": [{\"term\": \"program\", \"sim\": 0.95}, ...]}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path: Path, tokenizer, max_length: int = 64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "        \n",
    "        # Build set of special token IDs to exclude\n",
    "        self.special_ids = {\n",
    "            tokenizer.pad_token_id,\n",
    "            tokenizer.cls_token_id,\n",
    "            tokenizer.sep_token_id,\n",
    "            tokenizer.unk_token_id,\n",
    "            tokenizer.bos_token_id,\n",
    "            tokenizer.eos_token_id,\n",
    "        }\n",
    "        self.special_ids = {t for t in self.special_ids if t is not None}\n",
    "        \n",
    "        # Add tokens by name\n",
    "        for token_name in ['<s>', '</s>', '<pad>', '<unk>', '<mask>']:\n",
    "            tid = tokenizer.convert_tokens_to_ids(token_name)\n",
    "            if tid != tokenizer.unk_token_id:\n",
    "                self.special_ids.add(tid)\n",
    "\n",
    "        print(f\"Loading dataset from {data_path}...\")\n",
    "        print(f\"Special token IDs to exclude: {len(self.special_ids)}\")\n",
    "\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in tqdm(f, desc=\"Loading data\"):\n",
    "                item = json.loads(line.strip())\n",
    "\n",
    "                ko_term = item.get(\"ko\", \"\")\n",
    "                terms_data = item.get(\"terms\", [])\n",
    "\n",
    "                if not ko_term or not terms_data:\n",
    "                    continue\n",
    "\n",
    "                # Tokenize Korean source term (exclude special tokens)\n",
    "                ko_tokens = tokenizer.tokenize(ko_term)\n",
    "                ko_token_ids = tokenizer.convert_tokens_to_ids(ko_tokens)\n",
    "                ko_token_ids = [\n",
    "                    tid for tid in ko_token_ids \n",
    "                    if tid != tokenizer.unk_token_id and tid not in self.special_ids\n",
    "                ]\n",
    "\n",
    "                # Tokenize all target terms with similarity weights\n",
    "                # Format: {token_id: similarity_weight}\n",
    "                target_weights: dict = {}\n",
    "                for term_info in terms_data:\n",
    "                    # Handle both old format (string) and new format (dict)\n",
    "                    if isinstance(term_info, dict):\n",
    "                        term = term_info.get(\"term\", \"\")\n",
    "                        sim = term_info.get(\"sim\", 1.0)\n",
    "                    else:\n",
    "                        term = term_info\n",
    "                        sim = 1.0\n",
    "                    \n",
    "                    # Lowercase for consistency (affects English only)\n",
    "                    term_lower = term.lower() if term.isascii() else term\n",
    "                    tokens = tokenizer.tokenize(term_lower)\n",
    "                    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                    \n",
    "                    for tid in token_ids:\n",
    "                        # Exclude unknown and special tokens\n",
    "                        if tid != tokenizer.unk_token_id and tid not in self.special_ids:\n",
    "                            # Keep maximum similarity for each token\n",
    "                            target_weights[tid] = max(target_weights.get(tid, 0.0), sim)\n",
    "\n",
    "                if ko_token_ids and target_weights:\n",
    "                    self.data.append(\n",
    "                        {\n",
    "                            \"ko_term\": ko_term,\n",
    "                            \"ko_token_ids\": ko_token_ids,\n",
    "                            \"target_token_ids\": list(target_weights.keys()),\n",
    "                            \"target_weights\": list(target_weights.values()),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        print(f\"Loaded {len(self.data):,} valid term mappings\")\n",
    "        \n",
    "        # Statistics\n",
    "        n_targets = [len(d[\"target_token_ids\"]) for d in self.data]\n",
    "        all_weights = [w for d in self.data for w in d[\"target_weights\"]]\n",
    "        print(f\"Average target tokens per source: {sum(n_targets)/len(n_targets):.2f}\")\n",
    "        print(f\"Similarity weights: min={min(all_weights):.3f}, max={max(all_weights):.3f}, mean={sum(all_weights)/len(all_weights):.3f}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            item[\"ko_term\"],\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"ko_token_ids\": item[\"ko_token_ids\"],\n",
    "            \"target_token_ids\": item[\"target_token_ids\"],\n",
    "            \"target_weights\": item[\"target_weights\"],\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function.\"\"\"\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n",
    "        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n",
    "        \"ko_token_ids\": [item[\"ko_token_ids\"] for item in batch],\n",
    "        \"target_token_ids\": [item[\"target_token_ids\"] for item in batch],\n",
    "        \"target_weights\": [item[\"target_weights\"] for item in batch],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermLevelLoss(nn.Module):\n",
    "    \"\"\"Loss function for term-level cross-lingual training with similarity weights.\n",
    "    \n",
    "    Components:\n",
    "    - Self loss: Preserve Korean source term tokens\n",
    "    - Target loss: Activate target tokens with SIMILARITY-WEIGHTED loss\n",
    "    - Margin loss: Ensure minimum activation for target tokens\n",
    "    - Negative loss: Suppress non-target language tokens\n",
    "    \n",
    "    The target loss uses similarity scores as weights:\n",
    "    - Higher similarity = stronger loss signal for that token\n",
    "    - This focuses learning on the most semantically similar terms\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_margin: float = 2.0, non_target_ids: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.target_margin = target_margin\n",
    "        self.non_target_ids = non_target_ids\n",
    "\n",
    "    def forward(self, sparse_rep, ko_token_ids, target_token_ids, target_weights):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sparse_rep: Sparse representations [batch_size, vocab_size]\n",
    "            ko_token_ids: List of Korean source token IDs per sample\n",
    "            target_token_ids: List of target token IDs (mixed KO/EN) per sample\n",
    "            target_weights: List of similarity weights for each target token\n",
    "        \"\"\"\n",
    "        batch_size = sparse_rep.shape[0]\n",
    "        device = sparse_rep.device\n",
    "\n",
    "        self_loss = torch.tensor(0.0, device=device)\n",
    "        target_loss = torch.tensor(0.0, device=device)\n",
    "        margin_loss = torch.tensor(0.0, device=device)\n",
    "        negative_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "        n_valid = 0\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            rep = sparse_rep[i]\n",
    "\n",
    "            # Self loss: maximize activation of Korean source tokens\n",
    "            if ko_token_ids[i]:\n",
    "                ko_ids = torch.tensor(ko_token_ids[i], device=device)\n",
    "                ko_activations = rep[ko_ids]\n",
    "                self_loss = self_loss - torch.log(ko_activations + 1e-8).mean()\n",
    "\n",
    "            # Target loss: SIMILARITY-WEIGHTED activation of target tokens\n",
    "            if target_token_ids[i]:\n",
    "                tgt_ids = torch.tensor(target_token_ids[i], device=device)\n",
    "                tgt_weights = torch.tensor(target_weights[i], device=device, dtype=torch.float32)\n",
    "                tgt_activations = rep[tgt_ids]\n",
    "                \n",
    "                # Weighted log loss: higher similarity = stronger loss\n",
    "                weighted_log_loss = -torch.log(tgt_activations + 1e-8) * tgt_weights\n",
    "                target_loss = target_loss + weighted_log_loss.sum() / (tgt_weights.sum() + 1e-8)\n",
    "                \n",
    "                # Margin loss (weighted by similarity)\n",
    "                margin_violations = F.relu(self.target_margin - tgt_activations) * tgt_weights\n",
    "                margin_loss = margin_loss + margin_violations.sum() / (tgt_weights.sum() + 1e-8)\n",
    "\n",
    "            # Negative loss: suppress non-target language tokens\n",
    "            if self.non_target_ids is not None:\n",
    "                non_target_ids_device = self.non_target_ids.to(device)\n",
    "                non_target_activations = rep[non_target_ids_device]\n",
    "                negative_loss = negative_loss + F.relu(\n",
    "                    non_target_activations - 0.1\n",
    "                ).mean()\n",
    "\n",
    "            n_valid += 1\n",
    "\n",
    "        if n_valid > 0:\n",
    "            self_loss = self_loss / n_valid\n",
    "            target_loss = target_loss / n_valid\n",
    "            margin_loss = margin_loss / n_valid\n",
    "            negative_loss = negative_loss / n_valid\n",
    "\n",
    "        return {\n",
    "            \"self\": self_loss,\n",
    "            \"target\": target_loss,\n",
    "            \"margin\": margin_loss,\n",
    "            \"negative\": negative_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pairs for evaluation (Korean source -> expected Korean synonyms + English translations)\n",
    "TEST_PAIRS = [\n",
    "    # (source_ko, expected_english, expected_korean_synonyms)\n",
    "    (\"머신러닝\", [\"machine\", \"learning\"], [\"머신\", \"러닝\", \"기계학습\"]),\n",
    "    (\"딥러닝\", [\"deep\", \"learning\"], [\"딥\", \"러닝\", \"심층학습\"]),\n",
    "    (\"자연어처리\", [\"natural\", \"language\", \"processing\"], [\"자연어\", \"처리\"]),\n",
    "    (\"인공지능\", [\"artificial\", \"intelligence\"], [\"인공\", \"지능\"]),\n",
    "    (\"검색엔진\", [\"search\", \"engine\"], [\"검색\", \"엔진\"]),\n",
    "    (\"데이터베이스\", [\"database\"], [\"데이터\", \"베이스\"]),\n",
    "    (\"클라우드\", [\"cloud\"], [\"클라우드\"]),\n",
    "    (\"서버\", [\"server\"], [\"서버\"]),\n",
    "    (\"네트워크\", [\"network\"], [\"네트워크\"]),\n",
    "    (\"추천시스템\", [\"recommend\", \"system\"], [\"추천\", \"시스템\"]),\n",
    "    (\"추천\", [\"recommend\", \"recommendation\"], [\"추천\"]),\n",
    "    (\"신경망\", [\"neural\", \"network\"], [\"신경망\", \"신경\"]),\n",
    "    (\"강화학습\", [\"reinforcement\", \"learning\"], [\"강화\", \"학습\"]),\n",
    "    (\"컴퓨터비전\", [\"computer\", \"vision\"], [\"컴퓨터\", \"비전\"]),\n",
    "    (\"음성인식\", [\"speech\", \"recognition\"], [\"음성\", \"인식\"]),\n",
    "]\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, device, top_k=50):\n",
    "    \"\"\"Evaluate model on test pairs - check both Korean and English activation.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    ko_activated_total = 0\n",
    "    en_activated_total = 0\n",
    "    ko_expected_total = 0\n",
    "    en_expected_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ko_term, en_expected, ko_expected in TEST_PAIRS:\n",
    "            encoding = tokenizer(\n",
    "                ko_term,\n",
    "                max_length=64,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            with autocast(\"cuda\", enabled=CONFIG[\"use_fp16\"]):\n",
    "                sparse_rep, _ = model(\n",
    "                    encoding[\"input_ids\"].to(device),\n",
    "                    encoding[\"attention_mask\"].to(device),\n",
    "                )\n",
    "\n",
    "            sparse_rep = sparse_rep[0].float().cpu()\n",
    "            top_indices = torch.topk(sparse_rep, k=top_k).indices.tolist()\n",
    "            top_tokens = tokenizer.convert_ids_to_tokens(top_indices)\n",
    "            top_tokens_set = set(top_tokens)\n",
    "\n",
    "            # Check Korean synonym/preservation activation\n",
    "            for ko in ko_expected:\n",
    "                ko_toks = tokenizer.tokenize(ko)\n",
    "                for tok in ko_toks:\n",
    "                    ko_expected_total += 1\n",
    "                    if tok in top_tokens_set:\n",
    "                        ko_activated_total += 1\n",
    "\n",
    "            # Check English translation activation\n",
    "            for en in en_expected:\n",
    "                en_toks = tokenizer.tokenize(en.lower())\n",
    "                for tok in en_toks:\n",
    "                    en_expected_total += 1\n",
    "                    if tok in top_tokens_set:\n",
    "                        en_activated_total += 1\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    ko_rate = (\n",
    "        ko_activated_total / ko_expected_total * 100 if ko_expected_total > 0 else 0\n",
    "    )\n",
    "    en_rate = (\n",
    "        en_activated_total / en_expected_total * 100 if en_expected_total > 0 else 0\n",
    "    )\n",
    "\n",
    "    return ko_rate, en_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "Loading tokenizer: xlm-roberta-large...\n",
      "Vocab size: 250,002\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\nLoading tokenizer: {CONFIG['model_name']}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building token suppression lists...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0156c02abc46638d4f3482920cee5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scanning vocab:   0%|          | 0/250002 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-target language tokens: 76,209\n",
      "Special/punctuation tokens: 2,481\n",
      "Total tokens to suppress: 78,690\n"
     ]
    }
   ],
   "source": [
    "# Build non-target language token ID list AND special tokens to suppress\n",
    "print(\"Building token suppression lists...\")\n",
    "\n",
    "non_target_ids = []\n",
    "special_token_ids = []\n",
    "\n",
    "# Get special token IDs\n",
    "special_tokens = {\n",
    "    tokenizer.pad_token_id,\n",
    "    tokenizer.cls_token_id,\n",
    "    tokenizer.sep_token_id,\n",
    "    tokenizer.unk_token_id,\n",
    "    tokenizer.bos_token_id,\n",
    "    tokenizer.eos_token_id,\n",
    "}\n",
    "special_tokens = {t for t in special_tokens if t is not None}\n",
    "\n",
    "# Punctuation and symbols to suppress\n",
    "suppress_patterns = {\n",
    "    '.', ',', '!', '?', ':', ';', '-', '_', '(', ')', '[', ']', '{', '}',\n",
    "    '\"', \"'\", '`', '/', '\\\\', '@', '#', '$', '%', '^', '&', '*', '+', '=',\n",
    "    '<', '>', '~', '|', '▶', '►', '●', '○', '■', '□', '★', '☆', '→', '←',\n",
    "    '...', '..', '--', '==', '##', '@@',\n",
    "}\n",
    "\n",
    "for token_id in tqdm(range(tokenizer.vocab_size), desc=\"Scanning vocab\"):\n",
    "    token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "    \n",
    "    # Special tokens\n",
    "    if token_id in special_tokens:\n",
    "        special_token_ids.append(token_id)\n",
    "        continue\n",
    "    \n",
    "    # Check for special token markers\n",
    "    if token in ['<s>', '</s>', '<pad>', '<unk>', '<mask>', '[CLS]', '[SEP]', '[PAD]', '[UNK]', '[MASK]']:\n",
    "        special_token_ids.append(token_id)\n",
    "        continue\n",
    "    \n",
    "    # Punctuation and symbols\n",
    "    clean_token = token.replace('▁', '').replace('##', '').strip()\n",
    "    if clean_token in suppress_patterns or (clean_token and all(c in suppress_patterns or not c.isalnum() for c in clean_token)):\n",
    "        special_token_ids.append(token_id)\n",
    "        continue\n",
    "    \n",
    "    # Non-target language (not Korean, not English)\n",
    "    if is_non_target_token(token):\n",
    "        non_target_ids.append(token_id)\n",
    "\n",
    "# Combine for suppression\n",
    "all_suppress_ids = list(set(non_target_ids + special_token_ids))\n",
    "suppress_ids_tensor = torch.tensor(all_suppress_ids, dtype=torch.long)\n",
    "\n",
    "print(f\"Non-target language tokens: {len(non_target_ids):,}\")\n",
    "print(f\"Special/punctuation tokens: {len(special_token_ids):,}\")\n",
    "print(f\"Total tokens to suppress: {len(all_suppress_ids):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v19_high_quality/term_mappings.jsonl...\n",
      "Special token IDs to exclude: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b66a895be8346a0935e2e323e47da42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13,684 valid term mappings\n",
      "Average target tokens per source: 11.08\n",
      "Similarity weights: min=0.800, max=0.995, mean=0.888\n",
      "\n",
      "Dataset size: 13,684\n",
      "Batches per epoch: 214\n",
      "Effective batch size: 128\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (1:N mixed format)\n",
    "dataset = TermMappingDataset(CONFIG[\"data_path\"], tokenizer, CONFIG[\"max_length\"])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset):,}\")\n",
    "print(f\"Batches per epoch: {len(dataloader):,}\")\n",
    "print(f\"Effective batch size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating model: xlm-roberta-large...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 560,142,482 (560.1M)\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "print(f\"\\nCreating model: {CONFIG['model_name']}...\")\n",
    "model = create_splade_model(\n",
    "    model_name=CONFIG[\"model_name\"],\n",
    "    use_idf=False,\n",
    "    use_expansion=True,\n",
    "    expansion_mode=\"mlm\",\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Parameters: {n_params:,} ({n_params / 1e6:.1f}M)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total optimization steps: 1,605\n",
      "Warmup steps: 160\n",
      "Tokens to suppress: 78,690\n",
      "Output directory: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/outputs/v19_xlm_large\n"
     ]
    }
   ],
   "source": [
    "# Loss function - use combined suppression list\n",
    "loss_fn = TermLevelLoss(\n",
    "    target_margin=CONFIG[\"target_margin\"], \n",
    "    non_target_ids=suppress_ids_tensor  # Now includes special tokens + punctuation\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=CONFIG[\"learning_rate\"], \n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "total_steps = (\n",
    "    len(dataloader) * CONFIG[\"num_epochs\"] // CONFIG[\"gradient_accumulation_steps\"]\n",
    ")\n",
    "warmup_steps = int(total_steps * CONFIG[\"warmup_ratio\"])\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=warmup_steps, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Total optimization steps: {total_steps:,}\")\n",
    "print(f\"Warmup steps: {warmup_steps:,}\")\n",
    "print(f\"Tokens to suppress: {len(suppress_ids_tensor):,}\")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler(\"cuda\", enabled=CONFIG[\"use_fp16\"])\n",
    "\n",
    "# Create output directory\n",
    "CONFIG[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory: {CONFIG['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 7. Initial Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Performance:\n",
      "  Korean Preservation: 48.8%\n",
      "  English Activation: 9.1%\n",
      "  Combined Score: 57.9\n"
     ]
    }
   ],
   "source": [
    "# Evaluate before training\n",
    "ko_rate, en_rate = evaluate_model(model, tokenizer, device)\n",
    "print(f\"Initial Performance:\")\n",
    "print(f\"  Korean Preservation: {ko_rate:.1f}%\")\n",
    "print(f\"  English Activation: {en_rate:.1f}%\")\n",
    "print(f\"  Combined Score: {ko_rate + en_rate:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STARTING TRAINING\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Training variables\n",
    "history = []\n",
    "best_score = 0\n",
    "global_step = 0\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Epoch 1/15\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e549436f624163864d93280af93213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_534206/2706658435.py:54: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  scheduler.step()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "  Total Loss: -6.4263\n",
      "  Self Loss: -1.3848\n",
      "  Target Loss (weighted): -1.3229\n",
      "  Korean Activation: 62.8%\n",
      "  English Activation: 0.0%\n",
      "  Combined Score: 62.8\n",
      "  Saved: checkpoint_epoch1.pt\n",
      "  *** New best model! Score: 62.8 (KO:62.8% + EN:0.0%) ***\n",
      "\n",
      "======================================================================\n",
      "Epoch 2/15\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902cfa26b71d42d8821240df0e99ab27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "  Total Loss: -9.0627\n",
      "  Self Loss: -1.4000\n",
      "  Target Loss (weighted): -1.3385\n",
      "  Korean Activation: 51.2%\n",
      "  English Activation: 0.0%\n",
      "  Combined Score: 51.2\n",
      "  Saved: checkpoint_epoch2.pt\n",
      "\n",
      "======================================================================\n",
      "Epoch 3/15\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789e9b7c5448467cba6838063cc0b6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "  Total Loss: -9.5651\n",
      "  Self Loss: -1.4321\n",
      "  Target Loss (weighted): -1.3803\n",
      "  Korean Activation: 41.9%\n",
      "  English Activation: 0.0%\n",
      "  Combined Score: 41.9\n",
      "  Saved: checkpoint_epoch3.pt\n",
      "\n",
      "======================================================================\n",
      "Epoch 4/15\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5773df7f930c49369b86fc2b0c7c02cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "  Total Loss: -9.6967\n",
      "  Self Loss: -1.4433\n",
      "  Target Loss (weighted): -1.3972\n",
      "  Korean Activation: 41.9%\n",
      "  English Activation: 0.0%\n",
      "  Combined Score: 41.9\n",
      "  Saved: checkpoint_epoch4.pt\n",
      "\n",
      "======================================================================\n",
      "Epoch 5/15\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511ddb8eadaa478b99cbea4a4b977c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "  Total Loss: -9.7549\n",
      "  Self Loss: -1.4500\n",
      "  Target Loss (weighted): -1.4040\n",
      "  Korean Activation: 41.9%\n",
      "  English Activation: 0.0%\n",
      "  Combined Score: 41.9\n",
      "  Saved: checkpoint_epoch5.pt\n",
      "\n",
      "======================================================================\n",
      "Epoch 6/15\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fbbd76efcd4102aad74158d5b221eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(CONFIG[\"num_epochs\"]):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Epoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    model.train()\n",
    "    epoch_losses = defaultdict(float)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        with autocast(\"cuda\", enabled=CONFIG[\"use_fp16\"]):\n",
    "            sparse_rep, _ = model(input_ids, attention_mask)\n",
    "\n",
    "            # Pass target_weights for similarity-weighted loss\n",
    "            losses = loss_fn(\n",
    "                sparse_rep,\n",
    "                batch[\"ko_token_ids\"],\n",
    "                batch[\"target_token_ids\"],\n",
    "                batch[\"target_weights\"],  # Similarity weights\n",
    "            )\n",
    "\n",
    "            sparsity_loss = sparse_rep.mean()\n",
    "\n",
    "            total_loss = (\n",
    "                CONFIG[\"lambda_self\"] * losses[\"self\"]\n",
    "                + CONFIG[\"lambda_target\"] * losses[\"target\"]\n",
    "                + CONFIG[\"lambda_margin\"] * losses[\"margin\"]\n",
    "                + CONFIG[\"lambda_negative\"] * losses[\"negative\"]\n",
    "                + CONFIG[\"lambda_sparsity\"] * sparsity_loss\n",
    "            )\n",
    "\n",
    "            total_loss = total_loss / CONFIG[\"gradient_accumulation_steps\"]\n",
    "\n",
    "        scaler.scale(total_loss).backward()\n",
    "\n",
    "        epoch_losses[\"total\"] += total_loss.item() * CONFIG[\"gradient_accumulation_steps\"]\n",
    "        epoch_losses[\"self\"] += losses[\"self\"].item()\n",
    "        epoch_losses[\"target\"] += losses[\"target\"].item()\n",
    "        epoch_losses[\"margin\"] += losses[\"margin\"].item()\n",
    "        epoch_losses[\"negative\"] += losses[\"negative\"].item()\n",
    "\n",
    "        if (batch_idx + 1) % CONFIG[\"gradient_accumulation_steps\"] == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), CONFIG[\"max_grad_norm\"]\n",
    "            )\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            progress_bar.set_postfix(\n",
    "                {\n",
    "                    \"loss\": f\"{epoch_losses['total'] / (batch_idx + 1):.4f}\",\n",
    "                    \"tgt\": f\"{epoch_losses['target'] / (batch_idx + 1):.4f}\",\n",
    "                    \"step\": global_step,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Calculate average losses\n",
    "    n_batches = len(dataloader)\n",
    "    for key in epoch_losses:\n",
    "        epoch_losses[key] /= n_batches\n",
    "\n",
    "    history.append(dict(epoch_losses))\n",
    "\n",
    "    # Evaluate\n",
    "    ko_rate, en_rate = evaluate_model(model, tokenizer, device)\n",
    "    combined_score = ko_rate + en_rate\n",
    "\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"  Total Loss: {epoch_losses['total']:.4f}\")\n",
    "    print(f\"  Self Loss: {epoch_losses['self']:.4f}\")\n",
    "    print(f\"  Target Loss (weighted): {epoch_losses['target']:.4f}\")\n",
    "    print(f\"  Korean Activation: {ko_rate:.1f}%\")\n",
    "    print(f\"  English Activation: {en_rate:.1f}%\")\n",
    "    print(f\"  Combined Score: {combined_score:.1f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint_path = CONFIG[\"output_dir\"] / f\"checkpoint_epoch{epoch + 1}.pt\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"losses\": dict(epoch_losses),\n",
    "            \"ko_rate\": ko_rate,\n",
    "            \"en_rate\": en_rate,\n",
    "            \"config\": {\n",
    "                k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items()\n",
    "            },\n",
    "        },\n",
    "        checkpoint_path,\n",
    "    )\n",
    "    print(f\"  Saved: {checkpoint_path.name}\")\n",
    "\n",
    "    # Save best model\n",
    "    if combined_score > best_score:\n",
    "        best_score = combined_score\n",
    "        best_path = CONFIG[\"output_dir\"] / \"best_model.pt\"\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"ko_rate\": ko_rate,\n",
    "                \"en_rate\": en_rate,\n",
    "                \"combined_score\": combined_score,\n",
    "                \"config\": {\n",
    "                    k: str(v) if isinstance(v, Path) else v\n",
    "                    for k, v in CONFIG.items()\n",
    "                },\n",
    "            },\n",
    "            best_path,\n",
    "        )\n",
    "        print(f\"  *** New best model! Score: {combined_score:.1f} (KO:{ko_rate:.1f}% + EN:{en_rate:.1f}%) ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21218dee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 9. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_path = CONFIG[\"output_dir\"] / \"final_model.pt\"\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"config\": {\n",
    "            k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items()\n",
    "        },\n",
    "        \"history\": history,\n",
    "    },\n",
    "    final_path,\n",
    ")\n",
    "\n",
    "# Save training history\n",
    "with open(CONFIG[\"output_dir\"] / \"training_history.json\", \"w\") as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Final model saved: {final_path}\")\n",
    "print(f\"Best combined score: {best_score:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 10. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training curves\n",
    "if history:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "    epochs = range(1, len(history) + 1)\n",
    "\n",
    "    # Total loss\n",
    "    axes[0, 0].plot(epochs, [-h['total'] for h in history], '-o', color='#3498db')\n",
    "    axes[0, 0].set_title('Total Loss (negated)')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Self loss\n",
    "    axes[0, 1].plot(epochs, [-h['self'] for h in history], '-o', color='#2ecc71')\n",
    "    axes[0, 1].set_title('Self Loss (Korean Preservation)')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Target loss\n",
    "    axes[1, 0].plot(epochs, [-h['target'] for h in history], '-o', color='#e74c3c')\n",
    "    axes[1, 0].set_title('Target Loss (English Activation)')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Negative loss\n",
    "    axes[1, 1].plot(epochs, [h['negative'] for h in history], '-o', color='#9b59b6')\n",
    "    axes[1, 1].set_title('Negative Loss')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CONFIG[\"output_dir\"] / \"training_curves.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Training curves saved to: {CONFIG['output_dir'] / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After training completes:\n",
    "\n",
    "1. **Run inference tests** using `02_inference_test.ipynb`\n",
    "2. **Analyze results** and compare with previous versions\n",
    "3. **Fine-tune hyperparameters** if needed\n",
    "\n",
    "### Data Format Used\n",
    "This model was trained with **1:N mixed Korean/English mappings with similarity weights**:\n",
    "```json\n",
    "{\"ko\": \"프로그램\", \"terms\": [{\"term\": \"program\", \"sim\": 0.95}, {\"term\": \"소프트웨어\", \"sim\": 0.88}]}\n",
    "```\n",
    "\n",
    "### Key Improvements in v19\n",
    "- **Max 8 targets**: Limits each Korean term to top 8 most similar targets\n",
    "- **Similarity threshold 0.8**: Only high-quality pairs (cosine sim >= 0.8)\n",
    "- **Similarity-weighted loss**: Higher weight for more similar terms\n",
    "  - Focuses learning on the most semantically relevant pairs\n",
    "  - Reduces noise from borderline matches\n",
    "\n",
    "The model learns to activate both Korean synonyms and English translations, with stronger activation for more similar terms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

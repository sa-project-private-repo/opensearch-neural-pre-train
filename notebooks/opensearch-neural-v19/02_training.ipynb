{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v19 Training - Knowledge Distillation + FLOPS Sparse Retrieval\n",
    "\n",
    "This notebook trains the v19 Korean-English cross-lingual SPLADE model using **state-of-the-art** techniques.\n",
    "\n",
    "## Architecture: Dense Teacher → Sparse Student\n",
    "\n",
    "```\n",
    "┌─────────────────────┐     Distillation     ┌─────────────────────┐\n",
    "│    Dense Teacher    │  ─────────────────▶  │   Sparse Student    │\n",
    "│ (multilingual-e5)   │    soft labels       │     (SPLADE)        │\n",
    "└─────────────────────┘                      └─────────────────────┘\n",
    "```\n",
    "\n",
    "## Key Techniques:\n",
    "\n",
    "### 1. Knowledge Distillation\n",
    "- Teacher: `intfloat/multilingual-e5-base` (dense embeddings)\n",
    "- Student learns semantic similarity from teacher\n",
    "- No manual rules needed - teacher guides what's important\n",
    "\n",
    "### 2. FLOPS Regularization (Automatic Noise Suppression)\n",
    "- Penalizes tokens that activate frequently across batch\n",
    "- \"s\", \"the\", \"a\" naturally get suppressed (high avg activation → high penalty)\n",
    "- End-to-end learned, no manual token lists\n",
    "\n",
    "### 3. Cross-Lingual Term Mapping\n",
    "- Korean source → Korean synonyms + English translations\n",
    "- Separate loss weights for Korean/English targets\n",
    "\n",
    "## Why This Approach?\n",
    "- **SPLADE-v2, ColBERT-v2, OpenSearch neural-sparse** all use similar techniques\n",
    "- **Scalable**: No language-specific rules to maintain\n",
    "- **Effective**: Teacher provides rich semantic signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root\n",
    "def find_project_root():\n",
    "    \"\"\"Find project root by looking for markers like pyproject.toml or src/\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.model.splade_model import create_splade_model\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    \"model_name\": \"xlm-roberta-large\",\n",
    "    \"max_length\": 64,\n",
    "    \n",
    "    # Teacher model for Knowledge Distillation (upgraded to BGE-M3)\n",
    "    \"teacher_model\": \"BAAI/bge-m3\",  # Upgraded from e5-base (dim=1024, best cross-lingual)\n",
    "\n",
    "    # Data - 1:N mixed format (Korean -> [Korean + English terms])\n",
    "    \"data_path\": PROJECT_ROOT / \"dataset\" / \"v19_high_quality\" / \"term_mappings.jsonl\",\n",
    "\n",
    "    # Training\n",
    "    \"batch_size\": 64,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"num_epochs\": 15,\n",
    "    \"learning_rate\": 3e-6,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "\n",
    "    # Loss weights (tuned for better English activation)\n",
    "    \"lambda_self\": 3.0,         # Korean source preservation\n",
    "    \"lambda_ko_target\": 2.0,    # Korean synonym activation\n",
    "    \"lambda_en_target\": 12.0,   # English translation (increased from 8.0)\n",
    "    \"lambda_margin\": 2.0,       # Margin loss for minimum activation\n",
    "    \"lambda_distill\": 1.5,      # Knowledge distillation (increased from 1.0)\n",
    "    \"lambda_flops\": 5e-5,       # FLOPS regularization (auto noise suppression)\n",
    "    \"target_margin\": 2.0,\n",
    "\n",
    "    # Mixed precision\n",
    "    \"use_fp16\": True,\n",
    "\n",
    "    # Output\n",
    "    \"output_dir\": PROJECT_ROOT / \"outputs\" / \"v19_xlm_large\",\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Calculate training stats\n",
    "dataset_size = 13684\n",
    "batches_per_epoch = dataset_size // CONFIG[\"batch_size\"]\n",
    "opt_steps_per_epoch = batches_per_epoch // CONFIG[\"gradient_accumulation_steps\"]\n",
    "total_opt_steps = opt_steps_per_epoch * CONFIG[\"num_epochs\"]\n",
    "\n",
    "print(f\"\\nTraining Stats:\")\n",
    "print(f\"  Effective batch size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"  Batches per epoch: ~{batches_per_epoch}\")\n",
    "print(f\"  Total optimization steps: ~{total_opt_steps}\")\n",
    "print(f\"\\n*** Teacher: BGE-M3 (best cross-lingual), lambda_en_target=12.0 ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_korean_char(c: str) -> bool:\n",
    "    \"\"\"Check if character is Korean.\"\"\"\n",
    "    return (\n",
    "        \"\\uac00\" <= c <= \"\\ud7a3\"\n",
    "        or \"\\u1100\" <= c <= \"\\u11ff\"\n",
    "        or \"\\u3130\" <= c <= \"\\u318f\"\n",
    "    )\n",
    "\n",
    "\n",
    "def is_english_char(c: str) -> bool:\n",
    "    \"\"\"Check if character is English.\"\"\"\n",
    "    return c.isalpha() and c.isascii()\n",
    "\n",
    "\n",
    "def is_non_target_token(token: str) -> bool:\n",
    "    \"\"\"Check if token is from non-target language (not Korean or English).\"\"\"\n",
    "    clean = token.replace(\"\\u2581\", \"\").replace(\"##\", \"\")  # Remove subword markers\n",
    "    if not clean:\n",
    "        return False\n",
    "\n",
    "    has_korean = any(is_korean_char(c) for c in clean)\n",
    "    has_english = any(is_english_char(c) for c in clean)\n",
    "\n",
    "    if has_korean or has_english:\n",
    "        return False\n",
    "\n",
    "    # Check for other languages\n",
    "    has_japanese = any(\n",
    "        \"\\u3040\" <= c <= \"\\u309f\" or \"\\u30a0\" <= c <= \"\\u30ff\" for c in clean\n",
    "    )\n",
    "    has_cjk = any(\"\\u4e00\" <= c <= \"\\u9fff\" for c in clean)\n",
    "    has_cyrillic = any(\"\\u0400\" <= c <= \"\\u04ff\" for c in clean)\n",
    "    has_arabic = any(\"\\u0600\" <= c <= \"\\u06ff\" for c in clean)\n",
    "    has_thai = any(\"\\u0e00\" <= c <= \"\\u0e7f\" for c in clean)\n",
    "    has_greek = any(\"\\u0370\" <= c <= \"\\u03ff\" for c in clean)\n",
    "\n",
    "    return (\n",
    "        has_japanese or has_cjk or has_cyrillic or has_arabic or has_thai or has_greek\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Dataset Class - Separate Korean/English Targets\n",
    "\n",
    "Dataset for 1:N mixed Korean/English term mappings with similarity scores:\n",
    "- Input format: `{\"ko\": \"프로그램\", \"terms\": [{\"term\": \"program\", \"sim\": 0.95}, {\"term\": \"소프트웨어\", \"sim\": 0.88}]}`\n",
    "\n",
    "**Key Change**: Separates Korean and English targets into distinct lists:\n",
    "- `ko_target_ids` / `ko_target_weights`: Korean synonym tokens\n",
    "- `en_target_ids` / `en_target_weights`: English translation tokens\n",
    "\n",
    "This enables separate loss computation for cross-lingual training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermMappingDataset(Dataset):\n",
    "    \"\"\"Dataset for 1:N Korean to mixed Korean/English term mappings with similarity.\n",
    "    \n",
    "    Format: {\"ko\": \"프로그램\", \"terms\": [{\"term\": \"program\", \"sim\": 0.95}, ...]}\n",
    "    \n",
    "    Separates Korean and English targets for explicit cross-lingual training.\n",
    "    Returns text for knowledge distillation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path: Path, tokenizer, max_length: int = 64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "        \n",
    "        # Build set of special token IDs to exclude\n",
    "        self.special_ids = {\n",
    "            tokenizer.pad_token_id,\n",
    "            tokenizer.cls_token_id,\n",
    "            tokenizer.sep_token_id,\n",
    "            tokenizer.unk_token_id,\n",
    "            tokenizer.bos_token_id,\n",
    "            tokenizer.eos_token_id,\n",
    "        }\n",
    "        self.special_ids = {t for t in self.special_ids if t is not None}\n",
    "        \n",
    "        # Add tokens by name\n",
    "        for token_name in ['<s>', '</s>', '<pad>', '<unk>', '<mask>']:\n",
    "            tid = tokenizer.convert_tokens_to_ids(token_name)\n",
    "            if tid != tokenizer.unk_token_id:\n",
    "                self.special_ids.add(tid)\n",
    "\n",
    "        print(f\"Loading dataset from {data_path}...\")\n",
    "\n",
    "        def is_korean_term(text: str) -> bool:\n",
    "            \"\"\"Check if term contains Korean characters.\"\"\"\n",
    "            return any('\\uac00' <= c <= '\\ud7a3' for c in text)\n",
    "\n",
    "        total_ko_targets = 0\n",
    "        total_en_targets = 0\n",
    "\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in tqdm(f, desc=\"Loading data\"):\n",
    "                item = json.loads(line.strip())\n",
    "\n",
    "                ko_term = item.get(\"ko\", \"\")\n",
    "                terms_data = item.get(\"terms\", [])\n",
    "\n",
    "                if not ko_term or not terms_data:\n",
    "                    continue\n",
    "\n",
    "                # Tokenize Korean source term (exclude special tokens)\n",
    "                ko_tokens = tokenizer.tokenize(ko_term)\n",
    "                ko_token_ids = tokenizer.convert_tokens_to_ids(ko_tokens)\n",
    "                ko_token_ids = [\n",
    "                    tid for tid in ko_token_ids \n",
    "                    if tid != tokenizer.unk_token_id and tid not in self.special_ids\n",
    "                ]\n",
    "\n",
    "                # SEPARATE Korean and English targets\n",
    "                ko_target_weights: dict = {}\n",
    "                en_target_weights: dict = {}\n",
    "                \n",
    "                for term_info in terms_data:\n",
    "                    if isinstance(term_info, dict):\n",
    "                        term = term_info.get(\"term\", \"\")\n",
    "                        sim = term_info.get(\"sim\", 1.0)\n",
    "                    else:\n",
    "                        term = term_info\n",
    "                        sim = 1.0\n",
    "                    \n",
    "                    is_korean = is_korean_term(term)\n",
    "                    term_lower = term if is_korean else term.lower()\n",
    "                    tokens = tokenizer.tokenize(term_lower)\n",
    "                    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                    \n",
    "                    for tid in token_ids:\n",
    "                        if tid != tokenizer.unk_token_id and tid not in self.special_ids:\n",
    "                            if is_korean:\n",
    "                                ko_target_weights[tid] = max(ko_target_weights.get(tid, 0.0), sim)\n",
    "                            else:\n",
    "                                en_target_weights[tid] = max(en_target_weights.get(tid, 0.0), sim)\n",
    "\n",
    "                if ko_token_ids and (ko_target_weights or en_target_weights):\n",
    "                    total_ko_targets += len(ko_target_weights)\n",
    "                    total_en_targets += len(en_target_weights)\n",
    "                    \n",
    "                    self.data.append({\n",
    "                        \"ko_term\": ko_term,\n",
    "                        \"ko_token_ids\": ko_token_ids,\n",
    "                        \"ko_target_ids\": list(ko_target_weights.keys()),\n",
    "                        \"ko_target_weights\": list(ko_target_weights.values()),\n",
    "                        \"en_target_ids\": list(en_target_weights.keys()),\n",
    "                        \"en_target_weights\": list(en_target_weights.values()),\n",
    "                    })\n",
    "\n",
    "        print(f\"Loaded {len(self.data):,} valid term mappings\")\n",
    "        samples_with_english = sum(1 for d in self.data if d[\"en_target_ids\"])\n",
    "        print(f\"Samples with English targets: {samples_with_english:,} ({samples_with_english/len(self.data)*100:.1f}%)\")\n",
    "        print(f\"Total Korean target tokens: {total_ko_targets:,}\")\n",
    "        print(f\"Total English target tokens: {total_en_targets:,}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            item[\"ko_term\"],\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"text\": item[\"ko_term\"],  # For knowledge distillation\n",
    "            \"ko_token_ids\": item[\"ko_token_ids\"],\n",
    "            \"ko_target_ids\": item[\"ko_target_ids\"],\n",
    "            \"ko_target_weights\": item[\"ko_target_weights\"],\n",
    "            \"en_target_ids\": item[\"en_target_ids\"],\n",
    "            \"en_target_weights\": item[\"en_target_weights\"],\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function.\"\"\"\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n",
    "        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n",
    "        \"texts\": [item[\"text\"] for item in batch],  # List of texts for distillation\n",
    "        \"ko_token_ids\": [item[\"ko_token_ids\"] for item in batch],\n",
    "        \"ko_target_ids\": [item[\"ko_target_ids\"] for item in batch],\n",
    "        \"ko_target_weights\": [item[\"ko_target_weights\"] for item in batch],\n",
    "        \"en_target_ids\": [item[\"en_target_ids\"] for item in batch],\n",
    "        \"en_target_weights\": [item[\"en_target_weights\"] for item in batch],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Loss Function - Cross-Lingual with Separate Korean/English\n",
    "\n",
    "**Critical Design Decision**: Separate Korean and English target losses.\n",
    "\n",
    "Why? When the input is Korean (e.g., \"머신러닝\"):\n",
    "- Korean targets (e.g., \"딥러닝\") share subword tokens → naturally high activation\n",
    "- English targets (e.g., \"machine\", \"learning\") have no overlap → need explicit training\n",
    "\n",
    "Without separation, Korean dominates the loss and English is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationFLOPSLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Knowledge Distillation + FLOPS Loss for sparse retrieval.\n",
    "    \n",
    "    Key innovations:\n",
    "    1. Knowledge Distillation: Learn semantic similarity from dense teacher\n",
    "    2. FLOPS Regularization: Automatic noise suppression without manual rules\n",
    "    3. Cross-lingual term mapping: Separate Korean/English target losses\n",
    "    \n",
    "    The FLOPS loss naturally suppresses high-frequency noise tokens (s, the, a, etc.)\n",
    "    because they activate across many samples → high average activation → high penalty.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        teacher_model: SentenceTransformer,\n",
    "        target_margin: float = 2.0,\n",
    "        temperature: float = 0.05,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher_model\n",
    "        self.target_margin = target_margin\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Freeze teacher\n",
    "        self.teacher.eval()\n",
    "        for param in self.teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def compute_flops_loss(self, sparse_rep: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        FLOPS regularization - penalize frequently activated tokens.\n",
    "        \n",
    "        Logic: If a token activates for many samples in batch, it's likely noise.\n",
    "        - \"s\", \"the\" activate for almost everything → high avg → high penalty\n",
    "        - \"machine\", \"검색\" activate selectively → low avg → low penalty\n",
    "        \"\"\"\n",
    "        # Average activation per token across batch [vocab_size]\n",
    "        avg_activation = sparse_rep.mean(dim=0)\n",
    "        \n",
    "        # L2 penalty on average (stronger than L1)\n",
    "        flops_loss = (avg_activation ** 2).sum()\n",
    "        \n",
    "        return flops_loss\n",
    "\n",
    "    def compute_distillation_loss(\n",
    "        self, \n",
    "        sparse_rep: torch.Tensor, \n",
    "        texts: list[str],\n",
    "        device: torch.device,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Knowledge distillation from dense teacher.\n",
    "        \n",
    "        Student learns to produce similar pairwise similarities as teacher.\n",
    "        This teaches the model what tokens are semantically important.\n",
    "        \"\"\"\n",
    "        # Get teacher embeddings (already normalized)\n",
    "        with torch.no_grad():\n",
    "            teacher_emb = self.teacher.encode(\n",
    "                texts,\n",
    "                convert_to_tensor=True,\n",
    "                normalize_embeddings=True,\n",
    "                device=device,\n",
    "            )\n",
    "        \n",
    "        # Normalize student sparse representations for comparison\n",
    "        student_emb = F.normalize(sparse_rep.float(), p=2, dim=-1)\n",
    "        \n",
    "        # Compute similarity matrices\n",
    "        teacher_sim = teacher_emb @ teacher_emb.T  # [batch, batch]\n",
    "        student_sim = student_emb @ student_emb.T  # [batch, batch]\n",
    "        \n",
    "        # Scale by temperature\n",
    "        teacher_sim = teacher_sim / self.temperature\n",
    "        student_sim = student_sim / self.temperature\n",
    "        \n",
    "        # KL divergence on similarity distributions\n",
    "        teacher_probs = F.softmax(teacher_sim, dim=-1)\n",
    "        student_log_probs = F.log_softmax(student_sim, dim=-1)\n",
    "        \n",
    "        distill_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean')\n",
    "        \n",
    "        return distill_loss\n",
    "\n",
    "    def compute_term_losses(\n",
    "        self,\n",
    "        sparse_rep: torch.Tensor,\n",
    "        ko_token_ids: list,\n",
    "        ko_target_ids: list,\n",
    "        ko_target_weights: list,\n",
    "        en_target_ids: list,\n",
    "        en_target_weights: list,\n",
    "    ) -> dict:\n",
    "        \"\"\"Compute self, Korean target, English target, and margin losses.\"\"\"\n",
    "        batch_size = sparse_rep.shape[0]\n",
    "        device = sparse_rep.device\n",
    "\n",
    "        self_loss = torch.tensor(0.0, device=device)\n",
    "        ko_target_loss = torch.tensor(0.0, device=device)\n",
    "        en_target_loss = torch.tensor(0.0, device=device)\n",
    "        margin_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "        n_valid = 0\n",
    "        n_with_english = 0\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            rep = sparse_rep[i]\n",
    "\n",
    "            # Self loss: preserve Korean source tokens\n",
    "            if ko_token_ids[i]:\n",
    "                ko_ids = torch.tensor(ko_token_ids[i], device=device)\n",
    "                ko_activations = rep[ko_ids]\n",
    "                self_loss = self_loss - torch.log(ko_activations + 1e-8).mean()\n",
    "                \n",
    "                # Margin penalty for Korean source\n",
    "                ko_margin = F.relu(self.target_margin - ko_activations).mean()\n",
    "                self_loss = self_loss + ko_margin\n",
    "\n",
    "            # Korean target loss\n",
    "            if ko_target_ids[i]:\n",
    "                tgt_ids = torch.tensor(ko_target_ids[i], device=device)\n",
    "                tgt_weights = torch.tensor(ko_target_weights[i], device=device, dtype=torch.float32)\n",
    "                tgt_activations = rep[tgt_ids]\n",
    "                \n",
    "                weighted_log = -torch.log(tgt_activations + 1e-8) * tgt_weights\n",
    "                ko_target_loss = ko_target_loss + weighted_log.sum() / (tgt_weights.sum() + 1e-8)\n",
    "                \n",
    "                margin_violations = F.relu(self.target_margin - tgt_activations) * tgt_weights\n",
    "                margin_loss = margin_loss + margin_violations.sum() / (tgt_weights.sum() + 1e-8)\n",
    "\n",
    "            # English target loss\n",
    "            if en_target_ids[i]:\n",
    "                n_with_english += 1\n",
    "                tgt_ids = torch.tensor(en_target_ids[i], device=device)\n",
    "                tgt_weights = torch.tensor(en_target_weights[i], device=device, dtype=torch.float32)\n",
    "                tgt_activations = rep[tgt_ids]\n",
    "                \n",
    "                weighted_log = -torch.log(tgt_activations + 1e-8) * tgt_weights\n",
    "                en_target_loss = en_target_loss + weighted_log.sum() / (tgt_weights.sum() + 1e-8)\n",
    "                \n",
    "                en_margin = self.target_margin * 1.5\n",
    "                margin_violations = F.relu(en_margin - tgt_activations) * tgt_weights\n",
    "                margin_loss = margin_loss + margin_violations.sum() / (tgt_weights.sum() + 1e-8)\n",
    "\n",
    "            n_valid += 1\n",
    "\n",
    "        if n_valid > 0:\n",
    "            self_loss = self_loss / n_valid\n",
    "            ko_target_loss = ko_target_loss / n_valid\n",
    "            margin_loss = margin_loss / n_valid\n",
    "        \n",
    "        if n_with_english > 0:\n",
    "            en_target_loss = en_target_loss / n_with_english\n",
    "\n",
    "        return {\n",
    "            \"self\": self_loss,\n",
    "            \"ko_target\": ko_target_loss,\n",
    "            \"en_target\": en_target_loss,\n",
    "            \"margin\": margin_loss,\n",
    "        }\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        sparse_rep: torch.Tensor,\n",
    "        texts: list[str],\n",
    "        ko_token_ids: list,\n",
    "        ko_target_ids: list,\n",
    "        ko_target_weights: list,\n",
    "        en_target_ids: list,\n",
    "        en_target_weights: list,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Compute all losses.\n",
    "        \n",
    "        Returns dict with: self, ko_target, en_target, margin, distill, flops\n",
    "        \"\"\"\n",
    "        device = sparse_rep.device\n",
    "        \n",
    "        # Term-based losses\n",
    "        term_losses = self.compute_term_losses(\n",
    "            sparse_rep,\n",
    "            ko_token_ids,\n",
    "            ko_target_ids,\n",
    "            ko_target_weights,\n",
    "            en_target_ids,\n",
    "            en_target_weights,\n",
    "        )\n",
    "        \n",
    "        # Knowledge distillation loss\n",
    "        distill_loss = self.compute_distillation_loss(sparse_rep, texts, device)\n",
    "        \n",
    "        # FLOPS regularization (automatic noise suppression)\n",
    "        flops_loss = self.compute_flops_loss(sparse_rep)\n",
    "        \n",
    "        return {\n",
    "            **term_losses,\n",
    "            \"distill\": distill_loss,\n",
    "            \"flops\": flops_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pairs for evaluation (Korean source -> expected Korean synonyms + English translations)\n",
    "TEST_PAIRS = [\n",
    "    # (source_ko, expected_english, expected_korean_synonyms)\n",
    "    (\"머신러닝\", [\"machine\", \"learning\"], [\"머신\", \"러닝\", \"기계학습\"]),\n",
    "    (\"딥러닝\", [\"deep\", \"learning\"], [\"딥\", \"러닝\", \"심층학습\"]),\n",
    "    (\"자연어처리\", [\"natural\", \"language\", \"processing\"], [\"자연어\", \"처리\"]),\n",
    "    (\"인공지능\", [\"artificial\", \"intelligence\"], [\"인공\", \"지능\"]),\n",
    "    (\"검색엔진\", [\"search\", \"engine\"], [\"검색\", \"엔진\"]),\n",
    "    (\"데이터베이스\", [\"database\"], [\"데이터\", \"베이스\"]),\n",
    "    (\"클라우드\", [\"cloud\"], [\"클라우드\"]),\n",
    "    (\"서버\", [\"server\"], [\"서버\"]),\n",
    "    (\"네트워크\", [\"network\"], [\"네트워크\"]),\n",
    "    (\"추천시스템\", [\"recommend\", \"system\"], [\"추천\", \"시스템\"]),\n",
    "    (\"추천\", [\"recommend\", \"recommendation\"], [\"추천\"]),\n",
    "    (\"신경망\", [\"neural\", \"network\"], [\"신경망\", \"신경\"]),\n",
    "    (\"강화학습\", [\"reinforcement\", \"learning\"], [\"강화\", \"학습\"]),\n",
    "    (\"컴퓨터비전\", [\"computer\", \"vision\"], [\"컴퓨터\", \"비전\"]),\n",
    "    (\"음성인식\", [\"speech\", \"recognition\"], [\"음성\", \"인식\"]),\n",
    "]\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, device, top_k=50):\n",
    "    \"\"\"Evaluate model on test pairs - check both Korean and English activation.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    ko_activated_total = 0\n",
    "    en_activated_total = 0\n",
    "    ko_expected_total = 0\n",
    "    en_expected_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ko_term, en_expected, ko_expected in TEST_PAIRS:\n",
    "            encoding = tokenizer(\n",
    "                ko_term,\n",
    "                max_length=64,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            with autocast(\"cuda\", enabled=CONFIG[\"use_fp16\"]):\n",
    "                sparse_rep, _ = model(\n",
    "                    encoding[\"input_ids\"].to(device),\n",
    "                    encoding[\"attention_mask\"].to(device),\n",
    "                )\n",
    "\n",
    "            sparse_rep = sparse_rep[0].float().cpu()\n",
    "            top_indices = torch.topk(sparse_rep, k=top_k).indices.tolist()\n",
    "            top_tokens = tokenizer.convert_ids_to_tokens(top_indices)\n",
    "            top_tokens_set = set(top_tokens)\n",
    "\n",
    "            # Check Korean synonym/preservation activation\n",
    "            for ko in ko_expected:\n",
    "                ko_toks = tokenizer.tokenize(ko)\n",
    "                for tok in ko_toks:\n",
    "                    ko_expected_total += 1\n",
    "                    if tok in top_tokens_set:\n",
    "                        ko_activated_total += 1\n",
    "\n",
    "            # Check English translation activation\n",
    "            for en in en_expected:\n",
    "                en_toks = tokenizer.tokenize(en.lower())\n",
    "                for tok in en_toks:\n",
    "                    en_expected_total += 1\n",
    "                    if tok in top_tokens_set:\n",
    "                        en_activated_total += 1\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    ko_rate = (\n",
    "        ko_activated_total / ko_expected_total * 100 if ko_expected_total > 0 else 0\n",
    "    )\n",
    "    en_rate = (\n",
    "        en_activated_total / en_expected_total * 100 if en_expected_total > 0 else 0\n",
    "    )\n",
    "\n",
    "    return ko_rate, en_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\nLoading tokenizer: {CONFIG['model_name']}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Teacher Model for Knowledge Distillation\n",
    "print(f\"Loading teacher model: {CONFIG['teacher_model']}...\")\n",
    "teacher_model = SentenceTransformer(CONFIG[\"teacher_model\"])\n",
    "teacher_model = teacher_model.to(device)\n",
    "teacher_model.eval()\n",
    "\n",
    "print(f\"Teacher model loaded: {CONFIG['teacher_model']}\")\n",
    "print(f\"Teacher embedding dim: {teacher_model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Note: No manual noise token filtering needed!\n",
    "# FLOPS regularization will automatically suppress frequently-activated noise tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (1:N mixed format)\n",
    "dataset = TermMappingDataset(CONFIG[\"data_path\"], tokenizer, CONFIG[\"max_length\"])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset):,}\")\n",
    "print(f\"Batches per epoch: {len(dataloader):,}\")\n",
    "print(f\"Effective batch size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "print(f\"\\nCreating model: {CONFIG['model_name']}...\")\n",
    "model = create_splade_model(\n",
    "    model_name=CONFIG[\"model_name\"],\n",
    "    use_idf=False,\n",
    "    use_expansion=True,\n",
    "    expansion_mode=\"mlm\",\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Parameters: {n_params:,} ({n_params / 1e6:.1f}M)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function - Knowledge Distillation + FLOPS\n",
    "loss_fn = DistillationFLOPSLoss(\n",
    "    teacher_model=teacher_model,\n",
    "    target_margin=CONFIG[\"target_margin\"],\n",
    "    temperature=0.05,\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=CONFIG[\"learning_rate\"], \n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "total_steps = (\n",
    "    len(dataloader) * CONFIG[\"num_epochs\"] // CONFIG[\"gradient_accumulation_steps\"]\n",
    ")\n",
    "warmup_steps = int(total_steps * CONFIG[\"warmup_ratio\"])\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=warmup_steps, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Total optimization steps: {total_steps:,}\")\n",
    "print(f\"Warmup steps: {warmup_steps:,}\")\n",
    "print(f\"\\nLoss weights:\")\n",
    "print(f\"  Self (Korean source): {CONFIG['lambda_self']}\")\n",
    "print(f\"  Korean targets: {CONFIG['lambda_ko_target']}\")\n",
    "print(f\"  English targets: {CONFIG['lambda_en_target']}\")\n",
    "print(f\"  Margin: {CONFIG['lambda_margin']}\")\n",
    "print(f\"  Distillation: {CONFIG['lambda_distill']} (from teacher)\")\n",
    "print(f\"  FLOPS: {CONFIG['lambda_flops']} (auto noise suppression)\")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler(\"cuda\", enabled=CONFIG[\"use_fp16\"])\n",
    "\n",
    "# Create output directory\n",
    "CONFIG[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "print(f\"\\nOutput directory: {CONFIG['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 7. Initial Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate before training\n",
    "ko_rate, en_rate = evaluate_model(model, tokenizer, device)\n",
    "print(f\"Initial Performance:\")\n",
    "print(f\"  Korean Preservation: {ko_rate:.1f}%\")\n",
    "print(f\"  English Activation: {en_rate:.1f}%\")\n",
    "print(f\"  Combined Score: {ko_rate + en_rate:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training variables\n",
    "history = []\n",
    "best_score = 0\n",
    "global_step = 0\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(CONFIG[\"num_epochs\"]):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Epoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    model.train()\n",
    "    epoch_losses = defaultdict(float)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        texts = batch[\"texts\"]  # For distillation\n",
    "\n",
    "        with autocast(\"cuda\", enabled=CONFIG[\"use_fp16\"]):\n",
    "            sparse_rep, _ = model(input_ids, attention_mask)\n",
    "\n",
    "            # Compute all losses (including distillation and FLOPS)\n",
    "            losses = loss_fn(\n",
    "                sparse_rep,\n",
    "                texts,\n",
    "                batch[\"ko_token_ids\"],\n",
    "                batch[\"ko_target_ids\"],\n",
    "                batch[\"ko_target_weights\"],\n",
    "                batch[\"en_target_ids\"],\n",
    "                batch[\"en_target_weights\"],\n",
    "            )\n",
    "\n",
    "            # Total loss with all components\n",
    "            total_loss = (\n",
    "                CONFIG[\"lambda_self\"] * losses[\"self\"]\n",
    "                + CONFIG[\"lambda_ko_target\"] * losses[\"ko_target\"]\n",
    "                + CONFIG[\"lambda_en_target\"] * losses[\"en_target\"]\n",
    "                + CONFIG[\"lambda_margin\"] * losses[\"margin\"]\n",
    "                + CONFIG[\"lambda_distill\"] * losses[\"distill\"]\n",
    "                + CONFIG[\"lambda_flops\"] * losses[\"flops\"]\n",
    "            )\n",
    "\n",
    "            total_loss = total_loss / CONFIG[\"gradient_accumulation_steps\"]\n",
    "\n",
    "        scaler.scale(total_loss).backward()\n",
    "\n",
    "        # Track losses\n",
    "        epoch_losses[\"total\"] += total_loss.item() * CONFIG[\"gradient_accumulation_steps\"]\n",
    "        epoch_losses[\"self\"] += losses[\"self\"].item()\n",
    "        epoch_losses[\"ko_target\"] += losses[\"ko_target\"].item()\n",
    "        epoch_losses[\"en_target\"] += losses[\"en_target\"].item()\n",
    "        epoch_losses[\"margin\"] += losses[\"margin\"].item()\n",
    "        epoch_losses[\"distill\"] += losses[\"distill\"].item()\n",
    "        epoch_losses[\"flops\"] += losses[\"flops\"].item()\n",
    "\n",
    "        if (batch_idx + 1) % CONFIG[\"gradient_accumulation_steps\"] == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG[\"max_grad_norm\"])\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            progress_bar.set_postfix({\n",
    "                \"loss\": f\"{epoch_losses['total'] / (batch_idx + 1):.4f}\",\n",
    "                \"distill\": f\"{epoch_losses['distill'] / (batch_idx + 1):.4f}\",\n",
    "                \"flops\": f\"{epoch_losses['flops'] / (batch_idx + 1):.2f}\",\n",
    "            })\n",
    "\n",
    "    # Calculate average losses\n",
    "    n_batches = len(dataloader)\n",
    "    for key in epoch_losses:\n",
    "        epoch_losses[key] /= n_batches\n",
    "\n",
    "    history.append(dict(epoch_losses))\n",
    "\n",
    "    # Evaluate\n",
    "    ko_rate, en_rate = evaluate_model(model, tokenizer, device)\n",
    "    combined_score = ko_rate + en_rate\n",
    "\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"  Total Loss: {epoch_losses['total']:.4f}\")\n",
    "    print(f\"  Self Loss: {epoch_losses['self']:.4f}\")\n",
    "    print(f\"  KO Target: {epoch_losses['ko_target']:.4f}\")\n",
    "    print(f\"  EN Target: {epoch_losses['en_target']:.4f}\")\n",
    "    print(f\"  Distillation: {epoch_losses['distill']:.4f}\")\n",
    "    print(f\"  FLOPS: {epoch_losses['flops']:.2f}\")\n",
    "    print(f\"  Korean Activation: {ko_rate:.1f}%\")\n",
    "    print(f\"  English Activation: {en_rate:.1f}%\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint_path = CONFIG[\"output_dir\"] / f\"checkpoint_epoch{epoch + 1}.pt\"\n",
    "    torch.save({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"losses\": dict(epoch_losses),\n",
    "        \"ko_rate\": ko_rate,\n",
    "        \"en_rate\": en_rate,\n",
    "        \"config\": {k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items()},\n",
    "    }, checkpoint_path)\n",
    "    print(f\"  Saved: {checkpoint_path.name}\")\n",
    "\n",
    "    # Save best model (weighted: KO + 2*EN)\n",
    "    weighted_score = ko_rate + 2 * en_rate\n",
    "    if weighted_score > best_score:\n",
    "        best_score = weighted_score\n",
    "        best_path = CONFIG[\"output_dir\"] / \"best_model.pt\"\n",
    "        torch.save({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"ko_rate\": ko_rate,\n",
    "            \"en_rate\": en_rate,\n",
    "            \"combined_score\": combined_score,\n",
    "            \"config\": {k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items()},\n",
    "        }, best_path)\n",
    "        print(f\"  *** New best! Score: {weighted_score:.1f} (KO:{ko_rate:.1f}% + EN:{en_rate:.1f}%) ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 9. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_path = CONFIG[\"output_dir\"] / \"final_model.pt\"\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"config\": {\n",
    "            k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items()\n",
    "        },\n",
    "        \"history\": history,\n",
    "    },\n",
    "    final_path,\n",
    ")\n",
    "\n",
    "# Save training history\n",
    "with open(CONFIG[\"output_dir\"] / \"training_history.json\", \"w\") as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Final model saved: {final_path}\")\n",
    "print(f\"Best combined score: {best_score:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 10. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training curves\n",
    "if history:\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(18, 8))\n",
    "\n",
    "    epochs = range(1, len(history) + 1)\n",
    "\n",
    "    # Total loss\n",
    "    axes[0, 0].plot(epochs, [h['total'] for h in history], '-o', color='#3498db')\n",
    "    axes[0, 0].set_title('Total Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Self loss\n",
    "    axes[0, 1].plot(epochs, [h['self'] for h in history], '-o', color='#2ecc71')\n",
    "    axes[0, 1].set_title('Self Loss (Korean Source)')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Korean target loss\n",
    "    axes[0, 2].plot(epochs, [h['ko_target'] for h in history], '-o', color='#f39c12')\n",
    "    axes[0, 2].set_title('Korean Target Loss')\n",
    "    axes[0, 2].set_xlabel('Epoch')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # English target loss\n",
    "    axes[0, 3].plot(epochs, [h['en_target'] for h in history], '-o', color='#e74c3c')\n",
    "    axes[0, 3].set_title('English Target Loss')\n",
    "    axes[0, 3].set_xlabel('Epoch')\n",
    "    axes[0, 3].grid(True, alpha=0.3)\n",
    "\n",
    "    # Distillation loss\n",
    "    axes[1, 0].plot(epochs, [h['distill'] for h in history], '-o', color='#9b59b6')\n",
    "    axes[1, 0].set_title('Distillation Loss (Teacher)')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # FLOPS loss\n",
    "    axes[1, 1].plot(epochs, [h['flops'] for h in history], '-o', color='#1abc9c')\n",
    "    axes[1, 1].set_title('FLOPS Loss (Noise Suppression)')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Margin loss\n",
    "    axes[1, 2].plot(epochs, [h['margin'] for h in history], '-o', color='#e67e22')\n",
    "    axes[1, 2].set_title('Margin Loss')\n",
    "    axes[1, 2].set_xlabel('Epoch')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # Hide empty subplot\n",
    "    axes[1, 3].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CONFIG[\"output_dir\"] / \"training_curves.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Training curves saved to: {CONFIG['output_dir'] / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Summary: Knowledge Distillation + FLOPS Approach\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "                    ┌─────────────────┐\n",
    "                    │  Teacher Model  │\n",
    "                    │ (multilingual-  │\n",
    "                    │     e5-base)    │\n",
    "                    └────────┬────────┘\n",
    "                             │ Distillation\n",
    "                             ▼\n",
    "┌──────────┐         ┌─────────────────┐         ┌──────────┐\n",
    "│  Korean  │ ──────▶ │  Student Model  │ ──────▶ │  Sparse  │\n",
    "│  Input   │         │ (xlm-roberta-   │         │  Output  │\n",
    "│          │         │     large)      │         │          │\n",
    "└──────────┘         └─────────────────┘         └──────────┘\n",
    "                             │\n",
    "                             ▼\n",
    "                    ┌─────────────────┐\n",
    "                    │   FLOPS Loss    │\n",
    "                    │ (Auto Noise     │\n",
    "                    │  Suppression)   │\n",
    "                    └─────────────────┘\n",
    "```\n",
    "\n",
    "### Loss Components\n",
    "\n",
    "| Loss | Weight | Purpose |\n",
    "|------|--------|---------|\n",
    "| `lambda_self` | 3.0 | Preserve Korean source tokens |\n",
    "| `lambda_ko_target` | 2.0 | Activate Korean synonym tokens |\n",
    "| `lambda_en_target` | 8.0 | Activate English translation tokens |\n",
    "| `lambda_margin` | 2.0 | Ensure minimum activation |\n",
    "| `lambda_distill` | 1.0 | Learn from teacher model |\n",
    "| `lambda_flops` | 5e-5 | Automatic noise suppression |\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "1. **Knowledge Distillation**: Teacher model provides semantic similarity signal\n",
    "   - Student learns what tokens are semantically important\n",
    "   - No manual rules needed\n",
    "\n",
    "2. **FLOPS Regularization**: Automatic noise token suppression\n",
    "   - Tokens that activate frequently (s, the, a) get high penalty\n",
    "   - Meaningful tokens (machine, 검색) activate selectively → low penalty\n",
    "\n",
    "3. **Scalable**: Works across languages without language-specific rules\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Run inference tests with `03_inference_test.ipynb`\n",
    "2. Check if \"s\" noise is suppressed\n",
    "3. Verify Korean preservation rate improved\n",
    "4. Compare with previous rule-based approach"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

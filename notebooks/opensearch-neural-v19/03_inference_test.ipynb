{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v19 Inference Test - XLM-RoBERTa-large with High-Quality Data\n",
    "\n",
    "This notebook tests the v19 model trained with high-quality MUSE data (excluding wikidata).\n",
    "\n",
    "## v19 Key Features:\n",
    "- **Dataset**: v19_high_quality (~18K pairs, MUSE only, no wikidata)\n",
    "- **Model**: xlm-roberta-large\n",
    "- **Learning rate**: 2e-6 (same as v17)\n",
    "- **Epochs**: 10\n",
    "- **Loss weights**: self=2.0, target=5.0, margin=3.0, negative=0.5, sparsity=0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root\n",
    "def find_project_root():\n",
    "    \"\"\"Find project root by looking for markers like pyproject.toml or src/\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "from src.model.splade_model import create_splade_model\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load v19 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load v19 model\n",
    "model_path = PROJECT_ROOT / \"outputs\" / \"v19_xlm_large\" / \"best_model.pt\"\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "print(f\"Model exists: {model_path.exists()}\")\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=\"cpu\", weights_only=False)\n",
    "config = checkpoint[\"config\"]\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "for key, value in config.items():\n",
    "    if not isinstance(value, Path):\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nBest Model Info:\")\n",
    "print(f\"  Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "print(f\"  Korean Rate: {checkpoint.get('ko_rate', 'N/A'):.1f}%\")\n",
    "print(f\"  English Rate: {checkpoint.get('en_rate', 'N/A'):.1f}%\")\n",
    "print(f\"  Combined Score: {checkpoint.get('combined_score', 'N/A'):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "model_name = config.get(\"model_name\", \"xlm-roberta-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"Tokenizer: {model_name}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "# Create model\n",
    "model = create_splade_model(\n",
    "    model_name=model_name,\n",
    "    use_expansion=True,\n",
    "    expansion_mode=\"mlm\",\n",
    ")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Define Inference Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_term(term: str, top_k: int = 20) -> dict:\n",
    "    \"\"\"Encode a term and return top-k tokens with weights.\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        term, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=64\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        weights, _ = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "    \n",
    "    # Get top-k tokens\n",
    "    top_indices = weights[0].topk(top_k).indices.tolist()\n",
    "    top_values = weights[0].topk(top_k).values.tolist()\n",
    "    top_tokens = [tokenizer.decode([idx]).strip() for idx in top_indices]\n",
    "    \n",
    "    return {\n",
    "        \"term\": term,\n",
    "        \"tokens\": list(zip(top_tokens, top_values)),\n",
    "        \"top_indices\": top_indices,\n",
    "    }\n",
    "\n",
    "\n",
    "def display_result(result: dict, expected_en: list = None):\n",
    "    \"\"\"Display encoding result with formatting.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Input: {result['term']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Check if Korean is preserved\n",
    "    top_tokens = [t[0] for t in result['tokens'][:10]]\n",
    "    ko_preserved = any(result['term'] in tok or tok in result['term'] for tok in top_tokens if tok)\n",
    "    \n",
    "    # Check if English is activated\n",
    "    en_found = []\n",
    "    if expected_en:\n",
    "        for en in expected_en:\n",
    "            for tok in top_tokens:\n",
    "                if en.lower() in tok.lower() or tok.lower() in en.lower():\n",
    "                    en_found.append(en)\n",
    "                    break\n",
    "    \n",
    "    print(f\"Korean preserved: {'Yes' if ko_preserved else 'No'}\")\n",
    "    if expected_en:\n",
    "        print(f\"English activated: {en_found if en_found else 'None'}\")\n",
    "    \n",
    "    print(f\"\\nTop 10 tokens:\")\n",
    "    for i, (token, weight) in enumerate(result['tokens'][:10]):\n",
    "        marker = \"\"\n",
    "        if result['term'] in token or token in result['term']:\n",
    "            marker = \" [KO]\"\n",
    "        elif expected_en and any(en.lower() in token.lower() for en in expected_en):\n",
    "            marker = \" [EN]\"\n",
    "        print(f\"  {i+1:2}. {token:20} {weight:.4f}{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Test Korean-English Term Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases: (Korean term, expected English tokens)\n",
    "test_cases = [\n",
    "    (\"추천\", [\"recommend\", \"recommendation\", \"suggest\"]),\n",
    "    (\"검색\", [\"search\", \"retrieval\", \"query\"]),\n",
    "    (\"인공지능\", [\"artificial\", \"intelligence\", \"AI\"]),\n",
    "    (\"신경망\", [\"neural\", \"network\", \"deep\"]),\n",
    "    (\"기계학습\", [\"machine\", \"learning\", \"ML\"]),\n",
    "    (\"강화학습\", [\"reinforcement\", \"learning\", \"RL\"]),\n",
    "]\n",
    "\n",
    "for ko_term, en_terms in test_cases:\n",
    "    result = encode_term(ko_term)\n",
    "    display_result(result, en_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Test Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended test cases\n",
    "comprehensive_tests = [\n",
    "    # NLP/ML terms\n",
    "    (\"자연어처리\", [\"natural\", \"language\", \"NLP\", \"processing\"]),\n",
    "    (\"딥러닝\", [\"deep\", \"learning\"]),\n",
    "    (\"트랜스포머\", [\"transformer\", \"attention\"]),\n",
    "    (\"임베딩\", [\"embedding\", \"vector\"]),\n",
    "    \n",
    "    # Software Engineering terms\n",
    "    (\"데이터베이스\", [\"database\", \"DB\", \"data\"]),\n",
    "    (\"클라우드\", [\"cloud\", \"computing\"]),\n",
    "    (\"서버\", [\"server\", \"servers\"]),\n",
    "    (\"클라이언트\", [\"client\", \"clients\"]),\n",
    "    (\"프레임워크\", [\"framework\", \"frameworks\"]),\n",
    "    (\"라이브러리\", [\"library\", \"libraries\"]),\n",
    "    \n",
    "    # DevOps terms\n",
    "    (\"컨테이너\", [\"container\", \"docker\", \"kubernetes\"]),\n",
    "    (\"마이크로서비스\", [\"microservice\", \"micro\", \"service\"]),\n",
    "    (\"모니터링\", [\"monitoring\", \"monitor\"]),\n",
    "    (\"배포\", [\"deployment\", \"deploy\"]),\n",
    "    \n",
    "    # Development terms\n",
    "    (\"테스트\", [\"test\", \"testing\"]),\n",
    "    (\"디버깅\", [\"debug\", \"debugging\"]),\n",
    "    (\"리팩토링\", [\"refactoring\", \"refactor\"]),\n",
    "    (\"아키텍처\", [\"architecture\", \"architect\"]),\n",
    "    \n",
    "    # System terms\n",
    "    (\"네트워크\", [\"network\", \"networking\"]),\n",
    "    (\"운영체제\", [\"operating\", \"system\", \"OS\"]),\n",
    "    (\"컴파일러\", [\"compiler\", \"compile\"]),\n",
    "    (\"알고리즘\", [\"algorithm\", \"algorithms\"]),\n",
    "    (\"최적화\", [\"optimization\", \"optimize\"]),\n",
    "    \n",
    "    # Security terms\n",
    "    (\"보안\", [\"security\", \"secure\", \"protection\"]),\n",
    "    (\"암호화\", [\"encryption\", \"encrypt\", \"crypto\"]),\n",
    "    (\"인증\", [\"authentication\", \"auth\"]),\n",
    "    \n",
    "    # Data terms\n",
    "    (\"분석\", [\"analysis\", \"analytics\", \"analyze\"]),\n",
    "    (\"인덱싱\", [\"indexing\", \"index\"]),\n",
    "    (\"쿼리\", [\"query\", \"queries\"]),\n",
    "    (\"캐싱\", [\"caching\", \"cache\"]),\n",
    "    (\"스케일링\", [\"scaling\", \"scale\"]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive tests and calculate metrics\n",
    "print(\"=\" * 80)\n",
    "print(\"Comprehensive Test Results\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "ko_preserved_count = 0\n",
    "en_activated_count = 0\n",
    "total = len(comprehensive_tests)\n",
    "\n",
    "results_table = []\n",
    "\n",
    "for ko_term, en_terms in comprehensive_tests:\n",
    "    result = encode_term(ko_term, top_k=10)\n",
    "    top_tokens = [t[0] for t in result['tokens']]\n",
    "    top_values = [t[1] for t in result['tokens']]\n",
    "    \n",
    "    # Check Korean preservation\n",
    "    ko_preserved = any(ko_term in tok or tok in ko_term for tok in top_tokens if tok)\n",
    "    if ko_preserved:\n",
    "        ko_preserved_count += 1\n",
    "    \n",
    "    # Check English activation\n",
    "    en_found = []\n",
    "    for en in en_terms:\n",
    "        for tok in top_tokens:\n",
    "            if en.lower() in tok.lower() or tok.lower() in en.lower():\n",
    "                en_found.append(en)\n",
    "                break\n",
    "    en_activated = len(en_found) > 0\n",
    "    if en_activated:\n",
    "        en_activated_count += 1\n",
    "    \n",
    "    # Store result\n",
    "    ko_mark = \"o\" if ko_preserved else \"x\"\n",
    "    en_mark = \"o\" if en_activated else \"x\"\n",
    "    top_3 = [f\"{t}({v:.2f})\" for t, v in zip(top_tokens[:3], top_values[:3])]\n",
    "    \n",
    "    results_table.append({\n",
    "        \"term\": ko_term,\n",
    "        \"ko\": ko_mark,\n",
    "        \"en\": en_mark,\n",
    "        \"en_found\": en_found,\n",
    "        \"top_3\": top_3,\n",
    "    })\n",
    "    \n",
    "    print(f\"{ko_term:12} | KO:{ko_mark} EN:{en_mark} | {', '.join(top_3)}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Summary\")\n",
    "print(\"=\" * 80)\n",
    "ko_rate = ko_preserved_count / total * 100\n",
    "en_rate = en_activated_count / total * 100\n",
    "combined = ko_rate + en_rate\n",
    "\n",
    "print(f\"Korean Preservation: {ko_preserved_count}/{total} ({ko_rate:.1f}%)\")\n",
    "print(f\"English Activation:  {en_activated_count}/{total} ({en_rate:.1f}%)\")\n",
    "print(f\"Combined Score:      {combined:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Training History Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load training history\n",
    "history_path = PROJECT_ROOT / \"outputs\" / \"v19_xlm_large\" / \"training_history.json\"\n",
    "\n",
    "if history_path.exists():\n",
    "    with open(history_path, \"r\") as f:\n",
    "        history = json.load(f)\n",
    "\n",
    "    print(f\"Training epochs: {len(history)}\")\n",
    "    print(\"\\nLoss components per epoch:\")\n",
    "    for i, epoch in enumerate(history):\n",
    "        print(f\"  Epoch {i+1}: total={epoch['total']:.4f}, self={epoch['self']:.4f}, \"\n",
    "              f\"target={epoch['target']:.4f}, margin={epoch['margin']:.6f}, \"\n",
    "              f\"negative={epoch['negative']:.4f}\")\n",
    "else:\n",
    "    print(f\"Training history not found at: {history_path}\")\n",
    "    history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "if history:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "    epochs = range(1, len(history) + 1)\n",
    "\n",
    "    # Total loss\n",
    "    axes[0, 0].plot(epochs, [-h['total'] for h in history], '-o', color='#3498db')\n",
    "    axes[0, 0].set_title('Total Loss (negated for visualization)')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Self loss\n",
    "    axes[0, 1].plot(epochs, [-h['self'] for h in history], '-o', color='#2ecc71')\n",
    "    axes[0, 1].set_title('Self Loss (Korean Preservation)')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Target loss\n",
    "    axes[1, 0].plot(epochs, [-h['target'] for h in history], '-o', color='#e74c3c')\n",
    "    axes[1, 0].set_title('Target Loss (English Activation)')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Negative loss\n",
    "    axes[1, 1].plot(epochs, [h['negative'] for h in history], '-o', color='#9b59b6')\n",
    "    axes[1, 1].set_title('Negative Loss')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. Detailed Analysis of Specific Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze specific terms in detail\n",
    "print(\"=\" * 80)\n",
    "print(\"Detailed Analysis: Sample Terms\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "detail_terms = [\"추천\", \"검색\", \"인공지능\", \"데이터베이스\", \"보안\"]\n",
    "\n",
    "for term in detail_terms:\n",
    "    result = encode_term(term, top_k=15)\n",
    "    print(f\"\\n{term}:\")\n",
    "    for i, (token, weight) in enumerate(result[\"tokens\"][:10]):\n",
    "        print(f\"  {i+1:2}. {token:20} {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 7. Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze v19 dataset\n",
    "dataset_path = PROJECT_ROOT / \"dataset\" / \"v19_high_quality\" / \"term_pairs.jsonl\"\n",
    "\n",
    "if dataset_path.exists():\n",
    "    data = []\n",
    "    sources = {}\n",
    "    \n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line.strip())\n",
    "            data.append(item)\n",
    "            source = item.get(\"source\", \"unknown\")\n",
    "            sources[source] = sources.get(source, 0) + 1\n",
    "    \n",
    "    print(f\"Total pairs: {len(data):,}\")\n",
    "    print(f\"\\nData sources:\")\n",
    "    for source, count in sorted(sources.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {source}: {count:,} ({count/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nSample pairs:\")\n",
    "    for item in data[:10]:\n",
    "        print(f\"  {item['ko']} -> {item['en']} ({item.get('source', 'unknown')})\")\n",
    "else:\n",
    "    print(f\"Dataset not found: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "### v19 Results Summary\n",
    "\n",
    "v19 was trained with the following goals:\n",
    "\n",
    "1. **Remove wikidata noise** from v18 dataset\n",
    "2. **Focus on quality** over quantity (~18K vs 33K pairs)\n",
    "3. **Use same hyperparameters** as successful v17 model\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "- Korean preservation rate shows how well the model maintains the input Korean term\n",
    "- English activation rate shows cross-lingual term expansion capability\n",
    "- Combined score = Korean rate + English rate\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Analyze data quality**: Investigate what makes certain translation pairs effective\n",
    "2. **Loss function tuning**: Consider adjusting target loss weight for better English activation\n",
    "3. **Data filtering**: Apply stricter quality filters to identify high-impact pairs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

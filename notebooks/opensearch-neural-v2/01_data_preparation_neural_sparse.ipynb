{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenSearch Neural Sparse Model - 학습 데이터 준비\n",
    "\n",
    "## 목표\n",
    "한국어 공개 데이터셋(Wikipedia, Namuwiki)을 활용하여 OpenSearch neural sparse 모델 학습에 사용할 수 있는 JSONL 형식의 데이터셋을 생성합니다.\n",
    "\n",
    "## 데이터 포맷\n",
    "```json\n",
    "{\n",
    "    \"query\": \"질문 텍스트\",\n",
    "    \"docs\": [\"문서1\", \"문서2\", \"문서3\", ...],\n",
    "    \"scores\": [9.5, 7.2, 5.1, ...]\n",
    "}\n",
    "```\n",
    "\n",
    "## 주요 단계\n",
    "1. 데이터 로딩 및 전처리\n",
    "2. Query 생성 (문서 제목 활용)\n",
    "3. Embedding 생성 (intfloat/multilingual-e5-large)\n",
    "4. Hard Negatives Mining (FAISS 유사도 검색)\n",
    "5. K-means 클러스터링으로 관련 문서 그룹화\n",
    "6. 최종 JSONL 데이터셋 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# 프로젝트 루트 경로\n",
    "PROJECT_ROOT = Path(\"/home/west/Documents/cursor-workspace/opensearch-neural-pre-train\")\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# 시드 설정\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# 출력 디렉토리 생성\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"dataset\" / \"neural_sparse_training\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EMBEDDINGS_DIR = OUTPUT_DIR / \"embeddings\"\n",
    "EMBEDDINGS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로딩 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"문서 데이터 클래스\"\"\"\n",
    "    id: str\n",
    "    title: str\n",
    "    text: str\n",
    "    url: Optional[str] = None\n",
    "    source: Optional[str] = None\n",
    "    \n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"데이터 검증 및 정제\"\"\"\n",
    "        self.title = self.title.strip()\n",
    "        self.text = self.text.strip()\n",
    "\n",
    "\n",
    "def load_jsonl_file(file_path: Path) -> List[Dict]:\n",
    "    \"\"\"JSONL 파일을 로드합니다.\n",
    "    \n",
    "    Args:\n",
    "        file_path: JSONL 파일 경로\n",
    "        \n",
    "    Returns:\n",
    "        딕셔너리 리스트\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                documents.append(json.loads(line))\n",
    "    return documents\n",
    "\n",
    "\n",
    "def load_korean_datasets(\n",
    "    wiki_limit: Optional[int] = None,\n",
    "    namuwiki_limit: Optional[int] = None,\n",
    "    min_text_length: int = 100,\n",
    "    max_text_length: int = 2000,\n",
    ") -> List[Document]:\n",
    "    \"\"\"한국어 Wikipedia와 Namuwiki 데이터를 로드합니다.\n",
    "    \n",
    "    Args:\n",
    "        wiki_limit: Wikipedia 문서 개수 제한 (None이면 전체)\n",
    "        namuwiki_limit: Namuwiki 문서 개수 제한 (None이면 전체)\n",
    "        min_text_length: 최소 텍스트 길이\n",
    "        max_text_length: 최대 텍스트 길이\n",
    "        \n",
    "    Returns:\n",
    "        Document 객체 리스트\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Wikipedia 데이터 로드\n",
    "    wiki_dir = PROJECT_ROOT / \"dataset\" / \"wikipedia\"\n",
    "    wiki_files = sorted(wiki_dir.glob(\"ko_articles_chunk_*.jsonl\"))\n",
    "    \n",
    "    print(f\"Loading Wikipedia data from {len(wiki_files)} files...\")\n",
    "    wiki_count = 0\n",
    "    for file_path in tqdm(wiki_files, desc=\"Wikipedia\"):\n",
    "        for item in load_jsonl_file(file_path):\n",
    "            if wiki_limit and wiki_count >= wiki_limit:\n",
    "                break\n",
    "                \n",
    "            text = item.get(\"text\", \"\")\n",
    "            if min_text_length <= len(text) <= max_text_length:\n",
    "                documents.append(Document(\n",
    "                    id=f\"wiki_{item['id']}\",\n",
    "                    title=item.get(\"title\", \"\"),\n",
    "                    text=text,\n",
    "                    url=item.get(\"url\"),\n",
    "                    source=\"wikipedia\"\n",
    "                ))\n",
    "                wiki_count += 1\n",
    "        \n",
    "        if wiki_limit and wiki_count >= wiki_limit:\n",
    "            break\n",
    "    \n",
    "    print(f\"Loaded {wiki_count} Wikipedia documents\")\n",
    "    \n",
    "    # Namuwiki 데이터 로드\n",
    "    namuwiki_dir = PROJECT_ROOT / \"dataset\" / \"namuwiki\"\n",
    "    if namuwiki_dir.exists():\n",
    "        namuwiki_files = sorted(namuwiki_dir.glob(\"namuwiki_chunk_*.jsonl\"))\n",
    "        \n",
    "        print(f\"Loading Namuwiki data from {len(namuwiki_files)} files...\")\n",
    "        namuwiki_count = 0\n",
    "        for file_path in tqdm(namuwiki_files, desc=\"Namuwiki\"):\n",
    "            for item in load_jsonl_file(file_path):\n",
    "                if namuwiki_limit and namuwiki_count >= namuwiki_limit:\n",
    "                    break\n",
    "                    \n",
    "                text = item.get(\"text\", \"\")\n",
    "                if min_text_length <= len(text) <= max_text_length:\n",
    "                    documents.append(Document(\n",
    "                        id=f\"namu_{item.get('id', namuwiki_count)}\",\n",
    "                        title=item.get(\"title\", \"\"),\n",
    "                        text=text,\n",
    "                        url=item.get(\"url\"),\n",
    "                        source=\"namuwiki\"\n",
    "                    ))\n",
    "                    namuwiki_count += 1\n",
    "            \n",
    "            if namuwiki_limit and namuwiki_count >= namuwiki_limit:\n",
    "                break\n",
    "        \n",
    "        print(f\"Loaded {namuwiki_count} Namuwiki documents\")\n",
    "    \n",
    "    print(f\"Total documents loaded: {len(documents)}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 (처음에는 샘플로 시작)\n",
    "# 전체 데이터: wiki_limit=None, namuwiki_limit=None\n",
    "documents = load_korean_datasets(\n",
    "    wiki_limit=10000,  # 테스트용으로 제한\n",
    "    namuwiki_limit=5000,  # 테스트용으로 제한\n",
    "    min_text_length=100,\n",
    "    max_text_length=2000,\n",
    ")\n",
    "\n",
    "print(f\"\\nSample document:\")\n",
    "print(f\"Title: {documents[0].title}\")\n",
    "print(f\"Text length: {len(documents[0].text)}\")\n",
    "print(f\"Text preview: {documents[0].text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query 생성\n",
    "\n",
    "문서 제목을 query로 사용하고, 본문을 positive document로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QueryDocPair:\n",
    "    \"\"\"Query-Document 쌍\"\"\"\n",
    "    query_id: str\n",
    "    query_text: str\n",
    "    positive_doc_id: str\n",
    "    positive_doc_text: str\n",
    "    source: str\n",
    "\n",
    "\n",
    "def create_query_doc_pairs(documents: List[Document]) -> List[QueryDocPair]:\n",
    "    \"\"\"문서로부터 Query-Document 쌍을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        documents: Document 객체 리스트\n",
    "        \n",
    "    Returns:\n",
    "        QueryDocPair 객체 리스트\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    for doc in tqdm(documents, desc=\"Creating query-doc pairs\"):\n",
    "        if not doc.title or not doc.text:\n",
    "            continue\n",
    "            \n",
    "        pairs.append(QueryDocPair(\n",
    "            query_id=f\"q_{doc.id}\",\n",
    "            query_text=doc.title,\n",
    "            positive_doc_id=doc.id,\n",
    "            positive_doc_text=doc.text,\n",
    "            source=doc.source,\n",
    "        ))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "qd_pairs = create_query_doc_pairs(documents)\n",
    "print(f\"Created {len(qd_pairs)} query-document pairs\")\n",
    "\n",
    "print(f\"\\nSample pair:\")\n",
    "print(f\"Query: {qd_pairs[0].query_text}\")\n",
    "print(f\"Document: {qd_pairs[0].positive_doc_text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding 생성 (intfloat/multilingual-e5-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "MODEL_NAME = \"intfloat/multilingual-e5-large\"\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
    "print(f\"Model loaded successfully. Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(\n",
    "    texts: List[str],\n",
    "    model: SentenceTransformer,\n",
    "    batch_size: int = 32,\n",
    "    prefix: str = \"\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"텍스트 리스트에 대한 임베딩을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        texts: 텍스트 리스트\n",
    "        model: SentenceTransformer 모델\n",
    "        batch_size: 배치 크기\n",
    "        prefix: E5 모델용 prefix (\"query: \" 또는 \"passage: \")\n",
    "        \n",
    "    Returns:\n",
    "        임베딩 배열 (N, D)\n",
    "    \"\"\"\n",
    "    if prefix:\n",
    "        texts = [f\"{prefix}{text}\" for text in texts]\n",
    "    \n",
    "    embeddings = model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Query 임베딩 생성\n",
    "print(\"Generating query embeddings...\")\n",
    "query_texts = [pair.query_text for pair in qd_pairs]\n",
    "query_embeddings = generate_embeddings(\n",
    "    query_texts,\n",
    "    model,\n",
    "    batch_size=32,\n",
    "    prefix=\"query: \",  # E5 모델은 query/passage prefix 사용\n",
    ")\n",
    "\n",
    "print(f\"Query embeddings shape: {query_embeddings.shape}\")\n",
    "\n",
    "# Document 임베딩 생성\n",
    "print(\"\\nGenerating document embeddings...\")\n",
    "doc_texts = [pair.positive_doc_text for pair in qd_pairs]\n",
    "doc_embeddings = generate_embeddings(\n",
    "    doc_texts,\n",
    "    model,\n",
    "    batch_size=32,\n",
    "    prefix=\"passage: \",\n",
    ")\n",
    "\n",
    "print(f\"Document embeddings shape: {doc_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 저장\n",
    "np.save(EMBEDDINGS_DIR / \"query_embeddings.npy\", query_embeddings)\n",
    "np.save(EMBEDDINGS_DIR / \"document_embeddings.npy\", doc_embeddings)\n",
    "print(f\"Embeddings saved to {EMBEDDINGS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hard Negatives Mining (FAISS)\n",
    "\n",
    "각 query에 대해 유사하지만 관련 없는 문서를 hard negatives로 선정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faiss_index(embeddings: np.ndarray) -> faiss.Index:\n",
    "    \"\"\"FAISS 인덱스를 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: 임베딩 배열 (N, D)\n",
    "        \n",
    "    Returns:\n",
    "        FAISS 인덱스\n",
    "    \"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    # L2 정규화된 벡터이므로 Inner Product = Cosine Similarity\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    index.add(embeddings.astype(np.float32))\n",
    "    \n",
    "    return index\n",
    "\n",
    "\n",
    "# FAISS 인덱스 생성\n",
    "print(\"Building FAISS index...\")\n",
    "doc_index = build_faiss_index(doc_embeddings)\n",
    "print(f\"FAISS index built. Total documents: {doc_index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_hard_negatives(\n",
    "    query_embeddings: np.ndarray,\n",
    "    doc_index: faiss.Index,\n",
    "    k: int = 10,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"각 query에 대해 유사한 문서를 검색합니다.\n",
    "    \n",
    "    Args:\n",
    "        query_embeddings: Query 임베딩 (N, D)\n",
    "        doc_index: FAISS 인덱스\n",
    "        k: 검색할 문서 개수\n",
    "        \n",
    "    Returns:\n",
    "        (distances, indices) 튜플\n",
    "    \"\"\"\n",
    "    distances, indices = doc_index.search(\n",
    "        query_embeddings.astype(np.float32),\n",
    "        k,\n",
    "    )\n",
    "    \n",
    "    return distances, indices\n",
    "\n",
    "\n",
    "# Hard negatives 검색 (top-10)\n",
    "print(\"Searching for hard negatives...\")\n",
    "NUM_CANDIDATES = 10  # positive 1개 + hard negatives 9개 검색\n",
    "\n",
    "distances, indices = search_hard_negatives(\n",
    "    query_embeddings,\n",
    "    doc_index,\n",
    "    k=NUM_CANDIDATES,\n",
    ")\n",
    "\n",
    "print(f\"Search completed. Shape: {indices.shape}\")\n",
    "print(f\"Sample distances: {distances[0]}\")\n",
    "print(f\"Sample indices: {indices[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. K-means 클러스터링\n",
    "\n",
    "문서를 클러스터링하여 관련 문서를 그룹화하고, 클러스터 내 문서들에 대한 관련성 점수를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_kmeans_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    n_clusters: int = 100,\n",
    "    batch_size: int = 1000,\n",
    ") -> Tuple[MiniBatchKMeans, np.ndarray]:\n",
    "    \"\"\"K-means 클러스터링을 수행합니다.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: 임베딩 배열 (N, D)\n",
    "        n_clusters: 클러스터 개수\n",
    "        batch_size: MiniBatchKMeans 배치 크기\n",
    "        \n",
    "    Returns:\n",
    "        (kmeans 모델, cluster labels)\n",
    "    \"\"\"\n",
    "    print(f\"Performing K-means clustering with {n_clusters} clusters...\")\n",
    "    \n",
    "    kmeans = MiniBatchKMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        batch_size=batch_size,\n",
    "        random_state=SEED,\n",
    "        verbose=1,\n",
    "    )\n",
    "    \n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    return kmeans, cluster_labels\n",
    "\n",
    "\n",
    "# 클러스터 개수는 데이터 크기에 따라 조정\n",
    "n_clusters = min(100, len(documents) // 50)\n",
    "print(f\"Using {n_clusters} clusters for {len(documents)} documents\")\n",
    "\n",
    "kmeans, cluster_labels = perform_kmeans_clustering(\n",
    "    doc_embeddings,\n",
    "    n_clusters=n_clusters,\n",
    ")\n",
    "\n",
    "print(f\"Clustering completed.\")\n",
    "print(f\"Cluster distribution: {np.bincount(cluster_labels)[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 최종 JSONL 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingSample:\n",
    "    \"\"\"학습 샘플 데이터 클래스\"\"\"\n",
    "    query: str\n",
    "    docs: List[str]\n",
    "    scores: List[float]\n",
    "\n",
    "\n",
    "def create_training_samples(\n",
    "    qd_pairs: List[QueryDocPair],\n",
    "    search_indices: np.ndarray,\n",
    "    search_scores: np.ndarray,\n",
    "    num_docs_per_query: int = 8,\n",
    "    positive_score: float = 10.0,\n",
    ") -> List[TrainingSample]:\n",
    "    \"\"\"최종 학습 샘플을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        qd_pairs: Query-Document 쌍 리스트\n",
    "        search_indices: FAISS 검색 결과 인덱스\n",
    "        search_scores: FAISS 검색 결과 점수\n",
    "        num_docs_per_query: 각 query당 문서 개수\n",
    "        positive_score: Positive document 점수\n",
    "        \n",
    "    Returns:\n",
    "        TrainingSample 리스트\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    for i, pair in enumerate(tqdm(qd_pairs, desc=\"Creating training samples\")):\n",
    "        docs = []\n",
    "        scores = []\n",
    "        \n",
    "        # 1. Positive document (항상 첫 번째)\n",
    "        docs.append(pair.positive_doc_text)\n",
    "        scores.append(positive_score)\n",
    "        \n",
    "        # 2. Hard negatives (FAISS 검색 결과에서 선택)\n",
    "        # 첫 번째는 자기 자신이므로 제외\n",
    "        for j in range(1, min(num_docs_per_query, len(search_indices[i]))):\n",
    "            neg_idx = search_indices[i][j]\n",
    "            neg_score = search_scores[i][j]\n",
    "            \n",
    "            # 자기 자신이 아닌 경우만 추가\n",
    "            if neg_idx != i:\n",
    "                docs.append(qd_pairs[neg_idx].positive_doc_text)\n",
    "                # Cosine similarity를 0-10 스케일로 변환\n",
    "                # neg_score는 0~1 범위이므로 positive_score보다 낮게 설정\n",
    "                scaled_score = float(neg_score * (positive_score - 1.0))\n",
    "                scores.append(scaled_score)\n",
    "        \n",
    "        # 문서가 충분하지 않으면 랜덤 샘플링\n",
    "        while len(docs) < num_docs_per_query:\n",
    "            random_idx = random.randint(0, len(qd_pairs) - 1)\n",
    "            if random_idx != i:\n",
    "                docs.append(qd_pairs[random_idx].positive_doc_text)\n",
    "                scores.append(0.5)  # Low score for random negatives\n",
    "        \n",
    "        samples.append(TrainingSample(\n",
    "            query=pair.query_text,\n",
    "            docs=docs[:num_docs_per_query],\n",
    "            scores=scores[:num_docs_per_query],\n",
    "        ))\n",
    "    \n",
    "    return samples\n",
    "\n",
    "\n",
    "# 학습 샘플 생성\n",
    "NUM_DOCS_PER_QUERY = 8  # positive 1개 + negatives 7개\n",
    "\n",
    "training_samples = create_training_samples(\n",
    "    qd_pairs,\n",
    "    indices,\n",
    "    distances,\n",
    "    num_docs_per_query=NUM_DOCS_PER_QUERY,\n",
    ")\n",
    "\n",
    "print(f\"\\nCreated {len(training_samples)} training samples\")\n",
    "print(f\"\\nSample:\")\n",
    "sample = training_samples[0]\n",
    "print(f\"Query: {sample.query}\")\n",
    "print(f\"Num docs: {len(sample.docs)}\")\n",
    "print(f\"Scores: {sample.scores}\")\n",
    "print(f\"First doc preview: {sample.docs[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Val split\n",
    "random.shuffle(training_samples)\n",
    "split_idx = int(len(training_samples) * 0.9)\n",
    "\n",
    "train_samples = training_samples[:split_idx]\n",
    "val_samples = training_samples[split_idx:]\n",
    "\n",
    "print(f\"Train samples: {len(train_samples)}\")\n",
    "print(f\"Val samples: {len(val_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(\n",
    "    samples: List[TrainingSample],\n",
    "    file_path: Path,\n",
    ") -> None:\n",
    "    \"\"\"학습 샘플을 JSONL 파일로 저장합니다.\n",
    "    \n",
    "    Args:\n",
    "        samples: TrainingSample 리스트\n",
    "        file_path: 출력 파일 경로\n",
    "    \"\"\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sample in tqdm(samples, desc=f\"Saving to {file_path.name}\"):\n",
    "            json_obj = {\n",
    "                \"query\": sample.query,\n",
    "                \"docs\": sample.docs,\n",
    "                \"scores\": sample.scores,\n",
    "            }\n",
    "            f.write(json.dumps(json_obj, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    print(f\"Saved {len(samples)} samples to {file_path}\")\n",
    "\n",
    "\n",
    "# JSONL 파일로 저장\n",
    "save_jsonl(train_samples, OUTPUT_DIR / \"train.jsonl\")\n",
    "save_jsonl(val_samples, OUTPUT_DIR / \"val.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 메타데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메타데이터 생성\n",
    "metadata = {\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"total_documents\": len(documents),\n",
    "    \"total_queries\": len(qd_pairs),\n",
    "    \"train_samples\": len(train_samples),\n",
    "    \"val_samples\": len(val_samples),\n",
    "    \"docs_per_query\": NUM_DOCS_PER_QUERY,\n",
    "    \"embedding_model\": MODEL_NAME,\n",
    "    \"embedding_dimension\": query_embeddings.shape[1],\n",
    "    \"source_datasets\": [\"wikipedia_ko\", \"namuwiki\"],\n",
    "    \"num_clusters\": n_clusters,\n",
    "    \"min_text_length\": 100,\n",
    "    \"max_text_length\": 2000,\n",
    "    \"data_format\": \"pre-computed knowledge distillation\",\n",
    "    \"compatible_with\": \"opensearch-sparse-model-tuning-sample\",\n",
    "}\n",
    "\n",
    "# 메타데이터 저장\n",
    "with open(OUTPUT_DIR / \"metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\nMetadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 데이터 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 파일 검증\n",
    "print(\"Validating saved files...\\n\")\n",
    "\n",
    "# Train 파일 검증\n",
    "with open(OUTPUT_DIR / \"train.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    first_line = json.loads(f.readline())\n",
    "    print(\"Train sample:\")\n",
    "    print(f\"  Query: {first_line['query']}\")\n",
    "    print(f\"  Num docs: {len(first_line['docs'])}\")\n",
    "    print(f\"  Num scores: {len(first_line['scores'])}\")\n",
    "    print(f\"  Scores: {first_line['scores']}\")\n",
    "    print(f\"  Max score: {max(first_line['scores'])}\")\n",
    "    print(f\"  Min score: {min(first_line['scores'])}\")\n",
    "\n",
    "# 점수 분포 확인\n",
    "all_scores = []\n",
    "with open(OUTPUT_DIR / \"train.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        sample = json.loads(line)\n",
    "        all_scores.extend(sample['scores'])\n",
    "\n",
    "all_scores = np.array(all_scores)\n",
    "print(f\"\\nScore distribution:\")\n",
    "print(f\"  Mean: {all_scores.mean():.2f}\")\n",
    "print(f\"  Std: {all_scores.std():.2f}\")\n",
    "print(f\"  Min: {all_scores.min():.2f}\")\n",
    "print(f\"  Max: {all_scores.max():.2f}\")\n",
    "\n",
    "print(f\"\\n✅ Data preparation completed successfully!\")\n",
    "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "print(f\"Files created:\")\n",
    "print(f\"  - train.jsonl ({len(train_samples)} samples)\")\n",
    "print(f\"  - val.jsonl ({len(val_samples)} samples)\")\n",
    "print(f\"  - metadata.json\")\n",
    "print(f\"  - embeddings/query_embeddings.npy\")\n",
    "print(f\"  - embeddings/document_embeddings.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

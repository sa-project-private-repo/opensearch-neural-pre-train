{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenSearch Neural Sparse Model - 학습 데이터 준비\n",
    "\n",
    "## 목표\n",
    "한국어 공개 데이터셋(Wikipedia, Namuwiki)을 활용하여 OpenSearch neural sparse 모델 학습에 사용할 수 있는 JSONL 형식의 데이터셋을 생성합니다.\n",
    "\n",
    "## 데이터 포맷\n",
    "```json\n",
    "{\n",
    "    \"query\": \"질문 텍스트\",\n",
    "    \"docs\": [\"문서1\", \"문서2\", \"문서3\", ...],\n",
    "    \"scores\": [9.5, 7.2, 5.1, ...]\n",
    "}\n",
    "```\n",
    "\n",
    "## 주요 단계\n",
    "1. 데이터 로딩 및 전처리\n",
    "2. Query 생성 (문서 제목 활용)\n",
    "3. Embedding 생성 (intfloat/multilingual-e5-large)\n",
    "4. Hard Negatives Mining (FAISS 유사도 검색)\n",
    "5. K-means 클러스터링으로 관련 문서 그룹화\n",
    "6. 최종 JSONL 데이터셋 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# 프로젝트 루트 경로\n",
    "PROJECT_ROOT = Path(\"/home/west/Documents/cursor-workspace/opensearch-neural-pre-train\")\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# 시드 설정\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# 출력 디렉토리 생성\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"dataset\" / \"neural_sparse_training\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EMBEDDINGS_DIR = OUTPUT_DIR / \"embeddings\"\n",
    "EMBEDDINGS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로딩 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"문서 데이터 클래스\"\"\"\n",
    "    id: str\n",
    "    title: str\n",
    "    text: str\n",
    "    url: Optional[str] = None\n",
    "    source: Optional[str] = None\n",
    "    \n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"데이터 검증 및 정제\"\"\"\n",
    "        self.title = self.title.strip()\n",
    "        self.text = self.text.strip()\n",
    "\n",
    "\n",
    "def load_jsonl_file(file_path: Path) -> List[Dict]:\n",
    "    \"\"\"JSONL 파일을 로드합니다.\n",
    "    \n",
    "    Args:\n",
    "        file_path: JSONL 파일 경로\n",
    "        \n",
    "    Returns:\n",
    "        딕셔너리 리스트\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                documents.append(json.loads(line))\n",
    "    return documents\n",
    "\n",
    "\n",
    "def load_korean_datasets(\n",
    "    wiki_limit: Optional[int] = None,\n",
    "    namuwiki_limit: Optional[int] = None,\n",
    "    min_text_length: int = 100,\n",
    "    max_text_length: int = 2000,\n",
    ") -> List[Document]:\n",
    "    \"\"\"한국어 Wikipedia와 Namuwiki 데이터를 로드합니다.\n",
    "    \n",
    "    Args:\n",
    "        wiki_limit: Wikipedia 문서 개수 제한 (None이면 전체)\n",
    "        namuwiki_limit: Namuwiki 문서 개수 제한 (None이면 전체)\n",
    "        min_text_length: 최소 텍스트 길이\n",
    "        max_text_length: 최대 텍스트 길이\n",
    "        \n",
    "    Returns:\n",
    "        Document 객체 리스트\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Wikipedia 데이터 로드\n",
    "    wiki_dir = PROJECT_ROOT / \"dataset\" / \"wikipedia\"\n",
    "    wiki_files = sorted(wiki_dir.glob(\"ko_articles_chunk_*.jsonl\"))\n",
    "    \n",
    "    print(f\"Loading Wikipedia data from {len(wiki_files)} files...\")\n",
    "    wiki_count = 0\n",
    "    for file_path in tqdm(wiki_files, desc=\"Wikipedia\"):\n",
    "        for item in load_jsonl_file(file_path):\n",
    "            if wiki_limit and wiki_count >= wiki_limit:\n",
    "                break\n",
    "                \n",
    "            text = item.get(\"text\", \"\")\n",
    "            if min_text_length <= len(text) <= max_text_length:\n",
    "                documents.append(Document(\n",
    "                    id=f\"wiki_{item['id']}\",\n",
    "                    title=item.get(\"title\", \"\"),\n",
    "                    text=text,\n",
    "                    url=item.get(\"url\"),\n",
    "                    source=\"wikipedia\"\n",
    "                ))\n",
    "                wiki_count += 1\n",
    "        \n",
    "        if wiki_limit and wiki_count >= wiki_limit:\n",
    "            break\n",
    "    \n",
    "    print(f\"Loaded {wiki_count} Wikipedia documents\")\n",
    "    \n",
    "    # Namuwiki 데이터 로드\n",
    "    namuwiki_dir = PROJECT_ROOT / \"dataset\" / \"namuwiki\"\n",
    "    if namuwiki_dir.exists():\n",
    "        namuwiki_files = sorted(namuwiki_dir.glob(\"namuwiki_chunk_*.jsonl\"))\n",
    "        \n",
    "        print(f\"Loading Namuwiki data from {len(namuwiki_files)} files...\")\n",
    "        namuwiki_count = 0\n",
    "        for file_path in tqdm(namuwiki_files, desc=\"Namuwiki\"):\n",
    "            for item in load_jsonl_file(file_path):\n",
    "                if namuwiki_limit and namuwiki_count >= namuwiki_limit:\n",
    "                    break\n",
    "                    \n",
    "                text = item.get(\"text\", \"\")\n",
    "                if min_text_length <= len(text) <= max_text_length:\n",
    "                    documents.append(Document(\n",
    "                        id=f\"namu_{item.get('id', namuwiki_count)}\",\n",
    "                        title=item.get(\"title\", \"\"),\n",
    "                        text=text,\n",
    "                        url=item.get(\"url\"),\n",
    "                        source=\"namuwiki\"\n",
    "                    ))\n",
    "                    namuwiki_count += 1\n",
    "            \n",
    "            if namuwiki_limit and namuwiki_count >= namuwiki_limit:\n",
    "                break\n",
    "        \n",
    "        print(f\"Loaded {namuwiki_count} Namuwiki documents\")\n",
    "    \n",
    "    print(f\"Total documents loaded: {len(documents)}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 (처음에는 샘플로 시작)\n",
    "# 전체 데이터: wiki_limit=None, namuwiki_limit=None\n",
    "documents = load_korean_datasets(\n",
    "    wiki_limit=10000,  # 테스트용으로 제한\n",
    "    namuwiki_limit=5000,  # 테스트용으로 제한\n",
    "    min_text_length=100,\n",
    "    max_text_length=2000,\n",
    ")\n",
    "\n",
    "print(f\"\\nSample document:\")\n",
    "print(f\"Title: {documents[0].title}\")\n",
    "print(f\"Text length: {len(documents[0].text)}\")\n",
    "print(f\"Text preview: {documents[0].text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query 생성\n",
    "\n",
    "문서 제목을 query로 사용하고, 본문을 positive document로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QueryDocPair:\n",
    "    \"\"\"Query-Document 쌍\"\"\"\n",
    "    query_id: str\n",
    "    query_text: str\n",
    "    positive_doc_id: str\n",
    "    positive_doc_text: str\n",
    "    source: str\n",
    "\n",
    "\n",
    "def create_query_doc_pairs(documents: List[Document]) -> List[QueryDocPair]:\n",
    "    \"\"\"문서로부터 Query-Document 쌍을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        documents: Document 객체 리스트\n",
    "        \n",
    "    Returns:\n",
    "        QueryDocPair 객체 리스트\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    for doc in tqdm(documents, desc=\"Creating query-doc pairs\"):\n",
    "        if not doc.title or not doc.text:\n",
    "            continue\n",
    "            \n",
    "        pairs.append(QueryDocPair(\n",
    "            query_id=f\"q_{doc.id}\",\n",
    "            query_text=doc.title,\n",
    "            positive_doc_id=doc.id,\n",
    "            positive_doc_text=doc.text,\n",
    "            source=doc.source,\n",
    "        ))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "qd_pairs = create_query_doc_pairs(documents)\n",
    "print(f\"Created {len(qd_pairs)} query-document pairs\")\n",
    "\n",
    "print(f\"\\nSample pair:\")\n",
    "print(f\"Query: {qd_pairs[0].query_text}\")\n",
    "print(f\"Document: {qd_pairs[0].positive_doc_text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Query Augmentation using Ollama (LLM)\n",
    "\n",
    "원본 query(문서 제목)를 Ollama의 LLM 모델로 증강하여 다양한 검색 쿼리를 생성합니다.\n",
    "이를 통해 학습 데이터의 다양성을 높이고 모델의 robustness를 향상시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "# Ollama 설정\n",
    "OLLAMA_MODEL = \"qwen3:30b-a3b-instruct-2507-q8_0\"\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "def generate_with_ollama(\n",
    "    prompt: str,\n",
    "    model: str = OLLAMA_MODEL,\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: int = 200,\n",
    ") -> str:\n",
    "    \"\"\"Ollama API를 사용하여 텍스트를 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        prompt: 입력 프롬프트\n",
    "        model: Ollama 모델 이름\n",
    "        temperature: 생성 temperature\n",
    "        max_tokens: 최대 토큰 수\n",
    "        \n",
    "    Returns:\n",
    "        생성된 텍스트\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"num_predict\": max_tokens,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(OLLAMA_API_URL, json=payload, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return result.get(\"response\", \"\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Ollama API: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def augment_query(\n",
    "    original_query: str,\n",
    "    num_variations: int = 2,\n",
    ") -> List[str]:\n",
    "    \"\"\"LLM을 사용하여 query를 증강합니다.\n",
    "    \n",
    "    Args:\n",
    "        original_query: 원본 query\n",
    "        num_variations: 생성할 변형 query 개수\n",
    "        \n",
    "    Returns:\n",
    "        증강된 query 리스트 (원본 포함)\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"주어진 검색 쿼리에 대해 {num_variations}개의 다양한 변형 쿼리를 생성하세요.\n",
    "변형 쿼리는 원본 쿼리와 같은 의미를 가지지만, 다른 표현 방식을 사용해야 합니다.\n",
    "\n",
    "원본 쿼리: {original_query}\n",
    "\n",
    "다음 형식으로 출력하세요 (번호와 쿼리만, 추가 설명 없이):\n",
    "1. [변형 쿼리 1]\n",
    "2. [변형 쿼리 2]\n",
    "\"\"\"\n",
    "    \n",
    "    response = generate_with_ollama(prompt, temperature=0.8)\n",
    "    \n",
    "    # 응답에서 query 추출\n",
    "    augmented_queries = [original_query]  # 원본 query 포함\n",
    "    \n",
    "    if response:\n",
    "        lines = response.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            # \"1. \", \"2. \" 등의 형식에서 query 추출\n",
    "            if line and (line[0].isdigit() or line.startswith('-') or line.startswith('*')):\n",
    "                # 번호나 bullet 제거\n",
    "                query = line.split('.', 1)[-1].strip() if '.' in line else line.lstrip('-*').strip()\n",
    "                # 대괄호 제거\n",
    "                query = query.strip('[]')\n",
    "                if query and query != original_query:\n",
    "                    augmented_queries.append(query)\n",
    "    \n",
    "    # 정확히 num_variations + 1개 반환 (원본 + 변형들)\n",
    "    return augmented_queries[:num_variations + 1]\n",
    "\n",
    "\n",
    "# Ollama 연결 테스트\n",
    "print(\"Testing Ollama connection...\")\n",
    "test_response = generate_with_ollama(\"안녕하세요\", max_tokens=50)\n",
    "if test_response:\n",
    "    print(f\"✅ Ollama connected successfully!\")\n",
    "    print(f\"Test response: {test_response[:100]}...\")\n",
    "else:\n",
    "    print(\"⚠️ Ollama connection failed. Please check if Ollama is running.\")\n",
    "    print(f\"Model: {OLLAMA_MODEL}\")\n",
    "    print(f\"API URL: {OLLAMA_API_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Augmentation 수행\n",
    "# 전체 query에 대해 증강을 수행하면 시간이 오래 걸리므로 샘플링 옵션 제공\n",
    "ENABLE_QUERY_AUGMENTATION = True  # False로 설정하면 증강 건너뛰기\n",
    "AUGMENTATION_SAMPLE_RATE = 0.3  # 전체 query의 30%만 증강 (1.0이면 전체)\n",
    "NUM_QUERY_VARIATIONS = 2  # 각 query당 생성할 변형 개수\n",
    "\n",
    "if ENABLE_QUERY_AUGMENTATION:\n",
    "    print(f\"Query Augmentation Settings:\")\n",
    "    print(f\"  Sample rate: {AUGMENTATION_SAMPLE_RATE * 100:.0f}%\")\n",
    "    print(f\"  Variations per query: {NUM_QUERY_VARIATIONS}\")\n",
    "    print(f\"  Total queries to augment: {int(len(qd_pairs) * AUGMENTATION_SAMPLE_RATE)}\")\n",
    "    \n",
    "    # 증강할 query 인덱스 선택\n",
    "    num_to_augment = int(len(qd_pairs) * AUGMENTATION_SAMPLE_RATE)\n",
    "    augment_indices = random.sample(range(len(qd_pairs)), num_to_augment)\n",
    "    \n",
    "    # Query augmentation 실행\n",
    "    augmented_qd_pairs = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    for i, pair in enumerate(tqdm(qd_pairs, desc=\"Augmenting queries\")):\n",
    "        # 원본 query-doc pair는 항상 포함\n",
    "        augmented_qd_pairs.append(pair)\n",
    "        \n",
    "        # 선택된 인덱스만 증강\n",
    "        if i in augment_indices:\n",
    "            augmented_queries = augment_query(\n",
    "                pair.query_text,\n",
    "                num_variations=NUM_QUERY_VARIATIONS,\n",
    "            )\n",
    "            \n",
    "            # 증강된 query들 추가 (원본 제외)\n",
    "            for j, aug_query in enumerate(augmented_queries[1:], 1):\n",
    "                if aug_query and aug_query != pair.query_text:\n",
    "                    augmented_qd_pairs.append(QueryDocPair(\n",
    "                        query_id=f\"{pair.query_id}_aug{j}\",\n",
    "                        query_text=aug_query,\n",
    "                        positive_doc_id=pair.positive_doc_id,\n",
    "                        positive_doc_text=pair.positive_doc_text,\n",
    "                        source=pair.source,\n",
    "                    ))\n",
    "                else:\n",
    "                    failed_count += 1\n",
    "            \n",
    "            # API rate limiting 방지 (너무 빠르게 호출하지 않도록)\n",
    "            time.sleep(0.1)\n",
    "    \n",
    "    print(f\"\\n✅ Query augmentation completed!\")\n",
    "    print(f\"  Original queries: {len(qd_pairs)}\")\n",
    "    print(f\"  Augmented queries: {len(augmented_qd_pairs) - len(qd_pairs)}\")\n",
    "    print(f\"  Total queries: {len(augmented_qd_pairs)}\")\n",
    "    print(f\"  Failed augmentations: {failed_count}\")\n",
    "    \n",
    "    # 증강된 쌍으로 교체\n",
    "    qd_pairs = augmented_qd_pairs\n",
    "    \n",
    "    # 샘플 출력\n",
    "    print(f\"\\nSample augmented queries:\")\n",
    "    for i in range(min(3, len(augment_indices))):\n",
    "        idx = augment_indices[i]\n",
    "        original_query = qd_pairs[idx].query_text\n",
    "        aug_queries = [p.query_text for p in qd_pairs if p.positive_doc_id == qd_pairs[idx].positive_doc_id]\n",
    "        print(f\"\\n  Original: {original_query}\")\n",
    "        for j, aug_q in enumerate(aug_queries[1:], 1):\n",
    "            print(f\"  Variation {j}: {aug_q}\")\n",
    "else:\n",
    "    print(\"⚠️ Query augmentation is disabled. Using original queries only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding 생성 (intfloat/multilingual-e5-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "MODEL_NAME = \"intfloat/multilingual-e5-large\"\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
    "print(f\"Model loaded successfully. Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(\n",
    "    texts: List[str],\n",
    "    model: SentenceTransformer,\n",
    "    batch_size: int = 32,\n",
    "    prefix: str = \"\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"텍스트 리스트에 대한 임베딩을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        texts: 텍스트 리스트\n",
    "        model: SentenceTransformer 모델\n",
    "        batch_size: 배치 크기\n",
    "        prefix: E5 모델용 prefix (\"query: \" 또는 \"passage: \")\n",
    "        \n",
    "    Returns:\n",
    "        임베딩 배열 (N, D)\n",
    "    \"\"\"\n",
    "    if prefix:\n",
    "        texts = [f\"{prefix}{text}\" for text in texts]\n",
    "    \n",
    "    embeddings = model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Query 임베딩 생성\n",
    "print(\"Generating query embeddings...\")\n",
    "query_texts = [pair.query_text for pair in qd_pairs]\n",
    "query_embeddings = generate_embeddings(\n",
    "    query_texts,\n",
    "    model,\n",
    "    batch_size=32,\n",
    "    prefix=\"query: \",  # E5 모델은 query/passage prefix 사용\n",
    ")\n",
    "\n",
    "print(f\"Query embeddings shape: {query_embeddings.shape}\")\n",
    "\n",
    "# Document 임베딩 생성\n",
    "print(\"\\nGenerating document embeddings...\")\n",
    "doc_texts = [pair.positive_doc_text for pair in qd_pairs]\n",
    "doc_embeddings = generate_embeddings(\n",
    "    doc_texts,\n",
    "    model,\n",
    "    batch_size=32,\n",
    "    prefix=\"passage: \",\n",
    ")\n",
    "\n",
    "print(f\"Document embeddings shape: {doc_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 저장\n",
    "np.save(EMBEDDINGS_DIR / \"query_embeddings.npy\", query_embeddings)\n",
    "np.save(EMBEDDINGS_DIR / \"document_embeddings.npy\", doc_embeddings)\n",
    "print(f\"Embeddings saved to {EMBEDDINGS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hard Negatives Mining (FAISS)\n",
    "\n",
    "각 query에 대해 유사하지만 관련 없는 문서를 hard negatives로 선정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faiss_index(embeddings: np.ndarray) -> faiss.Index:\n",
    "    \"\"\"FAISS 인덱스를 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: 임베딩 배열 (N, D)\n",
    "        \n",
    "    Returns:\n",
    "        FAISS 인덱스\n",
    "    \"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    # L2 정규화된 벡터이므로 Inner Product = Cosine Similarity\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    index.add(embeddings.astype(np.float32))\n",
    "    \n",
    "    return index\n",
    "\n",
    "\n",
    "# FAISS 인덱스 생성\n",
    "print(\"Building FAISS index...\")\n",
    "doc_index = build_faiss_index(doc_embeddings)\n",
    "print(f\"FAISS index built. Total documents: {doc_index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_hard_negatives(\n",
    "    query_embeddings: np.ndarray,\n",
    "    doc_index: faiss.Index,\n",
    "    k: int = 10,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"각 query에 대해 유사한 문서를 검색합니다.\n",
    "    \n",
    "    Args:\n",
    "        query_embeddings: Query 임베딩 (N, D)\n",
    "        doc_index: FAISS 인덱스\n",
    "        k: 검색할 문서 개수\n",
    "        \n",
    "    Returns:\n",
    "        (distances, indices) 튜플\n",
    "    \"\"\"\n",
    "    distances, indices = doc_index.search(\n",
    "        query_embeddings.astype(np.float32),\n",
    "        k,\n",
    "    )\n",
    "    \n",
    "    return distances, indices\n",
    "\n",
    "\n",
    "# Hard negatives 검색 (top-10)\n",
    "print(\"Searching for hard negatives...\")\n",
    "NUM_CANDIDATES = 10  # positive 1개 + hard negatives 9개 검색\n",
    "\n",
    "distances, indices = search_hard_negatives(\n",
    "    query_embeddings,\n",
    "    doc_index,\n",
    "    k=NUM_CANDIDATES,\n",
    ")\n",
    "\n",
    "print(f\"Search completed. Shape: {indices.shape}\")\n",
    "print(f\"Sample distances: {distances[0]}\")\n",
    "print(f\"Sample indices: {indices[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. K-means 클러스터링\n",
    "\n",
    "문서를 클러스터링하여 관련 문서를 그룹화하고, 클러스터 내 문서들에 대한 관련성 점수를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_kmeans_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    n_clusters: int = 100,\n",
    "    batch_size: int = 1000,\n",
    ") -> Tuple[MiniBatchKMeans, np.ndarray]:\n",
    "    \"\"\"K-means 클러스터링을 수행합니다.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: 임베딩 배열 (N, D)\n",
    "        n_clusters: 클러스터 개수\n",
    "        batch_size: MiniBatchKMeans 배치 크기\n",
    "        \n",
    "    Returns:\n",
    "        (kmeans 모델, cluster labels)\n",
    "    \"\"\"\n",
    "    print(f\"Performing K-means clustering with {n_clusters} clusters...\")\n",
    "    \n",
    "    kmeans = MiniBatchKMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        batch_size=batch_size,\n",
    "        random_state=SEED,\n",
    "        verbose=1,\n",
    "    )\n",
    "    \n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    return kmeans, cluster_labels\n",
    "\n",
    "\n",
    "# 클러스터 개수는 데이터 크기에 따라 조정\n",
    "n_clusters = min(100, len(documents) // 50)\n",
    "print(f\"Using {n_clusters} clusters for {len(documents)} documents\")\n",
    "\n",
    "kmeans, cluster_labels = perform_kmeans_clustering(\n",
    "    doc_embeddings,\n",
    "    n_clusters=n_clusters,\n",
    ")\n",
    "\n",
    "print(f\"Clustering completed.\")\n",
    "print(f\"Cluster distribution: {np.bincount(cluster_labels)[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 최종 JSONL 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingSample:\n",
    "    \"\"\"학습 샘플 데이터 클래스\"\"\"\n",
    "    query: str\n",
    "    docs: List[str]\n",
    "    scores: List[float]\n",
    "\n",
    "\n",
    "def create_training_samples(\n",
    "    qd_pairs: List[QueryDocPair],\n",
    "    search_indices: np.ndarray,\n",
    "    search_scores: np.ndarray,\n",
    "    num_docs_per_query: int = 8,\n",
    "    positive_score: float = 10.0,\n",
    ") -> List[TrainingSample]:\n",
    "    \"\"\"최종 학습 샘플을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        qd_pairs: Query-Document 쌍 리스트\n",
    "        search_indices: FAISS 검색 결과 인덱스\n",
    "        search_scores: FAISS 검색 결과 점수\n",
    "        num_docs_per_query: 각 query당 문서 개수\n",
    "        positive_score: Positive document 점수\n",
    "        \n",
    "    Returns:\n",
    "        TrainingSample 리스트\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    for i, pair in enumerate(tqdm(qd_pairs, desc=\"Creating training samples\")):\n",
    "        docs = []\n",
    "        scores = []\n",
    "        \n",
    "        # 1. Positive document (항상 첫 번째)\n",
    "        docs.append(pair.positive_doc_text)\n",
    "        scores.append(positive_score)\n",
    "        \n",
    "        # 2. Hard negatives (FAISS 검색 결과에서 선택)\n",
    "        # 첫 번째는 자기 자신이므로 제외\n",
    "        for j in range(1, min(num_docs_per_query, len(search_indices[i]))):\n",
    "            neg_idx = search_indices[i][j]\n",
    "            neg_score = search_scores[i][j]\n",
    "            \n",
    "            # 자기 자신이 아닌 경우만 추가\n",
    "            if neg_idx != i:\n",
    "                docs.append(qd_pairs[neg_idx].positive_doc_text)\n",
    "                # Cosine similarity를 0-10 스케일로 변환\n",
    "                # neg_score는 0~1 범위이므로 positive_score보다 낮게 설정\n",
    "                scaled_score = float(neg_score * (positive_score - 1.0))\n",
    "                scores.append(scaled_score)\n",
    "        \n",
    "        # 문서가 충분하지 않으면 랜덤 샘플링\n",
    "        while len(docs) < num_docs_per_query:\n",
    "            random_idx = random.randint(0, len(qd_pairs) - 1)\n",
    "            if random_idx != i:\n",
    "                docs.append(qd_pairs[random_idx].positive_doc_text)\n",
    "                scores.append(0.5)  # Low score for random negatives\n",
    "        \n",
    "        samples.append(TrainingSample(\n",
    "            query=pair.query_text,\n",
    "            docs=docs[:num_docs_per_query],\n",
    "            scores=scores[:num_docs_per_query],\n",
    "        ))\n",
    "    \n",
    "    return samples\n",
    "\n",
    "\n",
    "# 학습 샘플 생성\n",
    "NUM_DOCS_PER_QUERY = 8  # positive 1개 + negatives 7개\n",
    "\n",
    "training_samples = create_training_samples(\n",
    "    qd_pairs,\n",
    "    indices,\n",
    "    distances,\n",
    "    num_docs_per_query=NUM_DOCS_PER_QUERY,\n",
    ")\n",
    "\n",
    "print(f\"\\nCreated {len(training_samples)} training samples\")\n",
    "print(f\"\\nSample:\")\n",
    "sample = training_samples[0]\n",
    "print(f\"Query: {sample.query}\")\n",
    "print(f\"Num docs: {len(sample.docs)}\")\n",
    "print(f\"Scores: {sample.scores}\")\n",
    "print(f\"First doc preview: {sample.docs[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Val split\n",
    "random.shuffle(training_samples)\n",
    "split_idx = int(len(training_samples) * 0.9)\n",
    "\n",
    "train_samples = training_samples[:split_idx]\n",
    "val_samples = training_samples[split_idx:]\n",
    "\n",
    "print(f\"Train samples: {len(train_samples)}\")\n",
    "print(f\"Val samples: {len(val_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(\n",
    "    samples: List[TrainingSample],\n",
    "    file_path: Path,\n",
    ") -> None:\n",
    "    \"\"\"학습 샘플을 JSONL 파일로 저장합니다.\n",
    "    \n",
    "    Args:\n",
    "        samples: TrainingSample 리스트\n",
    "        file_path: 출력 파일 경로\n",
    "    \"\"\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sample in tqdm(samples, desc=f\"Saving to {file_path.name}\"):\n",
    "            json_obj = {\n",
    "                \"query\": sample.query,\n",
    "                \"docs\": sample.docs,\n",
    "                \"scores\": sample.scores,\n",
    "            }\n",
    "            f.write(json.dumps(json_obj, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    print(f\"Saved {len(samples)} samples to {file_path}\")\n",
    "\n",
    "\n",
    "# JSONL 파일로 저장\n",
    "save_jsonl(train_samples, OUTPUT_DIR / \"train.jsonl\")\n",
    "save_jsonl(val_samples, OUTPUT_DIR / \"val.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 메타데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메타데이터 생성\n",
    "metadata = {\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"total_documents\": len(documents),\n",
    "    \"total_queries\": len(qd_pairs),\n",
    "    \"train_samples\": len(train_samples),\n",
    "    \"val_samples\": len(val_samples),\n",
    "    \"docs_per_query\": NUM_DOCS_PER_QUERY,\n",
    "    \"embedding_model\": MODEL_NAME,\n",
    "    \"embedding_dimension\": query_embeddings.shape[1],\n",
    "    \"source_datasets\": [\"wikipedia_ko\", \"namuwiki\"],\n",
    "    \"num_clusters\": n_clusters,\n",
    "    \"min_text_length\": 100,\n",
    "    \"max_text_length\": 2000,\n",
    "    \"data_format\": \"pre-computed knowledge distillation\",\n",
    "    \"compatible_with\": \"opensearch-sparse-model-tuning-sample\",\n",
    "}\n",
    "\n",
    "# 메타데이터 저장\n",
    "with open(OUTPUT_DIR / \"metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\nMetadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 데이터 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 파일 검증\n",
    "print(\"Validating saved files...\\n\")\n",
    "\n",
    "# Train 파일 검증\n",
    "with open(OUTPUT_DIR / \"train.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    first_line = json.loads(f.readline())\n",
    "    print(\"Train sample:\")\n",
    "    print(f\"  Query: {first_line['query']}\")\n",
    "    print(f\"  Num docs: {len(first_line['docs'])}\")\n",
    "    print(f\"  Num scores: {len(first_line['scores'])}\")\n",
    "    print(f\"  Scores: {first_line['scores']}\")\n",
    "    print(f\"  Max score: {max(first_line['scores'])}\")\n",
    "    print(f\"  Min score: {min(first_line['scores'])}\")\n",
    "\n",
    "# 점수 분포 확인\n",
    "all_scores = []\n",
    "with open(OUTPUT_DIR / \"train.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        sample = json.loads(line)\n",
    "        all_scores.extend(sample['scores'])\n",
    "\n",
    "all_scores = np.array(all_scores)\n",
    "print(f\"\\nScore distribution:\")\n",
    "print(f\"  Mean: {all_scores.mean():.2f}\")\n",
    "print(f\"  Std: {all_scores.std():.2f}\")\n",
    "print(f\"  Min: {all_scores.min():.2f}\")\n",
    "print(f\"  Max: {all_scores.max():.2f}\")\n",
    "\n",
    "print(f\"\\n✅ Data preparation completed successfully!\")\n",
    "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "print(f\"Files created:\")\n",
    "print(f\"  - train.jsonl ({len(train_samples)} samples)\")\n",
    "print(f\"  - val.jsonl ({len(val_samples)} samples)\")\n",
    "print(f\"  - metadata.json\")\n",
    "print(f\"  - embeddings/query_embeddings.npy\")\n",
    "print(f\"  - embeddings/document_embeddings.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

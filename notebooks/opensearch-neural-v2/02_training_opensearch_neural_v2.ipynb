{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenSearch Neural Sparse Model v2 Training\n",
    "\n",
    "이 노트북은 최신 연구 논문 \"Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers\" (arXiv:2411.04403v2)를 기반으로 OpenSearch Neural Sparse Model v2를 학습합니다.\n",
    "\n",
    "## 주요 개선사항\n",
    "\n",
    "### 1. IDF-Aware Penalty\n",
    "- 기존 FLOPS regularization의 문제점 해결\n",
    "- 중요한 저빈도 토큰 보존\n",
    "- IDF 가중치를 활용한 차별화된 패널티 적용\n",
    "\n",
    "### 2. Heterogeneous Ensemble Knowledge Distillation\n",
    "- Dense와 Sparse teacher 모델의 앙상블\n",
    "- 대규모 pre-training 데이터에 적용 가능\n",
    "- Cross-encoder보다 효율적인 teacher 모델\n",
    "\n",
    "### 3. Hardware Optimization\n",
    "- Nvidia DGX Spark GPU 최적화\n",
    "- Mixed Precision Training (FP16/BF16)\n",
    "- Gradient Accumulation\n",
    "- Distributed Training Support\n",
    "\n",
    "## 환경 정보\n",
    "- GPU: Nvidia GB10 (Compute Capability 12.1)\n",
    "- CUDA: 13.0\n",
    "- Python: 3.12.3\n",
    "- PyTorch: 2.5.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "# Import custom modules\n",
    "from src.models.neural_sparse_encoder import NeuralSparseEncoder\n",
    "from src.training.trainer import NeuralSparseTrainer\n",
    "from src.training.losses import CombinedLoss\n",
    "from src.training.data_collator import NeuralSparseDataCollator\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "### Training Configuration Based on Paper Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base configuration\n",
    "CONFIG = {\n",
    "    # Model configuration\n",
    "    \"model\": {\n",
    "        \"base_model\": \"opensearch-project/opensearch-neural-sparse-encoding-multilingual-v1\",\n",
    "        \"max_query_length\": 64,\n",
    "        \"max_doc_length\": 256,\n",
    "        \"use_relu\": True,\n",
    "    },\n",
    "    \n",
    "    # Pre-training configuration (based on paper Section 5.1.2)\n",
    "    \"pretraining\": {\n",
    "        \"enabled\": True,\n",
    "        \"num_steps\": 150000,\n",
    "        \"batch_size\": 48,  # Per device\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"num_hard_negatives\": 7,  # Paper: 7 hard negatives per query\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"lambda_flops\": 1e-7,  # Small coefficient for pre-training\n",
    "        \"warmup_steps\": 5000,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"scale_constant_S\": 10,  # For scaling ensemble scores\n",
    "    },\n",
    "    \n",
    "    # Fine-tuning configuration (based on paper Section 5.1.2)\n",
    "    \"finetuning\": {\n",
    "        \"enabled\": True,\n",
    "        \"num_steps\": 50000,\n",
    "        \"batch_size\": 40,  # Per device\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"num_hard_negatives\": 10,  # Paper: 10 hard negatives per query\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"lambda_flops\": 0.02,  # Balance relevance and efficiency\n",
    "        \"warmup_steps\": 2000,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"scale_constant_S\": 30,  # For fine-tuning\n",
    "    },\n",
    "    \n",
    "    # IDF-aware penalty configuration (Section 4.1)\n",
    "    \"idf_penalty\": {\n",
    "        \"enabled\": True,\n",
    "        \"idf_source\": \"msmarco\",  # Dataset to compute IDF from\n",
    "        \"default_idf\": 1.0,  # For unseen tokens\n",
    "    },\n",
    "    \n",
    "    # Knowledge distillation configuration (Section 4.2)\n",
    "    # IMPORTANT: OpenSearch sparse models on HuggingFace Hub may not have\n",
    "    # pretrained sparse projection head weights. Recommended approaches:\n",
    "    # Option 1 (Recommended): Use only dense teacher initially\n",
    "    #   - Set teacher_weights.sparse = 0\n",
    "    # Option 2: Train sparse teacher first, then use for distillation\n",
    "    #   - Train a model, save it, then set sparse_teacher to saved path\n",
    "    \"knowledge_distillation\": {\n",
    "        \"enabled\": True,\n",
    "        \"dense_teacher\": \"Alibaba-NLP/gte-large-en-v1.5\",\n",
    "        # Use None or set sparse weight to 0 to skip sparse teacher\n",
    "        \"sparse_teacher\": None,  # Changed from opensearch model ID\n",
    "        \"cross_encoder\": \"cross-encoder/ms-marco-MiniLM-L-12-v2\",  # For fine-tuning\n",
    "        \"teacher_weights\": {\n",
    "            \"dense\": 1.0,  # Use only dense teacher\n",
    "            \"sparse\": 0.0,  # Skip sparse teacher\n",
    "        },\n",
    "        \"temperature\": 1.0,\n",
    "        \"use_normalization\": True,  # Min-max normalization before ensemble\n",
    "    },\n",
    "    \n",
    "    # Hardware optimization for Nvidia DGX Spark\n",
    "    \"hardware\": {\n",
    "        \"mixed_precision\": True,\n",
    "        \"precision\": \"bf16\",  # BF16 for better stability\n",
    "        \"compile_model\": True,  # PyTorch 2.0 compile\n",
    "        \"num_workers\": 8,\n",
    "        \"pin_memory\": True,\n",
    "        \"persistent_workers\": True,\n",
    "    },\n",
    "    \n",
    "    # Logging and checkpointing\n",
    "    \"logging\": {\n",
    "        \"output_dir\": \"outputs/opensearch-neural-v2\",\n",
    "        \"logging_steps\": 100,\n",
    "        \"eval_steps\": 500,\n",
    "        \"save_steps\": 1000,\n",
    "        \"save_total_limit\": 5,\n",
    "    },\n",
    "    \n",
    "    # Data filtering (Section 4.2)\n",
    "    \"data_filtering\": {\n",
    "        \"enabled\": True,\n",
    "        \"top_k_filter\": 10,  # Filter samples where positive not in top-10\n",
    "    },\n",
    "}\n",
    "\n",
    "# Print configuration\n",
    "print(\"Training Configuration:\")\n",
    "print(json.dumps(CONFIG, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFIGURATION NOTES\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. Sparse Teacher Configuration:\")\n",
    "print(\"   - Currently set to use ONLY dense teacher (sparse_weight=0)\")\n",
    "print(\"   - This avoids issues with models lacking pretrained sparse heads\")\n",
    "print(\"   - For two-stage approach:\")\n",
    "print(\"     a) First train a sparse model (current config)\")\n",
    "print(\"     b) Then use trained model as sparse teacher for next iteration\")\n",
    "print(\"\\n2. Model Loading:\")\n",
    "print(\"   - Base model will be loaded from HuggingFace Hub\")\n",
    "print(\"   - Projection layer will be randomly initialized\")\n",
    "print(\"   - Training is required before using for retrieval\")\n",
    "print(\"\\n3. See docs/MODEL_LOADING_GUIDE.md for detailed information\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. IDF Computation\n",
    "\n",
    "### Compute IDF weights from corpus (Section 4.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf_weights(\n",
    "    documents: List[str],\n",
    "    tokenizer: AutoTokenizer,\n",
    "    save_path: Optional[str] = None,\n",
    ") -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Compute IDF weights for vocabulary.\n",
    "    \n",
    "    IDF(t) = log(N / df(t))\n",
    "    where N = total documents, df(t) = document frequency of term t\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document texts\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        save_path: Optional path to save IDF weights\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping token IDs to IDF weights\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    print(f\"Computing IDF weights from {len(documents)} documents...\")\n",
    "    \n",
    "    # Count document frequency for each token\n",
    "    df_counter = Counter()\n",
    "    \n",
    "    for i, doc in enumerate(tqdm(documents, desc=\"Processing documents\")):\n",
    "        # Tokenize\n",
    "        tokens = tokenizer.encode(doc, add_special_tokens=False)\n",
    "        # Count unique tokens in this document\n",
    "        unique_tokens = set(tokens)\n",
    "        df_counter.update(unique_tokens)\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Processed {i} documents, unique tokens: {len(df_counter)}\")\n",
    "    \n",
    "    # Compute IDF\n",
    "    N = len(documents)\n",
    "    idf_weights = {}\n",
    "    \n",
    "    for token_id, df in df_counter.items():\n",
    "        idf = np.log(N / df)\n",
    "        idf_weights[token_id] = float(idf)\n",
    "    \n",
    "    print(f\"Computed IDF for {len(idf_weights)} tokens\")\n",
    "    print(f\"IDF statistics:\")\n",
    "    idf_values = list(idf_weights.values())\n",
    "    print(f\"  Min: {min(idf_values):.4f}\")\n",
    "    print(f\"  Max: {max(idf_values):.4f}\")\n",
    "    print(f\"  Mean: {np.mean(idf_values):.4f}\")\n",
    "    print(f\"  Median: {np.median(idf_values):.4f}\")\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(idf_weights, f, indent=2)\n",
    "        print(f\"IDF weights saved to {save_path}\")\n",
    "    \n",
    "    return idf_weights\n",
    "\n",
    "\n",
    "def load_idf_weights(path: str) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Load pre-computed IDF weights.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to IDF weights JSON file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping token IDs to IDF weights\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        idf_weights = json.load(f)\n",
    "    \n",
    "    # Convert string keys to int\n",
    "    idf_weights = {int(k): float(v) for k, v in idf_weights.items()}\n",
    "    \n",
    "    print(f\"Loaded IDF weights for {len(idf_weights)} tokens from {path}\")\n",
    "    return idf_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. IDF-Aware Loss Functions\n",
    "\n",
    "### Implementation of IDF-aware penalty (Section 4.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDFAwareLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    IDF-aware ranking loss with FLOPS regularization.\n",
    "    \n",
    "    Based on paper Section 4.1:\n",
    "    - Ranking loss weighted by IDF values\n",
    "    - FLOPS regularization for sparsity\n",
    "    - Gradient composition encourages preserving high-IDF tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        idf_weights: Dict[int, float],\n",
    "        vocab_size: int,\n",
    "        lambda_flops: float = 0.02,\n",
    "        default_idf: float = 1.0,\n",
    "        device: Optional[torch.device] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize IDF-aware loss.\n",
    "        \n",
    "        Args:\n",
    "            idf_weights: Dictionary mapping token IDs to IDF values\n",
    "            vocab_size: Vocabulary size\n",
    "            lambda_flops: Weight for FLOPS regularization\n",
    "            default_idf: Default IDF for unseen tokens\n",
    "            device: Device to put IDF tensor on\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lambda_flops = lambda_flops\n",
    "        \n",
    "        # Create IDF tensor\n",
    "        idf_tensor = torch.ones(vocab_size) * default_idf\n",
    "        for token_id, idf_val in idf_weights.items():\n",
    "            if token_id < vocab_size:\n",
    "                idf_tensor[token_id] = idf_val\n",
    "        \n",
    "        # Register as buffer (not trainable parameter)\n",
    "        self.register_buffer('idf_weights', idf_tensor)\n",
    "        \n",
    "        if device is not None:\n",
    "            self.idf_weights = self.idf_weights.to(device)\n",
    "        \n",
    "        print(f\"Initialized IDF-aware loss with {len(idf_weights)} IDF weights\")\n",
    "    \n",
    "    def compute_idf_weighted_score(\n",
    "        self,\n",
    "        query_rep: torch.Tensor,\n",
    "        doc_rep: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute IDF-weighted similarity score (Equation 5 in paper).\n",
    "        \n",
    "        s(q, d) = Σ_t idf(t) * q_t * d_t\n",
    "        \n",
    "        Args:\n",
    "            query_rep: Query sparse representation [batch_size, vocab_size]\n",
    "            doc_rep: Document sparse representation [batch_size, vocab_size]\n",
    "            \n",
    "        Returns:\n",
    "            IDF-weighted scores [batch_size]\n",
    "        \"\"\"\n",
    "        # Element-wise multiplication with IDF weights\n",
    "        weighted_query = query_rep * self.idf_weights.unsqueeze(0)\n",
    "        \n",
    "        # Dot product\n",
    "        scores = torch.sum(weighted_query * doc_rep, dim=-1)\n",
    "        return scores\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        query_rep: torch.Tensor,\n",
    "        pos_doc_rep: torch.Tensor,\n",
    "        neg_doc_reps: torch.Tensor,\n",
    "        teacher_scores: Optional[torch.Tensor] = None,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute IDF-aware loss.\n",
    "        \n",
    "        Args:\n",
    "            query_rep: Query representations [batch_size, vocab_size]\n",
    "            pos_doc_rep: Positive doc representations [batch_size, vocab_size]\n",
    "            neg_doc_reps: Negative doc representations [batch_size, num_neg, vocab_size]\n",
    "            teacher_scores: Optional teacher scores [batch_size, 1+num_neg]\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with loss components\n",
    "        \"\"\"\n",
    "        batch_size = query_rep.shape[0]\n",
    "        num_neg = neg_doc_reps.shape[1]\n",
    "        \n",
    "        # Compute IDF-weighted scores\n",
    "        pos_scores = self.compute_idf_weighted_score(query_rep, pos_doc_rep)\n",
    "        \n",
    "        # Compute negative scores\n",
    "        neg_scores = torch.stack([\n",
    "            self.compute_idf_weighted_score(query_rep, neg_doc_reps[:, i, :])\n",
    "            for i in range(num_neg)\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Concatenate all scores\n",
    "        all_scores = torch.cat([pos_scores.unsqueeze(1), neg_scores], dim=1)\n",
    "        \n",
    "        # Ranking loss\n",
    "        if teacher_scores is not None:\n",
    "            # Knowledge distillation with KL divergence\n",
    "            student_log_probs = F.log_softmax(all_scores, dim=-1)\n",
    "            teacher_probs = F.softmax(teacher_scores, dim=-1)\n",
    "            ranking_loss = F.kl_div(\n",
    "                student_log_probs,\n",
    "                teacher_probs,\n",
    "                reduction='batchmean',\n",
    "            )\n",
    "        else:\n",
    "            # Standard cross-entropy (positive is first)\n",
    "            labels = torch.zeros(batch_size, dtype=torch.long, device=query_rep.device)\n",
    "            ranking_loss = F.cross_entropy(all_scores, labels)\n",
    "        \n",
    "        # FLOPS regularization (Equation 3 in paper)\n",
    "        # Average squared activation across batch\n",
    "        doc_flops = torch.sum(\n",
    "            (pos_doc_rep.mean(dim=0) ** 2)\n",
    "        )\n",
    "        \n",
    "        flops_loss = self.lambda_flops * doc_flops\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = ranking_loss + flops_loss\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss,\n",
    "            'ranking_loss': ranking_loss,\n",
    "            'flops_loss': flops_loss,\n",
    "        }\n",
    "\n",
    "print(\"IDF-aware loss functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Teacher Models for Knowledge Distillation\n",
    "\n",
    "### Heterogeneous ensemble of dense and sparse teachers (Section 4.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleTeacher:\n",
    "    \"\"\"\n",
    "    Ensemble of heterogeneous teacher models.\n",
    "    \n",
    "    Based on paper Section 4.2:\n",
    "    - Combines dense and sparse siamese retrievers\n",
    "    - Min-max normalization before ensemble\n",
    "    - Weighted sum of normalized scores\n",
    "    \n",
    "    Note: The sparse teacher may not have pretrained sparse head weights.\n",
    "    In that case, it will be initialized with random projection weights.\n",
    "    For best results, either:\n",
    "    1. Train a sparse teacher first, or\n",
    "    2. Use only the dense teacher (set sparse_weight=0)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dense_teacher_name: str,\n",
    "        sparse_teacher_name: Optional[str] = None,\n",
    "        dense_weight: float = 0.5,\n",
    "        sparse_weight: float = 0.5,\n",
    "        scale_constant: float = 10.0,\n",
    "        device: Optional[torch.device] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize ensemble teacher.\n",
    "        \n",
    "        Args:\n",
    "            dense_teacher_name: HuggingFace model name for dense teacher\n",
    "            sparse_teacher_name: HuggingFace model name for sparse teacher (optional)\n",
    "            dense_weight: Weight for dense teacher\n",
    "            sparse_weight: Weight for sparse teacher\n",
    "            scale_constant: Scaling constant S (paper Equation 9)\n",
    "            device: Device to load models on\n",
    "        \"\"\"\n",
    "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.dense_weight = dense_weight\n",
    "        self.sparse_weight = sparse_weight\n",
    "        self.scale_constant = scale_constant\n",
    "        self.use_sparse_teacher = sparse_teacher_name is not None and sparse_weight > 0\n",
    "        \n",
    "        # Load dense teacher\n",
    "        print(f\"Loading dense teacher: {dense_teacher_name}\")\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        self.dense_teacher = SentenceTransformer(\n",
    "            dense_teacher_name,\n",
    "            device=str(self.device),\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        self.dense_teacher.eval()\n",
    "        print(\"Dense teacher loaded successfully\")\n",
    "        \n",
    "        # Load sparse teacher if specified\n",
    "        if self.use_sparse_teacher:\n",
    "            print(f\"\\nLoading sparse teacher: {sparse_teacher_name}\")\n",
    "            try:\n",
    "                # Load sparse teacher (OpenSearch neural sparse model)\n",
    "                # Note: This will automatically handle models without pretrained head\n",
    "                self.sparse_teacher = NeuralSparseEncoder.from_pretrained(\n",
    "                    sparse_teacher_name\n",
    "                )\n",
    "                self.sparse_teacher.to(self.device)\n",
    "                self.sparse_teacher.eval()\n",
    "                print(\"Sparse teacher loaded successfully\")\n",
    "                print(\"WARNING: If the projection layer was randomly initialized,\")\n",
    "                print(\"         the sparse teacher may not provide meaningful scores.\")\n",
    "                print(\"         Consider training a sparse teacher first or using only\")\n",
    "                print(\"         the dense teacher (set sparse_weight=0 in CONFIG).\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR loading sparse teacher: {e}\")\n",
    "                print(\"Falling back to dense teacher only\")\n",
    "                self.use_sparse_teacher = False\n",
    "                self.dense_weight = 1.0\n",
    "                self.sparse_weight = 0.0\n",
    "        else:\n",
    "            print(\"\\nSparse teacher disabled (using dense teacher only)\")\n",
    "            self.sparse_teacher = None\n",
    "        \n",
    "        print(\"\\nEnsemble teacher initialized:\")\n",
    "        print(f\"  Dense weight: {self.dense_weight}\")\n",
    "        print(f\"  Sparse weight: {self.sparse_weight}\")\n",
    "        print(f\"  Scale constant: {scale_constant}\")\n",
    "    \n",
    "    def min_max_normalize(\n",
    "        self,\n",
    "        scores: torch.Tensor,\n",
    "        dim: int = -1,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Min-max normalization (paper Equation 8).\n",
    "        \n",
    "        ŝ_i = (s_i - min(s)) / (max(s) - min(s))\n",
    "        \n",
    "        Args:\n",
    "            scores: Scores tensor\n",
    "            dim: Dimension to normalize over\n",
    "            \n",
    "        Returns:\n",
    "            Normalized scores in [0, 1]\n",
    "        \"\"\"\n",
    "        min_scores = scores.min(dim=dim, keepdim=True)[0]\n",
    "        max_scores = scores.max(dim=dim, keepdim=True)[0]\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        range_scores = max_scores - min_scores\n",
    "        range_scores = torch.clamp(range_scores, min=1e-8)\n",
    "        \n",
    "        normalized = (scores - min_scores) / range_scores\n",
    "        return normalized\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_scores(\n",
    "        self,\n",
    "        queries: List[str],\n",
    "        documents: List[List[str]],  # [batch_size, num_docs]\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get ensemble teacher scores.\n",
    "        \n",
    "        Args:\n",
    "            queries: List of query strings\n",
    "            documents: List of document lists (pos + negs for each query)\n",
    "            \n",
    "        Returns:\n",
    "            Ensemble scores [batch_size, num_docs]\n",
    "        \"\"\"\n",
    "        batch_size = len(queries)\n",
    "        num_docs = len(documents[0])\n",
    "        \n",
    "        # Dense teacher scores\n",
    "        query_embeddings = self.dense_teacher.encode(\n",
    "            queries,\n",
    "            convert_to_tensor=True,\n",
    "            show_progress_bar=False,\n",
    "        )\n",
    "        \n",
    "        dense_scores = []\n",
    "        for i, docs in enumerate(documents):\n",
    "            doc_embeddings = self.dense_teacher.encode(\n",
    "                docs,\n",
    "                convert_to_tensor=True,\n",
    "                show_progress_bar=False,\n",
    "            )\n",
    "            # Cosine similarity\n",
    "            scores = torch.cosine_similarity(\n",
    "                query_embeddings[i].unsqueeze(0),\n",
    "                doc_embeddings,\n",
    "                dim=-1,\n",
    "            )\n",
    "            dense_scores.append(scores)\n",
    "        \n",
    "        dense_scores = torch.stack(dense_scores)  # [batch_size, num_docs]\n",
    "        \n",
    "        # Sparse teacher scores (if enabled)\n",
    "        if self.use_sparse_teacher:\n",
    "            query_sparse_reps = self.sparse_teacher.encode(queries, device=self.device)\n",
    "            \n",
    "            sparse_scores = []\n",
    "            for i, docs in enumerate(documents):\n",
    "                doc_sparse_reps = self.sparse_teacher.encode(docs, device=self.device)\n",
    "                # Dot product similarity\n",
    "                scores = torch.sum(\n",
    "                    query_sparse_reps[i].unsqueeze(0) * doc_sparse_reps,\n",
    "                    dim=-1,\n",
    "                )\n",
    "                sparse_scores.append(scores)\n",
    "            \n",
    "            sparse_scores = torch.stack(sparse_scores)  # [batch_size, num_docs]\n",
    "            \n",
    "            # Normalize scores (Equation 8)\n",
    "            dense_norm = self.min_max_normalize(dense_scores, dim=1)\n",
    "            sparse_norm = self.min_max_normalize(sparse_scores, dim=1)\n",
    "            \n",
    "            # Weighted ensemble (Equation 9)\n",
    "            ensemble_scores = (\n",
    "                self.dense_weight * dense_norm +\n",
    "                self.sparse_weight * sparse_norm\n",
    "            )\n",
    "        else:\n",
    "            # Use only dense teacher\n",
    "            ensemble_scores = self.min_max_normalize(dense_scores, dim=1)\n",
    "        \n",
    "        # Scale back with constant S\n",
    "        ensemble_scores = self.scale_constant * ensemble_scores\n",
    "        \n",
    "        return ensemble_scores\n",
    "\n",
    "print(\"Ensemble teacher class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Loading\n",
    "\n",
    "### Dataset with hard negative mining and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseRetrievalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for neural sparse retrieval training.\n",
    "    \n",
    "    Based on paper Section 4.2:\n",
    "    - Hard negative mining\n",
    "    - Consistency-based filtering (top-k)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        queries: List[str],\n",
    "        positive_docs: List[str],\n",
    "        negative_docs: List[List[str]],\n",
    "        filter_top_k: Optional[int] = None,\n",
    "        miner_model: Optional[nn.Module] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize dataset.\n",
    "        \n",
    "        Args:\n",
    "            queries: List of query strings\n",
    "            positive_docs: List of positive documents\n",
    "            negative_docs: List of negative document lists\n",
    "            filter_top_k: Filter samples where positive not in top-k\n",
    "            miner_model: Model for mining hard negatives\n",
    "        \"\"\"\n",
    "        self.queries = queries\n",
    "        self.positive_docs = positive_docs\n",
    "        self.negative_docs = negative_docs\n",
    "        \n",
    "        # Apply filtering if specified\n",
    "        if filter_top_k is not None and miner_model is not None:\n",
    "            self._apply_consistency_filter(filter_top_k, miner_model)\n",
    "        \n",
    "        print(f\"Dataset initialized with {len(self)} samples\")\n",
    "    \n",
    "    def _apply_consistency_filter(\n",
    "        self,\n",
    "        top_k: int,\n",
    "        miner_model: nn.Module,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Apply consistency-based filtering.\n",
    "        \n",
    "        Filters out samples where positive document is not in top-k results.\n",
    "        \"\"\"\n",
    "        print(f\"Applying consistency filter (top-{top_k})...\")\n",
    "        \n",
    "        filtered_indices = []\n",
    "        \n",
    "        miner_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(len(self)), desc=\"Filtering\"):\n",
    "                query = self.queries[i]\n",
    "                pos_doc = self.positive_docs[i]\n",
    "                neg_docs = self.negative_docs[i]\n",
    "                \n",
    "                # Get all documents\n",
    "                all_docs = [pos_doc] + neg_docs[:top_k-1]\n",
    "                \n",
    "                # Encode and score\n",
    "                query_rep = miner_model.encode([query])[0]\n",
    "                doc_reps = miner_model.encode(all_docs)\n",
    "                \n",
    "                scores = torch.sum(query_rep * doc_reps, dim=-1)\n",
    "                top_k_indices = torch.topk(scores, k=min(top_k, len(all_docs)))[1]\n",
    "                \n",
    "                # Check if positive (index 0) is in top-k\n",
    "                if 0 in top_k_indices:\n",
    "                    filtered_indices.append(i)\n",
    "        \n",
    "        # Update dataset\n",
    "        self.queries = [self.queries[i] for i in filtered_indices]\n",
    "        self.positive_docs = [self.positive_docs[i] for i in filtered_indices]\n",
    "        self.negative_docs = [self.negative_docs[i] for i in filtered_indices]\n",
    "        \n",
    "        print(f\"Filtered to {len(self)} samples ({len(filtered_indices)/len(self.queries)*100:.1f}% retained)\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.queries)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, any]:\n",
    "        # Return both tokenized inputs AND raw text for teacher model\n",
    "        return {\n",
    "            'query': self.queries[idx],\n",
    "            'positive_doc': self.positive_docs[idx],\n",
    "            'negative_docs': self.negative_docs[idx],\n",
    "        }\n",
    "\n",
    "print(\"Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Setup\n",
    "\n",
    "### Initialize model, tokenizer, and training components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG['model']['base_model']\n",
    ")\n",
    "print(f\"Loaded tokenizer: {CONFIG['model']['base_model']}\")\n",
    "\n",
    "# Initialize model\n",
    "model = NeuralSparseEncoder(\n",
    "    model_name=CONFIG['model']['base_model'],\n",
    "    max_length=CONFIG['model']['max_doc_length'],\n",
    "    use_relu=CONFIG['model']['use_relu'],\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Compile model for faster training (PyTorch 2.0+)\n",
    "if CONFIG['hardware']['compile_model'] and torch.__version__ >= '2.0':\n",
    "    print(\"Compiling model with torch.compile...\")\n",
    "    model = torch.compile(model)\n",
    "\n",
    "print(f\"Model initialized and moved to {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load or Compute IDF Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for IDF weights\n",
    "idf_path = project_root / \"data\" / \"idf_weights_msmarco.json\"\n",
    "\n",
    "if idf_path.exists():\n",
    "    print(f\"Loading pre-computed IDF weights from {idf_path}\")\n",
    "    idf_weights = load_idf_weights(str(idf_path))\n",
    "else:\n",
    "    print(\"IDF weights not found. You need to compute them from your corpus.\")\n",
    "    print(\"Example:\")\n",
    "    print(\"\"\"\\n\n",
    "# Load your corpus\n",
    "documents = [...]  # Your document corpus\n",
    "\n",
    "# Compute IDF\n",
    "idf_weights = compute_idf_weights(\n",
    "    documents=documents,\n",
    "    tokenizer=tokenizer,\n",
    "    save_path=str(idf_path),\n",
    ")\n",
    "    \"\"\")\n",
    "    \n",
    "    # For demonstration, create dummy IDF weights\n",
    "    print(\"\\nCreating dummy IDF weights for demonstration...\")\n",
    "    idf_weights = {i: 1.0 for i in range(tokenizer.vocab_size)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Initialize Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize IDF-aware loss\n",
    "loss_fn = IDFAwareLoss(\n",
    "    idf_weights=idf_weights,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    lambda_flops=CONFIG['finetuning']['lambda_flops'],\n",
    "    default_idf=CONFIG['idf_penalty']['default_idf'],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"Loss function initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Initialize Teacher Models (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['knowledge_distillation']['enabled']:\n",
    "    print(\"Initializing ensemble teacher models...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Determine if sparse teacher should be used\n",
    "    sparse_teacher_name = CONFIG['knowledge_distillation']['sparse_teacher']\n",
    "    sparse_weight = CONFIG['knowledge_distillation']['teacher_weights']['sparse']\n",
    "    \n",
    "    if sparse_weight == 0:\n",
    "        print(\"Note: Sparse teacher weight is 0 - using dense teacher only\")\n",
    "        sparse_teacher_name = None\n",
    "    \n",
    "    if sparse_teacher_name:\n",
    "        print(\"\\nIMPORTANT: Sparse Teacher Information\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"The OpenSearch sparse models on HuggingFace Hub may not have\")\n",
    "        print(\"pretrained sparse projection head weights (neural_sparse_head.pt).\")\n",
    "        print(\"\\nIf the model doesn't have pretrained weights, you have two options:\")\n",
    "        print(\"  1. Use only the dense teacher (recommended for initial training)\")\n",
    "        print(\"     Set CONFIG['knowledge_distillation']['teacher_weights']['sparse'] = 0\")\n",
    "        print(\"  2. Train a sparse teacher first, then use it for distillation\")\n",
    "        print(\"\\nProceeding with sparse teacher loading...\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    teacher = EnsembleTeacher(\n",
    "        dense_teacher_name=CONFIG['knowledge_distillation']['dense_teacher'],\n",
    "        sparse_teacher_name=sparse_teacher_name,\n",
    "        dense_weight=CONFIG['knowledge_distillation']['teacher_weights']['dense'],\n",
    "        sparse_weight=sparse_weight,\n",
    "        scale_constant=CONFIG['finetuning']['scale_constant_S'],\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Teacher models initialization complete\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    teacher = None\n",
    "    print(\"Training without knowledge distillation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load your training data\n",
    "# This is a placeholder - replace with your actual data loading\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "print(\"NOTE: This is a placeholder. Load your actual training data here.\")\n",
    "\n",
    "# Example data structure:\n",
    "train_queries = [\"query 1\", \"query 2\", ...]  # Your queries\n",
    "train_positive_docs = [\"pos doc 1\", \"pos doc 2\", ...]  # Positive documents\n",
    "train_negative_docs = [  # Hard negatives for each query\n",
    "    [\"neg 1\", \"neg 2\", ...],  # Negatives for query 1\n",
    "    [\"neg 1\", \"neg 2\", ...],  # Negatives for query 2\n",
    "    ...\n",
    "]\n",
    "\n",
    "# For demonstration, create dummy data\n",
    "print(\"Creating dummy data for demonstration...\")\n",
    "num_samples = 100\n",
    "train_queries = [f\"This is query {i}\" for i in range(num_samples)]\n",
    "train_positive_docs = [f\"This is positive document {i}\" for i in range(num_samples)]\n",
    "train_negative_docs = [\n",
    "    [f\"Negative doc {i}-{j}\" for j in range(CONFIG['finetuning']['num_hard_negatives'])]\n",
    "    for i in range(num_samples)\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(train_queries)} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "train_dataset = SparseRetrievalDataset(\n",
    "    queries=train_queries,\n",
    "    positive_docs=train_positive_docs,\n",
    "    negative_docs=train_negative_docs,\n",
    "    filter_top_k=CONFIG['data_filtering']['top_k_filter'] if CONFIG['data_filtering']['enabled'] else None,\n",
    "    miner_model=model if CONFIG['data_filtering']['enabled'] else None,\n",
    ")\n",
    "\n",
    "# Create data collator\n",
    "data_collator = NeuralSparseDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    query_max_length=CONFIG['model']['max_query_length'],\n",
    "    doc_max_length=CONFIG['model']['max_doc_length'],\n",
    "    num_negatives=CONFIG['finetuning']['num_hard_negatives'],\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['finetuning']['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=CONFIG['hardware']['num_workers'],\n",
    "    pin_memory=CONFIG['hardware']['pin_memory'],\n",
    "    persistent_workers=CONFIG['hardware']['persistent_workers'],\n",
    ")\n",
    "\n",
    "print(f\"Created dataloader with {len(train_dataloader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Override DataCollator to include raw text for teacher model\n# This fixes the KeyError: 'queries' issue\n\nclass FixedDataCollator(NeuralSparseDataCollator):\n    \"\"\"\n    Extended data collator that includes raw text for teacher model.\n    \n    Inherits from NeuralSparseDataCollator and adds raw text pass-through\n    for knowledge distillation.\n    \"\"\"\n    \n    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Collate features and add raw text for teacher model.\n        \n        Args:\n            features: List of dictionaries from Dataset.__getitem__()\n                Each dict has keys: 'query', 'positive_doc', 'negative_docs'\n        \n        Returns:\n            Batch dictionary with:\n                - Tokenized inputs (for student model)\n                - Raw text (for teacher model)\n        \"\"\"\n        # Extract raw text BEFORE tokenization\n        queries = [f['query'] for f in features]\n        pos_docs = [f['positive_doc'] for f in features]\n        neg_docs = [f['negative_docs'] for f in features]\n        \n        # Call parent class to get tokenized inputs\n        batch = super().__call__(features)\n        \n        # Add raw text for teacher model\n        batch['queries'] = queries\n        batch['positive_docs'] = pos_docs\n        batch['negative_docs'] = neg_docs\n        \n        return batch\n\n# Use fixed collator instead of original\ndata_collator = FixedDataCollator(\n    tokenizer=tokenizer,\n    query_max_length=CONFIG['model']['max_query_length'],\n    doc_max_length=CONFIG['model']['max_doc_length'],\n    num_negatives=CONFIG['finetuning']['num_hard_negatives'],\n)\n\nprint(\"✓ Using FixedDataCollator with raw text pass-through for teacher model\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate batch structure before training\n",
    "print(\"Validating batch structure...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get a sample batch\n",
    "sample_batch_iter = iter(train_dataloader)\n",
    "sample_batch = next(sample_batch_iter)\n",
    "\n",
    "print(\"\\nBatch keys:\")\n",
    "for key in sample_batch.keys():\n",
    "    if isinstance(sample_batch[key], torch.Tensor):\n",
    "        print(f\"  {key:30s}: torch.Tensor {sample_batch[key].shape}\")\n",
    "    elif isinstance(sample_batch[key], list):\n",
    "        print(f\"  {key:30s}: List[...] (length={len(sample_batch[key])})\")\n",
    "        if len(sample_batch[key]) > 0:\n",
    "            if isinstance(sample_batch[key][0], str):\n",
    "                print(f\"    └─ Sample: {sample_batch[key][0][:50]}...\")\n",
    "            elif isinstance(sample_batch[key][0], list):\n",
    "                print(f\"    └─ Nested list with {len(sample_batch[key][0])} items\")\n",
    "    else:\n",
    "        print(f\"  {key:30s}: {type(sample_batch[key])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check required keys for student model (tokenized inputs)\n",
    "student_keys = [\n",
    "    'query_input_ids', 'query_attention_mask',\n",
    "    'pos_doc_input_ids', 'pos_doc_attention_mask',\n",
    "    'neg_doc_input_ids', 'neg_doc_attention_mask'\n",
    "]\n",
    "\n",
    "print(\"\\nStudent model keys (tokenized):\")\n",
    "for key in student_keys:\n",
    "    status = \"✓\" if key in sample_batch else \"✗\"\n",
    "    print(f\"  {status} {key}\")\n",
    "\n",
    "# Check required keys for teacher model (raw text)\n",
    "teacher_keys = ['queries', 'positive_docs', 'negative_docs']\n",
    "\n",
    "print(\"\\nTeacher model keys (raw text):\")\n",
    "for key in teacher_keys:\n",
    "    status = \"✓\" if key in sample_batch else \"✗\"\n",
    "    print(f\"  {status} {key}\")\n",
    "\n",
    "# Validate shapes match\n",
    "batch_size = len(sample_batch['queries'])\n",
    "num_negatives = CONFIG['finetuning']['num_hard_negatives']\n",
    "\n",
    "print(f\"\\nBatch size: {batch_size}\")\n",
    "print(f\"Number of negatives: {num_negatives}\")\n",
    "\n",
    "shape_checks = [\n",
    "    (\"query_input_ids\", (batch_size, None)),\n",
    "    (\"pos_doc_input_ids\", (batch_size, None)),\n",
    "    (\"neg_doc_input_ids\", (batch_size, num_negatives, None)),\n",
    "]\n",
    "\n",
    "print(\"\\nShape validation:\")\n",
    "for key, expected_prefix in shape_checks:\n",
    "    actual_shape = sample_batch[key].shape\n",
    "    match = (actual_shape[0] == expected_prefix[0] and \n",
    "             (expected_prefix[1] is None or actual_shape[1] == expected_prefix[1]))\n",
    "    status = \"✓\" if match else \"✗\"\n",
    "    print(f\"  {status} {key:30s}: {actual_shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "if all(k in sample_batch for k in student_keys + teacher_keys):\n",
    "    print(\"SUCCESS: Batch structure is correct!\")\n",
    "    print(\"Ready to start training with knowledge distillation.\")\n",
    "else:\n",
    "    print(\"ERROR: Missing required keys in batch!\")\n",
    "    missing = [k for k in student_keys + teacher_keys if k not in sample_batch]\n",
    "    print(f\"Missing: {missing}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Initialize Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total training steps\n",
    "num_training_steps = CONFIG['finetuning']['num_steps']\n",
    "num_warmup_steps = CONFIG['finetuning']['warmup_steps']\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['finetuning']['learning_rate'],\n",
    "    weight_decay=0.01,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "# Initialize learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# Initialize gradient scaler for mixed precision\n",
    "scaler = GradScaler() if CONFIG['hardware']['mixed_precision'] else None\n",
    "\n",
    "print(f\"Optimizer and scheduler initialized\")\n",
    "print(f\"Total training steps: {num_training_steps}\")\n",
    "print(f\"Warmup steps: {num_warmup_steps}\")\n",
    "print(f\"Mixed precision: {CONFIG['hardware']['mixed_precision']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Training Loop\n",
    "\n",
    "### Main training loop with mixed precision and gradient accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    batch: Dict[str, torch.Tensor],\n",
    "    model: nn.Module,\n",
    "    loss_fn: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scaler: Optional[GradScaler],\n",
    "    teacher: Optional[EnsembleTeacher] = None,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Single training step.\n",
    "    \n",
    "    Args:\n",
    "        batch: Batch dictionary\n",
    "        model: Student model\n",
    "        loss_fn: Loss function\n",
    "        optimizer: Optimizer\n",
    "        scaler: Gradient scaler for mixed precision\n",
    "        teacher: Optional teacher model for knowledge distillation\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with loss values\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Move batch to device\n",
    "    batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "             for k, v in batch.items()}\n",
    "    \n",
    "    # Get teacher scores if available\n",
    "    teacher_scores = None\n",
    "    if teacher is not None:\n",
    "        queries = batch['queries']  # Assuming raw text is passed\n",
    "        positive_docs = batch['positive_docs']\n",
    "        negative_docs = batch['negative_docs']\n",
    "        \n",
    "        # Combine into document lists\n",
    "        all_docs = [[pos] + negs for pos, negs in zip(positive_docs, negative_docs)]\n",
    "        \n",
    "        teacher_scores = teacher.get_scores(queries, all_docs)\n",
    "    \n",
    "    # Forward pass with autocast\n",
    "    with autocast(enabled=scaler is not None, dtype=torch.bfloat16):\n",
    "        # Encode query\n",
    "        query_outputs = model(\n",
    "            input_ids=batch['query_input_ids'],\n",
    "            attention_mask=batch['query_attention_mask'],\n",
    "        )\n",
    "        query_rep = query_outputs['sparse_rep']\n",
    "        \n",
    "        # Encode positive document\n",
    "        pos_outputs = model(\n",
    "            input_ids=batch['pos_doc_input_ids'],\n",
    "            attention_mask=batch['pos_doc_attention_mask'],\n",
    "        )\n",
    "        pos_rep = pos_outputs['sparse_rep']\n",
    "        \n",
    "        # Encode negative documents\n",
    "        batch_size, num_neg, seq_len = batch['neg_doc_input_ids'].shape\n",
    "        neg_input_ids = batch['neg_doc_input_ids'].view(batch_size * num_neg, seq_len)\n",
    "        neg_attention_mask = batch['neg_doc_attention_mask'].view(batch_size * num_neg, seq_len)\n",
    "        \n",
    "        neg_outputs = model(\n",
    "            input_ids=neg_input_ids,\n",
    "            attention_mask=neg_attention_mask,\n",
    "        )\n",
    "        neg_rep = neg_outputs['sparse_rep'].view(batch_size, num_neg, -1)\n",
    "        \n",
    "        # Compute loss\n",
    "        losses = loss_fn(\n",
    "            query_rep=query_rep,\n",
    "            pos_doc_rep=pos_rep,\n",
    "            neg_doc_reps=neg_rep,\n",
    "            teacher_scores=teacher_scores,\n",
    "        )\n",
    "        \n",
    "        total_loss = losses['total_loss']\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if scaler is not None:\n",
    "        scaler.scale(total_loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(),\n",
    "            CONFIG['finetuning']['max_grad_norm'],\n",
    "        )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    else:\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(),\n",
    "            CONFIG['finetuning']['max_grad_norm'],\n",
    "        )\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Return losses as floats\n",
    "    return {k: v.item() for k, v in losses.items()}\n",
    "\n",
    "print(\"Training step function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "\n",
    "output_dir = Path(CONFIG['logging']['output_dir'])\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "global_step = 0\n",
    "training_losses = []\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(total=num_training_steps, desc=\"Training\")\n",
    "\n",
    "while global_step < num_training_steps:\n",
    "    for batch in train_dataloader:\n",
    "        # Training step\n",
    "        losses = train_step(\n",
    "            batch=batch,\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            scaler=scaler,\n",
    "            teacher=teacher,\n",
    "        )\n",
    "        \n",
    "        scheduler.step()\n",
    "        global_step += 1\n",
    "        \n",
    "        # Log losses\n",
    "        training_losses.append(losses)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{losses['total_loss']:.4f}\",\n",
    "            'rank': f\"{losses['ranking_loss']:.4f}\",\n",
    "            'flops': f\"{losses['flops_loss']:.6f}\",\n",
    "        })\n",
    "        \n",
    "        # Logging\n",
    "        if global_step % CONFIG['logging']['logging_steps'] == 0:\n",
    "            avg_loss = np.mean([l['total_loss'] for l in training_losses[-100:]])\n",
    "            print(f\"\\nStep {global_step}: Avg loss = {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if global_step % CONFIG['logging']['save_steps'] == 0:\n",
    "            checkpoint_dir = output_dir / f\"checkpoint-{global_step}\"\n",
    "            model.save_pretrained(str(checkpoint_dir))\n",
    "            print(f\"Checkpoint saved to {checkpoint_dir}\")\n",
    "        \n",
    "        # Stop if reached max steps\n",
    "        if global_step >= num_training_steps:\n",
    "            break\n",
    "\n",
    "pbar.close()\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_dir = output_dir / \"final_model\"\n",
    "model.save_pretrained(str(final_model_dir))\n",
    "print(f\"Final model saved to {final_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Training Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training losses\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Total loss\n",
    "total_losses = [l['total_loss'] for l in training_losses]\n",
    "axes[0].plot(total_losses, alpha=0.3, label='Raw')\n",
    "axes[0].plot(pd.Series(total_losses).rolling(100).mean(), label='Smoothed (100)')\n",
    "axes[0].set_title('Total Loss')\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Ranking loss\n",
    "ranking_losses = [l['ranking_loss'] for l in training_losses]\n",
    "axes[1].plot(ranking_losses, alpha=0.3, label='Raw')\n",
    "axes[1].plot(pd.Series(ranking_losses).rolling(100).mean(), label='Smoothed (100)')\n",
    "axes[1].set_title('Ranking Loss')\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# FLOPS loss\n",
    "flops_losses = [l['flops_loss'] for l in training_losses]\n",
    "axes[2].plot(flops_losses, alpha=0.3, label='Raw')\n",
    "axes[2].plot(pd.Series(flops_losses).rolling(100).mean(), label='Smoothed (100)')\n",
    "axes[2].set_title('FLOPS Loss')\n",
    "axes[2].set_xlabel('Step')\n",
    "axes[2].set_ylabel('Loss')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'training_losses.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training plots saved to {output_dir / 'training_losses.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Model Inference and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "model.eval()\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How to train neural networks?\",\n",
    "    \"Best practices for deep learning\",\n",
    "]\n",
    "\n",
    "test_documents = [\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Neural networks are trained using backpropagation.\",\n",
    "    \"Deep learning requires large datasets and GPU computing.\",\n",
    "]\n",
    "\n",
    "print(\"Testing model inference...\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Encode queries\n",
    "    query_reps = model.encode(test_queries, device=device)\n",
    "    \n",
    "    # Encode documents\n",
    "    doc_reps = model.encode(test_documents, device=device)\n",
    "    \n",
    "    # Compute similarities\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"Query: {query}\")\n",
    "        print(\"Similarities:\")\n",
    "        \n",
    "        query_rep = query_reps[i]\n",
    "        similarities = torch.sum(query_rep.unsqueeze(0) * doc_reps, dim=-1)\n",
    "        \n",
    "        for j, (doc, sim) in enumerate(zip(test_documents, similarities)):\n",
    "            print(f\"  [{sim:.4f}] {doc}\")\n",
    "        \n",
    "        # Get top activated terms\n",
    "        print(\"\\nTop activated terms:\")\n",
    "        top_terms = model.get_top_k_terms(query_rep, k=10)\n",
    "        for term, weight in top_terms:\n",
    "            print(f\"  {term:20s}: {weight:.4f}\")\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Sparsity statistics\n",
    "    print(\"Sparsity Statistics:\")\n",
    "    stats = model.get_sparsity_stats(doc_reps)\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key:30s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Summary and Next Steps\n",
    "\n",
    "### Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nModel: {CONFIG['model']['base_model']}\")\n",
    "print(f\"Total training steps: {global_step}\")\n",
    "print(f\"Final loss: {training_losses[-1]['total_loss']:.4f}\")\n",
    "print(f\"Final ranking loss: {training_losses[-1]['ranking_loss']:.4f}\")\n",
    "print(f\"Final FLOPS loss: {training_losses[-1]['flops_loss']:.6f}\")\n",
    "print(f\"\\nModel saved to: {final_model_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "1. Evaluate on BEIR benchmark datasets\n",
    "2. Measure retrieval efficiency (FLOPS, latency)\n",
    "3. Compare with baseline models (BM25, SPLADE-v3)\n",
    "4. Deploy to OpenSearch for production testing\n",
    "5. Fine-tune hyperparameters if needed\n",
    "\"\"\")\n",
    "\n",
    "# Save configuration\n",
    "config_path = output_dir / \"training_config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "print(f\"\\nConfiguration saved to: {config_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
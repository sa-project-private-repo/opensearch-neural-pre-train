{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenSearch Neural Sparse Model - í•œêµ­ì–´ ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
    "\n",
    "í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ ìƒ˜í”Œì— ëŒ€í•œ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ê³  í† í°í™” ì •ë³´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ëª©í‘œ\n",
    "- í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ë° ê²€ì¦\n",
    "- í•œêµ­ì–´ ì¿¼ë¦¬/ë¬¸ì„œ ìƒ˜í”Œ ì¶”ë¡ \n",
    "- í† í°í™” ì •ë³´ ìƒì„¸ ë¶„ì„\n",
    "- Sparse representation ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nfrom pathlib import Path\nimport json\nfrom typing import List, Dict, Tuple\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import AutoTokenizer\n\n# Add project root to path\nproject_root = Path.cwd().parent.parent\nsys.path.insert(0, str(project_root))\n\nfrom src.model.splade_model import SPLADEDoc\n\nprint(f\"Project root: {project_root}\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ëª¨ë¸ ê²½ë¡œ ì„¤ì • (best_model ì‚¬ìš©)\nmodel_path = project_root / \"outputs\" / \"baseline_dgx\" / \"best_model\"\ncheckpoint_path = model_path / \"checkpoint.pt\"\n\nprint(f\"Loading model from: {model_path}\")\nprint(f\"Checkpoint exists: {checkpoint_path.exists()}\")\n\nif not checkpoint_path.exists():\n    print(\"\\nâš ï¸ WARNING: Checkpoint not found!\")\n    print(\"Please check the model path or train the model first.\")\n    print(f\"Expected location: {checkpoint_path}\")\nelse:\n    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ\n    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n    \n    print(\"\\nğŸ“¦ Checkpoint loaded!\")\n    print(f\"  Epoch: {checkpoint['epoch']}\")\n    print(f\"  Global step: {checkpoint['global_step']}\")\n    print(f\"  Best validation loss: {checkpoint['best_val_loss']:.4f}\")\n    \n    # ì„¤ì • ì •ë³´\n    config = checkpoint['config']\n    model_name = config['model']['name']\n    \n    print(f\"\\nâš™ï¸ Model Configuration:\")\n    print(f\"  Base model: {model_name}\")\n    print(f\"  Max length: {config['data'].get('max_length', 256)}\")\n    \n    # SPLADEDoc ëª¨ë¸ ì´ˆê¸°í™”\n    model = SPLADEDoc(\n        model_name=model_name,\n        dropout=config['model'].get('dropout', 0.1),\n    )\n    \n    # í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model = model.to(device)\n    model.eval()\n    \n    # ëª¨ë¸ ì •ë³´ ì €ì¥\n    vocab_size = model.config.vocab_size\n    hidden_size = model.config.hidden_size\n    max_length = config['data'].get('max_length', 256)\n    \n    print(\"\\nâœ“ Model loaded successfully!\")\n    print(f\"  Model name: {model_name}\")\n    print(f\"  Vocab size: {vocab_size}\")\n    print(f\"  Hidden size: {hidden_size}\")\n    print(f\"  Max length: {max_length}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizer ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tokenizer ë¡œë“œ (ëª¨ë¸ê³¼ ë™ì¼í•œ í† í¬ë‚˜ì´ì € ì‚¬ìš©)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprint(\"Tokenizer loaded\")\nprint(f\"  Model: {model_name}\")\nprint(f\"  Vocab size: {len(tokenizer)}\")\nprint(f\"  Model max length: {tokenizer.model_max_length}\")\n\n# íŠ¹ìˆ˜ í† í° í™•ì¸\nprint(\"\\nSpecial tokens:\")\nprint(f\"  PAD: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\nprint(f\"  CLS: {tokenizer.cls_token} (ID: {tokenizer.cls_token_id})\")\nprint(f\"  SEP: {tokenizer.sep_token} (ID: {tokenizer.sep_token_id})\")\nprint(f\"  UNK: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. í•œêµ­ì–´ ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œêµ­ì–´ ìƒ˜í”Œ ì¿¼ë¦¬\n",
    "korean_queries = [\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹ì´ë€ ë¬´ì—‡ì¸ê°€\",\n",
    "    \"ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ë°©ë²•\",\n",
    "    \"ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ \",\n",
    "    \"ì»´í“¨í„° ë¹„ì „ ì•Œê³ ë¦¬ì¦˜\",\n",
    "    \"ë°ì´í„° ë¶„ì„ ë„êµ¬\",\n",
    "]\n",
    "\n",
    "# í•œêµ­ì–´ ìƒ˜í”Œ ë¬¸ì„œ\n",
    "korean_documents = [\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹ì€ ì»´í“¨í„°ê°€ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•˜ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.\",\n",
    "    \"ë”¥ëŸ¬ë‹ì€ ì¸ê³µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ì…ë‹ˆë‹¤.\",\n",
    "    \"ìì—°ì–´ ì²˜ë¦¬ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\",\n",
    "    \"ì»´í“¨í„° ë¹„ì „ì€ ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ë¥¼ ë¶„ì„í•˜ì—¬ ì˜ë¯¸ ìˆëŠ” ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\",\n",
    "    \"ë°ì´í„° ë¶„ì„ì€ ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘, ì²˜ë¦¬, ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\",\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(korean_queries)} queries and {len(korean_documents)} documents\")\n",
    "print(\"\\nSample query:\")\n",
    "print(f\"  {korean_queries[0]}\")\n",
    "print(\"\\nSample document:\")\n",
    "print(f\"  {korean_documents[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. í† í°í™” ì •ë³´ ë¶„ì„ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tokenization(\n",
    "    text: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    max_length: int = 64,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ì˜ í† í°í™” ì •ë³´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        text: ë¶„ì„í•  í…ìŠ¤íŠ¸\n",
    "        tokenizer: í† í¬ë‚˜ì´ì €\n",
    "        max_length: ìµœëŒ€ ê¸¸ì´\n",
    "        \n",
    "    Returns:\n",
    "        í† í°í™” ì •ë³´ ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "    # í† í°í™”\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'][0]\n",
    "    attention_mask = encoding['attention_mask'][0]\n",
    "    \n",
    "    # í† í° í…ìŠ¤íŠ¸ ë³€í™˜\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    \n",
    "    # ì‹¤ì œ í† í° (íŒ¨ë”© ì œì™¸)\n",
    "    actual_length = attention_mask.sum().item()\n",
    "    actual_tokens = tokens[:actual_length]\n",
    "    actual_ids = input_ids[:actual_length].tolist()\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'input_ids': input_ids.tolist(),\n",
    "        'attention_mask': attention_mask.tolist(),\n",
    "        'tokens': tokens,\n",
    "        'actual_tokens': actual_tokens,\n",
    "        'actual_ids': actual_ids,\n",
    "        'actual_length': actual_length,\n",
    "        'total_length': len(tokens),\n",
    "    }\n",
    "\n",
    "\n",
    "def display_tokenization(token_info: Dict) -> None:\n",
    "    \"\"\"\n",
    "    í† í°í™” ì •ë³´ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Text: {token_info['text']}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nLength: {token_info['actual_length']} tokens (padded to {token_info['total_length']})\")\n",
    "    print(\"\\nTokens:\")\n",
    "    \n",
    "    # í† í° í…Œì´ë¸” ìƒì„±\n",
    "    data = []\n",
    "    for i, (token, token_id) in enumerate(zip(token_info['actual_tokens'], token_info['actual_ids'])):\n",
    "        data.append({\n",
    "            'Position': i,\n",
    "            'Token': token,\n",
    "            'ID': token_id,\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(df.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "print(\"Tokenization analysis functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ìƒ˜í”Œ í† í°í™” í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²« ë²ˆì§¸ ì¿¼ë¦¬ í† í°í™” ë¶„ì„\n",
    "sample_query = korean_queries[0]\n",
    "query_token_info = analyze_tokenization(sample_query, tokenizer, max_length=64)\n",
    "display_tokenization(query_token_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²« ë²ˆì§¸ ë¬¸ì„œ í† í°í™” ë¶„ì„\n",
    "sample_doc = korean_documents[0]\n",
    "doc_token_info = analyze_tokenization(sample_doc, tokenizer, max_length=256)\n",
    "display_tokenization(doc_token_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ëª¨ë¸ ì¶”ë¡  ë° Sparse Representation ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def encode_text(text: str, model: SPLADEDoc, tokenizer: AutoTokenizer) -> torch.Tensor:\n    \"\"\"\n    í…ìŠ¤íŠ¸ë¥¼ sparse representationìœ¼ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤.\n    \n    Args:\n        text: ì…ë ¥ í…ìŠ¤íŠ¸\n        model: SPLADEDoc ëª¨ë¸\n        tokenizer: í† í¬ë‚˜ì´ì €\n        \n    Returns:\n        Sparse representation [vocab_size]\n    \"\"\"\n    # í† í°í™”\n    encoding = tokenizer(\n        text,\n        max_length=max_length,\n        padding='max_length',\n        truncation=True,\n        return_tensors='pt',\n    )\n    \n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    \n    # ëª¨ë¸ ì¶”ë¡ \n    with torch.no_grad():\n        sparse_rep, _ = model(input_ids, attention_mask)\n    \n    return sparse_rep[0]  # [vocab_size]\n\n\ndef analyze_sparse_representation(\n    text: str,\n    model: SPLADEDoc,\n    tokenizer: AutoTokenizer,\n    top_k: int = 20,\n) -> Dict:\n    \"\"\"\n    í…ìŠ¤íŠ¸ì˜ sparse representationì„ ë¶„ì„í•©ë‹ˆë‹¤.\n    \n    Args:\n        text: ë¶„ì„í•  í…ìŠ¤íŠ¸\n        model: SPLADEDoc ëª¨ë¸\n        tokenizer: í† í¬ë‚˜ì´ì €\n        top_k: ìƒìœ„ kê°œ í™œì„±í™” í† í°\n        \n    Returns:\n        ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬\n    \"\"\"\n    # í† í°í™” ì •ë³´\n    token_info = analyze_tokenization(text, tokenizer, max_length)\n    \n    # ëª¨ë¸ ì¶”ë¡ \n    sparse_rep = encode_text(text, model, tokenizer)\n    \n    # Sparse representation ë¶„ì„\n    sparse_rep_np = sparse_rep.cpu().numpy()\n    \n    # 0ì´ ì•„ë‹Œ ê°’ë§Œ ì¶”ì¶œ\n    nonzero_indices = np.nonzero(sparse_rep_np)[0]\n    nonzero_values = sparse_rep_np[nonzero_indices]\n    \n    # ìƒìœ„ kê°œ ì¶”ì¶œ\n    if len(nonzero_values) > 0:\n        top_k_actual = min(top_k, len(nonzero_values))\n        top_k_indices = np.argsort(nonzero_values)[-top_k_actual:][::-1]\n        top_k_token_ids = nonzero_indices[top_k_indices]\n        top_k_values = nonzero_values[top_k_indices]\n        top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_token_ids)\n    else:\n        top_k_token_ids = np.array([])\n        top_k_values = np.array([])\n        top_k_tokens = []\n    \n    # í†µê³„\n    stats = {\n        'num_activated': len(nonzero_indices),\n        'vocab_size': len(sparse_rep_np),\n        'sparsity': 1.0 - (len(nonzero_indices) / len(sparse_rep_np)),\n        'mean_activation': nonzero_values.mean() if len(nonzero_values) > 0 else 0.0,\n        'max_activation': nonzero_values.max() if len(nonzero_values) > 0 else 0.0,\n        'min_activation': nonzero_values.min() if len(nonzero_values) > 0 else 0.0,\n    }\n    \n    return {\n        'text': text,\n        'token_info': token_info,\n        'sparse_rep': sparse_rep_np,\n        'nonzero_indices': nonzero_indices,\n        'nonzero_values': nonzero_values,\n        'top_k_token_ids': top_k_token_ids,\n        'top_k_values': top_k_values,\n        'top_k_tokens': top_k_tokens,\n        'stats': stats,\n    }\n\n\ndef display_sparse_analysis(analysis: Dict) -> None:\n    \"\"\"\n    Sparse representation ë¶„ì„ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n    \"\"\"\n    print(\"=\"*80)\n    print(f\"Text: {analysis['text']}\")\n    print(\"=\"*80)\n    \n    stats = analysis['stats']\n    print(\"\\nğŸ“Š Sparsity Statistics:\")\n    print(f\"  Vocab size: {stats['vocab_size']:,}\")\n    print(f\"  Activated tokens: {stats['num_activated']:,}\")\n    print(f\"  Sparsity: {stats['sparsity']:.2%}\")\n    print(f\"  Mean activation: {stats['mean_activation']:.4f}\")\n    print(f\"  Max activation: {stats['max_activation']:.4f}\")\n    print(f\"  Min activation: {stats['min_activation']:.4f}\")\n    \n    print(f\"\\nğŸ”¥ Top {len(analysis['top_k_tokens'])} Activated Terms:\")\n    \n    # Top-k í…Œì´ë¸”\n    data = []\n    for i, (token_id, token, value) in enumerate(\n        zip(analysis['top_k_token_ids'], analysis['top_k_tokens'], analysis['top_k_values']),\n        1\n    ):\n        data.append({\n            'Rank': i,\n            'Token': token,\n            'ID': token_id,\n            'Weight': f\"{value:.4f}\",\n            'Bar': 'â–ˆ' * int(value * 20),\n        })\n    \n    df = pd.DataFrame(data)\n    print(df.to_string(index=False))\n    print()\n\nprint(\"Sparse representation analysis functions defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ì¿¼ë¦¬ Sparse Representation ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²« ë²ˆì§¸ ì¿¼ë¦¬ ë¶„ì„\n",
    "query_analysis = analyze_sparse_representation(\n",
    "    korean_queries[0],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    top_k=20,\n",
    ")\n",
    "display_sparse_analysis(query_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ë¬¸ì„œ Sparse Representation ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²« ë²ˆì§¸ ë¬¸ì„œ ë¶„ì„\n",
    "doc_analysis = analyze_sparse_representation(\n",
    "    korean_documents[0],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    top_k=20,\n",
    ")\n",
    "display_sparse_analysis(doc_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ì—¬ëŸ¬ ìƒ˜í”Œ ë¹„êµ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë“  ì¿¼ë¦¬ ë¶„ì„\n",
    "print(\"=\"*80)\n",
    "print(\"ALL QUERIES ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "query_analyses = []\n",
    "for i, query in enumerate(korean_queries, 1):\n",
    "    print(f\"\\n[Query {i}/{len(korean_queries)}]\")\n",
    "    analysis = analyze_sparse_representation(query, model, tokenizer, top_k=10)\n",
    "    query_analyses.append(analysis)\n",
    "    \n",
    "    # ê°„ë‹¨í•œ ìš”ì•½ë§Œ ì¶œë ¥\n",
    "    stats = analysis['stats']\n",
    "    print(f\"Text: {query}\")\n",
    "    print(f\"  Activated: {stats['num_activated']:,} tokens ({100-stats['sparsity']*100:.2f}%)\")\n",
    "    print(f\"  Top 5 terms: {', '.join(analysis['top_k_tokens'][:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sparsity ë¹„êµ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsity ë¹„êµ ê·¸ë˜í”„\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Query sparsity\n",
    "query_sparsities = [a['stats']['sparsity'] for a in query_analyses]\n",
    "query_activated = [a['stats']['num_activated'] for a in query_analyses]\n",
    "\n",
    "ax1 = axes[0]\n",
    "x = range(1, len(query_sparsities) + 1)\n",
    "ax1.bar(x, [1-s for s in query_sparsities], alpha=0.7, color='steelblue')\n",
    "ax1.set_xlabel('Query ID', fontsize=12)\n",
    "ax1.set_ylabel('Activation Rate', fontsize=12)\n",
    "ax1.set_title('Query Activation Rates', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, 0.05)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "for i, v in enumerate(query_activated):\n",
    "    ax1.text(i+1, (1-query_sparsities[i])+0.001, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Activated tokens\n",
    "ax2 = axes[1]\n",
    "ax2.bar(x, query_activated, alpha=0.7, color='coral')\n",
    "ax2.set_xlabel('Query ID', fontsize=12)\n",
    "ax2.set_ylabel('Number of Activated Tokens', fontsize=12)\n",
    "ax2.set_title('Activated Token Counts', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "for i, v in enumerate(query_activated):\n",
    "    ax2.text(i+1, v+5, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'outputs' / 'korean_query_sparsity.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Chart saved to: outputs/korean_query_sparsity.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Query-Document ìœ ì‚¬ë„ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ëª¨ë“  ì¿¼ë¦¬ì™€ ë¬¸ì„œ ì¸ì½”ë”©\nprint(\"Encoding all queries and documents...\")\n\ndef encode_batch(texts: List[str], model: SPLADEDoc, tokenizer: AutoTokenizer) -> torch.Tensor:\n    \"\"\"ë°°ì¹˜ í…ìŠ¤íŠ¸ë¥¼ ì¸ì½”ë”©í•©ë‹ˆë‹¤.\"\"\"\n    reps = []\n    for text in texts:\n        rep = encode_text(text, model, tokenizer)\n        reps.append(rep)\n    return torch.stack(reps)\n\nwith torch.no_grad():\n    query_reps = encode_batch(korean_queries, model, tokenizer)\n    doc_reps = encode_batch(korean_documents, model, tokenizer)\n\nprint(f\"Query representations: {query_reps.shape}\")\nprint(f\"Document representations: {doc_reps.shape}\")\n\n# ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚° (dot product)\nsimilarity_matrix = torch.mm(query_reps, doc_reps.t()).cpu().numpy()\n\nprint(f\"\\nSimilarity matrix: {similarity_matrix.shape}\")\nprint(\"\\nSimilarity scores (Query Ã— Document):\")\nprint(pd.DataFrame(\n    similarity_matrix,\n    index=[f\"Q{i+1}\" for i in range(len(korean_queries))],\n    columns=[f\"D{i+1}\" for i in range(len(korean_documents))],\n).round(4))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. ìœ ì‚¬ë„ íˆíŠ¸ë§µ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìœ ì‚¬ë„ íˆíŠ¸ë§µ\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(\n",
    "    similarity_matrix,\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    cmap='YlOrRd',\n",
    "    xticklabels=[f\"Doc {i+1}\" for i in range(len(korean_documents))],\n",
    "    yticklabels=[f\"Query {i+1}\" for i in range(len(korean_queries))],\n",
    "    cbar_kws={'label': 'Similarity Score'},\n",
    ")\n",
    "\n",
    "plt.title('Query-Document Similarity Matrix\\n(Neural Sparse Encoding)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Documents', fontsize=12)\n",
    "plt.ylabel('Queries', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'outputs' / 'similarity_heatmap_korean.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Heatmap saved to: outputs/similarity_heatmap_korean.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Top-K ê²€ìƒ‰ ê²°ê³¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê° ì¿¼ë¦¬ì— ëŒ€í•œ Top-3 ë¬¸ì„œ ê²€ìƒ‰\n",
    "print(\"=\"*80)\n",
    "print(\"TOP-3 RETRIEVAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, query in enumerate(korean_queries):\n",
    "    print(f\"\\n[Query {i+1}] {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # ìƒìœ„ 3ê°œ ë¬¸ì„œ\n",
    "    top_k_indices = np.argsort(similarity_matrix[i])[-3:][::-1]\n",
    "    \n",
    "    for rank, doc_idx in enumerate(top_k_indices, 1):\n",
    "        score = similarity_matrix[i][doc_idx]\n",
    "        doc = korean_documents[doc_idx]\n",
    "        print(f\"\\n  Rank {rank} [Score: {score:.4f}] (Doc {doc_idx+1})\")\n",
    "        print(f\"  {doc}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. ê²°ê³¼ ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*80)\nprint(\"INFERENCE TEST SUMMARY\")\nprint(\"=\"*80)\n\nprint(f\"\\nğŸ“ Test Configuration:\")\nprint(f\"  Model: {model_name}\")\nprint(f\"  Model path: {model_path}\")\nprint(f\"  Device: {device}\")\nprint(f\"  Vocab size: {vocab_size:,}\")\n\nprint(f\"\\nğŸ“Š Dataset:\")\nprint(f\"  Queries: {len(korean_queries)}\")\nprint(f\"  Documents: {len(korean_documents)}\")\nprint(f\"  Language: Korean\")\n\nprint(f\"\\nğŸ¯ Sparsity Statistics:\")\navg_sparsity = np.mean([a['stats']['sparsity'] for a in query_analyses])\navg_activated = np.mean([a['stats']['num_activated'] for a in query_analyses])\nprint(f\"  Average sparsity: {avg_sparsity:.2%}\")\nprint(f\"  Average activated tokens: {avg_activated:.1f}\")\nprint(f\"  Activation rate: {(1-avg_sparsity)*100:.3f}%\")\n\nprint(f\"\\nğŸ” Similarity Statistics:\")\nprint(f\"  Mean similarity: {similarity_matrix.mean():.4f}\")\nprint(f\"  Max similarity: {similarity_matrix.max():.4f}\")\nprint(f\"  Min similarity: {similarity_matrix.min():.4f}\")\nprint(f\"  Std similarity: {similarity_matrix.std():.4f}\")\n\nprint(f\"\\nâœ… Inference test completed successfully!\")\nprint(f\"\\nğŸ“ Generated files:\")\nprint(f\"  - outputs/korean_query_sparsity.png\")\nprint(f\"  - outputs/similarity_heatmap_korean.png\")\n\nprint(\"\\n\" + \"=\"*80)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
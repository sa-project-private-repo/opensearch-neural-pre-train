{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-lingual Synonym Data Synthesis\n",
    "\n",
    "Ollama gpt-oss:20b를 활용하여 한-영 동의어 데이터를 합성 생성합니다.\n",
    "\n",
    "## 목표\n",
    "- IT/ML 기술 용어 한-영 동의어 쌍 생성\n",
    "- 병렬 문장 데이터 생성\n",
    "- Cross-lingual KD 학습용 데이터셋 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "# Project root\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ollama Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OllamaConfig:\n",
    "    \"\"\"Ollama API configuration.\"\"\"\n",
    "    base_url: str = \"http://localhost:11434\"\n",
    "    model: str = \"gpt-oss:20b\"\n",
    "    temperature: float = 0.7\n",
    "    max_retries: int = 3\n",
    "    timeout: int = 120\n",
    "\n",
    "\n",
    "class OllamaClient:\n",
    "    \"\"\"Ollama API client for synonym generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: OllamaConfig):\n",
    "        self.config = config\n",
    "        self.api_url = f\"{config.base_url}/api/generate\"\n",
    "    \n",
    "    def generate(self, prompt: str, temperature: Optional[float] = None) -> str:\n",
    "        \"\"\"Generate text using Ollama API.\"\"\"\n",
    "        temp = temperature if temperature is not None else self.config.temperature\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": self.config.model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": temp}\n",
    "        }\n",
    "        \n",
    "        for attempt in range(self.config.max_retries):\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    self.api_url,\n",
    "                    json=payload,\n",
    "                    timeout=self.config.timeout\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                return response.json().get(\"response\", \"\").strip()\n",
    "            except Exception as e:\n",
    "                if attempt < self.config.max_retries - 1:\n",
    "                    time.sleep(2 ** attempt)\n",
    "                else:\n",
    "                    raise RuntimeError(f\"Ollama API failed: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    def health_check(self) -> bool:\n",
    "        \"\"\"Check if Ollama is running and model is available.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.config.base_url}/api/tags\", timeout=5)\n",
    "            models = [m[\"name\"] for m in response.json().get(\"models\", [])]\n",
    "            return self.config.model in models or any(self.config.model in m for m in models)\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "\n",
    "# Initialize client\n",
    "config = OllamaConfig()\n",
    "client = OllamaClient(config)\n",
    "\n",
    "# Health check\n",
    "print(f\"Ollama model: {config.model}\")\n",
    "print(f\"Health check: {'OK' if client.health_check() else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seed Terms (카테고리별 시드 용어)\n",
    "\n",
    "각 카테고리에서 시드 용어를 제공하고, LLM이 관련 용어들을 확장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리별 시드 용어 (LLM이 확장할 기준점)\n",
    "SEED_CATEGORIES = {\n",
    "    \"machine_learning\": [\n",
    "        \"머신러닝\", \"딥러닝\", \"신경망\", \"학습\", \"모델\", \"예측\", \"분류\", \"회귀\",\n",
    "        \"과적합\", \"정규화\", \"손실함수\", \"최적화\", \"경사하강법\", \"역전파\"\n",
    "    ],\n",
    "    \"natural_language_processing\": [\n",
    "        \"자연어처리\", \"토큰화\", \"임베딩\", \"트랜스포머\", \"어텐션\", \"언어모델\",\n",
    "        \"텍스트분류\", \"개체명인식\", \"기계번역\", \"질의응답\", \"요약\"\n",
    "    ],\n",
    "    \"computer_vision\": [\n",
    "        \"컴퓨터비전\", \"이미지분류\", \"객체탐지\", \"영상분할\", \"특징추출\",\n",
    "        \"합성곱\", \"풀링\", \"데이터증강\", \"전이학습\"\n",
    "    ],\n",
    "    \"data_science\": [\n",
    "        \"데이터분석\", \"데이터전처리\", \"특성공학\", \"차원축소\", \"군집화\",\n",
    "        \"이상탐지\", \"시각화\", \"통계분석\"\n",
    "    ],\n",
    "    \"software_engineering\": [\n",
    "        \"알고리즘\", \"자료구조\", \"프로그래밍\", \"소프트웨어\", \"데이터베이스\",\n",
    "        \"서버\", \"클라이언트\", \"API\", \"마이크로서비스\", \"컨테이너\"\n",
    "    ],\n",
    "    \"cloud_infrastructure\": [\n",
    "        \"클라우드\", \"가상화\", \"컨테이너\", \"쿠버네티스\", \"도커\",\n",
    "        \"로드밸런싱\", \"스케일링\", \"모니터링\"\n",
    "    ],\n",
    "    \"search_retrieval\": [\n",
    "        \"검색엔진\", \"정보검색\", \"인덱싱\", \"쿼리\", \"랭킹\", \"관련성\",\n",
    "        \"벡터검색\", \"시맨틱검색\", \"역색인\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"Total seed categories: {len(SEED_CATEGORIES)}\")\n",
    "print(f\"Total seed terms: {sum(len(v) for v in SEED_CATEGORIES.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Synonym Generation Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_translation_prompt(korean_term: str) -> str:\n",
    "    \"\"\"Create prompt for Korean to English translation.\"\"\"\n",
    "    return f\"\"\"Translate the Korean technical term to English.\n",
    "\n",
    "Korean term: {korean_term}\n",
    "\n",
    "Provide the English translation(s) as a JSON object with the following format:\n",
    "{{\n",
    "    \"primary\": \"main English translation\",\n",
    "    \"alternatives\": [\"alternative 1\", \"alternative 2\"],\n",
    "    \"abbreviation\": \"abbrev or null\"\n",
    "}}\n",
    "\n",
    "Only output the JSON, nothing else.\"\"\"\n",
    "\n",
    "\n",
    "def create_expansion_prompt(category: str, seed_terms: List[str]) -> str:\n",
    "    \"\"\"Create prompt to expand seed terms with related terms.\"\"\"\n",
    "    seeds = \", \".join(seed_terms[:5])\n",
    "    return f\"\"\"Generate 20 additional Korean technical terms related to the category \"{category}\".\n",
    "\n",
    "Example terms in this category: {seeds}\n",
    "\n",
    "Requirements:\n",
    "- Terms should be commonly used in Korean IT/tech context\n",
    "- Include both pure Korean terms and Konglish (외래어)\n",
    "- Terms should be specific enough to have clear English translations\n",
    "\n",
    "Output as JSON array of Korean terms only:\n",
    "[\"용어1\", \"용어2\", ...]\n",
    "\n",
    "Only output the JSON array, nothing else.\"\"\"\n",
    "\n",
    "\n",
    "def create_parallel_sentence_prompt(korean_term: str, english_term: str) -> str:\n",
    "    \"\"\"Create prompt for generating parallel sentences.\"\"\"\n",
    "    return f\"\"\"Generate 3 pairs of parallel sentences (Korean and English) using the terms:\n",
    "- Korean term: {korean_term}\n",
    "- English term: {english_term}\n",
    "\n",
    "Requirements:\n",
    "- Sentences should be technical/educational context\n",
    "- Korean and English sentences should have the same meaning\n",
    "- Use natural language in both languages\n",
    "\n",
    "Output as JSON array:\n",
    "[\n",
    "    {{\"ko\": \"한국어 문장\", \"en\": \"English sentence\"}},\n",
    "    ...\n",
    "]\n",
    "\n",
    "Only output the JSON array, nothing else.\"\"\"\n",
    "\n",
    "\n",
    "print(\"Prompt templates defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_response(response: str) -> Optional[dict | list]:\n",
    "    \"\"\"Parse JSON from LLM response, handling common issues.\"\"\"\n",
    "    # Try direct parse\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    # Try to extract JSON from response\n",
    "    import re\n",
    "    \n",
    "    # Look for JSON object\n",
    "    obj_match = re.search(r'\\{[^{}]*\\}', response, re.DOTALL)\n",
    "    if obj_match:\n",
    "        try:\n",
    "            return json.loads(obj_match.group())\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # Look for JSON array\n",
    "    arr_match = re.search(r'\\[[^\\[\\]]*\\]', response, re.DOTALL)\n",
    "    if arr_match:\n",
    "        try:\n",
    "            return json.loads(arr_match.group())\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def translate_term(client: OllamaClient, korean_term: str) -> Optional[Dict]:\n",
    "    \"\"\"Translate a Korean term to English using LLM.\"\"\"\n",
    "    prompt = create_translation_prompt(korean_term)\n",
    "    response = client.generate(prompt, temperature=0.3)\n",
    "    \n",
    "    parsed = parse_json_response(response)\n",
    "    if parsed and isinstance(parsed, dict):\n",
    "        return {\n",
    "            \"ko\": korean_term,\n",
    "            \"en_primary\": parsed.get(\"primary\", \"\"),\n",
    "            \"en_alternatives\": parsed.get(\"alternatives\", []),\n",
    "            \"abbreviation\": parsed.get(\"abbreviation\")\n",
    "        }\n",
    "    return None\n",
    "\n",
    "\n",
    "def expand_category(client: OllamaClient, category: str, seed_terms: List[str]) -> List[str]:\n",
    "    \"\"\"Expand a category with additional terms using LLM.\"\"\"\n",
    "    prompt = create_expansion_prompt(category, seed_terms)\n",
    "    response = client.generate(prompt, temperature=0.8)\n",
    "    \n",
    "    parsed = parse_json_response(response)\n",
    "    if parsed and isinstance(parsed, list):\n",
    "        return [t for t in parsed if isinstance(t, str) and len(t) > 1]\n",
    "    return []\n",
    "\n",
    "\n",
    "def generate_parallel_sentences(\n",
    "    client: OllamaClient, \n",
    "    korean_term: str, \n",
    "    english_term: str\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"Generate parallel sentences for a term pair.\"\"\"\n",
    "    prompt = create_parallel_sentence_prompt(korean_term, english_term)\n",
    "    response = client.generate(prompt, temperature=0.7)\n",
    "    \n",
    "    parsed = parse_json_response(response)\n",
    "    if parsed and isinstance(parsed, list):\n",
    "        return [\n",
    "            {\"ko\": item.get(\"ko\", \"\"), \"en\": item.get(\"en\", \"\")}\n",
    "            for item in parsed\n",
    "            if isinstance(item, dict) and item.get(\"ko\") and item.get(\"en\")\n",
    "        ]\n",
    "    return []\n",
    "\n",
    "\n",
    "print(\"Generation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test translation\n",
    "print(\"Testing translation...\")\n",
    "test_result = translate_term(client, \"머신러닝\")\n",
    "print(f\"Translation result: {json.dumps(test_result, ensure_ascii=False, indent=2)}\")\n",
    "\n",
    "print(\"\\nTesting category expansion...\")\n",
    "test_expansion = expand_category(client, \"machine_learning\", SEED_CATEGORIES[\"machine_learning\"][:5])\n",
    "print(f\"Expanded terms ({len(test_expansion)}): {test_expansion[:10]}\")\n",
    "\n",
    "print(\"\\nTesting parallel sentence generation...\")\n",
    "if test_result:\n",
    "    test_sentences = generate_parallel_sentences(client, \"머신러닝\", test_result[\"en_primary\"])\n",
    "    for sent in test_sentences:\n",
    "        print(f\"  KO: {sent['ko']}\")\n",
    "        print(f\"  EN: {sent['en']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Full Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "OUTPUT_DIR = project_root / \"dataset\" / \"synonyms\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generation settings\n",
    "EXPAND_CATEGORIES = True  # Whether to expand categories with LLM\n",
    "GENERATE_SENTENCES = True  # Whether to generate parallel sentences\n",
    "SAMPLE_RATE_SENTENCES = 0.3  # Fraction of terms to generate sentences for\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_full_dataset(\n",
    "    client: OllamaClient,\n",
    "    seed_categories: Dict[str, List[str]],\n",
    "    expand_categories: bool = True,\n",
    "    generate_sentences: bool = True,\n",
    "    sentence_sample_rate: float = 0.3,\n",
    ") -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Generate full cross-lingual dataset.\n",
    "    \n",
    "    Returns:\n",
    "        synonyms: List of synonym entries\n",
    "        parallel_sentences: List of parallel sentence pairs\n",
    "    \"\"\"\n",
    "    all_terms = []\n",
    "    synonyms = []\n",
    "    parallel_sentences = []\n",
    "    \n",
    "    # Step 1: Collect all terms (seed + expanded)\n",
    "    print(\"Step 1: Collecting terms...\")\n",
    "    for category, seeds in tqdm(seed_categories.items(), desc=\"Categories\"):\n",
    "        category_terms = list(seeds)  # Start with seeds\n",
    "        \n",
    "        if expand_categories:\n",
    "            try:\n",
    "                expanded = expand_category(client, category, seeds)\n",
    "                # Filter duplicates\n",
    "                new_terms = [t for t in expanded if t not in category_terms]\n",
    "                category_terms.extend(new_terms[:15])  # Limit expansion\n",
    "                print(f\"  {category}: {len(seeds)} seeds + {len(new_terms[:15])} expanded\")\n",
    "            except Exception as e:\n",
    "                print(f\"  {category}: expansion failed - {e}\")\n",
    "        \n",
    "        for term in category_terms:\n",
    "            all_terms.append({\"term\": term, \"category\": category})\n",
    "    \n",
    "    print(f\"\\nTotal terms to translate: {len(all_terms)}\")\n",
    "    \n",
    "    # Step 2: Translate all terms\n",
    "    print(\"\\nStep 2: Translating terms...\")\n",
    "    for item in tqdm(all_terms, desc=\"Translating\"):\n",
    "        try:\n",
    "            result = translate_term(client, item[\"term\"])\n",
    "            if result and result.get(\"en_primary\"):\n",
    "                result[\"category\"] = item[\"category\"]\n",
    "                synonyms.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to translate '{item['term']}': {e}\")\n",
    "        \n",
    "        # Rate limiting\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    print(f\"\\nSuccessfully translated: {len(synonyms)} terms\")\n",
    "    \n",
    "    # Step 3: Generate parallel sentences (sampled)\n",
    "    if generate_sentences:\n",
    "        print(\"\\nStep 3: Generating parallel sentences...\")\n",
    "        sampled = random.sample(synonyms, int(len(synonyms) * sentence_sample_rate))\n",
    "        \n",
    "        for entry in tqdm(sampled, desc=\"Generating sentences\"):\n",
    "            try:\n",
    "                sentences = generate_parallel_sentences(\n",
    "                    client, \n",
    "                    entry[\"ko\"], \n",
    "                    entry[\"en_primary\"]\n",
    "                )\n",
    "                for sent in sentences:\n",
    "                    sent[\"term_ko\"] = entry[\"ko\"]\n",
    "                    sent[\"term_en\"] = entry[\"en_primary\"]\n",
    "                    sent[\"category\"] = entry[\"category\"]\n",
    "                parallel_sentences.extend(sentences)\n",
    "            except Exception as e:\n",
    "                print(f\"  Failed for '{entry['ko']}': {e}\")\n",
    "            \n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        print(f\"\\nGenerated {len(parallel_sentences)} parallel sentences\")\n",
    "    \n",
    "    return synonyms, parallel_sentences\n",
    "\n",
    "\n",
    "print(\"Dataset generation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING FULL DATASET GENERATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "synonyms, parallel_sentences = generate_full_dataset(\n",
    "    client=client,\n",
    "    seed_categories=SEED_CATEGORIES,\n",
    "    expand_categories=EXPAND_CATEGORIES,\n",
    "    generate_sentences=GENERATE_SENTENCES,\n",
    "    sentence_sample_rate=SAMPLE_RATE_SENTENCES,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Synonym entries: {len(synonyms)}\")\n",
    "print(f\"Parallel sentences: {len(parallel_sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save synonyms\n",
    "synonyms_path = OUTPUT_DIR / \"ko_en_terms.jsonl\"\n",
    "with open(synonyms_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in synonyms:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved synonyms to: {synonyms_path}\")\n",
    "\n",
    "# Save parallel sentences\n",
    "sentences_path = OUTPUT_DIR / \"ko_en_parallel.jsonl\"\n",
    "with open(sentences_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in parallel_sentences:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved parallel sentences to: {sentences_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"total_synonyms\": len(synonyms),\n",
    "    \"total_parallel_sentences\": len(parallel_sentences),\n",
    "    \"categories\": list(SEED_CATEGORIES.keys()),\n",
    "    \"generation_model\": config.model,\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "}\n",
    "metadata_path = OUTPUT_DIR / \"metadata.json\"\n",
    "with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Saved metadata to: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and analyze\n",
    "df_synonyms = pd.DataFrame(synonyms)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SYNONYM DATASET ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTotal entries: {len(df_synonyms)}\")\n",
    "print(f\"\\nBy category:\")\n",
    "print(df_synonyms[\"category\"].value_counts())\n",
    "\n",
    "print(f\"\\nSample entries:\")\n",
    "for _, row in df_synonyms.sample(min(5, len(df_synonyms))).iterrows():\n",
    "    print(f\"  {row['ko']} → {row['en_primary']}\")\n",
    "    if row.get('en_alternatives'):\n",
    "        print(f\"    alternatives: {row['en_alternatives']}\")\n",
    "    if row.get('abbreviation'):\n",
    "        print(f\"    abbreviation: {row['abbreviation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if parallel_sentences:\n",
    "    df_sentences = pd.DataFrame(parallel_sentences)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"PARALLEL SENTENCES ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nTotal sentences: {len(df_sentences)}\")\n",
    "    print(f\"\\nBy category:\")\n",
    "    print(df_sentences[\"category\"].value_counts())\n",
    "    \n",
    "    print(f\"\\nSample sentences:\")\n",
    "    for _, row in df_sentences.sample(min(3, len(df_sentences))).iterrows():\n",
    "        print(f\"  KO: {row['ko']}\")\n",
    "        print(f\"  EN: {row['en']}\")\n",
    "        print(f\"  Term: {row['term_ko']} / {row['term_en']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Training Format\n",
    "\n",
    "Cross-lingual KD 학습에 사용할 형식으로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_pairs(synonyms: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Create training pairs for cross-lingual alignment.\n",
    "    \n",
    "    Format:\n",
    "    {\n",
    "        \"ko_term\": \"머신러닝\",\n",
    "        \"en_terms\": [\"machine learning\", \"ML\"],\n",
    "        \"category\": \"machine_learning\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for entry in synonyms:\n",
    "        en_terms = [entry[\"en_primary\"]]\n",
    "        if entry.get(\"en_alternatives\"):\n",
    "            en_terms.extend(entry[\"en_alternatives\"])\n",
    "        if entry.get(\"abbreviation\"):\n",
    "            en_terms.append(entry[\"abbreviation\"])\n",
    "        \n",
    "        # Filter empty/None\n",
    "        en_terms = [t for t in en_terms if t and isinstance(t, str)]\n",
    "        \n",
    "        if en_terms:\n",
    "            pairs.append({\n",
    "                \"ko_term\": entry[\"ko\"],\n",
    "                \"en_terms\": en_terms,\n",
    "                \"category\": entry.get(\"category\", \"unknown\")\n",
    "            })\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "training_pairs = create_training_pairs(synonyms)\n",
    "\n",
    "# Save training format\n",
    "training_path = OUTPUT_DIR / \"cross_lingual_pairs.jsonl\"\n",
    "with open(training_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for pair in training_pairs:\n",
    "        f.write(json.dumps(pair, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Created {len(training_pairs)} training pairs\")\n",
    "print(f\"Saved to: {training_path}\")\n",
    "\n",
    "# Show samples\n",
    "print(\"\\nSample training pairs:\")\n",
    "for pair in random.sample(training_pairs, min(5, len(training_pairs))):\n",
    "    print(f\"  {pair['ko_term']} → {pair['en_terms']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CROSS-LINGUAL DATA SYNTHESIS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  - {synonyms_path.name}: {len(synonyms)} synonym entries\")\n",
    "print(f\"  - {sentences_path.name}: {len(parallel_sentences)} parallel sentences\")\n",
    "print(f\"  - {training_path.name}: {len(training_pairs)} training pairs\")\n",
    "print(f\"  - {metadata_path.name}: metadata\")\n",
    "\n",
    "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Review generated data quality\")\n",
    "print(f\"  2. Implement CrossLingualKDLoss in src/training/losses.py\")\n",
    "print(f\"  3. Update training notebook to use cross-lingual data\")\n",
    "print(f\"  4. Run cross-lingual training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

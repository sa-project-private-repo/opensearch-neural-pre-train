{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-lingual Synonym Data Synthesis\n",
    "\n",
    "Ollama gpt-oss:20b를 활용하여 한-영 동의어 데이터를 합성 생성합니다.\n",
    "\n",
    "## 목표\n",
    "- IT/ML 기술 용어 한-영 동의어 쌍 생성\n",
    "- 병렬 문장 데이터 생성\n",
    "- Cross-lingual KD 학습용 데이터셋 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "# Project root\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ollama Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama model: gpt-oss:20b\n",
      "Health check: OK\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class OllamaConfig:\n",
    "    \"\"\"Ollama API configuration.\"\"\"\n",
    "    base_url: str = \"http://localhost:11434\"\n",
    "    model: str = \"gpt-oss:20b\"\n",
    "    temperature: float = 0.7\n",
    "    max_retries: int = 3\n",
    "    timeout: int = 120\n",
    "\n",
    "\n",
    "class OllamaClient:\n",
    "    \"\"\"Ollama API client for synonym generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: OllamaConfig):\n",
    "        self.config = config\n",
    "        self.api_url = f\"{config.base_url}/api/generate\"\n",
    "    \n",
    "    def generate(self, prompt: str, temperature: Optional[float] = None) -> str:\n",
    "        \"\"\"Generate text using Ollama API.\"\"\"\n",
    "        temp = temperature if temperature is not None else self.config.temperature\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": self.config.model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": temp}\n",
    "        }\n",
    "        \n",
    "        for attempt in range(self.config.max_retries):\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    self.api_url,\n",
    "                    json=payload,\n",
    "                    timeout=self.config.timeout\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                return response.json().get(\"response\", \"\").strip()\n",
    "            except Exception as e:\n",
    "                if attempt < self.config.max_retries - 1:\n",
    "                    time.sleep(2 ** attempt)\n",
    "                else:\n",
    "                    raise RuntimeError(f\"Ollama API failed: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    def health_check(self) -> bool:\n",
    "        \"\"\"Check if Ollama is running and model is available.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.config.base_url}/api/tags\", timeout=5)\n",
    "            models = [m[\"name\"] for m in response.json().get(\"models\", [])]\n",
    "            return self.config.model in models or any(self.config.model in m for m in models)\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "\n",
    "# Initialize client\n",
    "config = OllamaConfig()\n",
    "client = OllamaClient(config)\n",
    "\n",
    "# Health check\n",
    "print(f\"Ollama model: {config.model}\")\n",
    "print(f\"Health check: {'OK' if client.health_check() else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seed Terms (카테고리별 시드 용어)\n",
    "\n",
    "각 카테고리에서 시드 용어를 제공하고, LLM이 관련 용어들을 확장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total seed categories: 7\n",
      "Total seed terms: 69\n"
     ]
    }
   ],
   "source": [
    "# 카테고리별 시드 용어 (LLM이 확장할 기준점)\n",
    "SEED_CATEGORIES = {\n",
    "    \"machine_learning\": [\n",
    "        \"머신러닝\", \"딥러닝\", \"신경망\", \"학습\", \"모델\", \"예측\", \"분류\", \"회귀\",\n",
    "        \"과적합\", \"정규화\", \"손실함수\", \"최적화\", \"경사하강법\", \"역전파\"\n",
    "    ],\n",
    "    \"natural_language_processing\": [\n",
    "        \"자연어처리\", \"토큰화\", \"임베딩\", \"트랜스포머\", \"어텐션\", \"언어모델\",\n",
    "        \"텍스트분류\", \"개체명인식\", \"기계번역\", \"질의응답\", \"요약\"\n",
    "    ],\n",
    "    \"computer_vision\": [\n",
    "        \"컴퓨터비전\", \"이미지분류\", \"객체탐지\", \"영상분할\", \"특징추출\",\n",
    "        \"합성곱\", \"풀링\", \"데이터증강\", \"전이학습\"\n",
    "    ],\n",
    "    \"data_science\": [\n",
    "        \"데이터분석\", \"데이터전처리\", \"특성공학\", \"차원축소\", \"군집화\",\n",
    "        \"이상탐지\", \"시각화\", \"통계분석\"\n",
    "    ],\n",
    "    \"software_engineering\": [\n",
    "        \"알고리즘\", \"자료구조\", \"프로그래밍\", \"소프트웨어\", \"데이터베이스\",\n",
    "        \"서버\", \"클라이언트\", \"API\", \"마이크로서비스\", \"컨테이너\"\n",
    "    ],\n",
    "    \"cloud_infrastructure\": [\n",
    "        \"클라우드\", \"가상화\", \"컨테이너\", \"쿠버네티스\", \"도커\",\n",
    "        \"로드밸런싱\", \"스케일링\", \"모니터링\"\n",
    "    ],\n",
    "    \"search_retrieval\": [\n",
    "        \"검색엔진\", \"정보검색\", \"인덱싱\", \"쿼리\", \"랭킹\", \"관련성\",\n",
    "        \"벡터검색\", \"시맨틱검색\", \"역색인\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"Total seed categories: {len(SEED_CATEGORIES)}\")\n",
    "print(f\"Total seed terms: {sum(len(v) for v in SEED_CATEGORIES.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Synonym Generation Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt templates defined\n"
     ]
    }
   ],
   "source": [
    "def create_translation_prompt(korean_term: str) -> str:\n",
    "    \"\"\"Create prompt for Korean to English translation.\"\"\"\n",
    "    return f\"\"\"Translate the Korean technical term to English.\n",
    "\n",
    "Korean term: {korean_term}\n",
    "\n",
    "Provide the English translation(s) as a JSON object with the following format:\n",
    "{{\n",
    "    \"primary\": \"main English translation\",\n",
    "    \"alternatives\": [\"alternative 1\", \"alternative 2\"],\n",
    "    \"abbreviation\": \"abbrev or null\"\n",
    "}}\n",
    "\n",
    "Only output the JSON, nothing else.\"\"\"\n",
    "\n",
    "\n",
    "def create_expansion_prompt(category: str, seed_terms: List[str]) -> str:\n",
    "    \"\"\"Create prompt to expand seed terms with related terms.\"\"\"\n",
    "    seeds = \", \".join(seed_terms[:5])\n",
    "    return f\"\"\"Generate 20 additional Korean technical terms related to the category \"{category}\".\n",
    "\n",
    "Example terms in this category: {seeds}\n",
    "\n",
    "Requirements:\n",
    "- Terms should be commonly used in Korean IT/tech context\n",
    "- Include both pure Korean terms and Konglish (외래어)\n",
    "- Terms should be specific enough to have clear English translations\n",
    "\n",
    "Output as JSON array of Korean terms only:\n",
    "[\"용어1\", \"용어2\", ...]\n",
    "\n",
    "Only output the JSON array, nothing else.\"\"\"\n",
    "\n",
    "\n",
    "def create_parallel_sentence_prompt(korean_term: str, english_term: str) -> str:\n",
    "    \"\"\"Create prompt for generating parallel sentences.\"\"\"\n",
    "    return f\"\"\"Generate 3 pairs of parallel sentences (Korean and English) using the terms:\n",
    "- Korean term: {korean_term}\n",
    "- English term: {english_term}\n",
    "\n",
    "Requirements:\n",
    "- Sentences should be technical/educational context\n",
    "- Korean and English sentences should have the same meaning\n",
    "- Use natural language in both languages\n",
    "\n",
    "Output as JSON array:\n",
    "[\n",
    "    {{\"ko\": \"한국어 문장\", \"en\": \"English sentence\"}},\n",
    "    ...\n",
    "]\n",
    "\n",
    "Only output the JSON array, nothing else.\"\"\"\n",
    "\n",
    "\n",
    "print(\"Prompt templates defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation functions defined\n"
     ]
    }
   ],
   "source": [
    "def parse_json_response(response: str) -> Optional[dict | list]:\n",
    "    \"\"\"Parse JSON from LLM response, handling common issues.\"\"\"\n",
    "    # Try direct parse\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    # Try to extract JSON from response\n",
    "    import re\n",
    "    \n",
    "    # Look for JSON object\n",
    "    obj_match = re.search(r'\\{[^{}]*\\}', response, re.DOTALL)\n",
    "    if obj_match:\n",
    "        try:\n",
    "            return json.loads(obj_match.group())\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # Look for JSON array\n",
    "    arr_match = re.search(r'\\[[^\\[\\]]*\\]', response, re.DOTALL)\n",
    "    if arr_match:\n",
    "        try:\n",
    "            return json.loads(arr_match.group())\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def translate_term(client: OllamaClient, korean_term: str) -> Optional[Dict]:\n",
    "    \"\"\"Translate a Korean term to English using LLM.\"\"\"\n",
    "    prompt = create_translation_prompt(korean_term)\n",
    "    response = client.generate(prompt, temperature=0.3)\n",
    "    \n",
    "    parsed = parse_json_response(response)\n",
    "    if parsed and isinstance(parsed, dict):\n",
    "        return {\n",
    "            \"ko\": korean_term,\n",
    "            \"en_primary\": parsed.get(\"primary\", \"\"),\n",
    "            \"en_alternatives\": parsed.get(\"alternatives\", []),\n",
    "            \"abbreviation\": parsed.get(\"abbreviation\")\n",
    "        }\n",
    "    return None\n",
    "\n",
    "\n",
    "def expand_category(client: OllamaClient, category: str, seed_terms: List[str]) -> List[str]:\n",
    "    \"\"\"Expand a category with additional terms using LLM.\"\"\"\n",
    "    prompt = create_expansion_prompt(category, seed_terms)\n",
    "    response = client.generate(prompt, temperature=0.8)\n",
    "    \n",
    "    parsed = parse_json_response(response)\n",
    "    if parsed and isinstance(parsed, list):\n",
    "        return [t for t in parsed if isinstance(t, str) and len(t) > 1]\n",
    "    return []\n",
    "\n",
    "\n",
    "def generate_parallel_sentences(\n",
    "    client: OllamaClient, \n",
    "    korean_term: str, \n",
    "    english_term: str\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"Generate parallel sentences for a term pair.\"\"\"\n",
    "    prompt = create_parallel_sentence_prompt(korean_term, english_term)\n",
    "    response = client.generate(prompt, temperature=0.7)\n",
    "    \n",
    "    parsed = parse_json_response(response)\n",
    "    if parsed and isinstance(parsed, list):\n",
    "        return [\n",
    "            {\"ko\": item.get(\"ko\", \"\"), \"en\": item.get(\"en\", \"\")}\n",
    "            for item in parsed\n",
    "            if isinstance(item, dict) and item.get(\"ko\") and item.get(\"en\")\n",
    "        ]\n",
    "    return []\n",
    "\n",
    "\n",
    "print(\"Generation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing translation...\n",
      "Translation result: {\n",
      "  \"ko\": \"머신러닝\",\n",
      "  \"en_primary\": \"machine learning\",\n",
      "  \"en_alternatives\": [\n",
      "    \"ML\"\n",
      "  ],\n",
      "  \"abbreviation\": \"ML\"\n",
      "}\n",
      "\n",
      "Testing category expansion...\n",
      "Expanded terms (20): ['인공신경망', '컨볼루션', '전이학습', '과적합', '정규화', '하이퍼파라미터', '배치정규화', '피처엔지니어링', '특성추출', '분류기']\n",
      "\n",
      "Testing parallel sentence generation...\n",
      "  KO: 머신러닝은 대규모 데이터에서 패턴을 학습하여 예측 모델을 생성하는 기계 학습 기술이다.\n",
      "  EN: Machine learning is a technology that learns patterns from large-scale data to create predictive models.\n",
      "\n",
      "  KO: 머신러닝 알고리즘은 입력 데이터와 목표 변수 간의 관계를 자동으로 최적화한다.\n",
      "  EN: Machine learning algorithms automatically optimize the relationship between input data and target variables.\n",
      "\n",
      "  KO: 머신러닝 프로젝트를 성공적으로 수행하려면 데이터 전처리, 모델 선택, 하이퍼파라미터 튜닝이 필수적이다.\n",
      "  EN: To successfully execute a machine learning project, data preprocessing, model selection, and hyperparameter tuning are essential.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test translation\n",
    "print(\"Testing translation...\")\n",
    "test_result = translate_term(client, \"머신러닝\")\n",
    "print(f\"Translation result: {json.dumps(test_result, ensure_ascii=False, indent=2)}\")\n",
    "\n",
    "print(\"\\nTesting category expansion...\")\n",
    "test_expansion = expand_category(client, \"machine_learning\", SEED_CATEGORIES[\"machine_learning\"][:5])\n",
    "print(f\"Expanded terms ({len(test_expansion)}): {test_expansion[:10]}\")\n",
    "\n",
    "print(\"\\nTesting parallel sentence generation...\")\n",
    "if test_result:\n",
    "    test_sentences = generate_parallel_sentences(client, \"머신러닝\", test_result[\"en_primary\"])\n",
    "    for sent in test_sentences:\n",
    "        print(f\"  KO: {sent['ko']}\")\n",
    "        print(f\"  EN: {sent['en']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Full Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/synonyms\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "OUTPUT_DIR = project_root / \"dataset\" / \"synonyms\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generation settings\n",
    "EXPAND_CATEGORIES = True  # Whether to expand categories with LLM\n",
    "GENERATE_SENTENCES = True  # Whether to generate parallel sentences\n",
    "SAMPLE_RATE_SENTENCES = 0.3  # Fraction of terms to generate sentences for\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generation function defined\n"
     ]
    }
   ],
   "source": [
    "def generate_full_dataset(\n",
    "    client: OllamaClient,\n",
    "    seed_categories: Dict[str, List[str]],\n",
    "    expand_categories: bool = True,\n",
    "    generate_sentences: bool = True,\n",
    "    sentence_sample_rate: float = 0.3,\n",
    ") -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Generate full cross-lingual dataset.\n",
    "    \n",
    "    Returns:\n",
    "        synonyms: List of synonym entries\n",
    "        parallel_sentences: List of parallel sentence pairs\n",
    "    \"\"\"\n",
    "    all_terms = []\n",
    "    synonyms = []\n",
    "    parallel_sentences = []\n",
    "    \n",
    "    # Step 1: Collect all terms (seed + expanded)\n",
    "    print(\"Step 1: Collecting terms...\")\n",
    "    for category, seeds in tqdm(seed_categories.items(), desc=\"Categories\"):\n",
    "        category_terms = list(seeds)  # Start with seeds\n",
    "        \n",
    "        if expand_categories:\n",
    "            try:\n",
    "                expanded = expand_category(client, category, seeds)\n",
    "                # Filter duplicates\n",
    "                new_terms = [t for t in expanded if t not in category_terms]\n",
    "                category_terms.extend(new_terms[:15])  # Limit expansion\n",
    "                print(f\"  {category}: {len(seeds)} seeds + {len(new_terms[:15])} expanded\")\n",
    "            except Exception as e:\n",
    "                print(f\"  {category}: expansion failed - {e}\")\n",
    "        \n",
    "        for term in category_terms:\n",
    "            all_terms.append({\"term\": term, \"category\": category})\n",
    "    \n",
    "    print(f\"\\nTotal terms to translate: {len(all_terms)}\")\n",
    "    \n",
    "    # Step 2: Translate all terms\n",
    "    print(\"\\nStep 2: Translating terms...\")\n",
    "    for item in tqdm(all_terms, desc=\"Translating\"):\n",
    "        try:\n",
    "            result = translate_term(client, item[\"term\"])\n",
    "            if result and result.get(\"en_primary\"):\n",
    "                result[\"category\"] = item[\"category\"]\n",
    "                synonyms.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to translate '{item['term']}': {e}\")\n",
    "        \n",
    "        # Rate limiting\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    print(f\"\\nSuccessfully translated: {len(synonyms)} terms\")\n",
    "    \n",
    "    # Step 3: Generate parallel sentences (sampled)\n",
    "    if generate_sentences:\n",
    "        print(\"\\nStep 3: Generating parallel sentences...\")\n",
    "        sampled = random.sample(synonyms, int(len(synonyms) * sentence_sample_rate))\n",
    "        \n",
    "        for entry in tqdm(sampled, desc=\"Generating sentences\"):\n",
    "            try:\n",
    "                sentences = generate_parallel_sentences(\n",
    "                    client, \n",
    "                    entry[\"ko\"], \n",
    "                    entry[\"en_primary\"]\n",
    "                )\n",
    "                for sent in sentences:\n",
    "                    sent[\"term_ko\"] = entry[\"ko\"]\n",
    "                    sent[\"term_en\"] = entry[\"en_primary\"]\n",
    "                    sent[\"category\"] = entry[\"category\"]\n",
    "                parallel_sentences.extend(sentences)\n",
    "            except Exception as e:\n",
    "                print(f\"  Failed for '{entry['ko']}': {e}\")\n",
    "            \n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        print(f\"\\nGenerated {len(parallel_sentences)} parallel sentences\")\n",
    "    \n",
    "    return synonyms, parallel_sentences\n",
    "\n",
    "\n",
    "print(\"Dataset generation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING FULL DATASET GENERATION\n",
      "============================================================\n",
      "Step 1: Collecting terms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51694bfa0ce44f8a96746899d5aa621b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Categories:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  machine_learning: 14 seeds + 14 expanded\n",
      "  natural_language_processing: 11 seeds + 15 expanded\n",
      "  computer_vision: 9 seeds + 15 expanded\n",
      "  data_science: 8 seeds + 15 expanded\n",
      "  software_engineering: 10 seeds + 15 expanded\n",
      "  cloud_infrastructure: 8 seeds + 15 expanded\n",
      "  search_retrieval: 9 seeds + 15 expanded\n",
      "\n",
      "Total terms to translate: 173\n",
      "\n",
      "Step 2: Translating terms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7656eaf41d30472191344968752d4ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully translated: 173 terms\n",
      "\n",
      "Step 3: Generating parallel sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d04dc7dac94120ba357cb3e2051be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating sentences:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 153 parallel sentences\n",
      "\n",
      "============================================================\n",
      "GENERATION COMPLETE\n",
      "============================================================\n",
      "Synonym entries: 173\n",
      "Parallel sentences: 153\n"
     ]
    }
   ],
   "source": [
    "# Generate dataset\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING FULL DATASET GENERATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "synonyms, parallel_sentences = generate_full_dataset(\n",
    "    client=client,\n",
    "    seed_categories=SEED_CATEGORIES,\n",
    "    expand_categories=EXPAND_CATEGORIES,\n",
    "    generate_sentences=GENERATE_SENTENCES,\n",
    "    sentence_sample_rate=SAMPLE_RATE_SENTENCES,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Synonym entries: {len(synonyms)}\")\n",
    "print(f\"Parallel sentences: {len(parallel_sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved synonyms to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/synonyms/ko_en_terms.jsonl\n",
      "Saved parallel sentences to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/synonyms/ko_en_parallel.jsonl\n",
      "Saved metadata to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/synonyms/metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Save synonyms\n",
    "synonyms_path = OUTPUT_DIR / \"ko_en_terms.jsonl\"\n",
    "with open(synonyms_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in synonyms:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved synonyms to: {synonyms_path}\")\n",
    "\n",
    "# Save parallel sentences\n",
    "sentences_path = OUTPUT_DIR / \"ko_en_parallel.jsonl\"\n",
    "with open(sentences_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in parallel_sentences:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved parallel sentences to: {sentences_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"total_synonyms\": len(synonyms),\n",
    "    \"total_parallel_sentences\": len(parallel_sentences),\n",
    "    \"categories\": list(SEED_CATEGORIES.keys()),\n",
    "    \"generation_model\": config.model,\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "}\n",
    "metadata_path = OUTPUT_DIR / \"metadata.json\"\n",
    "with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Saved metadata to: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SYNONYM DATASET ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Total entries: 173\n",
      "\n",
      "By category:\n",
      "category\n",
      "machine_learning               28\n",
      "natural_language_processing    26\n",
      "software_engineering           25\n",
      "computer_vision                24\n",
      "search_retrieval               24\n",
      "data_science                   23\n",
      "cloud_infrastructure           23\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample entries:\n",
      "  어휘 정규화 → lexical normalization\n",
      "    alternatives: ['vocabulary normalization', 'lexicon normalization']\n",
      "  활성화함수 → activation function\n",
      "    alternatives: ['activation function']\n",
      "    abbreviation: AF\n",
      "  가중치 → weight\n",
      "    alternatives: ['weighted coefficient', 'weight factor']\n",
      "    abbreviation: wt\n",
      "  언어모델 → language model\n",
      "    alternatives: ['linguistic model', 'language modeling']\n",
      "    abbreviation: LM\n",
      "  회귀분석 → regression analysis\n",
      "    alternatives: ['regression analysis']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and analyze\n",
    "df_synonyms = pd.DataFrame(synonyms)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SYNONYM DATASET ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTotal entries: {len(df_synonyms)}\")\n",
    "print(f\"\\nBy category:\")\n",
    "print(df_synonyms[\"category\"].value_counts())\n",
    "\n",
    "print(f\"\\nSample entries:\")\n",
    "for _, row in df_synonyms.sample(min(5, len(df_synonyms))).iterrows():\n",
    "    print(f\"  {row['ko']} → {row['en_primary']}\")\n",
    "    if row.get('en_alternatives'):\n",
    "        print(f\"    alternatives: {row['en_alternatives']}\")\n",
    "    if row.get('abbreviation'):\n",
    "        print(f\"    abbreviation: {row['abbreviation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PARALLEL SENTENCES ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Total sentences: 153\n",
      "\n",
      "By category:\n",
      "category\n",
      "computer_vision                30\n",
      "data_science                   27\n",
      "natural_language_processing    24\n",
      "search_retrieval               24\n",
      "cloud_infrastructure           18\n",
      "machine_learning               18\n",
      "software_engineering           12\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample sentences:\n",
      "  KO: 검색엔진스파이더는 웹 페이지를 탐색하여 정보를 수집하고 색인에 저장합니다.\n",
      "  EN: The search engine spider navigates web pages, collects information, and stores it in the index.\n",
      "  Term: 검색엔진스파이더 / search engine spider\n",
      "\n",
      "  KO: 배치 정규화는 딥러닝 모델의 학습을 안정화시키기 위해 각 미니배치의 평균과 분산을 정규화하는 기법이다.\n",
      "  EN: Batch Normalization is a technique that normalizes the mean and variance of each mini-batch to stabilize training in deep learning models.\n",
      "  Term: 배치 정규화 / Batch Normalization\n",
      "\n",
      "  KO: 컴퓨터비전 응용 예로는 자율주행 차량의 차선 인식, 의료 영상에서 종양 검출 등이 있다.\n",
      "  EN: Examples of Computer Vision applications include lane detection in autonomous vehicles and tumor detection in medical imaging.\n",
      "  Term: 컴퓨터비전 / Computer Vision\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if parallel_sentences:\n",
    "    df_sentences = pd.DataFrame(parallel_sentences)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"PARALLEL SENTENCES ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nTotal sentences: {len(df_sentences)}\")\n",
    "    print(f\"\\nBy category:\")\n",
    "    print(df_sentences[\"category\"].value_counts())\n",
    "    \n",
    "    print(f\"\\nSample sentences:\")\n",
    "    for _, row in df_sentences.sample(min(3, len(df_sentences))).iterrows():\n",
    "        print(f\"  KO: {row['ko']}\")\n",
    "        print(f\"  EN: {row['en']}\")\n",
    "        print(f\"  Term: {row['term_ko']} / {row['term_en']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Training Format\n",
    "\n",
    "Cross-lingual KD 학습에 사용할 형식으로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 173 training pairs\n",
      "Saved to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/synonyms/cross_lingual_pairs.jsonl\n",
      "\n",
      "Sample training pairs:\n",
      "  오버피팅 → ['overfitting', 'overfit', 'over-fitting']\n",
      "  학습 → ['learning', 'study', 'training']\n",
      "  정규화 → ['normalization', 'standardization', 'regularization', 'norm.']\n",
      "  모듈화 → ['modularization', 'modularization']\n",
      "  검색결과 → ['search results', 'search result', 'search outcome']\n"
     ]
    }
   ],
   "source": [
    "def create_training_pairs(synonyms: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Create training pairs for cross-lingual alignment.\n",
    "    \n",
    "    Format:\n",
    "    {\n",
    "        \"ko_term\": \"머신러닝\",\n",
    "        \"en_terms\": [\"machine learning\", \"ML\"],\n",
    "        \"category\": \"machine_learning\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for entry in synonyms:\n",
    "        en_terms = [entry[\"en_primary\"]]\n",
    "        if entry.get(\"en_alternatives\"):\n",
    "            en_terms.extend(entry[\"en_alternatives\"])\n",
    "        if entry.get(\"abbreviation\"):\n",
    "            en_terms.append(entry[\"abbreviation\"])\n",
    "        \n",
    "        # Filter empty/None\n",
    "        en_terms = [t for t in en_terms if t and isinstance(t, str)]\n",
    "        \n",
    "        if en_terms:\n",
    "            pairs.append({\n",
    "                \"ko_term\": entry[\"ko\"],\n",
    "                \"en_terms\": en_terms,\n",
    "                \"category\": entry.get(\"category\", \"unknown\")\n",
    "            })\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "training_pairs = create_training_pairs(synonyms)\n",
    "\n",
    "# Save training format\n",
    "training_path = OUTPUT_DIR / \"cross_lingual_pairs.jsonl\"\n",
    "with open(training_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for pair in training_pairs:\n",
    "        f.write(json.dumps(pair, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Created {len(training_pairs)} training pairs\")\n",
    "print(f\"Saved to: {training_path}\")\n",
    "\n",
    "# Show samples\n",
    "print(\"\\nSample training pairs:\")\n",
    "for pair in random.sample(training_pairs, min(5, len(training_pairs))):\n",
    "    print(f\"  {pair['ko_term']} → {pair['en_terms']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CROSS-LINGUAL DATA SYNTHESIS COMPLETE\n",
      "============================================================\n",
      "\n",
      "Generated files:\n",
      "  - ko_en_terms.jsonl: 173 synonym entries\n",
      "  - ko_en_parallel.jsonl: 153 parallel sentences\n",
      "  - cross_lingual_pairs.jsonl: 173 training pairs\n",
      "  - metadata.json: metadata\n",
      "\n",
      "Output directory: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/synonyms\n",
      "\n",
      "Next steps:\n",
      "  1. Review generated data quality\n",
      "  2. Implement CrossLingualKDLoss in src/training/losses.py\n",
      "  3. Update training notebook to use cross-lingual data\n",
      "  4. Run cross-lingual training\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CROSS-LINGUAL DATA SYNTHESIS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  - {synonyms_path.name}: {len(synonyms)} synonym entries\")\n",
    "print(f\"  - {sentences_path.name}: {len(parallel_sentences)} parallel sentences\")\n",
    "print(f\"  - {training_path.name}: {len(training_pairs)} training pairs\")\n",
    "print(f\"  - {metadata_path.name}: metadata\")\n",
    "\n",
    "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Review generated data quality\")\n",
    "print(f\"  2. Implement CrossLingualKDLoss in src/training/losses.py\")\n",
    "print(f\"  3. Update training notebook to use cross-lingual data\")\n",
    "print(f\"  4. Run cross-lingual training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

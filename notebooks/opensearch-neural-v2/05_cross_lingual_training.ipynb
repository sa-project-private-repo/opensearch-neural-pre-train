{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-lingual Neural Sparse Training\n",
    "\n",
    "기존 학습된 모델을 기반으로 Cross-lingual Knowledge Distillation을 적용합니다.\n",
    "\n",
    "## 목표\n",
    "- 한국어 용어 입력 시 영어 동의어 토큰도 활성화\n",
    "- \"머신러닝\" → [머신, ##닝, machine, learning, ML]\n",
    "- \"학습\" → [학, ##습, training, learning]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport json\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.amp import autocast, GradScaler\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom transformers import AutoTokenizer, AutoModel\n\n# Project root\nproject_root = Path.cwd().parent.parent\nsys.path.insert(0, str(project_root))\n\nfrom src.model.splade_model import SPLADEDoc\nfrom src.data.synonym_dataset import SynonymDataset, SynonymCollator\nfrom src.training.losses import SynonymAlignmentLoss, CrossLingualKDLoss, TokenExpansionLoss\n\nprint(f\"Project root: {project_root}\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass CrossLingualConfig:\n    \"\"\"Configuration for cross-lingual training.\"\"\"\n    # Model\n    base_checkpoint: str = \"outputs/baseline_dgx/best_model/checkpoint.pt\"\n    teacher_model: str = \"intfloat/multilingual-e5-large\"\n    \n    # Data\n    synonym_data: str = \"dataset/synonyms/cross_lingual_pairs.jsonl\"\n    parallel_data: str = \"dataset/synonyms/ko_en_parallel.jsonl\"\n    \n    # Training\n    batch_size: int = 32\n    learning_rate: float = 1e-5\n    num_epochs: int = 5  # More epochs for token expansion\n    warmup_steps: int = 100\n    max_length: int = 64\n    \n    # Loss weights\n    lambda_expansion: float = 1.0    # Token expansion loss (main loss)\n    lambda_kd: float = 0.3           # Teacher KD loss  \n    lambda_sparsity: float = 0.0001  # Sparsity regularization\n    \n    # Loss types\n    expansion_loss_type: str = \"additive\"  # soft_target, hard_target, additive, mse_expansion\n    expansion_top_k: int = 10              # Only expand to top-k EN tokens\n    kd_loss_type: str = \"relation\"         # relation, mse_relation, kl_relation\n    \n    # Output\n    output_dir: str = \"outputs/cross_lingual\"\n    log_steps: int = 10\n    save_steps: int = 500\n\n\nconfig = CrossLingualConfig()\n\n# Convert to absolute paths\nconfig.base_checkpoint = str(project_root / config.base_checkpoint)\nconfig.synonym_data = str(project_root / config.synonym_data)\nconfig.parallel_data = str(project_root / config.parallel_data)\nconfig.output_dir = str(project_root / config.output_dir)\n\n# Create output directory\nPath(config.output_dir).mkdir(parents=True, exist_ok=True)\n\nprint(\"Configuration:\")\nfor field, value in config.__dict__.items():\n    print(f\"  {field}: {value}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "checkpoint_path = Path(config.base_checkpoint)\n",
    "print(f\"Loading base model from: {checkpoint_path}\")\n",
    "\n",
    "if not checkpoint_path.exists():\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "print(f\"Checkpoint info:\")\n",
    "print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "print(f\"  Best val loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "\n",
    "base_config = checkpoint['config']\n",
    "model_name = base_config['model']['name']\n",
    "\n",
    "# Initialize model\n",
    "model = SPLADEDoc(\n",
    "    model_name=model_name,\n",
    "    dropout=base_config['model'].get('dropout', 0.1),\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\\nModel loaded: {model_name}\")\n",
    "print(f\"  Vocab size: {model.config.vocab_size}\")\n",
    "print(f\"  Hidden size: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading teacher model: {config.teacher_model}\")\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    teacher = SentenceTransformer(\n",
    "        config.teacher_model,\n",
    "        device=str(device),\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    teacher.eval()\n",
    "    \n",
    "    # Freeze teacher\n",
    "    for param in teacher.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    print(f\"Teacher loaded successfully\")\n",
    "    print(f\"  Embedding dimension: {teacher.get_sentence_embedding_dimension()}\")\n",
    "    USE_TEACHER = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load teacher model: {e}\")\n",
    "    print(\"Will use synonym alignment only (no KD)\")\n",
    "    teacher = None\n",
    "    USE_TEACHER = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Synonym Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if synonym data exists\n",
    "synonym_path = Path(config.synonym_data)\n",
    "\n",
    "if not synonym_path.exists():\n",
    "    print(f\"Synonym data not found at: {synonym_path}\")\n",
    "    print(\"\\nPlease run notebook 04_cross_lingual_data_synthesis.ipynb first.\")\n",
    "    print(\"Or provide synonym data in the expected format.\")\n",
    "    raise FileNotFoundError(f\"Synonym data not found: {synonym_path}\")\n",
    "\n",
    "# Load dataset\n",
    "synonym_dataset = SynonymDataset(str(synonym_path))\n",
    "print(f\"Loaded {len(synonym_dataset)} synonym pairs\")\n",
    "\n",
    "# Show samples\n",
    "print(\"\\nSample pairs:\")\n",
    "for i in range(min(5, len(synonym_dataset))):\n",
    "    item = synonym_dataset[i]\n",
    "    print(f\"  {item['ko_term']} → {item['en_term']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer (same as base model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create collator\n",
    "collator = SynonymCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=config.max_length,\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "train_loader = DataLoader(\n",
    "    synonym_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created:\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Num batches: {len(train_loader)}\")\n",
    "\n",
    "# Test batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch:\")\n",
    "for key, value in sample_batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value).__name__} (len={len(value)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Token expansion loss (main loss for activating EN tokens)\nexpansion_loss_fn = TokenExpansionLoss(\n    expansion_type=config.expansion_loss_type,\n    top_k=config.expansion_top_k,\n)\n\n# KD loss (if teacher available)\nif USE_TEACHER:\n    kd_loss_fn = CrossLingualKDLoss(\n        loss_type=config.kd_loss_type,\n    )\nelse:\n    kd_loss_fn = None\n\nprint(f\"Loss functions initialized:\")\nprint(f\"  Token expansion: {config.expansion_loss_type} (top_k={config.expansion_top_k})\")\nprint(f\"  KD loss: {config.kd_loss_type if USE_TEACHER else 'disabled'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only train certain layers (optional: freeze transformer, only train projection)\n",
    "# For full fine-tuning:\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * config.num_epochs\n",
    "warmup_steps = config.warmup_steps\n",
    "\n",
    "def lr_lambda(step: int) -> float:\n",
    "    if step < warmup_steps:\n",
    "        return step / warmup_steps\n",
    "    return max(0.1, 1.0 - (step - warmup_steps) / (total_steps - warmup_steps))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "print(f\"Optimizer initialized:\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_step(\n    batch: Dict[str, torch.Tensor],\n    model: SPLADEDoc,\n    teacher: Optional[SentenceTransformer],\n    expansion_loss_fn: TokenExpansionLoss,\n    kd_loss_fn: Optional[CrossLingualKDLoss],\n    config: CrossLingualConfig,\n) -> Dict[str, float]:\n    \"\"\"\n    Single training step for cross-lingual token expansion.\n    \n    Key insight: We encode BOTH Korean and English through the model,\n    then train KO representation to also activate EN's top tokens.\n    \"\"\"\n    # Move inputs to device\n    ko_input_ids = batch['ko_input_ids'].to(device)\n    ko_attention_mask = batch['ko_attention_mask'].to(device)\n    en_input_ids = batch['en_input_ids'].to(device)\n    en_attention_mask = batch['en_attention_mask'].to(device)\n    \n    # Forward pass - get sparse representations for BOTH\n    ko_sparse, _ = model(ko_input_ids, ko_attention_mask)\n    \n    # Get EN sparse rep (as target, detach to not backprop through EN)\n    with torch.no_grad():\n        en_sparse, _ = model(en_input_ids, en_attention_mask)\n    \n    losses = {}\n    \n    # 1. Token expansion loss (MAIN LOSS)\n    # Goal: KO sparse should activate the same tokens as EN sparse\n    expansion_loss = expansion_loss_fn(ko_sparse, en_sparse)\n    losses['expansion_loss'] = expansion_loss.item()\n    \n    # 2. Teacher KD loss (optional, for cross-lingual guidance)\n    if teacher is not None and kd_loss_fn is not None:\n        with torch.no_grad():\n            ko_terms = batch['ko_terms']\n            en_terms = batch['en_terms']\n            \n            ko_teacher = teacher.encode(\n                ko_terms, \n                convert_to_tensor=True,\n                normalize_embeddings=True,\n            )\n            en_teacher = teacher.encode(\n                en_terms,\n                convert_to_tensor=True,\n                normalize_embeddings=True,\n            )\n        \n        # Need to re-encode EN with gradient for KD\n        en_sparse_grad, _ = model(en_input_ids, en_attention_mask)\n        kd_loss = kd_loss_fn(ko_sparse, en_sparse_grad, ko_teacher, en_teacher)\n        losses['kd_loss'] = kd_loss.item()\n    else:\n        kd_loss = torch.tensor(0.0, device=device)\n        losses['kd_loss'] = 0.0\n    \n    # 3. Sparsity regularization (L1) - only on KO since EN is target\n    sparsity_loss = ko_sparse.abs().mean()\n    losses['sparsity_loss'] = sparsity_loss.item()\n    \n    # Combined loss\n    total_loss = (\n        config.lambda_expansion * expansion_loss\n        + config.lambda_kd * kd_loss\n        + config.lambda_sparsity * sparsity_loss\n    )\n    losses['total_loss'] = total_loss.item()\n    \n    return total_loss, losses\n\n\nprint(\"Training step function defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_epoch(\n    model: SPLADEDoc,\n    teacher: Optional[SentenceTransformer],\n    train_loader: DataLoader,\n    optimizer: torch.optim.Optimizer,\n    scheduler: torch.optim.lr_scheduler.LRScheduler,\n    scaler: GradScaler,\n    expansion_loss_fn: TokenExpansionLoss,\n    kd_loss_fn: Optional[CrossLingualKDLoss],\n    config: CrossLingualConfig,\n    epoch: int,\n) -> Dict[str, float]:\n    \"\"\"\n    Train for one epoch.\n    \"\"\"\n    model.train()\n    \n    epoch_losses = {\n        'total_loss': 0.0,\n        'expansion_loss': 0.0,\n        'kd_loss': 0.0,\n        'sparsity_loss': 0.0,\n    }\n    \n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n    \n    for step, batch in enumerate(pbar):\n        optimizer.zero_grad()\n        \n        # Mixed precision forward\n        with autocast('cuda', dtype=torch.bfloat16):\n            total_loss, losses = train_step(\n                batch, model, teacher,\n                expansion_loss_fn, kd_loss_fn, config\n            )\n        \n        # Backward\n        scaler.scale(total_loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n        \n        # Accumulate losses\n        for key in epoch_losses:\n            epoch_losses[key] += losses.get(key, 0.0)\n        \n        # Update progress bar\n        if step % config.log_steps == 0:\n            pbar.set_postfix({\n                'loss': f\"{losses['total_loss']:.4f}\",\n                'exp': f\"{losses['expansion_loss']:.4f}\",\n                'lr': f\"{scheduler.get_last_lr()[0]:.2e}\",\n            })\n    \n    # Average losses\n    num_batches = len(train_loader)\n    for key in epoch_losses:\n        epoch_losses[key] /= num_batches\n    \n    return epoch_losses\n\n\nprint(\"Training epoch function defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"STARTING CROSS-LINGUAL TOKEN EXPANSION TRAINING\")\nprint(\"=\"*60)\n\nhistory = []\nbest_loss = float('inf')\n\nfor epoch in range(config.num_epochs):\n    print(f\"\\n--- Epoch {epoch+1}/{config.num_epochs} ---\")\n    \n    epoch_losses = train_epoch(\n        model=model,\n        teacher=teacher,\n        train_loader=train_loader,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        scaler=scaler,\n        expansion_loss_fn=expansion_loss_fn,\n        kd_loss_fn=kd_loss_fn,\n        config=config,\n        epoch=epoch,\n    )\n    \n    history.append(epoch_losses)\n    \n    print(f\"\\nEpoch {epoch+1} Results:\")\n    for key, value in epoch_losses.items():\n        print(f\"  {key}: {value:.4f}\")\n    \n    # Save checkpoint\n    if epoch_losses['total_loss'] < best_loss:\n        best_loss = epoch_losses['total_loss']\n        \n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'best_loss': best_loss,\n            'config': {\n                'model': {'name': model_name},\n                'cross_lingual': config.__dict__,\n            },\n        }\n        \n        save_path = Path(config.output_dir) / 'best_model' / 'checkpoint.pt'\n        save_path.parent.mkdir(parents=True, exist_ok=True)\n        torch.save(checkpoint, save_path)\n        print(f\"  Saved best model to: {save_path}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluation - Cross-lingual Activation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cross_lingual_activation(\n",
    "    model: SPLADEDoc,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    test_pairs: List[Tuple[str, List[str]]],\n",
    "    top_k: int = 20,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Test if Korean terms activate English synonym tokens.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        tokenizer: Tokenizer\n",
    "        test_pairs: List of (korean_term, [english_synonyms])\n",
    "        top_k: Number of top tokens to check\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation results\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for ko_term, en_synonyms in test_pairs:\n",
    "        # Encode Korean term\n",
    "        encoding = tokenizer(\n",
    "            ko_term,\n",
    "            max_length=64,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            sparse_rep, _ = model(\n",
    "                encoding['input_ids'].to(device),\n",
    "                encoding['attention_mask'].to(device),\n",
    "            )\n",
    "        \n",
    "        sparse_rep = sparse_rep[0].cpu()\n",
    "        \n",
    "        # Get top-k activated tokens\n",
    "        top_k_values, top_k_indices = torch.topk(sparse_rep, k=top_k)\n",
    "        top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices.tolist())\n",
    "        \n",
    "        # Check English synonym activation\n",
    "        en_activated = []\n",
    "        for en_syn in en_synonyms:\n",
    "            # Tokenize English synonym\n",
    "            en_tokens = tokenizer.tokenize(en_syn.lower())\n",
    "            \n",
    "            # Check if any token is in top-k\n",
    "            for en_tok in en_tokens:\n",
    "                if en_tok in top_k_tokens or en_tok.lower() in [t.lower() for t in top_k_tokens]:\n",
    "                    en_activated.append(en_tok)\n",
    "        \n",
    "        results.append({\n",
    "            'ko_term': ko_term,\n",
    "            'en_synonyms': en_synonyms,\n",
    "            'top_k_tokens': top_k_tokens,\n",
    "            'top_k_values': top_k_values.tolist(),\n",
    "            'en_activated': en_activated,\n",
    "            'activation_rate': len(set(en_activated)) / len(en_synonyms) if en_synonyms else 0,\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test pairs\n",
    "TEST_PAIRS = [\n",
    "    (\"머신러닝\", [\"machine\", \"learning\", \"ML\"]),\n",
    "    (\"딥러닝\", [\"deep\", \"learning\", \"DL\"]),\n",
    "    (\"자연어처리\", [\"natural\", \"language\", \"processing\", \"NLP\"]),\n",
    "    (\"학습\", [\"training\", \"learning\"]),\n",
    "    (\"모델\", [\"model\"]),\n",
    "    (\"데이터\", [\"data\"]),\n",
    "    (\"알고리즘\", [\"algorithm\"]),\n",
    "    (\"신경망\", [\"neural\", \"network\"]),\n",
    "]\n",
    "\n",
    "print(\"Test pairs defined:\")\n",
    "for ko, en_list in TEST_PAIRS:\n",
    "    print(f\"  {ko} → {en_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "print(\"=\"*60)\n",
    "print(\"CROSS-LINGUAL ACTIVATION TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = test_cross_lingual_activation(model, tokenizer, TEST_PAIRS, top_k=30)\n",
    "\n",
    "total_activation_rate = 0\n",
    "\n",
    "for result in results:\n",
    "    print(f\"\\n[{result['ko_term']}]\")\n",
    "    print(f\"  Expected EN: {result['en_synonyms']}\")\n",
    "    print(f\"  Top-10 activated: {result['top_k_tokens'][:10]}\")\n",
    "    print(f\"  EN activated: {result['en_activated']}\")\n",
    "    print(f\"  Activation rate: {result['activation_rate']:.1%}\")\n",
    "    total_activation_rate += result['activation_rate']\n",
    "\n",
    "avg_rate = total_activation_rate / len(results)\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"Average English Activation Rate: {avg_rate:.1%}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "history_path = Path(config.output_dir) / 'training_history.json'\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(f\"Saved training history to: {history_path}\")\n",
    "\n",
    "# Save evaluation results\n",
    "eval_path = Path(config.output_dir) / 'evaluation_results.json'\n",
    "eval_results = {\n",
    "    'test_pairs': TEST_PAIRS,\n",
    "    'results': results,\n",
    "    'average_activation_rate': avg_rate,\n",
    "}\n",
    "with open(eval_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(eval_results, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Saved evaluation results to: {eval_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CROSS-LINGUAL TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOutput directory: {config.output_dir}\")\n",
    "print(f\"Best model: {config.output_dir}/best_model/checkpoint.pt\")\n",
    "print(f\"Average EN activation: {avg_rate:.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-lingual Neural Sparse Training with Vocabulary Expansion\n",
    "\n",
    "기존 학습된 모델을 기반으로 Cross-lingual Training을 적용합니다.\n",
    "\n",
    "## 핵심 변경: SPLADEDocExpansion 사용\n",
    "기존 SPLADEDoc은 입력 토큰만 활성화할 수 있는 구조적 한계가 있습니다.\n",
    "SPLADEDocExpansion은 MLM 헤드를 사용해 전체 어휘 공간으로 투영하여 \n",
    "입력에 없는 토큰도 활성화할 수 있습니다.\n",
    "\n",
    "## 목표\n",
    "- 한국어 용어 입력 시 영어 동의어 토큰도 활성화\n",
    "- \"머신러닝\" → [머신, ##닝, machine, learning, ML]\n",
    "- \"학습\" → [학, ##습, training, learning]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Project root\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# IMPORTANT: Use SPLADEDocExpansion instead of SPLADEDoc\n",
    "# SPLADEDocExpansion uses MLM head to project to full vocabulary space,\n",
    "# enabling activation of ANY token (not just input tokens)\n",
    "from src.model.splade_model import SPLADEDocExpansion, create_splade_model\n",
    "from src.data.synonym_dataset import SynonymDataset, SynonymCollator\n",
    "from src.training.losses import (\n",
    "    SynonymAlignmentLoss, \n",
    "    CrossLingualKDLoss, \n",
    "    TokenExpansionLoss,\n",
    "    ExplicitNoiseTokenLoss,  # v4: Explicit noise token penalty (safer than frequency-based)\n",
    ")\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CrossLingualConfig:\n",
    "    \"\"\"Configuration for cross-lingual training with vocabulary expansion.\"\"\"\n",
    "    # Model - Use SPLADEDocExpansion (MLM-based)\n",
    "    base_model: str = \"bert-base-multilingual-cased\"\n",
    "    expansion_mode: str = \"mlm\"  # Use MLM head for vocabulary expansion\n",
    "    \n",
    "    # Teacher model for KD guidance\n",
    "    teacher_model: str = \"intfloat/multilingual-e5-large\"\n",
    "    \n",
    "    # Data - use LARGE-SCALE dataset (3.2M+ pairs from XLEnt, CCMatrix, Wikidata)\n",
    "    synonym_data: str = \"dataset/large_scale/ko_en_terms_merged.jsonl\"\n",
    "    parallel_data: str = \"dataset/synonyms/ko_en_parallel.jsonl\"\n",
    "    \n",
    "    # Training - adjusted for large-scale\n",
    "    batch_size: int = 128  # Increased for large dataset\n",
    "    learning_rate: float = 2e-5\n",
    "    num_epochs: int = 3  # Fewer epochs with more data\n",
    "    warmup_steps: int = 1000  # More warmup for larger dataset\n",
    "    max_length: int = 64\n",
    "    \n",
    "    # Loss weights\n",
    "    lambda_expansion: float = 1.0    # Token expansion loss (main loss)\n",
    "    lambda_kd: float = 0.3           # Teacher KD loss  \n",
    "    lambda_sparsity: float = 0.001   # L1 sparsity regularization\n",
    "    lambda_noise: float = 0.3        # v4: Explicit noise token penalty\n",
    "    \n",
    "    # Loss types\n",
    "    expansion_loss_type: str = \"additive\"\n",
    "    expansion_top_k: int = 10\n",
    "    kd_loss_type: str = \"relation\"\n",
    "    \n",
    "    # v4: Explicit noise token config (safer than frequency-based)\n",
    "    noise_penalty_type: str = \"sum\"  # 'sum', 'max', 'softmax'\n",
    "    # Custom noise tokens (observed in Top-10)\n",
    "    custom_noise_tokens: tuple = (\n",
    "        # Programming terms (high activation, low relevance)\n",
    "        \"function\", \"operator\", \"operation\", \"operations\",\n",
    "        \"programming\", \"integration\", \"organization\",\n",
    "        \"implementation\", \"configuration\", \"application\",\n",
    "        # Generic terms\n",
    "        \"system\", \"systems\", \"process\", \"processing\",\n",
    "        \"method\", \"methods\", \"type\", \"types\",\n",
    "        # Subword noise\n",
    "        \"##ing\", \"##tion\", \"##ation\", \"##ment\",\n",
    "        # Common but uninformative\n",
    "        \"the\", \"and\", \"for\", \"with\", \"from\",\n",
    "    )\n",
    "    \n",
    "    # Output\n",
    "    output_dir: str = \"outputs/cross_lingual_expansion_v5_largescale\"\n",
    "    log_steps: int = 100\n",
    "    save_steps: int = 5000\n",
    "\n",
    "\n",
    "config = CrossLingualConfig()\n",
    "\n",
    "# Convert to absolute paths\n",
    "config.synonym_data = str(project_root / config.synonym_data)\n",
    "config.parallel_data = str(project_root / config.parallel_data)\n",
    "config.output_dir = str(project_root / config.output_dir)\n",
    "\n",
    "# Create output directory\n",
    "Path(config.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configuration (v5 - LARGE-SCALE with 3.2M+ pairs):\")\n",
    "for field, value in config.__dict__.items():\n",
    "    print(f\"  {field}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create SPLADEDocExpansion Model\n",
    "\n",
    "SPLADEDocExpansion은 MLM 헤드를 사용하여 입력 토큰 외의 토큰도 활성화할 수 있습니다.\n",
    "\n",
    "**핵심 차이점:**\n",
    "- SPLADEDoc: `sparse_repr` 생성 시 `input_ids`에 있는 토큰만 활성화 가능 (scatter 연산)\n",
    "- SPLADEDocExpansion: MLM logits를 사용하여 전체 어휘(vocab_size)에 대해 활성화 점수 계산\n",
    "\n",
    "이를 통해 \"머신러닝\" 입력 시 \"machine\", \"learning\" 토큰도 활성화될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SPLADEDocExpansion model from scratch (using MLM head)\n",
    "# This model can activate ANY token in vocabulary, not just input tokens\n",
    "\n",
    "print(f\"Creating SPLADEDocExpansion model...\")\n",
    "print(f\"  Base model: {config.base_model}\")\n",
    "print(f\"  Expansion mode: {config.expansion_mode}\")\n",
    "\n",
    "# Use factory function with use_expansion=True\n",
    "model = create_splade_model(\n",
    "    model_name=config.base_model,\n",
    "    use_idf=False,\n",
    "    use_expansion=True,\n",
    "    expansion_mode=config.expansion_mode,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\\nModel created: SPLADEDocExpansion\")\n",
    "print(f\"  Vocab size: {model.config.vocab_size}\")\n",
    "print(f\"  Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"  Expansion mode: {model.expansion_mode}\")\n",
    "\n",
    "# Verify model can activate non-input tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model)\n",
    "test_input = tokenizer(\"머신러닝\", return_tensors=\"pt\", max_length=64, padding=\"max_length\")\n",
    "with torch.no_grad():\n",
    "    test_sparse, _ = model(\n",
    "        test_input['input_ids'].to(device),\n",
    "        test_input['attention_mask'].to(device),\n",
    "    )\n",
    "\n",
    "# Check if non-input tokens can be activated\n",
    "non_zero_count = (test_sparse[0] > 0).sum().item()\n",
    "print(f\"\\nInitial activation test:\")\n",
    "print(f\"  Non-zero tokens in sparse repr: {non_zero_count}\")\n",
    "print(f\"  (Pre-training: random activations from MLM head)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Teacher Model (Optional)\n",
    "\n",
    "Teacher 모델은 cross-lingual semantic similarity를 가이드하는 역할을 합니다.\n",
    "주요 기능: KO-EN 쌍의 semantic similarity를 relation-based KD로 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading teacher model: {config.teacher_model}\")\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    teacher = SentenceTransformer(\n",
    "        config.teacher_model,\n",
    "        device=str(device),\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    teacher.eval()\n",
    "    \n",
    "    # Freeze teacher\n",
    "    for param in teacher.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    print(f\"Teacher loaded successfully\")\n",
    "    print(f\"  Embedding dimension: {teacher.get_sentence_embedding_dimension()}\")\n",
    "    USE_TEACHER = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load teacher model: {e}\")\n",
    "    print(\"Will use synonym alignment only (no KD)\")\n",
    "    teacher = None\n",
    "    USE_TEACHER = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Synonym Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if synonym data exists\n",
    "synonym_path = Path(config.synonym_data)\n",
    "\n",
    "if not synonym_path.exists():\n",
    "    print(f\"Synonym data not found at: {synonym_path}\")\n",
    "    print(\"\\nPlease run notebook 04_cross_lingual_data_synthesis.ipynb first.\")\n",
    "    print(\"Or provide synonym data in the expected format.\")\n",
    "    raise FileNotFoundError(f\"Synonym data not found: {synonym_path}\")\n",
    "\n",
    "# Load dataset\n",
    "synonym_dataset = SynonymDataset(str(synonym_path))\n",
    "print(f\"Loaded {len(synonym_dataset)} synonym pairs\")\n",
    "\n",
    "# Show samples\n",
    "print(\"\\nSample pairs:\")\n",
    "for i in range(min(5, len(synonym_dataset))):\n",
    "    item = synonym_dataset[i]\n",
    "    print(f\"  {item['ko_term']} → {item['en_term']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer was already created in cell-6, reuse it\n",
    "# If running from this cell, uncomment the line below:\n",
    "# tokenizer = AutoTokenizer.from_pretrained(config.base_model)\n",
    "\n",
    "# Create collator\n",
    "collator = SynonymCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=config.max_length,\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "train_loader = DataLoader(\n",
    "    synonym_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created:\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Num batches: {len(train_loader)}\")\n",
    "\n",
    "# Test batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch:\")\n",
    "for key, value in sample_batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value).__name__} (len={len(value)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token expansion loss (main loss for activating EN tokens)\n",
    "expansion_loss_fn = TokenExpansionLoss(\n",
    "    expansion_type=config.expansion_loss_type,\n",
    "    top_k=config.expansion_top_k,\n",
    ")\n",
    "\n",
    "# KD loss (if teacher available)\n",
    "if USE_TEACHER:\n",
    "    kd_loss_fn = CrossLingualKDLoss(\n",
    "        loss_type=config.kd_loss_type,\n",
    "    )\n",
    "else:\n",
    "    kd_loss_fn = None\n",
    "\n",
    "# v4: Explicit noise token loss (safer than frequency-based)\n",
    "# This explicitly penalizes known noise tokens instead of using statistical patterns\n",
    "noise_loss_fn = ExplicitNoiseTokenLoss(\n",
    "    tokenizer=tokenizer,\n",
    "    noise_tokens=list(config.custom_noise_tokens),\n",
    "    lambda_noise=1.0,  # Will be scaled by config.lambda_noise in train_step\n",
    "    penalty_type=config.noise_penalty_type,\n",
    ")\n",
    "\n",
    "print(f\"\\nLoss functions initialized:\")\n",
    "print(f\"  Token expansion: {config.expansion_loss_type} (top_k={config.expansion_top_k})\")\n",
    "print(f\"  KD loss: {config.kd_loss_type if USE_TEACHER else 'disabled'}\")\n",
    "print(f\"  Noise penalty: ExplicitNoiseTokenLoss (type={config.noise_penalty_type}, lambda={config.lambda_noise})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only train certain layers (optional: freeze transformer, only train projection)\n",
    "# For full fine-tuning:\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * config.num_epochs\n",
    "warmup_steps = config.warmup_steps\n",
    "\n",
    "def lr_lambda(step: int) -> float:\n",
    "    if step < warmup_steps:\n",
    "        return step / warmup_steps\n",
    "    return max(0.1, 1.0 - (step - warmup_steps) / (total_steps - warmup_steps))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "print(f\"Optimizer initialized:\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    batch: Dict[str, torch.Tensor],\n",
    "    model: SPLADEDocExpansion,\n",
    "    teacher: Optional['SentenceTransformer'],\n",
    "    expansion_loss_fn: TokenExpansionLoss,\n",
    "    kd_loss_fn: Optional[CrossLingualKDLoss],\n",
    "    noise_loss_fn: ExplicitNoiseTokenLoss,\n",
    "    config: CrossLingualConfig,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Single training step for cross-lingual token expansion with noise suppression.\n",
    "    \n",
    "    Key insight: We encode BOTH Korean and English through the model,\n",
    "    then train KO representation to also activate EN's top tokens.\n",
    "    \n",
    "    v4: ExplicitNoiseTokenLoss directly penalizes known noise tokens.\n",
    "    This is safer than frequency-based approach which can suppress useful tokens.\n",
    "    \"\"\"\n",
    "    # Move inputs to device\n",
    "    ko_input_ids = batch['ko_input_ids'].to(device)\n",
    "    ko_attention_mask = batch['ko_attention_mask'].to(device)\n",
    "    en_input_ids = batch['en_input_ids'].to(device)\n",
    "    en_attention_mask = batch['en_attention_mask'].to(device)\n",
    "    \n",
    "    # Forward pass - get sparse representations for BOTH\n",
    "    ko_sparse, _ = model(ko_input_ids, ko_attention_mask)\n",
    "    \n",
    "    # Get EN sparse rep (as target, detach to not backprop through EN)\n",
    "    with torch.no_grad():\n",
    "        en_sparse, _ = model(en_input_ids, en_attention_mask)\n",
    "    \n",
    "    losses = {}\n",
    "    \n",
    "    # 1. Token expansion loss (MAIN LOSS)\n",
    "    expansion_loss = expansion_loss_fn(ko_sparse, en_sparse)\n",
    "    losses['expansion_loss'] = expansion_loss.item()\n",
    "    \n",
    "    # 2. Teacher KD loss (optional)\n",
    "    if teacher is not None and kd_loss_fn is not None:\n",
    "        with torch.no_grad():\n",
    "            ko_terms = batch['ko_terms']\n",
    "            en_terms = batch['en_terms']\n",
    "            \n",
    "            ko_teacher = teacher.encode(\n",
    "                ko_terms, \n",
    "                convert_to_tensor=True,\n",
    "                normalize_embeddings=True,\n",
    "            )\n",
    "            en_teacher = teacher.encode(\n",
    "                en_terms,\n",
    "                convert_to_tensor=True,\n",
    "                normalize_embeddings=True,\n",
    "            )\n",
    "        \n",
    "        en_sparse_grad, _ = model(en_input_ids, en_attention_mask)\n",
    "        kd_loss = kd_loss_fn(ko_sparse, en_sparse_grad, ko_teacher, en_teacher)\n",
    "        losses['kd_loss'] = kd_loss.item()\n",
    "    else:\n",
    "        kd_loss = torch.tensor(0.0, device=device)\n",
    "        losses['kd_loss'] = 0.0\n",
    "    \n",
    "    # 3. Sparsity regularization (L1)\n",
    "    sparsity_loss = ko_sparse.abs().mean()\n",
    "    losses['sparsity_loss'] = sparsity_loss.item()\n",
    "    \n",
    "    # 4. v4: Explicit noise token penalty (suppress known noise tokens)\n",
    "    # This directly penalizes tokens like 'function', 'operator', etc.\n",
    "    noise_loss = noise_loss_fn(ko_sparse)\n",
    "    losses['noise_loss'] = noise_loss.item()\n",
    "    \n",
    "    # Combined loss\n",
    "    total_loss = (\n",
    "        config.lambda_expansion * expansion_loss\n",
    "        + config.lambda_kd * kd_loss\n",
    "        + config.lambda_sparsity * sparsity_loss\n",
    "        + config.lambda_noise * noise_loss  # v4: explicit noise penalty\n",
    "    )\n",
    "    losses['total_loss'] = total_loss.item()\n",
    "    \n",
    "    return total_loss, losses\n",
    "\n",
    "\n",
    "print(\"Training step function defined (v4 with ExplicitNoiseTokenLoss)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: SPLADEDocExpansion,\n",
    "    teacher: Optional['SentenceTransformer'],\n",
    "    train_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "    scaler: GradScaler,\n",
    "    expansion_loss_fn: TokenExpansionLoss,\n",
    "    kd_loss_fn: Optional[CrossLingualKDLoss],\n",
    "    noise_loss_fn: ExplicitNoiseTokenLoss,\n",
    "    config: CrossLingualConfig,\n",
    "    epoch: int,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Train for one epoch with SPLADEDocExpansion and explicit noise suppression.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    epoch_losses = {\n",
    "        'total_loss': 0.0,\n",
    "        'expansion_loss': 0.0,\n",
    "        'kd_loss': 0.0,\n",
    "        'sparsity_loss': 0.0,\n",
    "        'noise_loss': 0.0,  # v4: explicit noise loss\n",
    "    }\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n",
    "    \n",
    "    for step, batch in enumerate(pbar):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed precision forward\n",
    "        with autocast('cuda', dtype=torch.bfloat16):\n",
    "            total_loss, losses = train_step(\n",
    "                batch, model, teacher,\n",
    "                expansion_loss_fn, kd_loss_fn, noise_loss_fn, config\n",
    "            )\n",
    "        \n",
    "        # Backward\n",
    "        scaler.scale(total_loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Accumulate losses\n",
    "        for key in epoch_losses:\n",
    "            epoch_losses[key] += losses.get(key, 0.0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        if step % config.log_steps == 0:\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{losses['total_loss']:.4f}\",\n",
    "                'exp': f\"{losses['expansion_loss']:.4f}\",\n",
    "                'noise': f\"{losses['noise_loss']:.4f}\",\n",
    "                'lr': f\"{scheduler.get_last_lr()[0]:.2e}\",\n",
    "            })\n",
    "    \n",
    "    # Average losses\n",
    "    num_batches = len(train_loader)\n",
    "    for key in epoch_losses:\n",
    "        epoch_losses[key] /= num_batches\n",
    "    \n",
    "    return epoch_losses\n",
    "\n",
    "\n",
    "print(\"Training epoch function defined (v4 with ExplicitNoiseTokenLoss)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STARTING CROSS-LINGUAL TRAINING (v4 with ExplicitNoiseTokenLoss)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    print(f\"\\n--- Epoch {epoch+1}/{config.num_epochs} ---\")\n",
    "    \n",
    "    epoch_losses = train_epoch(\n",
    "        model=model,\n",
    "        teacher=teacher,\n",
    "        train_loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        scaler=scaler,\n",
    "        expansion_loss_fn=expansion_loss_fn,\n",
    "        kd_loss_fn=kd_loss_fn,\n",
    "        noise_loss_fn=noise_loss_fn,  # v4: explicit noise penalty\n",
    "        config=config,\n",
    "        epoch=epoch,\n",
    "    )\n",
    "    \n",
    "    history.append(epoch_losses)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} Results:\")\n",
    "    for key, value in epoch_losses.items():\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if epoch_losses['total_loss'] < best_loss:\n",
    "        best_loss = epoch_losses['total_loss']\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_loss': best_loss,\n",
    "            'config': {\n",
    "                'model': {'name': config.base_model},\n",
    "                'cross_lingual': config.__dict__,\n",
    "            },\n",
    "        }\n",
    "        \n",
    "        save_path = Path(config.output_dir) / 'best_model' / 'checkpoint.pt'\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(checkpoint, save_path)\n",
    "        print(f\"  Saved best model to: {save_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluation - Cross-lingual Activation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cross_lingual_activation(\n",
    "    model: SPLADEDocExpansion,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    test_pairs: List[Tuple[str, List[str]]],\n",
    "    top_k: int = 30,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Test if Korean terms activate English synonym tokens.\n",
    "    \n",
    "    With SPLADEDocExpansion, the model should be able to activate\n",
    "    English tokens even when the input is only Korean.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained SPLADEDocExpansion model\n",
    "        tokenizer: Tokenizer\n",
    "        test_pairs: List of (korean_term, [english_synonyms])\n",
    "        top_k: Number of top tokens to check\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation results\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for ko_term, en_synonyms in test_pairs:\n",
    "        # Encode Korean term\n",
    "        encoding = tokenizer(\n",
    "            ko_term,\n",
    "            max_length=64,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            sparse_rep, _ = model(\n",
    "                encoding['input_ids'].to(device),\n",
    "                encoding['attention_mask'].to(device),\n",
    "            )\n",
    "        \n",
    "        sparse_rep = sparse_rep[0].cpu()\n",
    "        \n",
    "        # Get top-k activated tokens\n",
    "        top_k_values, top_k_indices = torch.topk(sparse_rep, k=top_k)\n",
    "        top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices.tolist())\n",
    "        \n",
    "        # Check English synonym activation\n",
    "        en_activated = []\n",
    "        en_scores = {}\n",
    "        \n",
    "        for en_syn in en_synonyms:\n",
    "            # Tokenize English synonym\n",
    "            en_tokens = tokenizer.tokenize(en_syn.lower())\n",
    "            \n",
    "            # Check if any token is activated (in top-k or has positive score)\n",
    "            for en_tok in en_tokens:\n",
    "                # Check in top-k\n",
    "                if en_tok in top_k_tokens or en_tok.lower() in [t.lower() for t in top_k_tokens]:\n",
    "                    en_activated.append(en_tok)\n",
    "                    # Get score\n",
    "                    if en_tok in top_k_tokens:\n",
    "                        idx = top_k_tokens.index(en_tok)\n",
    "                        en_scores[en_tok] = top_k_values[idx].item()\n",
    "                \n",
    "                # Also check direct token ID score\n",
    "                en_tok_ids = tokenizer.convert_tokens_to_ids([en_tok])\n",
    "                if en_tok_ids and en_tok_ids[0] is not None:\n",
    "                    score = sparse_rep[en_tok_ids[0]].item()\n",
    "                    if score > 0 and en_tok not in en_scores:\n",
    "                        en_scores[en_tok] = score\n",
    "                        if en_tok not in en_activated:\n",
    "                            en_activated.append(en_tok)\n",
    "        \n",
    "        results.append({\n",
    "            'ko_term': ko_term,\n",
    "            'en_synonyms': en_synonyms,\n",
    "            'top_10_tokens': top_k_tokens[:10],\n",
    "            'top_10_values': top_k_values[:10].tolist(),\n",
    "            'en_activated': en_activated,\n",
    "            'en_scores': en_scores,\n",
    "            'activation_rate': len(set(en_activated)) / len(en_synonyms) if en_synonyms else 0,\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test pairs - UPDATED: Added abbreviations for testing\n",
    "TEST_PAIRS = [\n",
    "    (\"머신러닝\", [\"machine\", \"learning\", \"ML\"]),\n",
    "    (\"딥러닝\", [\"deep\", \"learning\", \"DL\"]),\n",
    "    (\"자연어처리\", [\"natural\", \"language\", \"processing\", \"NLP\"]),\n",
    "    (\"학습\", [\"training\", \"learning\"]),\n",
    "    (\"모델\", [\"model\"]),\n",
    "    (\"데이터\", [\"data\"]),\n",
    "    (\"알고리즘\", [\"algorithm\"]),\n",
    "    (\"신경망\", [\"neural\", \"network\", \"NN\"]),\n",
    "    (\"분류\", [\"classification\", \"classify\"]),\n",
    "    (\"회귀\", [\"regression\"]),\n",
    "    # New abbreviation tests\n",
    "    (\"강화학습\", [\"reinforcement\", \"learning\", \"RL\"]),\n",
    "    (\"컴퓨터비전\", [\"computer\", \"vision\", \"CV\"]),\n",
    "    (\"개체명인식\", [\"named\", \"entity\", \"recognition\", \"NER\"]),\n",
    "    (\"순환신경망\", [\"recurrent\", \"neural\", \"network\", \"RNN\"]),\n",
    "]\n",
    "\n",
    "print(\"Test pairs defined (with abbreviations):\")\n",
    "for ko, en_list in TEST_PAIRS:\n",
    "    print(f\"  {ko} → {en_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "print(\"=\"*60)\n",
    "print(\"CROSS-LINGUAL ACTIVATION TEST (SPLADEDocExpansion)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = test_cross_lingual_activation(model, tokenizer, TEST_PAIRS, top_k=30)\n",
    "\n",
    "total_activation_rate = 0\n",
    "\n",
    "for result in results:\n",
    "    print(f\"\\n[{result['ko_term']}]\")\n",
    "    print(f\"  Expected EN: {result['en_synonyms']}\")\n",
    "    print(f\"  Top-10 activated: {result['top_10_tokens']}\")\n",
    "    \n",
    "    if result['en_scores']:\n",
    "        print(f\"  EN token scores:\")\n",
    "        for tok, score in sorted(result['en_scores'].items(), key=lambda x: -x[1]):\n",
    "            print(f\"    ✓ {tok}: {score:.4f}\")\n",
    "    else:\n",
    "        print(f\"  EN activated: None (need more training)\")\n",
    "    \n",
    "    print(f\"  Activation rate: {result['activation_rate']:.1%}\")\n",
    "    total_activation_rate += result['activation_rate']\n",
    "\n",
    "avg_rate = total_activation_rate / len(results)\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"Average English Activation Rate: {avg_rate:.1%}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if avg_rate < 0.3:\n",
    "    print(\"\\n⚠️  Low activation rate - model needs training!\")\n",
    "    print(\"    Run the training loop above to improve cross-lingual activation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "history_path = Path(config.output_dir) / 'training_history.json'\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(f\"Saved training history to: {history_path}\")\n",
    "\n",
    "# Save evaluation results\n",
    "eval_path = Path(config.output_dir) / 'evaluation_results.json'\n",
    "eval_results = {\n",
    "    'model_type': 'SPLADEDocExpansion',\n",
    "    'expansion_mode': config.expansion_mode,\n",
    "    'noise_suppression': 'ExplicitNoiseTokenLoss',  # v4\n",
    "    'noise_tokens': list(config.custom_noise_tokens),\n",
    "    'test_pairs': TEST_PAIRS,\n",
    "    'results': results,\n",
    "    'average_activation_rate': avg_rate,\n",
    "}\n",
    "with open(eval_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(eval_results, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Saved evaluation results to: {eval_path}\")\n",
    "\n",
    "# Save tokenizer for later use\n",
    "tokenizer.save_pretrained(Path(config.output_dir) / 'best_model')\n",
    "print(f\"Saved tokenizer to: {config.output_dir}/best_model\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CROSS-LINGUAL TRAINING COMPLETE (v4)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: SPLADEDocExpansion (MLM-based vocabulary expansion)\")\n",
    "print(f\"Noise suppression: ExplicitNoiseTokenLoss\")\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"Best model: {config.output_dir}/best_model/checkpoint.pt\")\n",
    "print(f\"Average EN activation: {avg_rate:.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

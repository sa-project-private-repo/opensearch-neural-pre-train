{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v21.1 Data Ingestion - Knowledge Distillation Dataset (General Purpose)\n",
    "\n",
    "This notebook creates a **general-purpose** SPLADE training dataset using Knowledge Distillation approach.\n",
    "\n",
    "## Methodology (Based on Sentence Transformers v5)\n",
    "\n",
    "1. **Data Collection**: 다양한 도메인의 한국어 데이터셋 수집\n",
    "2. **Term Extraction**: Kiwi 형태소 분석기로 고유명사/복합명사 추출\n",
    "3. **Embedding**: BGE-M3 (Teacher) 로 의미적 유사도 계산\n",
    "4. **Clustering**: K-means로 동의어 그룹 추출\n",
    "5. **Dataset Format**: Triplet (anchor, positive, negative) 생성\n",
    "\n",
    "## Data Sources (Diverse Domains)\n",
    "\n",
    "| Domain | Dataset | Description |\n",
    "|--------|---------|-------------|\n",
    "| 백과사전 | Korean Wikipedia | 일반 지식, 역사, 과학, 문화 |\n",
    "| 뉴스 | KLUE-MRC, KorQuAD | 뉴스 기반 질의응답 |\n",
    "| 법률 | Korean Legal QA | 법률 용어, 판례 |\n",
    "| 의료 | Korean Medical QA | 의학 용어, 증상, 질병 |\n",
    "| 금융 | Korean Finance | 금융/경제 용어 |\n",
    "| 대화 | Korean Dialogue | 일상 대화, 고객 상담 |\n",
    "| 리뷰 | NSMC | 영화/제품 리뷰 |\n",
    "| 과학 | AI Hub Science QA | 과학 용어, 개념 |\n",
    "\n",
    "## Dataset Format for SPLADE Training\n",
    "\n",
    "```python\n",
    "# Triplet format for SparseTripletLoss\n",
    "{\n",
    "    \"anchor\": \"인공지능\",           # Query\n",
    "    \"positive\": \"AI\",              # Synonym (정답)\n",
    "    \"negative\": \"컴퓨터\"            # Hard negative (유사하지만 오답)\n",
    "}\n",
    "```\n",
    "\n",
    "## Reference\n",
    "- [HuggingFace: Training Sparse Encoders](https://huggingface.co/blog/train-sparse-encoder)\n",
    "- SPLADE v3: Knowledge Distillation with SparseDistillKLDivLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Set, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.1_korean_general\n",
      "Config: {'min_term_freq': 3, 'max_terms': 100000, 'embedding_batch_size': 64, 'n_clusters': 10000, 'min_cluster_size': 2, 'max_cluster_size': 10, 'similarity_threshold': 0.7}\n"
     ]
    }
   ],
   "source": [
    "# Output directory\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"dataset\" / \"v21.1_korean_general\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Configuration (increased for diverse data)\n",
    "CONFIG = {\n",
    "    \"min_term_freq\": 3,           # Minimum frequency for a term\n",
    "    \"max_terms\": 100000,          # Increased to 100K terms for diversity\n",
    "    \"embedding_batch_size\": 64,   # Batch size for BGE-M3 embeddings\n",
    "    \"n_clusters\": 10000,          # Increased clusters for more terms\n",
    "    \"min_cluster_size\": 2,        # Minimum terms per cluster to form synonyms\n",
    "    \"max_cluster_size\": 10,       # Maximum terms per cluster\n",
    "    \"similarity_threshold\": 0.7,  # Minimum cosine similarity for synonym pairs\n",
    "}\n",
    "print(f\"Config: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Diverse Korean Datasets from HuggingFace\n",
    "\n",
    "Load Korean text data from various domains to create a general-purpose model:\n",
    "\n",
    "### Domain Coverage\n",
    "- **백과사전**: Wikipedia (일반 지식)\n",
    "- **뉴스/QA**: KLUE-MRC, KorQuAD (뉴스, 질의응답)\n",
    "- **법률**: Legal QA datasets\n",
    "- **의료**: Medical QA, health-related texts\n",
    "- **금융**: Financial news, reports\n",
    "- **대화**: Dialogue, customer service\n",
    "- **리뷰**: Movie/product reviews (NSMC)\n",
    "- **과학/기술**: Science QA, technical documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[1/10] Loading Korean Wikipedia (백과사전)...\n",
      "  ✓ Wikipedia: 96,359 texts\n",
      "\n",
      "[2/10] Loading KLUE-MRC (뉴스 기반 QA)...\n",
      "  ✓ KLUE-MRC: 35,108 texts\n",
      "\n",
      "[3/10] Loading KorQuAD (한국어 QA)...\n",
      "  ✓ KorQuAD: 120,814 texts\n",
      "\n",
      "[4/10] Loading NSMC (영화 리뷰)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04293754a2a7411fa90d1d336d502a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074aa04c1ad74eb98f7dc814b1d9c347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/3.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddc5f0cbb084d52b2d76023b318b6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.33M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91899f87528d4dee93db3bfb4ad188f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.89M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6485a144a22844848c94544053675631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/150000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1e4593cbc545ad93b9ae911568e4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ NSMC: 134,112 texts\n",
      "\n",
      "[5/10] Loading Korean Dialogue (대화)...\n",
      "  ✓ KorHate (dialogue): 7,896 texts\n",
      "\n",
      "[6/10] Loading Korean News (뉴스)...\n",
      "  ✓ KLUE-YNAT (news): 45,392 texts\n",
      "\n",
      "[7/10] Loading KLUE-STS (문장 유사도)...\n",
      "  ✓ KLUE-STS: 23,336 texts\n",
      "\n",
      "[8/10] Loading KLUE-NLI (자연어 추론)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12bddfb322947e78406bc8627a9a0bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ade4ee48fdd4d8fbc23a72db2843fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/224k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2338f6c2744c2eb32f870fbcf2f0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/24998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7497976b81f40f49d62cb128c7b7429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ KLUE-NLI: 49,996 texts\n",
      "\n",
      "[9/10] Loading Korean Medical Data (의료)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5baeefa684ac4988b53b8d1eb6d0f19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d506d0a007e34330990908f224afe7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.49M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d63db53cb1a4095b0ffbc57b94d36f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/49620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ KoAlpaca (general instructions): 50,000 texts\n",
      "\n",
      "[10/10] Loading Korean Science/Tech Data (과학/기술)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0f04f49ab24515a19373ee2b90f452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a2f194a6d14637b13587567555e311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/23.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e50903ec9a41d9940105d4d859a1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9619 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ KorQuAD-Chat: 0 texts\n",
      "\n",
      "============================================================\n",
      "Data Collection Summary\n",
      "============================================================\n",
      "  NSMC                    134,112 texts ( 23.8%)\n",
      "  KorQuAD                 120,814 texts ( 21.5%)\n",
      "  Wikipedia                96,359 texts ( 17.1%)\n",
      "  KoAlpaca                 50,000 texts (  8.9%)\n",
      "  KLUE-NLI                 49,996 texts (  8.9%)\n",
      "  KLUE-YNAT                45,392 texts (  8.1%)\n",
      "  KLUE-MRC                 35,108 texts (  6.2%)\n",
      "  KLUE-STS                 23,336 texts (  4.1%)\n",
      "  KorHate                   7,896 texts (  1.4%)\n",
      "  KorQuAD-Chat                  0 texts (  0.0%)\n",
      "------------------------------------------------------------\n",
      "  TOTAL                   563,013 texts\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "def load_diverse_korean_datasets() -> List[str]:\n",
    "    \"\"\"Load diverse Korean text data from multiple domains via HuggingFace.\"\"\"\n",
    "    all_texts = []\n",
    "    domain_stats = {}\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 1. 백과사전 (Encyclopedia) - Korean Wikipedia\n",
    "    # ========================================================================\n",
    "    print(\"=\" * 60)\n",
    "    print(\"[1/10] Loading Korean Wikipedia (백과사전)...\")\n",
    "    try:\n",
    "        wiki_dataset = load_dataset(\n",
    "            \"wikimedia/wikipedia\", \n",
    "            \"20231101.ko\",\n",
    "            split=\"train\",\n",
    "            streaming=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        wiki_texts = []\n",
    "        for i, item in enumerate(wiki_dataset):\n",
    "            if i >= 100000:  # Increased to 100K articles\n",
    "                break\n",
    "            text = item.get(\"text\", \"\")\n",
    "            if text and len(text) > 100:\n",
    "                wiki_texts.append(text[:3000])  # Longer truncation\n",
    "        all_texts.extend(wiki_texts)\n",
    "        domain_stats[\"Wikipedia\"] = len(wiki_texts)\n",
    "        print(f\"  ✓ Wikipedia: {len(wiki_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Wikipedia failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 2. 뉴스/QA (News/Question Answering) - KLUE-MRC\n",
    "    # ========================================================================\n",
    "    print(\"\\n[2/10] Loading KLUE-MRC (뉴스 기반 QA)...\")\n",
    "    try:\n",
    "        klue_dataset = load_dataset(\n",
    "            \"klue\",\n",
    "            \"mrc\",\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        klue_texts = []\n",
    "        for item in klue_dataset:\n",
    "            context = item.get(\"context\", \"\")\n",
    "            question = item.get(\"question\", \"\")\n",
    "            if context and len(context) > 50:\n",
    "                klue_texts.append(context[:2000])\n",
    "            if question:\n",
    "                klue_texts.append(question)\n",
    "        all_texts.extend(klue_texts)\n",
    "        domain_stats[\"KLUE-MRC\"] = len(klue_texts)\n",
    "        print(f\"  ✓ KLUE-MRC: {len(klue_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ KLUE-MRC failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 3. QA - KorQuAD (Korean Question Answering Dataset)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[3/10] Loading KorQuAD (한국어 QA)...\")\n",
    "    try:\n",
    "        korquad_dataset = load_dataset(\n",
    "            \"squad_kor_v1\",\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        korquad_texts = []\n",
    "        for item in korquad_dataset:\n",
    "            context = item.get(\"context\", \"\")\n",
    "            question = item.get(\"question\", \"\")\n",
    "            if context and len(context) > 50:\n",
    "                korquad_texts.append(context[:2000])\n",
    "            if question:\n",
    "                korquad_texts.append(question)\n",
    "        all_texts.extend(korquad_texts)\n",
    "        domain_stats[\"KorQuAD\"] = len(korquad_texts)\n",
    "        print(f\"  ✓ KorQuAD: {len(korquad_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ KorQuAD failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 4. 리뷰 (Reviews) - NSMC (Naver Sentiment Movie Corpus)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[4/10] Loading NSMC (영화 리뷰)...\")\n",
    "    try:\n",
    "        nsmc_dataset = load_dataset(\n",
    "            \"nsmc\",\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        nsmc_texts = []\n",
    "        for item in nsmc_dataset:\n",
    "            text = item.get(\"document\", \"\")\n",
    "            if text and len(text) > 10:\n",
    "                nsmc_texts.append(text)\n",
    "        all_texts.extend(nsmc_texts)\n",
    "        domain_stats[\"NSMC\"] = len(nsmc_texts)\n",
    "        print(f\"  ✓ NSMC: {len(nsmc_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ NSMC failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 5. 대화 (Dialogue) - Korean Dialogue Dataset\n",
    "    # ========================================================================\n",
    "    print(\"\\n[5/10] Loading Korean Dialogue (대화)...\")\n",
    "    try:\n",
    "        # Try Korean hate speech for diverse vocabulary\n",
    "        hate_dataset = load_dataset(\n",
    "            \"kor_hate\",\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        hate_texts = [item.get(\"comments\", \"\") for item in hate_dataset if item.get(\"comments\")]\n",
    "        all_texts.extend(hate_texts)\n",
    "        domain_stats[\"KorHate\"] = len(hate_texts)\n",
    "        print(f\"  ✓ KorHate (dialogue): {len(hate_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ KorHate failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 6. 뉴스 (News) - Korean News Dataset\n",
    "    # ========================================================================\n",
    "    print(\"\\n[6/10] Loading Korean News (뉴스)...\")\n",
    "    try:\n",
    "        news_dataset = load_dataset(\n",
    "            \"klue\",\n",
    "            \"ynat\",  # YNAT: Korean news topic classification\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        news_texts = []\n",
    "        for item in news_dataset:\n",
    "            title = item.get(\"title\", \"\")\n",
    "            if title and len(title) > 10:\n",
    "                news_texts.append(title)\n",
    "        all_texts.extend(news_texts)\n",
    "        domain_stats[\"KLUE-YNAT\"] = len(news_texts)\n",
    "        print(f\"  ✓ KLUE-YNAT (news): {len(news_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ KLUE-YNAT failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 7. 문장 유사도 (Sentence Similarity) - KLUE-STS\n",
    "    # ========================================================================\n",
    "    print(\"\\n[7/10] Loading KLUE-STS (문장 유사도)...\")\n",
    "    try:\n",
    "        sts_dataset = load_dataset(\n",
    "            \"klue\",\n",
    "            \"sts\",\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        sts_texts = []\n",
    "        for item in sts_dataset:\n",
    "            sent1 = item.get(\"sentence1\", \"\")\n",
    "            sent2 = item.get(\"sentence2\", \"\")\n",
    "            if sent1:\n",
    "                sts_texts.append(sent1)\n",
    "            if sent2:\n",
    "                sts_texts.append(sent2)\n",
    "        all_texts.extend(sts_texts)\n",
    "        domain_stats[\"KLUE-STS\"] = len(sts_texts)\n",
    "        print(f\"  ✓ KLUE-STS: {len(sts_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ KLUE-STS failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 8. 자연어 추론 (NLI) - KLUE-NLI\n",
    "    # ========================================================================\n",
    "    print(\"\\n[8/10] Loading KLUE-NLI (자연어 추론)...\")\n",
    "    try:\n",
    "        nli_dataset = load_dataset(\n",
    "            \"klue\",\n",
    "            \"nli\",\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        nli_texts = []\n",
    "        for item in nli_dataset:\n",
    "            premise = item.get(\"premise\", \"\")\n",
    "            hypothesis = item.get(\"hypothesis\", \"\")\n",
    "            if premise:\n",
    "                nli_texts.append(premise)\n",
    "            if hypothesis:\n",
    "                nli_texts.append(hypothesis)\n",
    "        all_texts.extend(nli_texts)\n",
    "        domain_stats[\"KLUE-NLI\"] = len(nli_texts)\n",
    "        print(f\"  ✓ KLUE-NLI: {len(nli_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ KLUE-NLI failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 9. 의료 (Medical) - Korean Medical QA\n",
    "    # ========================================================================\n",
    "    print(\"\\n[9/10] Loading Korean Medical Data (의료)...\")\n",
    "    try:\n",
    "        # Korean medical/health related dataset\n",
    "        medical_dataset = load_dataset(\n",
    "            \"Bingsu/ko_alpaca_data\",\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        medical_texts = []\n",
    "        for item in medical_dataset:\n",
    "            instruction = item.get(\"instruction\", \"\")\n",
    "            output = item.get(\"output\", \"\")\n",
    "            if instruction:\n",
    "                medical_texts.append(instruction)\n",
    "            if output and len(output) > 20:\n",
    "                medical_texts.append(output[:1000])\n",
    "        # Limit to 50K to balance\n",
    "        medical_texts = medical_texts[:50000]\n",
    "        all_texts.extend(medical_texts)\n",
    "        domain_stats[\"KoAlpaca\"] = len(medical_texts)\n",
    "        print(f\"  ✓ KoAlpaca (general instructions): {len(medical_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ KoAlpaca failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 10. 과학/기술 (Science/Tech) - Korean Science QA\n",
    "    # ========================================================================\n",
    "    print(\"\\n[10/10] Loading Korean Science/Tech Data (과학/기술)...\")\n",
    "    try:\n",
    "        # Korean open-domain QA dataset\n",
    "        openqa_dataset = load_dataset(\n",
    "            \"heegyu/korquad-chat-v1\",\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        science_texts = []\n",
    "        for item in openqa_dataset:\n",
    "            # Try different field names\n",
    "            for field in [\"context\", \"question\", \"answer\", \"input\", \"output\"]:\n",
    "                text = item.get(field, \"\")\n",
    "                if text and len(text) > 20:\n",
    "                    science_texts.append(text[:1000])\n",
    "        # Limit to balance\n",
    "        science_texts = science_texts[:30000]\n",
    "        all_texts.extend(science_texts)\n",
    "        domain_stats[\"KorQuAD-Chat\"] = len(science_texts)\n",
    "        print(f\"  ✓ KorQuAD-Chat: {len(science_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ KorQuAD-Chat failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Summary\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Data Collection Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total = 0\n",
    "    for domain, count in sorted(domain_stats.items(), key=lambda x: -x[1]):\n",
    "        pct = count / sum(domain_stats.values()) * 100\n",
    "        print(f\"  {domain:20} {count:>10,} texts ({pct:5.1f}%)\")\n",
    "        total += count\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"  {'TOTAL':20} {total:>10,} texts\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return all_texts\n",
    "\n",
    "texts = load_diverse_korean_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Korean Terms with Morphological Analysis\n",
    "\n",
    "Use **Kiwi** morphological analyzer to extract meaningful terms:\n",
    "- **NNG**: 일반명사 (General Nouns) - for compound nouns\n",
    "- **NNP**: 고유명사 (Proper Nouns)\n",
    "- **SL**: 외래어 (Foreign words like \"AI\", \"ML\")\n",
    "\n",
    "Filter out:\n",
    "- 조사 (JKS, JKC, JKG, JKO, JKB, JKV, JKQ, JX, JC)\n",
    "- 어미 (EP, EF, EC, ETN, ETM)\n",
    "- 접사 (XPN, XSN, XSV, XSA)\n",
    "- 기호, 부호 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Kiwi morphological analyzer...\n",
      "Kiwi loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantization is not supported for ArchType::neon. Fall back to non-quantized model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: skt/A.X-Encoder-base\n",
      "Vocab size: 49,999\n",
      "\n",
      "Test text: 인공지능 기술이 발전하면서 기계학습과 딥러닝이 주목받고 있습니다.\n",
      "Extracted nouns: ['인공', '지능', '기술', '발전', '기계', '학습', '러닝', '주목']\n",
      "Compound nouns: ['인공지능기술', '기계학습', '딥러닝']\n"
     ]
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize Kiwi morphological analyzer\n",
    "print(\"Loading Kiwi morphological analyzer...\")\n",
    "kiwi = Kiwi()\n",
    "print(\"Kiwi loaded successfully\")\n",
    "\n",
    "# Load tokenizer for later use\n",
    "MODEL_NAME = \"skt/A.X-Encoder-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer: {MODEL_NAME}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "# POS tags to keep (nouns and foreign words)\n",
    "VALID_POS_TAGS = {\n",
    "    'NNG',   # 일반명사 (General Noun)\n",
    "    'NNP',   # 고유명사 (Proper Noun)\n",
    "    'NNB',   # 의존명사 (Dependent Noun) - sometimes useful\n",
    "    'SL',    # 외래어 (Foreign words: AI, ML, API, etc.)\n",
    "    'SH',    # 한자 (Chinese characters)\n",
    "}\n",
    "\n",
    "# POS tags to explicitly filter out\n",
    "FILTER_OUT_POS_TAGS = {\n",
    "    # 조사 (Particles)\n",
    "    'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ',  # 격조사\n",
    "    'JX',   # 보조사\n",
    "    'JC',   # 접속조사\n",
    "    # 어미 (Endings)\n",
    "    'EP', 'EF', 'EC', 'ETN', 'ETM',\n",
    "    # 접사 (Affixes)\n",
    "    'XPN', 'XSN', 'XSV', 'XSA',\n",
    "    # 기호 (Symbols)\n",
    "    'SF', 'SP', 'SS', 'SE', 'SO', 'SW',\n",
    "    # 기타\n",
    "    'NR', 'NP',  # 수사, 대명사\n",
    "}\n",
    "\n",
    "def extract_nouns_with_kiwi(text: str, kiwi_instance: Kiwi) -> List[str]:\n",
    "    \"\"\"Extract nouns and proper nouns using Kiwi morphological analyzer.\"\"\"\n",
    "    try:\n",
    "        result = kiwi_instance.tokenize(text)\n",
    "        nouns = []\n",
    "        for token in result:\n",
    "            # token.form: 형태, token.tag: 품사\n",
    "            if token.tag in VALID_POS_TAGS:\n",
    "                word = token.form.strip()\n",
    "                # Filter by length (2-15 characters for compound nouns)\n",
    "                if 2 <= len(word) <= 15:\n",
    "                    nouns.append(word)\n",
    "        return nouns\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def extract_compound_nouns(text: str, kiwi_instance: Kiwi) -> List[str]:\n",
    "    \"\"\"Extract compound nouns by joining consecutive nouns.\"\"\"\n",
    "    try:\n",
    "        result = kiwi_instance.tokenize(text)\n",
    "        compounds = []\n",
    "        current_compound = []\n",
    "        \n",
    "        for token in result:\n",
    "            if token.tag in {'NNG', 'NNP', 'SL', 'SH'}:\n",
    "                current_compound.append(token.form)\n",
    "            else:\n",
    "                if len(current_compound) >= 2:\n",
    "                    compound = ''.join(current_compound)\n",
    "                    if 2 <= len(compound) <= 15:\n",
    "                        compounds.append(compound)\n",
    "                current_compound = []\n",
    "        \n",
    "        # Don't forget the last compound\n",
    "        if len(current_compound) >= 2:\n",
    "            compound = ''.join(current_compound)\n",
    "            if 2 <= len(compound) <= 15:\n",
    "                compounds.append(compound)\n",
    "        \n",
    "        return compounds\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def extract_terms(texts: List[str], kiwi_instance: Kiwi, min_freq: int = 3) -> List[Tuple[str, int]]:\n",
    "    \"\"\"Extract Korean terms using morphological analysis.\"\"\"\n",
    "    term_freq = Counter()\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Processing text {i:,}/{len(texts):,}...\")\n",
    "        \n",
    "        # Extract individual nouns\n",
    "        nouns = extract_nouns_with_kiwi(text, kiwi_instance)\n",
    "        for noun in nouns:\n",
    "            term_freq[noun] += 1\n",
    "        \n",
    "        # Extract compound nouns\n",
    "        compounds = extract_compound_nouns(text, kiwi_instance)\n",
    "        for compound in compounds:\n",
    "            term_freq[compound] += 1\n",
    "    \n",
    "    # Filter by frequency\n",
    "    filtered_terms = [\n",
    "        (term, freq) for term, freq in term_freq.items() \n",
    "        if freq >= min_freq\n",
    "    ]\n",
    "    \n",
    "    # Sort by frequency\n",
    "    filtered_terms.sort(key=lambda x: -x[1])\n",
    "    \n",
    "    return filtered_terms\n",
    "\n",
    "# Test morphological analysis\n",
    "test_text = \"인공지능 기술이 발전하면서 기계학습과 딥러닝이 주목받고 있습니다.\"\n",
    "print(f\"\\nTest text: {test_text}\")\n",
    "print(f\"Extracted nouns: {extract_nouns_with_kiwi(test_text, kiwi)}\")\n",
    "print(f\"Compound nouns: {extract_compound_nouns(test_text, kiwi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting terms from 563,013 texts...\n",
      "Processing text 0/563,013...\n",
      "Processing text 10,000/563,013...\n",
      "Processing text 20,000/563,013...\n",
      "Processing text 30,000/563,013...\n",
      "Processing text 40,000/563,013...\n",
      "Processing text 50,000/563,013...\n",
      "Processing text 60,000/563,013...\n",
      "Processing text 70,000/563,013...\n",
      "Processing text 80,000/563,013...\n",
      "Processing text 90,000/563,013...\n",
      "Processing text 100,000/563,013...\n",
      "Processing text 110,000/563,013...\n",
      "Processing text 120,000/563,013...\n",
      "Processing text 130,000/563,013...\n",
      "Processing text 140,000/563,013...\n",
      "Processing text 150,000/563,013...\n",
      "Processing text 160,000/563,013...\n",
      "Processing text 170,000/563,013...\n",
      "Processing text 180,000/563,013...\n",
      "Processing text 190,000/563,013...\n",
      "Processing text 200,000/563,013...\n",
      "Processing text 210,000/563,013...\n",
      "Processing text 220,000/563,013...\n",
      "Processing text 230,000/563,013...\n",
      "Processing text 240,000/563,013...\n",
      "Processing text 250,000/563,013...\n",
      "Processing text 260,000/563,013...\n",
      "Processing text 270,000/563,013...\n",
      "Processing text 280,000/563,013...\n",
      "Processing text 290,000/563,013...\n",
      "Processing text 300,000/563,013...\n",
      "Processing text 310,000/563,013...\n",
      "Processing text 320,000/563,013...\n",
      "Processing text 330,000/563,013...\n",
      "Processing text 340,000/563,013...\n",
      "Processing text 350,000/563,013...\n",
      "Processing text 360,000/563,013...\n",
      "Processing text 370,000/563,013...\n",
      "Processing text 380,000/563,013...\n",
      "Processing text 390,000/563,013...\n",
      "Processing text 400,000/563,013...\n",
      "Processing text 410,000/563,013...\n",
      "Processing text 420,000/563,013...\n",
      "Processing text 430,000/563,013...\n",
      "Processing text 440,000/563,013...\n",
      "Processing text 450,000/563,013...\n",
      "Processing text 460,000/563,013...\n",
      "Processing text 470,000/563,013...\n",
      "Processing text 480,000/563,013...\n",
      "Processing text 490,000/563,013...\n",
      "Processing text 500,000/563,013...\n",
      "Processing text 510,000/563,013...\n",
      "Processing text 520,000/563,013...\n",
      "Processing text 530,000/563,013...\n",
      "Processing text 540,000/563,013...\n",
      "Processing text 550,000/563,013...\n",
      "Processing text 560,000/563,013...\n",
      "\n",
      "Extracted 589,443 unique terms\n",
      "\n",
      "Top 50 terms (고유명사/복합명사):\n",
      "  영화: 106,162\n",
      "  대한민국: 100,294\n",
      "  사람: 82,902\n",
      "  미국: 82,538\n",
      "  일본: 78,016\n",
      "  사용: 78,009\n",
      "  이후: 76,495\n",
      "  때문: 62,762\n",
      "  지역: 58,041\n",
      "  시작: 57,583\n",
      "  선수: 56,949\n",
      "  한국: 53,822\n",
      "  이름: 50,536\n",
      "  세계: 49,453\n",
      "  경우: 48,690\n",
      "  당시: 48,367\n",
      "  국가: 46,327\n",
      "  다음: 46,094\n",
      "  자신: 45,676\n",
      "  기록: 44,655\n",
      "  활동: 42,604\n",
      "  세기: 40,830\n",
      "  대표: 40,499\n",
      "  정부: 40,487\n",
      "  중국: 40,132\n",
      "  역사: 37,791\n",
      "  경기: 35,565\n",
      "  도시: 35,333\n",
      "  외부: 34,921\n",
      "  대학: 34,780\n",
      "  이상: 33,806\n",
      "  축구: 33,447\n",
      "  시간: 33,149\n",
      "  가능: 33,092\n",
      "  게임: 32,339\n",
      "  링크: 32,039\n",
      "  전쟁: 31,994\n",
      "  작품: 31,637\n",
      "  방송: 31,561\n",
      "  시대: 31,434\n",
      "  서울: 31,289\n",
      "  문제: 31,283\n",
      "  프랑스: 30,662\n",
      "  영국: 30,596\n",
      "  일부: 30,486\n",
      "  처음: 30,220\n",
      "  사이: 30,041\n",
      "  배우: 30,001\n",
      "  사건: 29,907\n",
      "  포함: 29,624\n"
     ]
    }
   ],
   "source": [
    "# Extract terms from collected texts\n",
    "print(f\"\\nExtracting terms from {len(texts):,} texts...\")\n",
    "terms_with_freq = extract_terms(texts, kiwi, CONFIG[\"min_term_freq\"])\n",
    "\n",
    "print(f\"\\nExtracted {len(terms_with_freq):,} unique terms\")\n",
    "print(f\"\\nTop 50 terms (고유명사/복합명사):\")\n",
    "for term, freq in terms_with_freq[:50]:\n",
    "    print(f\"  {term}: {freq:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute Term Embeddings with BGE-M3\n",
    "\n",
    "Use BAAI/bge-m3 model to compute dense embeddings for each term.\n",
    "BGE-M3 supports Korean and provides high-quality multilingual embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100,000 terms for embeddings\n",
      "\n",
      "Loading BGE-M3 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a10997b3f8411cac1e084efbb1a56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGE-M3 loaded on cuda\n",
      "Embedding batch 0/100,000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch 8,000/100,000...\n",
      "Embedding batch 16,000/100,000...\n",
      "Embedding batch 24,000/100,000...\n",
      "Embedding batch 32,000/100,000...\n",
      "Embedding batch 40,000/100,000...\n",
      "Embedding batch 48,000/100,000...\n",
      "Embedding batch 56,000/100,000...\n",
      "Embedding batch 64,000/100,000...\n",
      "Embedding batch 72,000/100,000...\n",
      "Embedding batch 80,000/100,000...\n",
      "Embedding batch 88,000/100,000...\n",
      "Embedding batch 96,000/100,000...\n",
      "\n",
      "Embeddings shape: (100000, 1024)\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import torch\n",
    "\n",
    "# Limit terms to process\n",
    "terms = [t[0] for t in terms_with_freq[:CONFIG[\"max_terms\"]]]\n",
    "print(f\"Processing {len(terms):,} terms for embeddings\")\n",
    "\n",
    "# Load BGE-M3 model\n",
    "print(\"\\nLoading BGE-M3 model...\")\n",
    "bge_model = BGEM3FlagModel(\n",
    "    \"BAAI/bge-m3\",\n",
    "    use_fp16=True,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"BGE-M3 loaded on {bge_model.device}\")\n",
    "\n",
    "def compute_embeddings(terms: List[str], model, batch_size: int = 64) -> np.ndarray:\n",
    "    \"\"\"Compute BGE-M3 embeddings for terms.\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(terms), batch_size):\n",
    "        batch = terms[i:i + batch_size]\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Embedding batch {i:,}/{len(terms):,}...\")\n",
    "        \n",
    "        # Get dense embeddings\n",
    "        output = model.encode(\n",
    "            batch,\n",
    "            return_dense=True,\n",
    "            return_sparse=False,\n",
    "            return_colbert_vecs=False\n",
    "        )\n",
    "        embeddings = output[\"dense_vecs\"]\n",
    "        all_embeddings.append(embeddings)\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "embeddings = compute_embeddings(terms, bge_model, CONFIG[\"embedding_batch_size\"])\n",
    "print(f\"\\nEmbeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. K-means Clustering\n",
    "\n",
    "Apply K-means clustering to group semantically similar terms together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering 100,000 terms into 10,000 clusters...\n",
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 15257.434805494704\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 15244.326776531336\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 15278.869513165158\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 1/9765: mean batch inertia: 0.5095442725708315\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 2/9765: mean batch inertia: 0.511518103086497, ewa inertia: 0.511518103086497\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 3/9765: mean batch inertia: 0.5251314584798107, ewa inertia: 0.5117969018169648\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 4/9765: mean batch inertia: 0.4898227214431942, ewa inertia: 0.511346875103177\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 5/9765: mean batch inertia: 0.49313930029080527, ewa inertia: 0.5109739876998937\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 6/9765: mean batch inertia: 0.49089144599370005, ewa inertia: 0.5105627013586143\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 7/9765: mean batch inertia: 0.4684691307473435, ewa inertia: 0.5097006336531725\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 8/9765: mean batch inertia: 0.46032198884782244, ewa inertia: 0.5086893691202043\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 9/9765: mean batch inertia: 0.46535393321615726, ewa inertia: 0.507801868267898\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 10/9765: mean batch inertia: 0.4754606496328592, ewa inertia: 0.5071395267336677\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 11/9765: mean batch inertia: 0.45971485371838366, ewa inertia: 0.5061682791427906\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 12/9765: mean batch inertia: 0.4529573231153395, ewa inertia: 0.5050785296608432\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 13/9765: mean batch inertia: 0.46461842847171414, ewa inertia: 0.5042499150746357\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 14/9765: mean batch inertia: 0.44866337240882903, ewa inertia: 0.50311151406485\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 15/9765: mean batch inertia: 0.436029459065267, ewa inertia: 0.501737687316726\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 16/9765: mean batch inertia: 0.4391723746639741, ewa inertia: 0.5004563625268454\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 17/9765: mean batch inertia: 0.44234674911278915, ewa inertia: 0.4992662895448554\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 18/9765: mean batch inertia: 0.4421319449032019, ewa inertia: 0.4980961898675911\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 19/9765: mean batch inertia: 0.42943661828840823, ewa inertia: 0.49669005590298904\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 20/9765: mean batch inertia: 0.43923211902786424, ewa inertia: 0.4955133291230543\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 21/9765: mean batch inertia: 0.4374236810947185, ewa inertia: 0.4943236650280749\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 22/9765: mean batch inertia: 0.4173759965296688, ewa inertia: 0.49274779253595247\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 23/9765: mean batch inertia: 0.42362125170415726, ewa inertia: 0.4913320951366913\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 24/9765: mean batch inertia: 0.41918207410368225, ewa inertia: 0.4898544774821118\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 25/9765: mean batch inertia: 0.41654492421141487, ewa inertia: 0.4883531128447743\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 26/9765: mean batch inertia: 0.4170671802029853, ewa inertia: 0.48689319154348343\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 27/9765: mean batch inertia: 0.4108894005272872, ewa inertia: 0.48533664946889243\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 28/9765: mean batch inertia: 0.4103749169847276, ewa inertia: 0.48380144853962603\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 29/9765: mean batch inertia: 0.405941414669247, ewa inertia: 0.48220689099153613\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 30/9765: mean batch inertia: 0.40552942198851927, ewa inertia: 0.48063655212974293\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 31/9765: mean batch inertia: 0.4019033605453867, ewa inertia: 0.4790241124904917\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 32/9765: mean batch inertia: 0.4012230618586482, ewa inertia: 0.4774307629070474\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 33/9765: mean batch inertia: 0.40882320207299555, ewa inertia: 0.47602569411185397\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 34/9765: mean batch inertia: 0.41437170394707884, ewa inertia: 0.4747630330198903\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 35/9765: mean batch inertia: 0.41095528884065635, ewa inertia: 0.4734562634867949\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 36/9765: mean batch inertia: 0.4044583241712318, ewa inertia: 0.47204319982024884\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 37/9765: mean batch inertia: 0.40283920487275215, ewa inertia: 0.4706259161765605\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 38/9765: mean batch inertia: 0.4002292326146164, ewa inertia: 0.46918420651430853\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 39/9765: mean batch inertia: 0.3966292001666544, ewa inertia: 0.46769829484342523\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 40/9765: mean batch inertia: 0.3985820630587988, ewa inertia: 0.46628280857133875\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 41/9765: mean batch inertia: 0.3988202947740519, ewa inertia: 0.46490119010495495\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 42/9765: mean batch inertia: 0.3954769113293873, ewa inertia: 0.46347939509358144\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 43/9765: mean batch inertia: 0.3929044720720978, ewa inertia: 0.4620340351237011\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 44/9765: mean batch inertia: 0.39150818993190495, ewa inertia: 0.4605896802577218\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 45/9765: mean batch inertia: 0.38855438325289726, ewa inertia: 0.4591144121277443\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 46/9765: mean batch inertia: 0.39039787884570204, ewa inertia: 0.45770711159913335\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 47/9765: mean batch inertia: 0.3919404658443, ewa inertia: 0.45636022416294875\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 48/9765: mean batch inertia: 0.3921388894965806, ewa inertia: 0.45504498438137936\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 49/9765: mean batch inertia: 0.3876289921374754, ewa inertia: 0.45366431866688134\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 50/9765: mean batch inertia: 0.39180508749438914, ewa inertia: 0.45239745428111255\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 51/9765: mean batch inertia: 0.3876479173998806, ewa inertia: 0.45107139702635746\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 52/9765: mean batch inertia: 0.3968150595546357, ewa inertia: 0.4499602383465234\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 53/9765: mean batch inertia: 0.385940373890112, ewa inertia: 0.4486491246335932\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 54/9765: mean batch inertia: 0.38597638851437077, ewa inertia: 0.4473655998331195\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 55/9765: mean batch inertia: 0.3761755741490635, ewa inertia: 0.4459076426866815\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 56/9765: mean batch inertia: 0.38646518018613024, ewa inertia: 0.44469027322836474\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 57/9765: mean batch inertia: 0.3899040960537313, ewa inertia: 0.4435682635399251\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 58/9765: mean batch inertia: 0.38629214571926307, ewa inertia: 0.44239526037698956\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 59/9765: mean batch inertia: 0.3943363195284269, ewa inertia: 0.4414110231107836\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 60/9765: mean batch inertia: 0.38154126855590176, ewa inertia: 0.4401849027987027\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 61/9765: mean batch inertia: 0.386256273189748, ewa inertia: 0.4390804555087842\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 62/9765: mean batch inertia: 0.37530474735371283, ewa inertia: 0.4377743420669028\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 63/9765: mean batch inertia: 0.38385882299430185, ewa inertia: 0.4366701632780838\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 64/9765: mean batch inertia: 0.3851851950214733, ewa inertia: 0.4356157616722044\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 65/9765: mean batch inertia: 0.3843841626654397, ewa inertia: 0.43456654901667235\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 66/9765: mean batch inertia: 0.3780061708150176, ewa inertia: 0.4334082040545521\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 67/9765: mean batch inertia: 0.375953942110735, ewa inertia: 0.4322315525364579\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 68/9765: mean batch inertia: 0.3657062952131544, ewa inertia: 0.4308691288907131\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 69/9765: mean batch inertia: 0.3829085875842323, ewa inertia: 0.429886906826977\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 70/9765: mean batch inertia: 0.3763915071133931, ewa inertia: 0.4287913319965911\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 71/9765: mean batch inertia: 0.37185335881264797, ewa inertia: 0.4276252539665642\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 72/9765: mean batch inertia: 0.36739807831180604, ewa inertia: 0.42639181374355695\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 73/9765: mean batch inertia: 0.3785078335706453, ewa inertia: 0.4254111596361568\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 74/9765: mean batch inertia: 0.37825468153129155, ewa inertia: 0.42444540462211927\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 75/9765: mean batch inertia: 0.37258760508992905, ewa inertia: 0.4233833675080711\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 76/9765: mean batch inertia: 0.3817719937810279, ewa inertia: 0.42253117509606536\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 77/9765: mean batch inertia: 0.3738286935889693, ewa inertia: 0.4215337582489685\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 78/9765: mean batch inertia: 0.3668279937586754, ewa inertia: 0.42041339539583583\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 79/9765: mean batch inertia: 0.3737965140530408, ewa inertia: 0.4194586912129772\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 80/9765: mean batch inertia: 0.3783095787241999, ewa inertia: 0.418615965816461\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 81/9765: mean batch inertia: 0.3713412128861226, ewa inertia: 0.4176477885582202\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 82/9765: mean batch inertia: 0.3777063778714542, ewa inertia: 0.4168297966472743\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 83/9765: mean batch inertia: 0.3717802994068802, ewa inertia: 0.41590719216983585\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 84/9765: mean batch inertia: 0.37591302662395604, ewa inertia: 0.4150881198501794\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 85/9765: mean batch inertia: 0.37418342445762115, ewa inertia: 0.41425040006573766\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 86/9765: mean batch inertia: 0.37479343215758165, ewa inertia: 0.4134423294436848\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 87/9765: mean batch inertia: 0.3680828597796949, ewa inertia: 0.41251337679449274\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 88/9765: mean batch inertia: 0.36428756660157363, ewa inertia: 0.4115257220782889\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 89/9765: mean batch inertia: 0.3727284136995022, ewa inertia: 0.41073116114830066\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 90/9765: mean batch inertia: 0.3665314800431224, ewa inertia: 0.40982596073127076\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 91/9765: mean batch inertia: 0.37847179088637484, ewa inertia: 0.40918383375411704\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 92/9765: mean batch inertia: 0.37058832229155564, ewa inertia: 0.4083934055836455\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 93/9765: mean batch inertia: 0.37476338473295423, ewa inertia: 0.4077046696439827\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 94/9765: mean batch inertia: 0.36710914436471687, ewa inertia: 0.4068732816001438\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 95/9765: mean batch inertia: 0.36488862938974237, ewa inertia: 0.40601344452124555\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 96/9765: mean batch inertia: 0.36522605706654554, ewa inertia: 0.4051781271793467\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 97/9765: mean batch inertia: 0.36372648929199486, ewa inertia: 0.4043292061246243\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 98/9765: mean batch inertia: 0.36930335096090705, ewa inertia: 0.40361188378409474\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 99/9765: mean batch inertia: 0.3656911402161637, ewa inertia: 0.4028352747219141\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 100/9765: mean batch inertia: 0.36100887225478295, ewa inertia: 0.40197867856534886\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 101/9765: mean batch inertia: 0.3556547410812774, ewa inertia: 0.4010299738127226\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 102/9765: mean batch inertia: 0.36288466092339855, ewa inertia: 0.40024876561683115\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 103/9765: mean batch inertia: 0.3729655243818539, ewa inertia: 0.39969001042389074\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 104/9765: mean batch inertia: 0.36022579165355356, ewa inertia: 0.3988817913056654\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 105/9765: mean batch inertia: 0.3673726333691224, ewa inertia: 0.39823649020413604\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 106/9765: mean batch inertia: 0.36648470844351255, ewa inertia: 0.39758622021637835\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 107/9765: mean batch inertia: 0.36478251529945527, ewa inertia: 0.39691440705781134\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 108/9765: mean batch inertia: 0.3541139901174126, ewa inertia: 0.3960378632843097\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 109/9765: mean batch inertia: 0.35568799555292346, ewa inertia: 0.3952115062567412\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 110/9765: mean batch inertia: 0.3669186659504864, ewa inertia: 0.3946320746815848\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 111/9765: mean batch inertia: 0.3622362602221187, ewa inertia: 0.39396861503605135\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 112/9765: mean batch inertia: 0.36582969243675845, ewa inertia: 0.39339233566401155\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 113/9765: mean batch inertia: 0.3617306840615022, ewa inertia: 0.3927439115234335\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 114/9765: mean batch inertia: 0.3658868358840254, ewa inertia: 0.39219388411461253\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 115/9765: mean batch inertia: 0.35326641051468327, ewa inertia: 0.39139665742755286\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 116/9765: mean batch inertia: 0.36327254602767267, ewa inertia: 0.3908206813858437\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 117/9765: mean batch inertia: 0.36042110093997076, ewa inertia: 0.390198104204084\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 118/9765: mean batch inertia: 0.3587472213389593, ewa inertia: 0.3895539965640827\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 119/9765: mean batch inertia: 0.36790269163113265, ewa inertia: 0.3891105822731988\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 120/9765: mean batch inertia: 0.36413238544239185, ewa inertia: 0.38859903391758743\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 121/9765: mean batch inertia: 0.36030346157686877, ewa inertia: 0.3880195463909248\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 122/9765: mean batch inertia: 0.35729419607588053, ewa inertia: 0.3873902975089615\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 123/9765: mean batch inertia: 0.3593048006776854, ewa inertia: 0.38681511228570914\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 124/9765: mean batch inertia: 0.36227153493105485, ewa inertia: 0.38631246484796017\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 125/9765: mean batch inertia: 0.3514363881460039, ewa inertia: 0.38559820993965316\n",
      "Minibatch step 126/9765: mean batch inertia: 0.3573623494782614, ewa inertia: 0.38501994530005024\n",
      "Minibatch step 127/9765: mean batch inertia: 0.3504563649428538, ewa inertia: 0.38431209025288526\n",
      "Minibatch step 128/9765: mean batch inertia: 0.3604354599335107, ewa inertia: 0.38382310175382944\n",
      "Minibatch step 129/9765: mean batch inertia: 0.35473557612933987, ewa inertia: 0.38322739518610555\n",
      "Minibatch step 130/9765: mean batch inertia: 0.3585881798017142, ewa inertia: 0.3827227891010941\n",
      "Minibatch step 131/9765: mean batch inertia: 0.3605063973800101, ewa inertia: 0.3822678019485178\n",
      "Minibatch step 132/9765: mean batch inertia: 0.3507090993216733, ewa inertia: 0.3816214861818777\n",
      "Minibatch step 133/9765: mean batch inertia: 0.35335806088040633, ewa inertia: 0.3810426570199952\n",
      "Minibatch step 134/9765: mean batch inertia: 0.36006381781073266, ewa inertia: 0.3806130146894128\n",
      "Minibatch step 135/9765: mean batch inertia: 0.35667303424118013, ewa inertia: 0.38012272879269193\n",
      "Minibatch step 136/9765: mean batch inertia: 0.3562709430208083, ewa inertia: 0.37963424910488064\n",
      "Minibatch step 137/9765: mean batch inertia: 0.36203691560041906, ewa inertia: 0.37927385931860713\n",
      "Minibatch step 138/9765: mean batch inertia: 0.35598637568907676, ewa inertia: 0.3787969364231033\n",
      "Minibatch step 139/9765: mean batch inertia: 0.3577643857712342, ewa inertia: 0.3783661940931763\n",
      "Minibatch step 140/9765: mean batch inertia: 0.3585400470251013, ewa inertia: 0.37796015866157645\n",
      "Minibatch step 141/9765: mean batch inertia: 0.36755683215159796, ewa inertia: 0.37774710066523204\n",
      "Minibatch step 142/9765: mean batch inertia: 0.35761295815574135, ewa inertia: 0.3773347575500688\n",
      "Minibatch step 143/9765: mean batch inertia: 0.3587566748689913, ewa inertia: 0.3769542822215136\n",
      "Minibatch step 144/9765: mean batch inertia: 0.3462712966213395, ewa inertia: 0.37632590096023466\n",
      "Minibatch step 145/9765: mean batch inertia: 0.3533602139048413, ewa inertia: 0.3758555683926659\n",
      "Minibatch step 146/9765: mean batch inertia: 0.35670524888367594, ewa inertia: 0.375463373771068\n",
      "Minibatch step 147/9765: mean batch inertia: 0.3534807387524259, ewa inertia: 0.3750131739078848\n",
      "Minibatch step 148/9765: mean batch inertia: 0.34946151159205846, ewa inertia: 0.37448988109658476\n",
      "Minibatch step 149/9765: mean batch inertia: 0.34406366794054605, ewa inertia: 0.3738667584823752\n",
      "Minibatch step 150/9765: mean batch inertia: 0.351368855739382, ewa inertia: 0.37340600604172314\n",
      "Minibatch step 151/9765: mean batch inertia: 0.3586525882337168, ewa inertia: 0.3731038590664849\n",
      "Minibatch step 152/9765: mean batch inertia: 0.35004428073532035, ewa inertia: 0.37263160362481706\n",
      "Minibatch step 153/9765: mean batch inertia: 0.35036953497208784, ewa inertia: 0.3721756810180352\n",
      "Minibatch step 154/9765: mean batch inertia: 0.3532468532385726, ewa inertia: 0.371788022501697\n",
      "Minibatch step 155/9765: mean batch inertia: 0.36011529984016055, ewa inertia: 0.37154896753213845\n",
      "Minibatch step 156/9765: mean batch inertia: 0.34422977395840065, ewa inertia: 0.3709894760426632\n",
      "Minibatch step 157/9765: mean batch inertia: 0.3582779122072524, ewa inertia: 0.3707291458186162\n",
      "Minibatch step 158/9765: mean batch inertia: 0.3598960778395419, ewa inertia: 0.3705072868049949\n",
      "Minibatch step 159/9765: mean batch inertia: 0.3562516955954268, ewa inertia: 0.3702153352165388\n",
      "Minibatch step 160/9765: mean batch inertia: 0.35080775993478197, ewa inertia: 0.3698178720494001\n",
      "Minibatch step 161/9765: mean batch inertia: 0.351377459780598, ewa inertia: 0.3694402161826937\n",
      "Minibatch step 162/9765: mean batch inertia: 0.34626573137936756, ewa inertia: 0.36896560748000856\n",
      "Minibatch step 163/9765: mean batch inertia: 0.34812716374166436, ewa inertia: 0.36853884041991786\n",
      "Minibatch step 164/9765: mean batch inertia: 0.34942553222125705, ewa inertia: 0.36814740378237565\n",
      "Minibatch step 165/9765: mean batch inertia: 0.3500529368782893, ewa inertia: 0.36777683280588974\n",
      "Minibatch step 166/9765: mean batch inertia: 0.3492798526929559, ewa inertia: 0.3673980184413205\n",
      "Minibatch step 167/9765: mean batch inertia: 0.353731554406343, ewa inertia: 0.367118132056748\n",
      "Minibatch step 168/9765: mean batch inertia: 0.3463176085333717, ewa inertia: 0.36669214159489383\n",
      "Minibatch step 169/9765: mean batch inertia: 0.3492271506188738, ewa inertia: 0.36633446215649934\n",
      "Minibatch step 170/9765: mean batch inertia: 0.3529667846292269, ewa inertia: 0.3660606948584138\n",
      "Minibatch step 171/9765: mean batch inertia: 0.3538063859556718, ewa inertia: 0.365809729121743\n",
      "Minibatch step 172/9765: mean batch inertia: 0.34495582799716673, ewa inertia: 0.36538264549754784\n",
      "Minibatch step 173/9765: mean batch inertia: 0.3484883579191545, ewa inertia: 0.36503665394785784\n",
      "Minibatch step 174/9765: mean batch inertia: 0.3543291213835913, ewa inertia: 0.3648173658738224\n",
      "Minibatch step 175/9765: mean batch inertia: 0.35561721200975693, ewa inertia: 0.364628948606859\n",
      "Minibatch step 176/9765: mean batch inertia: 0.33833666309850385, ewa inertia: 0.3640904879842541\n",
      "Minibatch step 177/9765: mean batch inertia: 0.34253407546962233, ewa inertia: 0.36364901707066355\n",
      "Minibatch step 178/9765: mean batch inertia: 0.3497334100112937, ewa inertia: 0.3633640282879754\n",
      "Minibatch step 179/9765: mean batch inertia: 0.3509147355451975, ewa inertia: 0.36310906932219295\n",
      "Minibatch step 180/9765: mean batch inertia: 0.35102040565756987, ewa inertia: 0.36286149596607503\n",
      "Minibatch step 181/9765: mean batch inertia: 0.3561101594296, ewa inertia: 0.3627232299764679\n",
      "Minibatch step 182/9765: mean batch inertia: 0.34765982888314023, ewa inertia: 0.3624147346070302\n",
      "Minibatch step 183/9765: mean batch inertia: 0.3563571150628899, ewa inertia: 0.3622906757993543\n",
      "Minibatch step 184/9765: mean batch inertia: 0.34040472839284996, ewa inertia: 0.36184245607866633\n",
      "Minibatch step 185/9765: mean batch inertia: 0.34495077020294374, ewa inertia: 0.3614965178113142\n",
      "Minibatch step 186/9765: mean batch inertia: 0.35151309573354367, ewa inertia: 0.36129205937174586\n",
      "Minibatch step 187/9765: mean batch inertia: 0.3426798030253003, ewa inertia: 0.36091088417352263\n",
      "Minibatch step 188/9765: mean batch inertia: 0.34806598929138466, ewa inertia: 0.36064782335694456\n",
      "Minibatch step 189/9765: mean batch inertia: 0.345794989745374, ewa inertia: 0.36034364036640953\n",
      "Minibatch step 190/9765: mean batch inertia: 0.3542236576406671, ewa inertia: 0.36021830437354624\n",
      "Minibatch step 191/9765: mean batch inertia: 0.34917663146680733, ewa inertia: 0.3599921731737282\n",
      "Minibatch step 192/9765: mean batch inertia: 0.35069634480746253, ewa inertia: 0.3598017965125537\n",
      "Minibatch step 193/9765: mean batch inertia: 0.3503560807382843, ewa inertia: 0.3596083501879599\n",
      "Minibatch step 194/9765: mean batch inertia: 0.34894824940786906, ewa inertia: 0.35939003350715043\n",
      "Minibatch step 195/9765: mean batch inertia: 0.3576748191543601, ewa inertia: 0.35935490626847766\n",
      "Minibatch step 196/9765: mean batch inertia: 0.352324094043141, ewa inertia: 0.35921091667399874\n",
      "Minibatch step 197/9765: mean batch inertia: 0.35433561006460773, ewa inertia: 0.35911107139309123\n",
      "Minibatch step 198/9765: mean batch inertia: 0.3520361862420888, ewa inertia: 0.3589661791941207\n",
      "Minibatch step 199/9765: mean batch inertia: 0.3383088677448929, ewa inertia: 0.3585431216862156\n",
      "Minibatch step 200/9765: mean batch inertia: 0.3500157823041769, ewa inertia: 0.3583684835220531\n",
      "Minibatch step 201/9765: mean batch inertia: 0.34341926080447277, ewa inertia: 0.3580623265023672\n",
      "Minibatch step 202/9765: mean batch inertia: 0.34412150382551426, ewa inertia: 0.3577768213089972\n",
      "Minibatch step 203/9765: mean batch inertia: 0.3475261611340434, ewa inertia: 0.35756688988792834\n",
      "Minibatch step 204/9765: mean batch inertia: 0.3482043238849385, ewa inertia: 0.3573751464536214\n",
      "Minibatch step 205/9765: mean batch inertia: 0.34697643220890917, ewa inertia: 0.3571621829155251\n",
      "Minibatch step 206/9765: mean batch inertia: 0.34696240240554876, ewa inertia: 0.35695329349957494\n",
      "Minibatch step 207/9765: mean batch inertia: 0.34506303032010105, ewa inertia: 0.35670978334476083\n",
      "Minibatch step 208/9765: mean batch inertia: 0.3503323259261935, ewa inertia: 0.35657917432291875\n",
      "Minibatch step 209/9765: mean batch inertia: 0.34928372818554965, ewa inertia: 0.35642976508011787\n",
      "Minibatch step 210/9765: mean batch inertia: 0.35391904762528387, ewa inertia: 0.35637834610083263\n",
      "Minibatch step 211/9765: mean batch inertia: 0.3534942087860838, ewa inertia: 0.356319279559292\n",
      "Minibatch step 212/9765: mean batch inertia: 0.3523586159886478, ewa inertia: 0.35623816598050095\n",
      "Minibatch step 213/9765: mean batch inertia: 0.35659458589249765, ewa inertia: 0.35624546538730456\n",
      "Minibatch step 214/9765: mean batch inertia: 0.34440658999871787, ewa inertia: 0.35600300764392373\n",
      "Minibatch step 215/9765: mean batch inertia: 0.34157302456203564, ewa inertia: 0.3557074845456376\n",
      "Minibatch step 216/9765: mean batch inertia: 0.35292861006790704, ewa inertia: 0.3556505737654415\n",
      "Minibatch step 217/9765: mean batch inertia: 0.34664326316827415, ewa inertia: 0.3554661058890903\n",
      "Minibatch step 218/9765: mean batch inertia: 0.3466472272183178, ewa inertia: 0.3552854970600011\n",
      "Minibatch step 219/9765: mean batch inertia: 0.34865425431690955, ewa inertia: 0.35514969056668755\n",
      "Minibatch step 220/9765: mean batch inertia: 0.34880193528983444, ewa inertia: 0.35501968983862486\n",
      "Minibatch step 221/9765: mean batch inertia: 0.35058202597802923, ewa inertia: 0.35492880739158433\n",
      "Minibatch step 222/9765: mean batch inertia: 0.351316098816978, ewa inertia: 0.3548548198598517\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 223/9765: mean batch inertia: 0.35303048303015133, ewa inertia: 0.35481745781519985\n",
      "Minibatch step 224/9765: mean batch inertia: 0.3571872377900121, ewa inertia: 0.3548659904237579\n",
      "Minibatch step 225/9765: mean batch inertia: 0.34971907111328154, ewa inertia: 0.3547605825703578\n",
      "Minibatch step 226/9765: mean batch inertia: 0.3441004837241152, ewa inertia: 0.3545422659291531\n",
      "Minibatch step 227/9765: mean batch inertia: 0.34497442596281136, ewa inertia: 0.3543463185261165\n",
      "Minibatch step 228/9765: mean batch inertia: 0.34252896156847273, ewa inertia: 0.3541043014757944\n",
      "Minibatch step 229/9765: mean batch inertia: 0.33685919117981833, ewa inertia: 0.35375112514869606\n",
      "Minibatch step 230/9765: mean batch inertia: 0.35483667944095754, ewa inertia: 0.3537733570782823\n",
      "Minibatch step 231/9765: mean batch inertia: 0.33672485081857045, ewa inertia: 0.35342420716158257\n",
      "Minibatch step 232/9765: mean batch inertia: 0.3495354701846296, ewa inertia: 0.35334456662469993\n",
      "Minibatch step 233/9765: mean batch inertia: 0.3437304851992524, ewa inertia: 0.3531476722060509\n",
      "Minibatch step 234/9765: mean batch inertia: 0.3406243978188056, ewa inertia: 0.35289119811134106\n",
      "Minibatch step 235/9765: mean batch inertia: 0.34844511647893783, ewa inertia: 0.3528001432700578\n",
      "Minibatch step 236/9765: mean batch inertia: 0.34574472230877995, ewa inertia: 0.3526556496937066\n",
      "Minibatch step 237/9765: mean batch inertia: 0.34635375117628586, ewa inertia: 0.3525265881026857\n",
      "Minibatch step 238/9765: mean batch inertia: 0.344671265309261, ewa inertia: 0.35236571270063044\n",
      "Minibatch step 239/9765: mean batch inertia: 0.3420710030240951, ewa inertia: 0.35215487915479043\n",
      "Minibatch step 240/9765: mean batch inertia: 0.3490132050860755, ewa inertia: 0.35209053831327153\n",
      "Minibatch step 241/9765: mean batch inertia: 0.340129003346509, ewa inertia: 0.35184556852685006\n",
      "Minibatch step 242/9765: mean batch inertia: 0.3431720330136874, ewa inertia: 0.3516679362958628\n",
      "Minibatch step 243/9765: mean batch inertia: 0.34295522490113445, ewa inertia: 0.3514895017508442\n",
      "Minibatch step 244/9765: mean batch inertia: 0.34903212885193924, ewa inertia: 0.3514391752571396\n",
      "Minibatch step 245/9765: mean batch inertia: 0.3413604342293134, ewa inertia: 0.35123276470499526\n",
      "Minibatch step 246/9765: mean batch inertia: 0.34394547064340497, ewa inertia: 0.35108352241503676\n",
      "Minibatch step 247/9765: mean batch inertia: 0.3441933588956927, ewa inertia: 0.35094241327725195\n",
      "Minibatch step 248/9765: mean batch inertia: 0.3458808483392463, ewa inertia: 0.3508387534639197\n",
      "Minibatch step 249/9765: mean batch inertia: 0.3424788074015201, ewa inertia: 0.35066754348066154\n",
      "Minibatch step 250/9765: mean batch inertia: 0.35220610758858517, ewa inertia: 0.350699052958497\n",
      "Minibatch step 251/9765: mean batch inertia: 0.3424647950969246, ewa inertia: 0.3505304170438512\n",
      "Minibatch step 252/9765: mean batch inertia: 0.3448531292800741, ewa inertia: 0.3504141473531459\n",
      "Minibatch step 253/9765: mean batch inertia: 0.34866679582778143, ewa inertia: 0.3503783619517605\n",
      "Minibatch step 254/9765: mean batch inertia: 0.3422689543635495, ewa inertia: 0.350212282945144\n",
      "Minibatch step 255/9765: mean batch inertia: 0.3338820176340085, ewa inertia: 0.34987784245597686\n",
      "Minibatch step 256/9765: mean batch inertia: 0.34781710956072986, ewa inertia: 0.3498356390683161\n",
      "Minibatch step 257/9765: mean batch inertia: 0.3423391009880206, ewa inertia: 0.3496821115037073\n",
      "Minibatch step 258/9765: mean batch inertia: 0.3364985447139158, ewa inertia: 0.34941211475581985\n",
      "Minibatch step 259/9765: mean batch inertia: 0.3407088980664919, ewa inertia: 0.3492338746604233\n",
      "Minibatch step 260/9765: mean batch inertia: 0.34378816678757906, ewa inertia: 0.3491223476784573\n",
      "Minibatch step 261/9765: mean batch inertia: 0.3462812832267844, ewa inertia: 0.3490641632603312\n",
      "Minibatch step 262/9765: mean batch inertia: 0.3433228503800042, ewa inertia: 0.3489465823483512\n",
      "Minibatch step 263/9765: mean batch inertia: 0.34020962191139353, ewa inertia: 0.34876765118791386\n",
      "Minibatch step 264/9765: mean batch inertia: 0.34551790937671667, ewa inertia: 0.348701097141161\n",
      "Minibatch step 265/9765: mean batch inertia: 0.3461633164578182, ewa inertia: 0.3486491239124984\n",
      "Minibatch step 266/9765: mean batch inertia: 0.3406919792340181, ewa inertia: 0.34848616321909004\n",
      "Minibatch step 267/9765: mean batch inertia: 0.339029395585604, ewa inertia: 0.3482924905546829\n",
      "Minibatch step 268/9765: mean batch inertia: 0.3346831344050048, ewa inertia: 0.34801377372790576\n",
      "Minibatch step 269/9765: mean batch inertia: 0.34014005053105173, ewa inertia: 0.3478525214893565\n",
      "Minibatch step 270/9765: mean batch inertia: 0.3448810994252155, ewa inertia: 0.34779166737402406\n",
      "Minibatch step 271/9765: mean batch inertia: 0.3400629166603593, ewa inertia: 0.34763338414224054\n",
      "Minibatch step 272/9765: mean batch inertia: 0.347554934143003, ewa inertia: 0.34763177750232255\n",
      "Minibatch step 273/9765: mean batch inertia: 0.3401960549623525, ewa inertia: 0.3474794954275247\n",
      "Minibatch step 274/9765: mean batch inertia: 0.3403589603566618, ewa inertia: 0.3473336683275444\n",
      "Minibatch step 275/9765: mean batch inertia: 0.3415298665586359, ewa inertia: 0.3472148076559239\n",
      "Minibatch step 276/9765: mean batch inertia: 0.3389570395965346, ewa inertia: 0.3470456902572416\n",
      "Minibatch step 277/9765: mean batch inertia: 0.3409334795088706, ewa inertia: 0.34692051343288316\n",
      "Minibatch step 278/9765: mean batch inertia: 0.3386361196970812, ewa inertia: 0.3467508507458008\n",
      "Minibatch step 279/9765: mean batch inertia: 0.34988796539651856, ewa inertia: 0.3468150982113728\n",
      "Minibatch step 280/9765: mean batch inertia: 0.33702712259301776, ewa inertia: 0.3466146424752662\n",
      "Minibatch step 281/9765: mean batch inertia: 0.3428913364749351, ewa inertia: 0.34653838993090486\n",
      "Minibatch step 282/9765: mean batch inertia: 0.33660177462187174, ewa inertia: 0.34633489008437435\n",
      "Minibatch step 283/9765: mean batch inertia: 0.34220392843996983, ewa inertia: 0.3462502888359094\n",
      "Minibatch step 284/9765: mean batch inertia: 0.34733372884251634, ewa inertia: 0.34627247746535844\n",
      "Minibatch step 285/9765: mean batch inertia: 0.33281736653185223, ewa inertia: 0.3459969195490194\n",
      "Minibatch step 286/9765: mean batch inertia: 0.3477745249207198, ewa inertia: 0.34603332454298186\n",
      "Minibatch step 287/9765: mean batch inertia: 0.34029002637334177, ewa inertia: 0.3459157029726833\n",
      "Minibatch step 288/9765: mean batch inertia: 0.3525378856450533, ewa inertia: 0.34605132391760396\n",
      "Minibatch step 289/9765: mean batch inertia: 0.33552310634300003, ewa inertia: 0.3458357081778335\n",
      "Minibatch step 290/9765: mean batch inertia: 0.34164803048947445, ewa inertia: 0.3457499453964037\n",
      "Minibatch step 291/9765: mean batch inertia: 0.3443535801762453, ewa inertia: 0.3457213481226676\n",
      "Minibatch step 292/9765: mean batch inertia: 0.3497307672035927, ewa inertia: 0.3458034602043241\n",
      "Minibatch step 293/9765: mean batch inertia: 0.3365207507238249, ewa inertia: 0.3456133522152434\n",
      "Minibatch step 294/9765: mean batch inertia: 0.3452382292199374, ewa inertia: 0.34560566977312396\n",
      "Minibatch step 295/9765: mean batch inertia: 0.34022787825536743, ewa inertia: 0.345495533704201\n",
      "Minibatch step 296/9765: mean batch inertia: 0.33174817639509996, ewa inertia: 0.3452139906419412\n",
      "Minibatch step 297/9765: mean batch inertia: 0.3421118871954582, ewa inertia: 0.34515046019866163\n",
      "Minibatch step 298/9765: mean batch inertia: 0.34424558472627226, ewa inertia: 0.34513192853430374\n",
      "Minibatch step 299/9765: mean batch inertia: 0.33636796655709916, ewa inertia: 0.34495244438785205\n",
      "Minibatch step 300/9765: mean batch inertia: 0.337430184113539, ewa inertia: 0.34479839003797763\n",
      "Minibatch step 301/9765: mean batch inertia: 0.3459465540774642, ewa inertia: 0.3448219042023647\n",
      "Minibatch step 302/9765: mean batch inertia: 0.34225184243524864, ewa inertia: 0.34476926986371753\n",
      "Minibatch step 303/9765: mean batch inertia: 0.3381333326428939, ewa inertia: 0.3446333672284614\n",
      "Minibatch step 304/9765: mean batch inertia: 0.3390559772719007, ewa inertia: 0.34451914342438905\n",
      "Minibatch step 305/9765: mean batch inertia: 0.3445765305579054, ewa inertia: 0.3445203187011307\n",
      "Minibatch step 306/9765: mean batch inertia: 0.3380758569965317, ewa inertia: 0.34438833744523306\n",
      "Minibatch step 307/9765: mean batch inertia: 0.33762550573492295, ewa inertia: 0.34424983603681997\n",
      "Minibatch step 308/9765: mean batch inertia: 0.3394438241860964, ewa inertia: 0.3441514098983785\n",
      "Minibatch step 309/9765: mean batch inertia: 0.3322407581192295, ewa inertia: 0.3439074821892186\n",
      "Minibatch step 310/9765: mean batch inertia: 0.3383768719772332, ewa inertia: 0.3437942164247348\n",
      "Minibatch step 311/9765: mean batch inertia: 0.3404524262101211, ewa inertia: 0.34372577724553127\n",
      "Minibatch step 312/9765: mean batch inertia: 0.3435581319157696, ewa inertia: 0.34372234390351114\n",
      "Minibatch step 313/9765: mean batch inertia: 0.33275040406221057, ewa inertia: 0.3434976408225921\n",
      "Minibatch step 314/9765: mean batch inertia: 0.34180871225467985, ewa inertia: 0.34346305191141036\n",
      "Minibatch step 315/9765: mean batch inertia: 0.3366303398711294, ewa inertia: 0.34332311936815085\n",
      "Minibatch step 316/9765: mean batch inertia: 0.33877861413413934, ewa inertia: 0.34323004883166364\n",
      "Minibatch step 317/9765: mean batch inertia: 0.3433525224891979, ewa inertia: 0.34323255706708755\n",
      "Minibatch step 318/9765: mean batch inertia: 0.3447513359062995, ewa inertia: 0.3432636613466718\n",
      "Minibatch step 319/9765: mean batch inertia: 0.33497777098934356, ewa inertia: 0.3430939680090871\n",
      "Minibatch step 320/9765: mean batch inertia: 0.3428399632978239, ewa inertia: 0.3430887660446201\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 321/9765: mean batch inertia: 0.34051047964018494, ewa inertia: 0.343035963267085\n",
      "Minibatch step 322/9765: mean batch inertia: 0.34402477843576235, ewa inertia: 0.3430562139992322\n",
      "Minibatch step 323/9765: mean batch inertia: 0.33671297347531576, ewa inertia: 0.34292630573238503\n",
      "Minibatch step 324/9765: mean batch inertia: 0.3362338985092954, ewa inertia: 0.34278924660304744\n",
      "Minibatch step 325/9765: mean batch inertia: 0.34350165668347704, ewa inertia: 0.3428038366155945\n",
      "Minibatch step 326/9765: mean batch inertia: 0.3382107807845, ewa inertia: 0.34270977177282214\n",
      "Minibatch step 327/9765: mean batch inertia: 0.335675672255832, ewa inertia: 0.3425657148552833\n",
      "Minibatch step 328/9765: mean batch inertia: 0.34264824990350357, ewa inertia: 0.34256740515616785\n",
      "Minibatch step 329/9765: mean batch inertia: 0.3533741030929923, ewa inertia: 0.3427887241167244\n",
      "Minibatch step 330/9765: mean batch inertia: 0.33729689730052054, ewa inertia: 0.3426762526282434\n",
      "Minibatch step 331/9765: mean batch inertia: 0.342540467655039, ewa inertia: 0.3426734717798007\n",
      "Minibatch step 332/9765: mean batch inertia: 0.33770736290971654, ewa inertia: 0.34257176688719027\n",
      "Minibatch step 333/9765: mean batch inertia: 0.3376961321536616, ewa inertia: 0.34247191488636763\n",
      "Minibatch step 334/9765: mean batch inertia: 0.3304265896044052, ewa inertia: 0.342225229091451\n",
      "Minibatch step 335/9765: mean batch inertia: 0.3338667968161874, ewa inertia: 0.34205405011024337\n",
      "Minibatch step 336/9765: mean batch inertia: 0.3435592657190084, ewa inertia: 0.3420848766176458\n",
      "Minibatch step 337/9765: mean batch inertia: 0.33793139174621695, ewa inertia: 0.34199981409810415\n",
      "Minibatch step 338/9765: mean batch inertia: 0.3481194939621244, ewa inertia: 0.3421251438884214\n",
      "Minibatch step 339/9765: mean batch inertia: 0.3466251132265553, ewa inertia: 0.3422173023388819\n",
      "Minibatch step 340/9765: mean batch inertia: 0.3441772391890167, ewa inertia: 0.3422574414441816\n",
      "Minibatch step 341/9765: mean batch inertia: 0.347483845052576, ewa inertia: 0.34236447711972473\n",
      "Minibatch step 342/9765: mean batch inertia: 0.34318724223740266, ewa inertia: 0.3423813271808342\n",
      "Minibatch step 343/9765: mean batch inertia: 0.33414265563516604, ewa inertia: 0.34221260087484195\n",
      "Minibatch step 344/9765: mean batch inertia: 0.3475000320363482, ewa inertia: 0.3423208863821745\n",
      "Minibatch step 345/9765: mean batch inertia: 0.3422657488024478, ewa inertia: 0.34231975717583374\n",
      "Minibatch step 346/9765: mean batch inertia: 0.339090387143289, ewa inertia: 0.3422536203389356\n",
      "Minibatch step 347/9765: mean batch inertia: 0.3433400770917004, ewa inertia: 0.34227587075072813\n",
      "Converged (lack of improvement in inertia) at step 347/9765\n",
      "\n",
      "Clustering complete\n",
      "Valid clusters (size 2-10): 5,083\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "embeddings_normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Determine optimal number of clusters\n",
    "n_clusters = min(CONFIG[\"n_clusters\"], len(terms) // 2)\n",
    "print(f\"Clustering {len(terms):,} terms into {n_clusters:,} clusters...\")\n",
    "\n",
    "# Apply K-means\n",
    "kmeans = MiniBatchKMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    batch_size=1024,\n",
    "    n_init=3,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "cluster_labels = kmeans.fit_predict(embeddings_normalized)\n",
    "print(f\"\\nClustering complete\")\n",
    "\n",
    "# Group terms by cluster\n",
    "clusters = defaultdict(list)\n",
    "for i, label in enumerate(cluster_labels):\n",
    "    clusters[label].append((terms[i], i))\n",
    "\n",
    "# Filter clusters by size\n",
    "valid_clusters = {\n",
    "    label: terms_list \n",
    "    for label, terms_list in clusters.items()\n",
    "    if CONFIG[\"min_cluster_size\"] <= len(terms_list) <= CONFIG[\"max_cluster_size\"]\n",
    "}\n",
    "print(f\"Valid clusters (size {CONFIG['min_cluster_size']}-{CONFIG['max_cluster_size']}): {len(valid_clusters):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract Synonym Pairs from Clusters\n",
    "\n",
    "For each cluster, compute pairwise cosine similarity and extract high-quality synonym pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting synonym pairs (similarity >= 0.7)...\n",
      "Extracted 66,346 synonym pairs from clusters\n"
     ]
    }
   ],
   "source": [
    "def extract_synonym_pairs_from_clusters(\n",
    "    valid_clusters: Dict[int, List[Tuple[str, int]]],\n",
    "    embeddings_normalized: np.ndarray,\n",
    "    similarity_threshold: float = 0.7\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Extract synonym pairs from clusters based on cosine similarity.\"\"\"\n",
    "    synonym_pairs = []\n",
    "    \n",
    "    for cluster_id, terms_list in valid_clusters.items():\n",
    "        if len(terms_list) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Get embeddings for this cluster\n",
    "        cluster_terms = [t[0] for t in terms_list]\n",
    "        cluster_indices = [t[1] for t in terms_list]\n",
    "        cluster_embeddings = embeddings_normalized[cluster_indices]\n",
    "        \n",
    "        # Compute pairwise similarities\n",
    "        similarities = cosine_similarity(cluster_embeddings)\n",
    "        \n",
    "        # Extract pairs above threshold\n",
    "        for i in range(len(cluster_terms)):\n",
    "            for j in range(i + 1, len(cluster_terms)):\n",
    "                sim = similarities[i][j]\n",
    "                if sim >= similarity_threshold:\n",
    "                    # Bidirectional pairs\n",
    "                    synonym_pairs.append({\n",
    "                        \"source\": cluster_terms[i],\n",
    "                        \"target\": cluster_terms[j],\n",
    "                        \"similarity\": float(sim),\n",
    "                        \"relation\": \"synonym\",\n",
    "                        \"category\": \"cluster\"\n",
    "                    })\n",
    "                    synonym_pairs.append({\n",
    "                        \"source\": cluster_terms[j],\n",
    "                        \"target\": cluster_terms[i],\n",
    "                        \"similarity\": float(sim),\n",
    "                        \"relation\": \"synonym\",\n",
    "                        \"category\": \"cluster\"\n",
    "                    })\n",
    "    \n",
    "    return synonym_pairs\n",
    "\n",
    "print(f\"Extracting synonym pairs (similarity >= {CONFIG['similarity_threshold']})...\")\n",
    "cluster_synonym_pairs = extract_synonym_pairs_from_clusters(\n",
    "    valid_clusters, \n",
    "    embeddings_normalized,\n",
    "    CONFIG[\"similarity_threshold\"]\n",
    ")\n",
    "print(f\"Extracted {len(cluster_synonym_pairs):,} synonym pairs from clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Handle OOV Words with BPE\n",
    "\n",
    "For terms not in the tokenizer vocabulary, use BPE subword decomposition\n",
    "to map them to known vocabulary terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting BPE expansion pairs...\n",
      "OOV terms (multi-token): 88,757\n",
      "Extracted 57,148 BPE expansion pairs\n"
     ]
    }
   ],
   "source": [
    "def get_bpe_decomposition(term: str, tokenizer) -> List[str]:\n",
    "    \"\"\"Get BPE subword decomposition for a term.\"\"\"\n",
    "    tokens = tokenizer.tokenize(term)\n",
    "    # Clean subword markers\n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        clean_token = token.replace(\"##\", \"\").replace(\"▁\", \"\").strip()\n",
    "        if clean_token and len(clean_token) >= 2:\n",
    "            clean_tokens.append(clean_token)\n",
    "    return clean_tokens\n",
    "\n",
    "def create_bpe_expansion_pairs(\n",
    "    terms: List[str],\n",
    "    tokenizer,\n",
    "    embeddings_normalized: np.ndarray,\n",
    "    term_to_idx: Dict[str, int]\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Create expansion pairs for OOV terms using BPE decomposition.\"\"\"\n",
    "    bpe_pairs = []\n",
    "    oov_count = 0\n",
    "    \n",
    "    for term in terms:\n",
    "        # Check if term is in vocabulary as a single token\n",
    "        token_ids = tokenizer.encode(term, add_special_tokens=False)\n",
    "        \n",
    "        # If it's tokenized into multiple subwords\n",
    "        if len(token_ids) > 1:\n",
    "            oov_count += 1\n",
    "            subwords = get_bpe_decomposition(term, tokenizer)\n",
    "            \n",
    "            # Create pairs from term to each meaningful subword\n",
    "            for subword in subwords:\n",
    "                if subword in term_to_idx and subword != term:\n",
    "                    # Check semantic similarity\n",
    "                    term_idx = term_to_idx.get(term)\n",
    "                    subword_idx = term_to_idx.get(subword)\n",
    "                    \n",
    "                    if term_idx is not None and subword_idx is not None:\n",
    "                        sim = np.dot(\n",
    "                            embeddings_normalized[term_idx],\n",
    "                            embeddings_normalized[subword_idx]\n",
    "                        )\n",
    "                        if sim >= 0.5:  # Lower threshold for BPE pairs\n",
    "                            bpe_pairs.append({\n",
    "                                \"source\": term,\n",
    "                                \"target\": subword,\n",
    "                                \"similarity\": float(sim),\n",
    "                                \"relation\": \"bpe_expansion\",\n",
    "                                \"category\": \"BPE\"\n",
    "                            })\n",
    "    \n",
    "    print(f\"OOV terms (multi-token): {oov_count:,}\")\n",
    "    return bpe_pairs\n",
    "\n",
    "# Create term to index mapping\n",
    "term_to_idx = {term: i for i, term in enumerate(terms)}\n",
    "\n",
    "# Extract BPE expansion pairs\n",
    "print(\"Extracting BPE expansion pairs...\")\n",
    "bpe_pairs = create_bpe_expansion_pairs(\n",
    "    terms, tokenizer, embeddings_normalized, term_to_idx\n",
    ")\n",
    "print(f\"Extracted {len(bpe_pairs):,} BPE expansion pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Sample Clusters\n",
    "\n",
    "Inspect some example clusters to verify quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample clusters with synonym potential:\n",
      "\n",
      "Cluster 8929 (avg_sim=0.795):\n",
      "  Terms: 시즌, 계절, Season, 시즌전, season, 시즌리그, 시즌팀, 시즌시즌, Seasons, 시즌기간\n",
      "\n",
      "Cluster 6451 (avg_sim=0.732):\n",
      "  Terms: 지방, 지방산, 지방대, 지방관리, 지방조직, 지방질, 지방채, 지방간, 지방비, 지방어\n",
      "\n",
      "Cluster 7079 (avg_sim=0.724):\n",
      "  Terms: 내용, 콘텐츠, 내용물, 내용전개, 내용자체, Content, 내용면, content, 내용이해, Contents\n",
      "\n",
      "Cluster 5116 (avg_sim=0.710):\n",
      "  Terms: 현재, 현행, 현직, 현행법, 현행범, current, Current, 현재상태, 현행맞춤법, 현재시제\n",
      "\n",
      "Cluster 1833 (avg_sim=0.703):\n",
      "  Terms: 위원회, 위원, 심사위원, 위원단, IOC위원, 위원회위원, 위원군, 구미위원부, 심사위원대상, 주최고위원\n",
      "\n",
      "Cluster 2060 (avg_sim=0.708):\n",
      "  Terms: 성공, 성공사례, 성공요인, 성공비결, 성공회사제, 성공여부, 성공작, 명랑소녀 성공기, 성공스토리, 성공가도\n",
      "\n",
      "Cluster 6357 (avg_sim=0.658):\n",
      "  Terms: 황제, 로마황제, 시황제, 역대황제, 중국황제, 독일황제, 러시아황제, 서방황제, 오스트리아황제, 차기황제\n",
      "\n",
      "Cluster 4799 (avg_sim=0.787):\n",
      "  Terms: 달러, 달러화, 달러환율, 미국달러, 홍콩달러, USD, Dollar, 달러강세, 미국달러화, dollar\n",
      "\n",
      "Cluster 1081 (avg_sim=0.784):\n",
      "  Terms: 추진, 추진력, 추진체, 추진단, 추진제, 추진위, 추진키, 추진위원회, 추진비, 추진기관\n",
      "\n",
      "Cluster 3965 (avg_sim=0.671):\n",
      "  Terms: 계속, Again, Another, Still, again, still, IDidItAgain, Bio, Encore, Continue\n",
      "\n",
      "Cluster 2092 (avg_sim=0.732):\n",
      "  Terms: 연속, 연속함수, 연속체, 연속점, 연속체가설, 연속대상, 연속낭독, 연속점프, 연속바코드, 연속스펙트럼\n",
      "\n",
      "Cluster 3546 (avg_sim=0.808):\n",
      "  Terms: 준비, 준비과정, Ready, 준비물, 준비기간, 준비작업, 준비끝, ready, 준비단, 준비단계\n",
      "\n",
      "Cluster 8377 (avg_sim=0.704):\n",
      "  Terms: 물질, 오염물질, 신경전달물질, 이물질, 독성물질, 유해물질, 발암물질, 대기오염물질, 인화물질, 분자물질\n",
      "\n",
      "Cluster 5568 (avg_sim=0.656):\n",
      "  Terms: 신라, 신리, 호텔신라, 신라군, 신라호텔, 신라왕, 신라대학교, 신라어, 신라김, 신라왕족\n",
      "\n",
      "Cluster 7162 (avg_sim=0.757):\n",
      "  Terms: 결국, 결국자신, 결국실패, 결국정부, 결국사망, 결국서인, 결국프랑스, 결국시즌, 결국패배, 결국죽음\n",
      "\n",
      "Cluster 7887 (avg_sim=0.715):\n",
      "  Terms: 유일, 유일신, 유일무이, 유일한, 국내유일, 세계유일, 유일종, 대한민국유일, 유일호, 당시유일\n",
      "\n",
      "Cluster 2219 (avg_sim=0.769):\n",
      "  Terms: 캐나다, 캐나다인, Canada, Canadian, 사람캐나다, 캐나디안, Swiss, 캐나다군, 캐나디언, 캐나다선수\n",
      "\n",
      "Cluster 3275 (avg_sim=0.753):\n",
      "  Terms: 입장, 관람, 입장료, 관람료, 입장권, 입장차이, 입장차, 입장객, 입장인원수, 입장면\n",
      "\n",
      "Cluster 7151 (avg_sim=0.763):\n",
      "  Terms: 지난해, 작년, 지난해매출, 올해처음, 작년시즌, 올해매출, 지난해기준, 작년매출, 지난해한국, 지난해처음\n",
      "\n",
      "Cluster 1483 (avg_sim=0.763):\n",
      "  Terms: 명령, 명령어, 명령어집합, 커맨드, Command, command, Commander, 명령형, 커맨더, 명령줄\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display sample clusters\n",
    "print(\"Sample clusters with synonym potential:\\n\")\n",
    "\n",
    "sample_count = 0\n",
    "for cluster_id, terms_list in sorted(valid_clusters.items(), key=lambda x: -len(x[1])):\n",
    "    if sample_count >= 20:\n",
    "        break\n",
    "    \n",
    "    cluster_terms = [t[0] for t in terms_list]\n",
    "    cluster_indices = [t[1] for t in terms_list]\n",
    "    \n",
    "    # Compute average similarity within cluster\n",
    "    if len(cluster_terms) >= 2:\n",
    "        cluster_embeddings = embeddings_normalized[cluster_indices]\n",
    "        sims = cosine_similarity(cluster_embeddings)\n",
    "        avg_sim = (sims.sum() - len(cluster_terms)) / (len(cluster_terms) * (len(cluster_terms) - 1))\n",
    "        \n",
    "        if avg_sim >= 0.6:  # Only show high-quality clusters\n",
    "            print(f\"Cluster {cluster_id} (avg_sim={avg_sim:.3f}):\")\n",
    "            print(f\"  Terms: {', '.join(cluster_terms)}\")\n",
    "            print()\n",
    "            sample_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Merge and Save All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total pairs collected:\n",
      "  Cluster synonyms: 66,346\n",
      "  BPE expansions:   57,148\n",
      "  ------------------------------\n",
      "  Total:            123,494\n"
     ]
    }
   ],
   "source": [
    "# Merge all pairs\n",
    "all_pairs = cluster_synonym_pairs + bpe_pairs\n",
    "\n",
    "print(f\"\\nTotal pairs collected:\")\n",
    "print(f\"  Cluster synonyms: {len(cluster_synonym_pairs):,}\")\n",
    "print(f\"  BPE expansions:   {len(bpe_pairs):,}\")\n",
    "print(f\"  \" + \"-\" * 30)\n",
    "print(f\"  Total:            {len(all_pairs):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique pairs after deduplication: 119,753\n",
      "Saved to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.1_korean_general/korean_synonym_pairs.jsonl\n",
      "Total pairs: 119,753\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates and self-pairs\n",
    "seen = set()\n",
    "unique_pairs = []\n",
    "\n",
    "for pair in all_pairs:\n",
    "    # Skip self-pairs\n",
    "    if pair[\"source\"] == pair[\"target\"]:\n",
    "        continue\n",
    "    \n",
    "    key = (pair[\"source\"], pair[\"target\"])\n",
    "    if key not in seen:\n",
    "        seen.add(key)\n",
    "        unique_pairs.append(pair)\n",
    "\n",
    "print(f\"Unique pairs after deduplication: {len(unique_pairs):,}\")\n",
    "\n",
    "# Sort by similarity (highest first)\n",
    "unique_pairs.sort(key=lambda x: -x.get(\"similarity\", 0))\n",
    "\n",
    "# Save to JSONL\n",
    "output_path = OUTPUT_DIR / \"korean_synonym_pairs.jsonl\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for pair in unique_pairs:\n",
    "        f.write(json.dumps(pair, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved to: {output_path}\")\n",
    "print(f\"Total pairs: {len(unique_pairs):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "By Category:\n",
      "  cluster: 66,346\n",
      "  BPE: 53,407\n",
      "\n",
      "By Relation:\n",
      "  synonym: 66,346\n",
      "  bpe_expansion: 53,407\n",
      "\n",
      "Similarity statistics:\n",
      "  Min: 0.500\n",
      "  Max: 0.994\n",
      "  Mean: 0.749\n",
      "  Median: 0.757\n"
     ]
    }
   ],
   "source": [
    "# Statistics by category and relation\n",
    "categories = Counter(p[\"category\"] for p in unique_pairs)\n",
    "relations = Counter(p[\"relation\"] for p in unique_pairs)\n",
    "\n",
    "print(\"\\nBy Category:\")\n",
    "for cat, count in categories.most_common():\n",
    "    print(f\"  {cat}: {count:,}\")\n",
    "\n",
    "print(\"\\nBy Relation:\")\n",
    "for rel, count in relations.most_common():\n",
    "    print(f\"  {rel}: {count:,}\")\n",
    "\n",
    "# Similarity distribution\n",
    "if unique_pairs:\n",
    "    similarities = [p.get(\"similarity\", 0) for p in unique_pairs]\n",
    "    print(f\"\\nSimilarity statistics:\")\n",
    "    print(f\"  Min: {min(similarities):.3f}\")\n",
    "    print(f\"  Max: {max(similarities):.3f}\")\n",
    "    print(f\"  Mean: {np.mean(similarities):.3f}\")\n",
    "    print(f\"  Median: {np.median(similarities):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 30 synonym pairs (highest similarity):\n",
      "  다큐멘터리 -> 다큐멘타리 (sim=0.994, synonym)\n",
      "  다큐멘타리 -> 다큐멘터리 (sim=0.994, synonym)\n",
      "  인터컨티넨탈 -> 인터콘티넨탈 (sim=0.993, synonym)\n",
      "  인터콘티넨탈 -> 인터컨티넨탈 (sim=0.993, synonym)\n",
      "  세키가하라전투 -> 세키가하라 전투 (sim=0.993, synonym)\n",
      "  세키가하라 전투 -> 세키가하라전투 (sim=0.993, synonym)\n",
      "  헐리우드영화 -> 헐리웃영화 (sim=0.991, synonym)\n",
      "  헐리웃영화 -> 헐리우드영화 (sim=0.991, synonym)\n",
      "  하버드대학교 -> 하버드대학 (sim=0.991, synonym)\n",
      "  하버드대학 -> 하버드대학교 (sim=0.991, synonym)\n",
      "  O.S.T -> O.S.T. (sim=0.989, synonym)\n",
      "  O.S.T. -> O.S.T (sim=0.989, synonym)\n",
      "  국가사회주의독일 노동자당 -> 국가사회주의독일노동자당 (sim=0.988, synonym)\n",
      "  국가사회주의독일노동자당 -> 국가사회주의독일 노동자당 (sim=0.988, synonym)\n",
      "  컨트롤 -> 콘트롤 (sim=0.988, synonym)\n",
      "  콘트롤 -> 컨트롤 (sim=0.988, synonym)\n",
      "  Encyclopaedia -> Encyclopædia (sim=0.988, synonym)\n",
      "  Encyclopædia -> Encyclopaedia (sim=0.988, synonym)\n",
      "  엘리베이터 -> 엘레베이터 (sim=0.987, synonym)\n",
      "  엘레베이터 -> 엘리베이터 (sim=0.987, synonym)\n",
      "  외무장관 -> 외무부장관 (sim=0.987, synonym)\n",
      "  외무부장관 -> 외무장관 (sim=0.987, synonym)\n",
      "  취임후 -> 취임이후 (sim=0.987, synonym)\n",
      "  취임이후 -> 취임후 (sim=0.987, synonym)\n",
      "  인터내셔널 -> 인터내셔날 (sim=0.987, synonym)\n",
      "  인터내셔날 -> 인터내셔널 (sim=0.987, synonym)\n",
      "  고속도로나들목충청남도 -> 고속도로나들목충청북도 (sim=0.987, synonym)\n",
      "  고속도로나들목충청북도 -> 고속도로나들목충청남도 (sim=0.987, synonym)\n",
      "  H.O.T. -> H.O.T (sim=0.986, synonym)\n",
      "  H.O.T -> H.O.T. (sim=0.986, synonym)\n"
     ]
    }
   ],
   "source": [
    "# Sample high-quality synonym pairs\n",
    "print(\"\\nTop 30 synonym pairs (highest similarity):\")\n",
    "for p in unique_pairs[:30]:\n",
    "    print(f\"  {p['source']} -> {p['target']} (sim={p.get('similarity', 0):.3f}, {p['relation']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create SPLADE Training Dataset with Hard Negatives\n",
    "\n",
    "Generate triplet dataset (anchor, positive, negative) using Hard Negative Mining.\n",
    "\n",
    "**Hard Negative**: 의미적으로 유사하지만 정답이 아닌 문서\n",
    "- BGE-M3 임베딩으로 유사도 계산\n",
    "- Top-K 유사 용어 중 동의어가 아닌 것을 Hard Negative로 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating synonym lookup table...\n",
      "Lookup table size: 64,226 terms\n",
      "\n",
      "Generating triplet dataset with hard negatives...\n",
      "Processing pair 0/119,753...\n",
      "Processing pair 10,000/119,753...\n",
      "Processing pair 20,000/119,753...\n",
      "Processing pair 30,000/119,753...\n",
      "Processing pair 40,000/119,753...\n",
      "Processing pair 50,000/119,753...\n",
      "Processing pair 60,000/119,753...\n",
      "Processing pair 70,000/119,753...\n",
      "Processing pair 80,000/119,753...\n",
      "Processing pair 90,000/119,753...\n",
      "Processing pair 100,000/119,753...\n",
      "Processing pair 110,000/119,753...\n",
      "\n",
      "Generated 119,753 triplets\n",
      "Dataset features: {'anchor': Value(dtype='string', id=None), 'positive': Value(dtype='string', id=None), 'negative': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "def create_synonym_lookup(unique_pairs: List[Dict]) -> Dict[str, Set[str]]:\n",
    "    \"\"\"Create a lookup table: source -> set of synonyms.\"\"\"\n",
    "    synonym_lookup = defaultdict(set)\n",
    "    for pair in unique_pairs:\n",
    "        synonym_lookup[pair[\"source\"]].add(pair[\"target\"])\n",
    "        synonym_lookup[pair[\"target\"]].add(pair[\"source\"])\n",
    "    return synonym_lookup\n",
    "\n",
    "def find_hard_negatives(\n",
    "    term: str,\n",
    "    term_to_idx: Dict[str, int],\n",
    "    embeddings_normalized: np.ndarray,\n",
    "    synonym_lookup: Dict[str, Set[str]],\n",
    "    terms: List[str],\n",
    "    top_k: int = 50,\n",
    "    n_negatives: int = 3\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Find hard negatives for a term using embedding similarity.\n",
    "    \n",
    "    Hard Negative: Similar in embedding space but NOT a synonym.\n",
    "    \"\"\"\n",
    "    if term not in term_to_idx:\n",
    "        return []\n",
    "    \n",
    "    term_idx = term_to_idx[term]\n",
    "    term_embedding = embeddings_normalized[term_idx]\n",
    "    \n",
    "    # Compute similarities with all terms\n",
    "    similarities = np.dot(embeddings_normalized, term_embedding)\n",
    "    \n",
    "    # Get top-k similar terms (excluding self)\n",
    "    top_indices = np.argsort(similarities)[::-1][1:top_k+1]\n",
    "    \n",
    "    # Filter: similar but NOT synonym\n",
    "    synonyms = synonym_lookup.get(term, set())\n",
    "    hard_negatives = []\n",
    "    \n",
    "    for idx in top_indices:\n",
    "        candidate = terms[idx]\n",
    "        # Hard negative: similar (top-k) but not a synonym\n",
    "        if candidate not in synonyms and candidate != term:\n",
    "            hard_negatives.append(candidate)\n",
    "            if len(hard_negatives) >= n_negatives:\n",
    "                break\n",
    "    \n",
    "    return hard_negatives\n",
    "\n",
    "# Create synonym lookup\n",
    "print(\"Creating synonym lookup table...\")\n",
    "synonym_lookup = create_synonym_lookup(unique_pairs)\n",
    "print(f\"Lookup table size: {len(synonym_lookup):,} terms\")\n",
    "\n",
    "# Generate triplet dataset\n",
    "print(\"\\nGenerating triplet dataset with hard negatives...\")\n",
    "triplets = []\n",
    "\n",
    "for i, pair in enumerate(unique_pairs):\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"Processing pair {i:,}/{len(unique_pairs):,}...\")\n",
    "    \n",
    "    anchor = pair[\"source\"]\n",
    "    positive = pair[\"target\"]\n",
    "    \n",
    "    # Find hard negatives\n",
    "    hard_negs = find_hard_negatives(\n",
    "        anchor, term_to_idx, embeddings_normalized, \n",
    "        synonym_lookup, terms, top_k=50, n_negatives=1\n",
    "    )\n",
    "    \n",
    "    if hard_negs:\n",
    "        triplets.append({\n",
    "            \"anchor\": anchor,\n",
    "            \"positive\": positive,\n",
    "            \"negative\": hard_negs[0]  # Use first hard negative\n",
    "        })\n",
    "\n",
    "print(f\"\\nGenerated {len(triplets):,} triplets\")\n",
    "\n",
    "# Create HuggingFace Dataset\n",
    "triplet_dataset = Dataset.from_dict({\n",
    "    \"anchor\": [t[\"anchor\"] for t in triplets],\n",
    "    \"positive\": [t[\"positive\"] for t in triplets],\n",
    "    \"negative\": [t[\"negative\"] for t in triplets],\n",
    "})\n",
    "print(f\"Dataset features: {triplet_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a62c6bebce4120b552a33138225fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/119753 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved triplet dataset to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.1_korean_general/splade_triplet_dataset\n",
      "Saved JSONL to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.1_korean_general/splade_triplet_dataset.jsonl\n",
      "\n",
      "================================================================================\n",
      "Sample Triplets (Anchor -> Positive vs Negative)\n",
      "================================================================================\n",
      "  Anchor:   다큐멘터리               \n",
      "  Positive: 다큐멘타리                (synonym)\n",
      "  Negative: 다큐                   (hard negative)\n",
      "\n",
      "  Anchor:   다큐멘타리               \n",
      "  Positive: 다큐멘터리                (synonym)\n",
      "  Negative: 다큐영화                 (hard negative)\n",
      "\n",
      "  Anchor:   인터컨티넨탈              \n",
      "  Positive: 인터콘티넨탈               (synonym)\n",
      "  Negative: 인터내셔날                (hard negative)\n",
      "\n",
      "  Anchor:   인터콘티넨탈              \n",
      "  Positive: 인터컨티넨탈               (synonym)\n",
      "  Negative: 인터내셔날                (hard negative)\n",
      "\n",
      "  Anchor:   세키가하라전투             \n",
      "  Positive: 세키가하라 전투             (synonym)\n",
      "  Negative: 오케하자마 전투             (hard negative)\n",
      "\n",
      "  Anchor:   세키가하라 전투            \n",
      "  Positive: 세키가하라전투              (synonym)\n",
      "  Negative: 오케하자마 전투             (hard negative)\n",
      "\n",
      "  Anchor:   헐리우드영화              \n",
      "  Positive: 헐리웃영화                (synonym)\n",
      "  Negative: 미국영화                 (hard negative)\n",
      "\n",
      "  Anchor:   헐리웃영화               \n",
      "  Positive: 헐리우드영화               (synonym)\n",
      "  Negative: 미국영화                 (hard negative)\n",
      "\n",
      "  Anchor:   하버드대학교              \n",
      "  Positive: 하버드대학                (synonym)\n",
      "  Negative: 하이델베르크대학             (hard negative)\n",
      "\n",
      "  Anchor:   하버드대학               \n",
      "  Positive: 하버드대학교               (synonym)\n",
      "  Negative: 하이델베르크대학             (hard negative)\n",
      "\n",
      "  Anchor:   O.S.T               \n",
      "  Positive: O.S.T.               (synonym)\n",
      "  Negative: H.O.T                (hard negative)\n",
      "\n",
      "  Anchor:   O.S.T.              \n",
      "  Positive: O.S.T                (synonym)\n",
      "  Negative: H.O.T.               (hard negative)\n",
      "\n",
      "  Anchor:   국가사회주의독일 노동자당       \n",
      "  Positive: 국가사회주의독일노동자당         (synonym)\n",
      "  Negative: 독일사회민주당              (hard negative)\n",
      "\n",
      "  Anchor:   국가사회주의독일노동자당        \n",
      "  Positive: 국가사회주의독일 노동자당        (synonym)\n",
      "  Negative: 독일사회민주당              (hard negative)\n",
      "\n",
      "  Anchor:   컨트롤                 \n",
      "  Positive: 콘트롤                  (synonym)\n",
      "  Negative: 컨트롤러                 (hard negative)\n",
      "\n",
      "  Anchor:   콘트롤                 \n",
      "  Positive: 컨트롤                  (synonym)\n",
      "  Negative: 컨트롤러                 (hard negative)\n",
      "\n",
      "  Anchor:   Encyclopaedia       \n",
      "  Positive: Encyclopædia         (synonym)\n",
      "  Negative: 위키피디아                (hard negative)\n",
      "\n",
      "  Anchor:   Encyclopædia        \n",
      "  Positive: Encyclopaedia        (synonym)\n",
      "  Negative: 위키피디아                (hard negative)\n",
      "\n",
      "  Anchor:   엘리베이터               \n",
      "  Positive: 엘레베이터                (synonym)\n",
      "  Negative: 리프트                  (hard negative)\n",
      "\n",
      "  Anchor:   엘레베이터               \n",
      "  Positive: 엘리베이터                (synonym)\n",
      "  Negative: 리프트                  (hard negative)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save triplet dataset\n",
    "triplet_dataset_path = OUTPUT_DIR / \"splade_triplet_dataset\"\n",
    "triplet_dataset.save_to_disk(str(triplet_dataset_path))\n",
    "print(f\"Saved triplet dataset to: {triplet_dataset_path}\")\n",
    "\n",
    "# Also save as JSONL\n",
    "triplet_jsonl_path = OUTPUT_DIR / \"splade_triplet_dataset.jsonl\"\n",
    "with open(triplet_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for t in triplets:\n",
    "        f.write(json.dumps(t, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved JSONL to: {triplet_jsonl_path}\")\n",
    "\n",
    "# Display sample triplets\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Sample Triplets (Anchor -> Positive vs Negative)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for t in triplets[:20]:\n",
    "    print(f\"  Anchor:   {t['anchor']:20}\")\n",
    "    print(f\"  Positive: {t['positive']:20} (synonym)\")\n",
    "    print(f\"  Negative: {t['negative']:20} (hard negative)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Split Dataset into Train/Test (7:3)\n",
    "\n",
    "Split the triplet dataset into training (70%) and test (30%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset into train (70%) and test (30%)...\n",
      "\n",
      "Dataset split:\n",
      "  Train: 83,827 samples (70%)\n",
      "  Test:  35,926 samples (30%)\n",
      "  Total: 119,753 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c95f74f5e78427d920b707bd175e796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/83827 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d575849fb7f04cb988e6bca73755bc98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/35926 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved train dataset to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.1_korean_general/train_dataset\n",
      "Saved test dataset to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.1_korean_general/test_dataset\n",
      "Saved train JSONL to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.1_korean_general/train_triplets.jsonl\n",
      "Saved test JSONL to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.1_korean_general/test_triplets.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Split into train (70%) and test (30%)\n",
    "print(\"Splitting dataset into train (70%) and test (30%)...\")\n",
    "dataset_split = triplet_dataset.train_test_split(test_size=0.3, seed=42)\n",
    "\n",
    "train_dataset = dataset_split[\"train\"]\n",
    "test_dataset = dataset_split[\"test\"]\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"  Train: {len(train_dataset):,} samples (70%)\")\n",
    "print(f\"  Test:  {len(test_dataset):,} samples (30%)\")\n",
    "print(f\"  Total: {len(train_dataset) + len(test_dataset):,} samples\")\n",
    "\n",
    "# Save train and test datasets separately\n",
    "train_path = OUTPUT_DIR / \"train_dataset\"\n",
    "test_path = OUTPUT_DIR / \"test_dataset\"\n",
    "\n",
    "train_dataset.save_to_disk(str(train_path))\n",
    "test_dataset.save_to_disk(str(test_path))\n",
    "\n",
    "print(f\"\\nSaved train dataset to: {train_path}\")\n",
    "print(f\"Saved test dataset to: {test_path}\")\n",
    "\n",
    "# Also save as JSONL for easy inspection\n",
    "train_jsonl_path = OUTPUT_DIR / \"train_triplets.jsonl\"\n",
    "test_jsonl_path = OUTPUT_DIR / \"test_triplets.jsonl\"\n",
    "\n",
    "with open(train_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in train_dataset:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(test_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in test_dataset:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved train JSONL to: {train_jsonl_path}\")\n",
    "print(f\"Saved test JSONL to: {test_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "ML-based data collection with diverse domains complete.\n",
    "\n",
    "### Data Pipeline\n",
    "1. **Data Sources**: 다양한 도메인의 한국어 데이터셋\n",
    "2. **Term Extraction**: Kiwi 형태소 분석기 (고유명사/복합명사)\n",
    "3. **Embeddings**: BGE-M3 (Teacher model)\n",
    "4. **Clustering**: K-means → 동의어 추출\n",
    "5. **Hard Negative Mining**: BGE-M3 유사도 기반\n",
    "6. **Train/Test Split**: 7:3 비율 (seed=42)\n",
    "\n",
    "### Data Sources (Diverse Domains)\n",
    "\n",
    "| Domain | Dataset | Content |\n",
    "|--------|---------|---------|\n",
    "| 백과사전 | Wikipedia | 일반 지식, 역사, 과학, 문화 (100K) |\n",
    "| 뉴스/QA | KLUE-MRC | 뉴스 기반 질의응답 |\n",
    "| QA | KorQuAD | 한국어 질의응답 |\n",
    "| 리뷰 | NSMC | 영화 리뷰 (150K) |\n",
    "| 대화 | KorHate | 온라인 대화 |\n",
    "| 뉴스 | KLUE-YNAT | 뉴스 제목 분류 |\n",
    "| 유사도 | KLUE-STS | 문장 유사도 |\n",
    "| NLI | KLUE-NLI | 자연어 추론 |\n",
    "| 지시 | KoAlpaca | 다양한 지시문 (50K) |\n",
    "| QA | KorQuAD-Chat | 대화형 QA (30K) |\n",
    "\n",
    "### Output Files\n",
    "\n",
    "| File | Format | Description |\n",
    "|------|--------|-------------|\n",
    "| `korean_synonym_pairs.jsonl` | `{source, target, similarity}` | Raw pairs |\n",
    "| `splade_triplet_dataset/` | `{anchor, positive, negative}` | Full triplet dataset |\n",
    "| `train_dataset/` | HuggingFace Dataset | **Train set (70%)** |\n",
    "| `test_dataset/` | HuggingFace Dataset | **Test set (30%)** |\n",
    "| `train_triplets.jsonl` | JSONL | Train backup |\n",
    "| `test_triplets.jsonl` | JSONL | Test backup |\n",
    "\n",
    "### Dataset Format\n",
    "\n",
    "```python\n",
    "# Triplet format for SparseTripletLoss\n",
    "{\n",
    "    \"anchor\": \"추천\",              # Query\n",
    "    \"positive\": \"권장\",            # Synonym (정답)\n",
    "    \"negative\": \"제안\"             # Hard negative (유사하지만 오답)\n",
    "}\n",
    "```\n",
    "\n",
    "### Usage with Sentence Transformers v5\n",
    "\n",
    "```python\n",
    "from datasets import load_from_disk\n",
    "from sentence_transformers.sparse_encoder import SparseEncoder\n",
    "from sentence_transformers.sparse_encoder.losses import SpladeLoss, SparseTripletLoss\n",
    "\n",
    "# Load train/test datasets\n",
    "train_dataset = load_from_disk(\"dataset/v21.1_korean_general/train_dataset\")\n",
    "test_dataset = load_from_disk(\"dataset/v21.1_korean_general/test_dataset\")\n",
    "\n",
    "# SparseTripletLoss with hard negatives\n",
    "loss = SpladeLoss(\n",
    "    model=model, \n",
    "    loss=SparseTripletLoss(model=model)\n",
    ")\n",
    "```\n",
    "\n",
    "Next step: Run `02_training.ipynb` with train/test datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

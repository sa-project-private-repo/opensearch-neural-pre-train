{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v21.2 Data Ingestion - Knowledge Distillation Dataset (General + Legal/Medical)\n",
    "\n",
    "This notebook creates a **general-purpose** SPLADE training dataset with **legal and medical domain** enhancement.\n",
    "\n",
    "## Changes from v21.1\n",
    "| Feature | v21.1 | v21.2 |\n",
    "|---------|-------|-------|\n",
    "| Domains | 10 general domains | **12 domains (+ë²•ë¥ , +ì˜ë£Œ)** |\n",
    "| Legal Data | None | **Korean Law Precedents, Law.go.kr** |\n",
    "| Medical Data | KoAlpaca (general) | **KorMedLawQA, Medical Collection** |\n",
    "| Similarity Threshold | 0.7 | **0.75 (stricter)** |\n",
    "| Clusters | 10,000 | **12,000 (more diversity)** |\n",
    "\n",
    "## Methodology (Based on Sentence Transformers v5)\n",
    "\n",
    "1. **Data Collection**: ë‹¤ì–‘í•œ ë„ë©”ì¸ì˜ í•œêµ­ì–´ ë°ì´í„°ì…‹ ìˆ˜ì§‘ (ë²•ë¥ /ì˜ë£Œ ê°•í™”)\n",
    "2. **Term Extraction**: Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°ë¡œ ê³ ìœ ëª…ì‚¬/ë³µí•©ëª…ì‚¬ ì¶”ì¶œ\n",
    "3. **Embedding**: BGE-M3 (Teacher) ë¡œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°\n",
    "4. **Clustering**: K-meansë¡œ ë™ì˜ì–´ ê·¸ë£¹ ì¶”ì¶œ\n",
    "5. **Dataset Format**: Triplet (anchor, positive, negative) ìƒì„±\n",
    "\n",
    "## Data Sources (Diverse Domains + Legal/Medical)\n",
    "\n",
    "| Domain | Dataset | Description |\n",
    "|--------|---------|-------------|\n",
    "| ë°±ê³¼ì‚¬ì „ | Korean Wikipedia | ì¼ë°˜ ì§€ì‹, ì—­ì‚¬, ê³¼í•™, ë¬¸í™” |\n",
    "| ë‰´ìŠ¤ | KLUE-MRC, KorQuAD | ë‰´ìŠ¤ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ |\n",
    "| **ë²•ë¥ ** | **Korean Law Precedents** | **ë²•ë¥  ìš©ì–´, íŒë¡€** |\n",
    "| **ë²•ë¥ ** | **Law.go.kr** | **ë²•ë¥  ì¡°ë¬¸, ë²•ë ¹** |\n",
    "| **ì˜ë£Œ** | **KorMedLawQA** | **ì˜ë£Œë²• QA (SNUH 2025)** |\n",
    "| **ì˜ë£Œ** | **Korean Medical Collection** | **ì˜í•™ ìš©ì–´, ì¦ìƒ, ì§ˆë³‘** |\n",
    "| ê¸ˆìœµ | Korean Finance | ê¸ˆìœµ/ê²½ì œ ìš©ì–´ |\n",
    "| ëŒ€í™” | Korean Dialogue | ì¼ìƒ ëŒ€í™”, ê³ ê° ìƒë‹´ |\n",
    "| ë¦¬ë·° | NSMC | ì˜í™”/ì œí’ˆ ë¦¬ë·° |\n",
    "| ê³¼í•™ | AI Hub Science QA | ê³¼í•™ ìš©ì–´, ê°œë… |\n",
    "\n",
    "## Dataset Format for SPLADE Training\n",
    "\n",
    "```python\n",
    "# Triplet format for SparseTripletLoss\n",
    "{\n",
    "    \"anchor\": \"ì†í•´ë°°ìƒ\",           # Query (ë²•ë¥  ìš©ì–´)\n",
    "    \"positive\": \"ë°°ìƒì±…ì„\",         # Synonym (ì •ë‹µ)\n",
    "    \"negative\": \"ì†í•´ë³´í—˜\"          # Hard negative (ìœ ì‚¬í•˜ì§€ë§Œ ì˜¤ë‹µ)\n",
    "}\n",
    "```\n",
    "\n",
    "## Reference\n",
    "- [HuggingFace: Training Sparse Encoders](https://huggingface.co/blog/train-sparse-encoder)\n",
    "- SPLADE v3: Knowledge Distillation with SparseDistillKLDivLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Set, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.2_korean_legal_medical\n",
      "Config: {'min_term_freq': 3, 'max_terms': 120000, 'embedding_batch_size': 64, 'n_clusters': 12000, 'min_cluster_size': 2, 'max_cluster_size': 10, 'similarity_threshold': 0.75}\n"
     ]
    }
   ],
   "source": [
    "# Output directory - v21.2 with legal/medical domains\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"dataset\" / \"v21.2_korean_legal_medical\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Configuration (tuned for v21.2 with stricter threshold)\n",
    "CONFIG = {\n",
    "    \"min_term_freq\": 3,           # Minimum frequency for a term\n",
    "    \"max_terms\": 120000,          # Increased to 120K terms for legal/medical\n",
    "    \"embedding_batch_size\": 64,   # Batch size for BGE-M3 embeddings\n",
    "    \"n_clusters\": 12000,          # Increased clusters for more terms\n",
    "    \"min_cluster_size\": 2,        # Minimum terms per cluster to form synonyms\n",
    "    \"max_cluster_size\": 10,       # Maximum terms per cluster\n",
    "    \"similarity_threshold\": 0.75, # Increased from 0.7 for higher quality pairs\n",
    "}\n",
    "print(f\"Config: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Diverse Korean Datasets from HuggingFace\n",
    "\n",
    "Load Korean text data from various domains to create a general-purpose model:\n",
    "\n",
    "### Domain Coverage\n",
    "- **ë°±ê³¼ì‚¬ì „**: Wikipedia (ì¼ë°˜ ì§€ì‹)\n",
    "- **ë‰´ìŠ¤/QA**: KLUE-MRC, KorQuAD (ë‰´ìŠ¤, ì§ˆì˜ì‘ë‹µ)\n",
    "- **ë²•ë¥ **: Legal QA datasets\n",
    "- **ì˜ë£Œ**: Medical QA, health-related texts\n",
    "- **ê¸ˆìœµ**: Financial news, reports\n",
    "- **ëŒ€í™”**: Dialogue, customer service\n",
    "- **ë¦¬ë·°**: Movie/product reviews (NSMC)\n",
    "- **ê³¼í•™/ê¸°ìˆ **: Science QA, technical documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[1/12] Loading Korean Wikipedia (ë°±ê³¼ì‚¬ì „)...\n",
      "  âœ“ Wikipedia: 96,359 texts\n",
      "\n",
      "[2/12] Loading KLUE-MRC (ë‰´ìŠ¤ ê¸°ë°˜ QA)...\n",
      "  âœ“ KLUE-MRC: 35,108 texts\n",
      "\n",
      "[3/12] Loading KorQuAD (í•œêµ­ì–´ QA)...\n",
      "  âœ“ KorQuAD: 120,814 texts\n",
      "\n",
      "[4/12] Loading NSMC (ì˜í™” ë¦¬ë·°)...\n",
      "  âœ“ NSMC: 134,112 texts\n",
      "\n",
      "[5/12] Loading Korean Dialogue (ëŒ€í™”)...\n",
      "  âœ“ KorHate: 7,896 texts\n",
      "\n",
      "[6/12] Loading Korean News (ë‰´ìŠ¤)...\n",
      "  âœ“ KLUE-YNAT: 45,392 texts\n",
      "\n",
      "[7/12] Loading KLUE-STS (ë¬¸ì¥ ìœ ì‚¬ë„)...\n",
      "  âœ“ KLUE-STS: 23,336 texts\n",
      "\n",
      "[8/12] Loading KLUE-NLI (ìì—°ì–´ ì¶”ë¡ )...\n",
      "  âœ“ KLUE-NLI: 49,996 texts\n",
      "\n",
      "[9/12] Loading KoAlpaca (ì§€ì‹œë¬¸)...\n",
      "  âœ“ KoAlpaca: 50,000 texts\n",
      "\n",
      "[10/12] Loading Korean Law Precedents (ë²•ë¥  íŒë¡€)...\n",
      "  âœ“ Law Precedents: 80,000 texts\n",
      "\n",
      "[11/12] Loading KorMedMCQA (ì˜ë£Œ ìê²©ì‹œí—˜ QA)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faea8406e5fc4f51b7a7fe623e7c7323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ— KorMedMCQA failed: Config name is missing.\n",
      "Please pick one among the available configs: ['dentist', 'doctor', 'nurse', 'pharm']\n",
      "Example of usage:\n",
      "\t`load_dataset('sean0042/KorMedMCQA', 'dentist')`\n",
      "\n",
      "[12/12] Loading Open Korean Instructions (ë‹¤ì–‘í•œ ì§€ì‹œë¬¸)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e201f32b11e4f82b994456319b22b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/3.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd64fc7da1d4ece8e9e298dcdec0905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/106M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5112b6ebe20248908f4c89debe2b2b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/38.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78362ff5c594bb6ac4019fc841da573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/23.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b5e999970d45cc97b7aa9b634bb37b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/611M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa06185f30a4bff9d60a7a55f25fd46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/375159 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Open Korean Instructions: 0 texts\n",
      "\n",
      "============================================================\n",
      "Data Collection Summary (v21.2 - Legal/Medical Enhanced)\n",
      "============================================================\n",
      "   NSMC                    134,112 texts ( 20.9%)\n",
      "   KorQuAD                 120,814 texts ( 18.8%)\n",
      "   Wikipedia                96,359 texts ( 15.0%)\n",
      "ğŸ“š LawPrecedents            80,000 texts ( 12.4%)\n",
      "   KoAlpaca                 50,000 texts (  7.8%)\n",
      "   KLUE-NLI                 49,996 texts (  7.8%)\n",
      "   KLUE-YNAT                45,392 texts (  7.1%)\n",
      "   KLUE-MRC                 35,108 texts (  5.5%)\n",
      "   KLUE-STS                 23,336 texts (  3.6%)\n",
      "   KorHate                   7,896 texts (  1.2%)\n",
      "ğŸ¥ OpenKorInst                   0 texts (  0.0%)\n",
      "------------------------------------------------------------\n",
      "   TOTAL                   643,013 texts\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "import signal\n",
    "\n",
    "class TimeoutError(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError(\"Dataset loading timeout\")\n",
    "\n",
    "def load_dataset_with_timeout(dataset_name, timeout_seconds=120, **kwargs):\n",
    "    \"\"\"Load dataset with timeout to prevent infinite hanging.\"\"\"\n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    signal.alarm(timeout_seconds)\n",
    "    try:\n",
    "        dataset = load_dataset(dataset_name, **kwargs)\n",
    "        signal.alarm(0)  # Cancel timeout\n",
    "        return dataset\n",
    "    except TimeoutError:\n",
    "        print(f\"  âœ— Timeout after {timeout_seconds}s\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        signal.alarm(0)\n",
    "        raise e\n",
    "\n",
    "def load_diverse_korean_datasets() -> List[str]:\n",
    "    \"\"\"Load diverse Korean text data from multiple domains via HuggingFace.\n",
    "    \n",
    "    v21.2: Added legal and medical domain datasets with timeout protection.\n",
    "    \"\"\"\n",
    "    all_texts = []\n",
    "    domain_stats = {}\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 1. ë°±ê³¼ì‚¬ì „ (Encyclopedia) - Korean Wikipedia\n",
    "    # ========================================================================\n",
    "    print(\"=\" * 60)\n",
    "    print(\"[1/12] Loading Korean Wikipedia (ë°±ê³¼ì‚¬ì „)...\")\n",
    "    try:\n",
    "        wiki_dataset = load_dataset(\n",
    "            \"wikimedia/wikipedia\", \n",
    "            \"20231101.ko\",\n",
    "            split=\"train\",\n",
    "            streaming=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        wiki_texts = []\n",
    "        for i, item in enumerate(wiki_dataset):\n",
    "            if i >= 100000:\n",
    "                break\n",
    "            text = item.get(\"text\", \"\")\n",
    "            if text and len(text) > 100:\n",
    "                wiki_texts.append(text[:3000])\n",
    "        all_texts.extend(wiki_texts)\n",
    "        domain_stats[\"Wikipedia\"] = len(wiki_texts)\n",
    "        print(f\"  âœ“ Wikipedia: {len(wiki_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Wikipedia failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 2. ë‰´ìŠ¤/QA (News/Question Answering) - KLUE-MRC\n",
    "    # ========================================================================\n",
    "    print(\"\\n[2/12] Loading KLUE-MRC (ë‰´ìŠ¤ ê¸°ë°˜ QA)...\")\n",
    "    try:\n",
    "        klue_dataset = load_dataset(\"klue\", \"mrc\", split=\"train\", trust_remote_code=True)\n",
    "        klue_texts = []\n",
    "        for item in klue_dataset:\n",
    "            context = item.get(\"context\", \"\")\n",
    "            question = item.get(\"question\", \"\")\n",
    "            if context and len(context) > 50:\n",
    "                klue_texts.append(context[:2000])\n",
    "            if question:\n",
    "                klue_texts.append(question)\n",
    "        all_texts.extend(klue_texts)\n",
    "        domain_stats[\"KLUE-MRC\"] = len(klue_texts)\n",
    "        print(f\"  âœ“ KLUE-MRC: {len(klue_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— KLUE-MRC failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 3. QA - KorQuAD\n",
    "    # ========================================================================\n",
    "    print(\"\\n[3/12] Loading KorQuAD (í•œêµ­ì–´ QA)...\")\n",
    "    try:\n",
    "        korquad_dataset = load_dataset(\"squad_kor_v1\", split=\"train\", trust_remote_code=True)\n",
    "        korquad_texts = []\n",
    "        for item in korquad_dataset:\n",
    "            context = item.get(\"context\", \"\")\n",
    "            question = item.get(\"question\", \"\")\n",
    "            if context and len(context) > 50:\n",
    "                korquad_texts.append(context[:2000])\n",
    "            if question:\n",
    "                korquad_texts.append(question)\n",
    "        all_texts.extend(korquad_texts)\n",
    "        domain_stats[\"KorQuAD\"] = len(korquad_texts)\n",
    "        print(f\"  âœ“ KorQuAD: {len(korquad_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— KorQuAD failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 4. ë¦¬ë·° - NSMC\n",
    "    # ========================================================================\n",
    "    print(\"\\n[4/12] Loading NSMC (ì˜í™” ë¦¬ë·°)...\")\n",
    "    try:\n",
    "        nsmc_dataset = load_dataset(\"nsmc\", split=\"train\", trust_remote_code=True)\n",
    "        nsmc_texts = [item.get(\"document\", \"\") for item in nsmc_dataset \n",
    "                     if item.get(\"document\") and len(item.get(\"document\", \"\")) > 10]\n",
    "        all_texts.extend(nsmc_texts)\n",
    "        domain_stats[\"NSMC\"] = len(nsmc_texts)\n",
    "        print(f\"  âœ“ NSMC: {len(nsmc_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— NSMC failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 5. ëŒ€í™” - KorHate\n",
    "    # ========================================================================\n",
    "    print(\"\\n[5/12] Loading Korean Dialogue (ëŒ€í™”)...\")\n",
    "    try:\n",
    "        hate_dataset = load_dataset(\"kor_hate\", split=\"train\", trust_remote_code=True)\n",
    "        hate_texts = [item.get(\"comments\", \"\") for item in hate_dataset if item.get(\"comments\")]\n",
    "        all_texts.extend(hate_texts)\n",
    "        domain_stats[\"KorHate\"] = len(hate_texts)\n",
    "        print(f\"  âœ“ KorHate: {len(hate_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— KorHate failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 6. ë‰´ìŠ¤ - KLUE-YNAT\n",
    "    # ========================================================================\n",
    "    print(\"\\n[6/12] Loading Korean News (ë‰´ìŠ¤)...\")\n",
    "    try:\n",
    "        news_dataset = load_dataset(\"klue\", \"ynat\", split=\"train\", trust_remote_code=True)\n",
    "        news_texts = [item.get(\"title\", \"\") for item in news_dataset \n",
    "                     if item.get(\"title\") and len(item.get(\"title\", \"\")) > 10]\n",
    "        all_texts.extend(news_texts)\n",
    "        domain_stats[\"KLUE-YNAT\"] = len(news_texts)\n",
    "        print(f\"  âœ“ KLUE-YNAT: {len(news_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— KLUE-YNAT failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 7. ë¬¸ì¥ ìœ ì‚¬ë„ - KLUE-STS\n",
    "    # ========================================================================\n",
    "    print(\"\\n[7/12] Loading KLUE-STS (ë¬¸ì¥ ìœ ì‚¬ë„)...\")\n",
    "    try:\n",
    "        sts_dataset = load_dataset(\"klue\", \"sts\", split=\"train\", trust_remote_code=True)\n",
    "        sts_texts = []\n",
    "        for item in sts_dataset:\n",
    "            if item.get(\"sentence1\"): sts_texts.append(item[\"sentence1\"])\n",
    "            if item.get(\"sentence2\"): sts_texts.append(item[\"sentence2\"])\n",
    "        all_texts.extend(sts_texts)\n",
    "        domain_stats[\"KLUE-STS\"] = len(sts_texts)\n",
    "        print(f\"  âœ“ KLUE-STS: {len(sts_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— KLUE-STS failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 8. ìì—°ì–´ ì¶”ë¡  - KLUE-NLI\n",
    "    # ========================================================================\n",
    "    print(\"\\n[8/12] Loading KLUE-NLI (ìì—°ì–´ ì¶”ë¡ )...\")\n",
    "    try:\n",
    "        nli_dataset = load_dataset(\"klue\", \"nli\", split=\"train\", trust_remote_code=True)\n",
    "        nli_texts = []\n",
    "        for item in nli_dataset:\n",
    "            if item.get(\"premise\"): nli_texts.append(item[\"premise\"])\n",
    "            if item.get(\"hypothesis\"): nli_texts.append(item[\"hypothesis\"])\n",
    "        all_texts.extend(nli_texts)\n",
    "        domain_stats[\"KLUE-NLI\"] = len(nli_texts)\n",
    "        print(f\"  âœ“ KLUE-NLI: {len(nli_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— KLUE-NLI failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 9. ì§€ì‹œë¬¸ - KoAlpaca\n",
    "    # ========================================================================\n",
    "    print(\"\\n[9/12] Loading KoAlpaca (ì§€ì‹œë¬¸)...\")\n",
    "    try:\n",
    "        alpaca_dataset = load_dataset(\"Bingsu/ko_alpaca_data\", split=\"train\", trust_remote_code=True)\n",
    "        alpaca_texts = []\n",
    "        for item in alpaca_dataset:\n",
    "            if item.get(\"instruction\"): alpaca_texts.append(item[\"instruction\"])\n",
    "            if item.get(\"output\") and len(item.get(\"output\", \"\")) > 20:\n",
    "                alpaca_texts.append(item[\"output\"][:1000])\n",
    "        alpaca_texts = alpaca_texts[:50000]\n",
    "        all_texts.extend(alpaca_texts)\n",
    "        domain_stats[\"KoAlpaca\"] = len(alpaca_texts)\n",
    "        print(f\"  âœ“ KoAlpaca: {len(alpaca_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— KoAlpaca failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 10. ë²•ë¥  (Legal) - Korean Law Precedents (íŒë¡€)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[10/12] Loading Korean Law Precedents (ë²•ë¥  íŒë¡€)...\")\n",
    "    try:\n",
    "        law_precedents = load_dataset(\n",
    "            \"joonhok-exo-ai/korean_law_open_data_precedents\",\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        law_texts = []\n",
    "        for item in law_precedents:\n",
    "            for field in [\"íŒì‹œì‚¬í•­\", \"íŒê²°ìš”ì§€\", \"ì „ë¬¸\", \"ì‚¬ê±´ëª…\", \"ì‚¬ê±´ê°œìš”\", \n",
    "                         \"content\", \"text\", \"summary\", \"title\"]:\n",
    "                text = item.get(field, \"\")\n",
    "                if text and len(text) > 30:\n",
    "                    law_texts.append(text[:2000])\n",
    "        law_texts = law_texts[:80000]\n",
    "        all_texts.extend(law_texts)\n",
    "        domain_stats[\"LawPrecedents\"] = len(law_texts)\n",
    "        print(f\"  âœ“ Law Precedents: {len(law_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Law Precedents failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 11. ì˜ë£Œ (Medical) - KorMedMCQA (ê²€ì¦ëœ ë°ì´í„°ì…‹)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[11/12] Loading KorMedMCQA (ì˜ë£Œ ìê²©ì‹œí—˜ QA)...\")\n",
    "    try:\n",
    "        kormed_dataset = load_dataset(\n",
    "            \"sean0042/KorMedMCQA\",\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        kormed_texts = []\n",
    "        for item in kormed_dataset:\n",
    "            for field in [\"question\", \"answer\", \"options\", \"explanation\"]:\n",
    "                text = item.get(field, \"\")\n",
    "                if isinstance(text, str) and len(text) > 20:\n",
    "                    kormed_texts.append(text[:1500])\n",
    "                elif isinstance(text, list):\n",
    "                    for t in text:\n",
    "                        if isinstance(t, str) and len(t) > 10:\n",
    "                            kormed_texts.append(t[:500])\n",
    "        kormed_texts = kormed_texts[:50000]\n",
    "        all_texts.extend(kormed_texts)\n",
    "        domain_stats[\"KorMedMCQA\"] = len(kormed_texts)\n",
    "        print(f\"  âœ“ KorMedMCQA: {len(kormed_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— KorMedMCQA failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 12. ì˜ë£Œ (Medical) - Open Korean Instructions (ì˜ë£Œ í¬í•¨)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[12/12] Loading Open Korean Instructions (ë‹¤ì–‘í•œ ì§€ì‹œë¬¸)...\")\n",
    "    try:\n",
    "        openkor_dataset = load_dataset(\n",
    "            \"heegyu/open-korean-instructions\",\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        openkor_texts = []\n",
    "        for item in openkor_dataset:\n",
    "            for field in [\"instruction\", \"output\", \"input\"]:\n",
    "                text = item.get(field, \"\")\n",
    "                if text and len(text) > 20:\n",
    "                    openkor_texts.append(text[:1500])\n",
    "        openkor_texts = openkor_texts[:60000]\n",
    "        all_texts.extend(openkor_texts)\n",
    "        domain_stats[\"OpenKorInst\"] = len(openkor_texts)\n",
    "        print(f\"  âœ“ Open Korean Instructions: {len(openkor_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Open Korean Instructions failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Summary\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Data Collection Summary (v21.2 - Legal/Medical Enhanced)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total = 0\n",
    "    for domain, count in sorted(domain_stats.items(), key=lambda x: -x[1]):\n",
    "        pct = count / sum(domain_stats.values()) * 100 if sum(domain_stats.values()) > 0 else 0\n",
    "        marker = \"ğŸ“š\" if domain == \"LawPrecedents\" else \\\n",
    "                 \"ğŸ¥\" if domain in [\"KorMedMCQA\", \"OpenKorInst\"] else \"  \"\n",
    "        print(f\"{marker} {domain:20} {count:>10,} texts ({pct:5.1f}%)\")\n",
    "        total += count\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"   {'TOTAL':20} {total:>10,} texts\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return all_texts\n",
    "\n",
    "texts = load_diverse_korean_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Korean Terms with Morphological Analysis\n",
    "\n",
    "Use **Kiwi** morphological analyzer to extract meaningful terms:\n",
    "- **NNG**: ì¼ë°˜ëª…ì‚¬ (General Nouns) - for compound nouns\n",
    "- **NNP**: ê³ ìœ ëª…ì‚¬ (Proper Nouns)\n",
    "- **SL**: ì™¸ë˜ì–´ (Foreign words like \"AI\", \"ML\")\n",
    "\n",
    "Filter out:\n",
    "- ì¡°ì‚¬ (JKS, JKC, JKG, JKO, JKB, JKV, JKQ, JX, JC)\n",
    "- ì–´ë¯¸ (EP, EF, EC, ETN, ETM)\n",
    "- ì ‘ì‚¬ (XPN, XSN, XSV, XSA)\n",
    "- ê¸°í˜¸, ë¶€í˜¸ ë“±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Kiwi morphological analyzer...\n",
      "Kiwi loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantization is not supported for ArchType::neon. Fall back to non-quantized model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: skt/A.X-Encoder-base\n",
      "Vocab size: 49,999\n",
      "\n",
      "Test text: ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ë°œì „í•˜ë©´ì„œ ê¸°ê³„í•™ìŠµê³¼ ë”¥ëŸ¬ë‹ì´ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "Extracted nouns: ['ì¸ê³µ', 'ì§€ëŠ¥', 'ê¸°ìˆ ', 'ë°œì „', 'ê¸°ê³„', 'í•™ìŠµ', 'ëŸ¬ë‹', 'ì£¼ëª©']\n",
      "Compound nouns: ['ì¸ê³µì§€ëŠ¥ê¸°ìˆ ', 'ê¸°ê³„í•™ìŠµ', 'ë”¥ëŸ¬ë‹']\n"
     ]
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize Kiwi morphological analyzer\n",
    "print(\"Loading Kiwi morphological analyzer...\")\n",
    "kiwi = Kiwi()\n",
    "print(\"Kiwi loaded successfully\")\n",
    "\n",
    "# Load tokenizer for later use\n",
    "MODEL_NAME = \"skt/A.X-Encoder-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer: {MODEL_NAME}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "# POS tags to keep (nouns and foreign words)\n",
    "VALID_POS_TAGS = {\n",
    "    'NNG',   # ì¼ë°˜ëª…ì‚¬ (General Noun)\n",
    "    'NNP',   # ê³ ìœ ëª…ì‚¬ (Proper Noun)\n",
    "    'NNB',   # ì˜ì¡´ëª…ì‚¬ (Dependent Noun) - sometimes useful\n",
    "    'SL',    # ì™¸ë˜ì–´ (Foreign words: AI, ML, API, etc.)\n",
    "    'SH',    # í•œì (Chinese characters)\n",
    "}\n",
    "\n",
    "# POS tags to explicitly filter out\n",
    "FILTER_OUT_POS_TAGS = {\n",
    "    # ì¡°ì‚¬ (Particles)\n",
    "    'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ',  # ê²©ì¡°ì‚¬\n",
    "    'JX',   # ë³´ì¡°ì‚¬\n",
    "    'JC',   # ì ‘ì†ì¡°ì‚¬\n",
    "    # ì–´ë¯¸ (Endings)\n",
    "    'EP', 'EF', 'EC', 'ETN', 'ETM',\n",
    "    # ì ‘ì‚¬ (Affixes)\n",
    "    'XPN', 'XSN', 'XSV', 'XSA',\n",
    "    # ê¸°í˜¸ (Symbols)\n",
    "    'SF', 'SP', 'SS', 'SE', 'SO', 'SW',\n",
    "    # ê¸°íƒ€\n",
    "    'NR', 'NP',  # ìˆ˜ì‚¬, ëŒ€ëª…ì‚¬\n",
    "}\n",
    "\n",
    "def extract_nouns_with_kiwi(text: str, kiwi_instance: Kiwi) -> List[str]:\n",
    "    \"\"\"Extract nouns and proper nouns using Kiwi morphological analyzer.\"\"\"\n",
    "    try:\n",
    "        result = kiwi_instance.tokenize(text)\n",
    "        nouns = []\n",
    "        for token in result:\n",
    "            # token.form: í˜•íƒœ, token.tag: í’ˆì‚¬\n",
    "            if token.tag in VALID_POS_TAGS:\n",
    "                word = token.form.strip()\n",
    "                # Filter by length (2-15 characters for compound nouns)\n",
    "                if 2 <= len(word) <= 15:\n",
    "                    nouns.append(word)\n",
    "        return nouns\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def extract_compound_nouns(text: str, kiwi_instance: Kiwi) -> List[str]:\n",
    "    \"\"\"Extract compound nouns by joining consecutive nouns.\"\"\"\n",
    "    try:\n",
    "        result = kiwi_instance.tokenize(text)\n",
    "        compounds = []\n",
    "        current_compound = []\n",
    "        \n",
    "        for token in result:\n",
    "            if token.tag in {'NNG', 'NNP', 'SL', 'SH'}:\n",
    "                current_compound.append(token.form)\n",
    "            else:\n",
    "                if len(current_compound) >= 2:\n",
    "                    compound = ''.join(current_compound)\n",
    "                    if 2 <= len(compound) <= 15:\n",
    "                        compounds.append(compound)\n",
    "                current_compound = []\n",
    "        \n",
    "        # Don't forget the last compound\n",
    "        if len(current_compound) >= 2:\n",
    "            compound = ''.join(current_compound)\n",
    "            if 2 <= len(compound) <= 15:\n",
    "                compounds.append(compound)\n",
    "        \n",
    "        return compounds\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def extract_terms(texts: List[str], kiwi_instance: Kiwi, min_freq: int = 3) -> List[Tuple[str, int]]:\n",
    "    \"\"\"Extract Korean terms using morphological analysis.\"\"\"\n",
    "    term_freq = Counter()\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Processing text {i:,}/{len(texts):,}...\")\n",
    "        \n",
    "        # Extract individual nouns\n",
    "        nouns = extract_nouns_with_kiwi(text, kiwi_instance)\n",
    "        for noun in nouns:\n",
    "            term_freq[noun] += 1\n",
    "        \n",
    "        # Extract compound nouns\n",
    "        compounds = extract_compound_nouns(text, kiwi_instance)\n",
    "        for compound in compounds:\n",
    "            term_freq[compound] += 1\n",
    "    \n",
    "    # Filter by frequency\n",
    "    filtered_terms = [\n",
    "        (term, freq) for term, freq in term_freq.items() \n",
    "        if freq >= min_freq\n",
    "    ]\n",
    "    \n",
    "    # Sort by frequency\n",
    "    filtered_terms.sort(key=lambda x: -x[1])\n",
    "    \n",
    "    return filtered_terms\n",
    "\n",
    "# Test morphological analysis\n",
    "\n",
    "test_text = \"ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ë°œì „í•˜ë©´ì„œ ê¸°ê³„í•™ìŠµê³¼ ë”¥ëŸ¬ë‹ì´ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.\"\n",
    "print(f\"\\nTest text: {test_text}\")\n",
    "print(f\"Extracted nouns: {extract_nouns_with_kiwi(test_text, kiwi)}\")\n",
    "print(f\"Compound nouns: {extract_compound_nouns(test_text, kiwi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting terms from 643,013 texts...\n",
      "Processing text 0/643,013...\n",
      "Processing text 10,000/643,013...\n",
      "Processing text 20,000/643,013...\n",
      "Processing text 30,000/643,013...\n",
      "Processing text 40,000/643,013...\n",
      "Processing text 50,000/643,013...\n",
      "Processing text 60,000/643,013...\n",
      "Processing text 70,000/643,013...\n",
      "Processing text 80,000/643,013...\n",
      "Processing text 90,000/643,013...\n",
      "Processing text 100,000/643,013...\n",
      "Processing text 110,000/643,013...\n",
      "Processing text 120,000/643,013...\n",
      "Processing text 130,000/643,013...\n",
      "Processing text 140,000/643,013...\n",
      "Processing text 150,000/643,013...\n",
      "Processing text 160,000/643,013...\n",
      "Processing text 170,000/643,013...\n",
      "Processing text 180,000/643,013...\n",
      "Processing text 190,000/643,013...\n",
      "Processing text 200,000/643,013...\n",
      "Processing text 210,000/643,013...\n",
      "Processing text 220,000/643,013...\n",
      "Processing text 230,000/643,013...\n",
      "Processing text 240,000/643,013...\n",
      "Processing text 250,000/643,013...\n",
      "Processing text 260,000/643,013...\n",
      "Processing text 270,000/643,013...\n",
      "Processing text 280,000/643,013...\n",
      "Processing text 290,000/643,013...\n",
      "Processing text 300,000/643,013...\n",
      "Processing text 310,000/643,013...\n",
      "Processing text 320,000/643,013...\n",
      "Processing text 330,000/643,013...\n",
      "Processing text 340,000/643,013...\n",
      "Processing text 350,000/643,013...\n",
      "Processing text 360,000/643,013...\n",
      "Processing text 370,000/643,013...\n",
      "Processing text 380,000/643,013...\n",
      "Processing text 390,000/643,013...\n",
      "Processing text 400,000/643,013...\n",
      "Processing text 410,000/643,013...\n",
      "Processing text 420,000/643,013...\n",
      "Processing text 430,000/643,013...\n",
      "Processing text 440,000/643,013...\n",
      "Processing text 450,000/643,013...\n",
      "Processing text 460,000/643,013...\n",
      "Processing text 470,000/643,013...\n",
      "Processing text 480,000/643,013...\n",
      "Processing text 490,000/643,013...\n",
      "Processing text 500,000/643,013...\n",
      "Processing text 510,000/643,013...\n",
      "Processing text 520,000/643,013...\n",
      "Processing text 530,000/643,013...\n",
      "Processing text 540,000/643,013...\n",
      "Processing text 550,000/643,013...\n",
      "Processing text 560,000/643,013...\n",
      "Processing text 570,000/643,013...\n",
      "Processing text 580,000/643,013...\n",
      "Processing text 590,000/643,013...\n",
      "Processing text 600,000/643,013...\n",
      "Processing text 610,000/643,013...\n",
      "Processing text 620,000/643,013...\n",
      "Processing text 630,000/643,013...\n",
      "Processing text 640,000/643,013...\n",
      "\n",
      "Extracted 660,334 unique terms\n",
      "\n",
      "Top 50 terms (ê³ ìœ ëª…ì‚¬/ë³µí•©ëª…ì‚¬):\n",
      "  ì›ê³ : 187,566\n",
      "  íŒê²°: 162,371\n",
      "  í”¼ê³ : 160,178\n",
      "  ì›ì‹¬: 126,252\n",
      "  ì‚¬ì‹¤: 116,434\n",
      "  ìƒê³ : 116,392\n",
      "  ì´ìœ : 112,870\n",
      "  ì‚¬ê±´: 107,709\n",
      "  ì˜í™”: 106,547\n",
      "  ëŒ€í•œë¯¼êµ­: 103,742\n",
      "  ê²½ìš°: 97,244\n",
      "  ì‚¬ìš©: 94,658\n",
      "  ì‚¬ëŒ: 90,037\n",
      "  ë¯¸êµ­: 83,233\n",
      "  ì´í›„: 81,055\n",
      "  ì¼ë³¸: 78,947\n",
      "  ì¸ì •: 78,245\n",
      "  í”¼ê³ ì¸: 74,745\n",
      "  ë‹¹ì‹œ: 68,189\n",
      "  ë•Œë¬¸: 65,996\n",
      "  ê¸°ë¡: 65,692\n",
      "  ì§€ì—­: 62,051\n",
      "  ì†Œì™¸: 60,907\n",
      "  ë‹¤ìŒ: 58,356\n",
      "  ì‹œì‘: 58,138\n",
      "  ì„ ìˆ˜: 57,093\n",
      "  í•œêµ­: 56,220\n",
      "  ì£¼ì¥: 55,185\n",
      "  êµ­ê°€: 54,933\n",
      "  ê·œì •: 53,281\n",
      "  ë¶€ë¶„: 52,599\n",
      "  íŒë‹¨: 52,379\n",
      "  ì†Œìœ : 52,222\n",
      "  ë“±ê¸°: 52,079\n",
      "  ì´ë¦„: 51,966\n",
      "  ì£¼ë¬¸: 51,940\n",
      "  ì²˜ë¶„: 51,443\n",
      "  ì†Œì†¡: 50,331\n",
      "  ì„¸ê³„: 49,644\n",
      "  ì´ì „: 48,985\n",
      "  ìì‹ : 48,645\n",
      "  íšŒì‚¬: 48,641\n",
      "  í† ì§€: 48,127\n",
      "  ì²­êµ¬: 47,104\n",
      "  ì´ìƒ: 46,843\n",
      "  ì„ ê³ : 46,534\n",
      "  ëŒ€í‘œ: 45,590\n",
      "  ê²°ì •: 45,554\n",
      "  ì„œìš¸: 45,260\n",
      "  ê´€ê³„: 45,088\n"
     ]
    }
   ],
   "source": [
    "# Extract terms from collected texts\n",
    "print(f\"\\nExtracting terms from {len(texts):,} texts...\")\n",
    "terms_with_freq = extract_terms(texts, kiwi, CONFIG[\"min_term_freq\"])\n",
    "\n",
    "print(f\"\\nExtracted {len(terms_with_freq):,} unique terms\")\n",
    "print(f\"\\nTop 50 terms (ê³ ìœ ëª…ì‚¬/ë³µí•©ëª…ì‚¬):\")\n",
    "for term, freq in terms_with_freq[:50]:\n",
    "    print(f\"  {term}: {freq:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute Term Embeddings with BGE-M3\n",
    "\n",
    "Use BAAI/bge-m3 model to compute dense embeddings for each term.\n",
    "BGE-M3 supports Korean and provides high-quality multilingual embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 120,000 terms for embeddings\n",
      "\n",
      "Loading BGE-M3 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bea8db2f5c4bef85533144400bb3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGE-M3 loaded on cuda\n",
      "Embedding batch 0/120,000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch 8,000/120,000...\n",
      "Embedding batch 16,000/120,000...\n",
      "Embedding batch 24,000/120,000...\n",
      "Embedding batch 32,000/120,000...\n",
      "Embedding batch 40,000/120,000...\n",
      "Embedding batch 48,000/120,000...\n",
      "Embedding batch 56,000/120,000...\n",
      "Embedding batch 64,000/120,000...\n",
      "Embedding batch 72,000/120,000...\n",
      "Embedding batch 80,000/120,000...\n",
      "Embedding batch 88,000/120,000...\n",
      "Embedding batch 96,000/120,000...\n",
      "Embedding batch 104,000/120,000...\n",
      "Embedding batch 112,000/120,000...\n",
      "\n",
      "Embeddings shape: (120000, 1024)\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import torch\n",
    "\n",
    "# Limit terms to process\n",
    "terms = [t[0] for t in terms_with_freq[:CONFIG[\"max_terms\"]]]\n",
    "print(f\"Processing {len(terms):,} terms for embeddings\")\n",
    "\n",
    "# Load BGE-M3 model\n",
    "print(\"\\nLoading BGE-M3 model...\")\n",
    "bge_model = BGEM3FlagModel(\n",
    "    \"BAAI/bge-m3\",\n",
    "    use_fp16=True,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"BGE-M3 loaded on {bge_model.device}\")\n",
    "\n",
    "def compute_embeddings(terms: List[str], model, batch_size: int = 64) -> np.ndarray:\n",
    "    \"\"\"Compute BGE-M3 embeddings for terms.\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(terms), batch_size):\n",
    "        batch = terms[i:i + batch_size]\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Embedding batch {i:,}/{len(terms):,}...\")\n",
    "        \n",
    "        # Get dense embeddings\n",
    "        output = model.encode(\n",
    "            batch,\n",
    "            return_dense=True,\n",
    "            return_sparse=False,\n",
    "            return_colbert_vecs=False\n",
    "        )\n",
    "        embeddings = output[\"dense_vecs\"]\n",
    "        all_embeddings.append(embeddings)\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "embeddings = compute_embeddings(terms, bge_model, CONFIG[\"embedding_batch_size\"])\n",
    "print(f\"\\nEmbeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. K-means Clustering\n",
    "\n",
    "Apply K-means clustering to group semantically similar terms together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering 120,000 terms into 12,000 clusters...\n",
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 17893.457911910264\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 17921.676437992806\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 17909.82313917774\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 1/11718: mean batch inertia: 0.48734163023797156\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 2/11718: mean batch inertia: 0.5186286581685076, ewa inertia: 0.5186286581685076\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 3/11718: mean batch inertia: 0.501104318799685, ewa inertia: 0.518329578602276\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 4/11718: mean batch inertia: 0.493209540661632, ewa inertia: 0.5179008668606869\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 5/11718: mean batch inertia: 0.4744108108968102, ewa inertia: 0.5171586427574376\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 6/11718: mean batch inertia: 0.4779043028799729, ewa inertia: 0.5164887076063218\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 7/11718: mean batch inertia: 0.47707359554431733, ewa inertia: 0.5158160286327884\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 8/11718: mean batch inertia: 0.484877210426083, ewa inertia: 0.5152880105355447\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 9/11718: mean batch inertia: 0.4627116173383375, ewa inertia: 0.5143907142357815\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 10/11718: mean batch inertia: 0.4791933207698531, ewa inertia: 0.5137900170597729\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 11/11718: mean batch inertia: 0.46633663940078, ewa inertia: 0.512980152829928\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 12/11718: mean batch inertia: 0.45586999461396827, ewa inertia: 0.5120054809186415\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 13/11718: mean batch inertia: 0.44429025652511056, ewa inertia: 0.5108498173861881\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 14/11718: mean batch inertia: 0.4559221174783681, ewa inertia: 0.509912392452969\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 15/11718: mean batch inertia: 0.4483734401406362, ewa inertia: 0.5088621364189723\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 16/11718: mean batch inertia: 0.4365470366861554, ewa inertia: 0.507627969001594\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 17/11718: mean batch inertia: 0.4346445347204609, ewa inertia: 0.506382395436309\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 18/11718: mean batch inertia: 0.4392667257146879, ewa inertia: 0.5052369642183202\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 19/11718: mean batch inertia: 0.435479901550119, ewa inertia: 0.504046453603038\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 20/11718: mean batch inertia: 0.4306656967720985, ewa inertia: 0.502794099122744\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 21/11718: mean batch inertia: 0.42730643932030266, ewa inertia: 0.5015057871313823\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 22/11718: mean batch inertia: 0.42670551925925715, ewa inertia: 0.5002292065312031\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 23/11718: mean batch inertia: 0.42223462107524845, ewa inertia: 0.4988981100318923\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 24/11718: mean batch inertia: 0.42593996673596946, ewa inertia: 0.49765296809582465\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 25/11718: mean batch inertia: 0.4192558130867714, ewa inertia: 0.49631500113339483\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 26/11718: mean batch inertia: 0.410325640026963, ewa inertia: 0.4948474616000078\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 27/11718: mean batch inertia: 0.41333152861315814, ewa inertia: 0.49345626793697944\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 28/11718: mean batch inertia: 0.4170505031367649, ewa inertia: 0.4921522870842296\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 29/11718: mean batch inertia: 0.4125579322640648, ewa inertia: 0.4907938880819571\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 30/11718: mean batch inertia: 0.40344147688042237, ewa inertia: 0.4893030860208014\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 31/11718: mean batch inertia: 0.40211592096947246, ewa inertia: 0.48781510413710777\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 32/11718: mean batch inertia: 0.40483893744963134, ewa inertia: 0.4863989893599313\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 33/11718: mean batch inertia: 0.4070765008741162, ewa inertia: 0.48504523017109996\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 34/11718: mean batch inertia: 0.3986065214313103, ewa inertia: 0.4835700218353437\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 35/11718: mean batch inertia: 0.3877927076149326, ewa inertia: 0.4819354359608643\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 36/11718: mean batch inertia: 0.40144974061644845, ewa inertia: 0.4805618248737454\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 37/11718: mean batch inertia: 0.4029715276427692, ewa inertia: 0.47923762816930926\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 38/11718: mean batch inertia: 0.3948301068827895, ewa inertia: 0.47779708514387786\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 39/11718: mean batch inertia: 0.3970078123829407, ewa inertia: 0.47641829304535865\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 40/11718: mean batch inertia: 0.39532859979763296, ewa inertia: 0.47503437381325775\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 41/11718: mean batch inertia: 0.39309445682831795, ewa inertia: 0.4736359442169614\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 42/11718: mean batch inertia: 0.3950448377986295, ewa inertia: 0.47229466717806384\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 43/11718: mean batch inertia: 0.4021007418947607, ewa inertia: 0.47109670083628163\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 44/11718: mean batch inertia: 0.39232364325466457, ewa inertia: 0.469752318523408\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 45/11718: mean batch inertia: 0.39229294649731705, ewa inertia: 0.46843035625718155\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 46/11718: mean batch inertia: 0.39564166000101886, ewa inertia: 0.46718810619316026\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 47/11718: mean batch inertia: 0.3874846708297977, ewa inertia: 0.4658278455651308\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 48/11718: mean batch inertia: 0.39758566682256563, ewa inertia: 0.46466318875339774\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 49/11718: mean batch inertia: 0.386930591491372, ewa inertia: 0.46333656348200314\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 50/11718: mean batch inertia: 0.3830656881696036, ewa inertia: 0.461966618626212\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 51/11718: mean batch inertia: 0.39416794562551133, ewa inertia: 0.46080953091606436\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 52/11718: mean batch inertia: 0.38281971995829017, ewa inertia: 0.4594785159008435\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 53/11718: mean batch inertia: 0.3805809456831702, ewa inertia: 0.4581320085900228\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 54/11718: mean batch inertia: 0.3842345344188846, ewa inertia: 0.4568708355406108\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 55/11718: mean batch inertia: 0.37842662960546825, ewa inertia: 0.45553206558240067\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 56/11718: mean batch inertia: 0.37204592815391685, ewa inertia: 0.4541072473771062\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 57/11718: mean batch inertia: 0.37480323736083626, ewa inertia: 0.4527538035515271\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 58/11718: mean batch inertia: 0.3741406826333601, ewa inertia: 0.45141215080163\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 59/11718: mean batch inertia: 0.3854637802544063, ewa inertia: 0.45028664132353635\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 60/11718: mean batch inertia: 0.38206373767385715, ewa inertia: 0.4491223134706473\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 61/11718: mean batch inertia: 0.38170866905648204, ewa inertia: 0.44797179686028393\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 62/11718: mean batch inertia: 0.3679927079254046, ewa inertia: 0.44660683178383764\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 63/11718: mean batch inertia: 0.3795568496513925, ewa inertia: 0.44546252162469524\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 64/11718: mean batch inertia: 0.37888656283241207, ewa inertia: 0.4443263013964755\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 65/11718: mean batch inertia: 0.37256238479999587, ewa inertia: 0.4431015407595676\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 66/11718: mean batch inertia: 0.3736455149728875, ewa inertia: 0.4419161677975829\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 67/11718: mean batch inertia: 0.37833425707104607, ewa inertia: 0.4408310455638686\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 68/11718: mean batch inertia: 0.36430792757066566, ewa inertia: 0.4395250618999818\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 69/11718: mean batch inertia: 0.3729657458455701, ewa inertia: 0.4383891257054548\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 70/11718: mean batch inertia: 0.38392176519091503, ewa inertia: 0.4374595571657445\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 71/11718: mean batch inertia: 0.3625788621533284, ewa inertia: 0.43618160395380934\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 72/11718: mean batch inertia: 0.3739536309729728, ewa inertia: 0.43511958873172996\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 73/11718: mean batch inertia: 0.3679250008694125, ewa inertia: 0.43397281065536375\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 74/11718: mean batch inertia: 0.36413865357528313, ewa inertia: 0.43278098430641665\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 75/11718: mean batch inertia: 0.3563112585639876, ewa inertia: 0.43147591186268286\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 76/11718: mean batch inertia: 0.3619034958980846, ewa inertia: 0.4302885525248815\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 77/11718: mean batch inertia: 0.3704568135729051, ewa inertia: 0.42926743268943307\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 78/11718: mean batch inertia: 0.3585218053139165, ewa inertia: 0.4280600507104074\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 79/11718: mean batch inertia: 0.36836239934752596, ewa inertia: 0.4270412192840761\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 80/11718: mean batch inertia: 0.3721117157583612, ewa inertia: 0.4261037635693682\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 81/11718: mean batch inertia: 0.3682335915617025, ewa inertia: 0.4251161208641266\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 82/11718: mean batch inertia: 0.36362978449132316, ewa inertia: 0.4240667628013479\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 83/11718: mean batch inertia: 0.37529565645402146, ewa inertia: 0.42323440952263086\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 84/11718: mean batch inertia: 0.37137075294920174, ewa inertia: 0.4223492771598807\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 85/11718: mean batch inertia: 0.3631764149072708, ewa inertia: 0.42133940205972864\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 86/11718: mean batch inertia: 0.3684189910063813, ewa inertia: 0.4204362345708139\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 87/11718: mean batch inertia: 0.3638133216688925, ewa inertia: 0.4194698782435905\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 88/11718: mean batch inertia: 0.36465329649308115, ewa inertia: 0.4185343497111196\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 89/11718: mean batch inertia: 0.366521773324215, ewa inertia: 0.4176466758047323\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 90/11718: mean batch inertia: 0.36887466466714436, ewa inertia: 0.4168143070843901\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 91/11718: mean batch inertia: 0.36555385587756983, ewa inertia: 0.4159394693407749\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 92/11718: mean batch inertia: 0.35654007341620864, ewa inertia: 0.41492572809817263\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 93/11718: mean batch inertia: 0.35813544928162316, ewa inertia: 0.4139565154164759\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 94/11718: mean batch inertia: 0.3713208354394792, ewa inertia: 0.4132288725418924\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 95/11718: mean batch inertia: 0.36115818392047094, ewa inertia: 0.41234020686163414\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 96/11718: mean batch inertia: 0.3638891534231568, ewa inertia: 0.41151331577370986\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 97/11718: mean batch inertia: 0.36012143915618994, ewa inertia: 0.4106362350551102\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 98/11718: mean batch inertia: 0.35164395643322444, ewa inertia: 0.4096294418899064\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 99/11718: mean batch inertia: 0.3702717862726873, ewa inertia: 0.40895774349819247\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 100/11718: mean batch inertia: 0.3639476906245752, ewa inertia: 0.40818957833052577\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 101/11718: mean batch inertia: 0.36503441755005195, ewa inertia: 0.4074530697241108\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 102/11718: mean batch inertia: 0.35216673422052913, ewa inertia: 0.4065095241277296\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 103/11718: mean batch inertia: 0.36019179020698, ewa inertia: 0.405719041389505\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 104/11718: mean batch inertia: 0.3567561913915809, ewa inertia: 0.4048834157130877\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 105/11718: mean batch inertia: 0.3617272799975625, ewa inertia: 0.4041468904679198\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 106/11718: mean batch inertia: 0.35810913799691824, ewa inertia: 0.40336118603995164\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 107/11718: mean batch inertia: 0.3636635976868703, ewa inertia: 0.4026836861778912\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 108/11718: mean batch inertia: 0.3573991480450521, ewa inertia: 0.4019108365008381\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 109/11718: mean batch inertia: 0.36155734528304373, ewa inertia: 0.40122214265650313\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 110/11718: mean batch inertia: 0.3559274321435512, ewa inertia: 0.4004491193722761\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 111/11718: mean batch inertia: 0.3595882800045654, ewa inertia: 0.39975176685833813\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 112/11718: mean batch inertia: 0.35314958314320033, ewa inertia: 0.398956429550744\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 113/11718: mean batch inertia: 0.3526796508241287, ewa inertia: 0.3981666457753412\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 114/11718: mean batch inertia: 0.3580061310727694, ewa inertia: 0.3974812453694207\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 115/11718: mean batch inertia: 0.36495209178746524, ewa inertia: 0.3969260857746186\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 116/11718: mean batch inertia: 0.3553062246224154, ewa inertia: 0.39621577939684083\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 117/11718: mean batch inertia: 0.3622360194018698, ewa inertia: 0.39563586299223\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 118/11718: mean batch inertia: 0.3553589403096294, ewa inertia: 0.3949484759066727\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 119/11718: mean batch inertia: 0.3442757718202056, ewa inertia: 0.3940836689636548\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 120/11718: mean batch inertia: 0.3613520903853321, ewa inertia: 0.3935250546777038\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 121/11718: mean batch inertia: 0.3622191349504624, ewa inertia: 0.39299077143338595\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 122/11718: mean batch inertia: 0.35640353909456135, ewa inertia: 0.39236635453827745\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 123/11718: mean batch inertia: 0.3597701347033739, ewa inertia: 0.39181005035562994\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 124/11718: mean batch inertia: 0.3511116311546575, ewa inertia: 0.39111546978943806\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 125/11718: mean batch inertia: 0.35170653877117375, ewa inertia: 0.3904428963048387\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 126/11718: mean batch inertia: 0.3539377619424452, ewa inertia: 0.3898198805368519\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 127/11718: mean batch inertia: 0.349293461604915, ewa inertia: 0.38912823541745617\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 128/11718: mean batch inertia: 0.34786281983561823, ewa inertia: 0.3884239781936697\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 129/11718: mean batch inertia: 0.35070487399079053, ewa inertia: 0.387780244179724\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 130/11718: mean batch inertia: 0.3423034867860371, ewa inertia: 0.3870041139879567\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 131/11718: mean batch inertia: 0.35401077293333183, ewa inertia: 0.38644103232630495\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 132/11718: mean batch inertia: 0.3539177270904825, ewa inertia: 0.3858859725424451\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 133/11718: mean batch inertia: 0.35262910045797646, ewa inertia: 0.38531839332203033\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 134/11718: mean batch inertia: 0.3527953736736652, ewa inertia: 0.3847633384121558\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 135/11718: mean batch inertia: 0.3504598802721274, ewa inertia: 0.3841778976052394\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 136/11718: mean batch inertia: 0.3520324218149528, ewa inertia: 0.3836292860568481\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 137/11718: mean batch inertia: 0.34476742971756097, ewa inertia: 0.3829660492356311\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 138/11718: mean batch inertia: 0.3526942807046725, ewa inertia: 0.38244941535798505\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 139/11718: mean batch inertia: 0.34448832694728887, ewa inertia: 0.38180155151464124\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 140/11718: mean batch inertia: 0.3545286657142061, ewa inertia: 0.38133609814242525\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 141/11718: mean batch inertia: 0.35238857601258966, ewa inertia: 0.380842064548356\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 142/11718: mean batch inertia: 0.349586696313488, ewa inertia: 0.38030864404231846\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 143/11718: mean batch inertia: 0.3494436748689245, ewa inertia: 0.37978188629140713\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 144/11718: mean batch inertia: 0.3455187104691932, ewa inertia: 0.37919713296365243\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 145/11718: mean batch inertia: 0.3404133767169254, ewa inertia: 0.3785352290395743\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 146/11718: mean batch inertia: 0.34890835357766936, ewa inertia: 0.37802960124525614\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 147/11718: mean batch inertia: 0.34594736865523823, ewa inertia: 0.37748206903848824\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 148/11718: mean batch inertia: 0.3463094299816342, ewa inertia: 0.37695006043198964\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 149/11718: mean batch inertia: 0.34723588582956316, ewa inertia: 0.37644294274475565\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 150/11718: mean batch inertia: 0.3442723738188529, ewa inertia: 0.3758939029437519\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 151/11718: mean batch inertia: 0.34813600489136043, ewa inertia: 0.3754201720980815\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 152/11718: mean batch inertia: 0.34182698086959973, ewa inertia: 0.37484685307877386\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 153/11718: mean batch inertia: 0.3474454154300882, ewa inertia: 0.37437920577329714\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 154/11718: mean batch inertia: 0.340953782492375, ewa inertia: 0.37380874996976776\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 155/11718: mean batch inertia: 0.3419036583683086, ewa inertia: 0.3732642409440114\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 156/11718: mean batch inertia: 0.3513643835217045, ewa inertia: 0.37289048649195783\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 157/11718: mean batch inertia: 0.3477225578711547, ewa inertia: 0.372460957422905\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 158/11718: mean batch inertia: 0.3536409843370815, ewa inertia: 0.37213976589216974\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 159/11718: mean batch inertia: 0.3480370799905185, ewa inertia: 0.3717284168140239\n",
      "Minibatch step 160/11718: mean batch inertia: 0.34473101516696864, ewa inertia: 0.37126766499884595\n",
      "Minibatch step 161/11718: mean batch inertia: 0.3454440676150535, ewa inertia: 0.37082694594282134\n",
      "Minibatch step 162/11718: mean batch inertia: 0.33957686411069427, ewa inertia: 0.3702936156573054\n",
      "Minibatch step 163/11718: mean batch inertia: 0.3400542389131146, ewa inertia: 0.36977753459488005\n",
      "Minibatch step 164/11718: mean batch inertia: 0.3593899903324845, ewa inertia: 0.3696002553167958\n",
      "Minibatch step 165/11718: mean batch inertia: 0.35584148085321227, ewa inertia: 0.36936544085607115\n",
      "Minibatch step 166/11718: mean batch inertia: 0.34210264667660556, ewa inertia: 0.36890015971275114\n",
      "Minibatch step 167/11718: mean batch inertia: 0.34694829976524805, ewa inertia: 0.36852551775833003\n",
      "Minibatch step 168/11718: mean batch inertia: 0.34029109078115233, ewa inertia: 0.3680436542201157\n",
      "Minibatch step 169/11718: mean batch inertia: 0.3434106653109281, ewa inertia: 0.3676232547127281\n",
      "Minibatch step 170/11718: mean batch inertia: 0.34545043334067405, ewa inertia: 0.36724484171475336\n",
      "Minibatch step 171/11718: mean batch inertia: 0.3403497362588954, ewa inertia: 0.36678583574002316\n",
      "Minibatch step 172/11718: mean batch inertia: 0.3513935435529061, ewa inertia: 0.36652314280913745\n",
      "Minibatch step 173/11718: mean batch inertia: 0.33896070416078916, ewa inertia: 0.36605274777616426\n",
      "Minibatch step 174/11718: mean batch inertia: 0.35652482788260464, ewa inertia: 0.3658901392983848\n",
      "Minibatch step 175/11718: mean batch inertia: 0.34713505184269017, ewa inertia: 0.3655700551398423\n",
      "Minibatch step 176/11718: mean batch inertia: 0.3445850246972002, ewa inertia: 0.36521191360480065\n",
      "Minibatch step 177/11718: mean batch inertia: 0.34676892706173446, ewa inertia: 0.364897155924113\n",
      "Minibatch step 178/11718: mean batch inertia: 0.34194432831378274, ewa inertia: 0.364505430930605\n",
      "Minibatch step 179/11718: mean batch inertia: 0.34214578808200485, ewa inertia: 0.3641238295393338\n",
      "Minibatch step 180/11718: mean batch inertia: 0.33917031474905246, ewa inertia: 0.3636979597691611\n",
      "Minibatch step 181/11718: mean batch inertia: 0.3399759977831969, ewa inertia: 0.3632931083250293\n",
      "Minibatch step 182/11718: mean batch inertia: 0.3433733021187598, ewa inertia: 0.36295314646545784\n",
      "Minibatch step 183/11718: mean batch inertia: 0.3531740771431112, ewa inertia: 0.3627862517398125\n",
      "Minibatch step 184/11718: mean batch inertia: 0.3403969237508765, ewa inertia: 0.3624041437263681\n",
      "Minibatch step 185/11718: mean batch inertia: 0.3476585924694256, ewa inertia: 0.3621524884153772\n",
      "Minibatch step 186/11718: mean batch inertia: 0.34391118474571114, ewa inertia: 0.3618411727603787\n",
      "Minibatch step 187/11718: mean batch inertia: 0.3562192065275186, ewa inertia: 0.3617452253362331\n",
      "Minibatch step 188/11718: mean batch inertia: 0.34468560265673553, ewa inertia: 0.36145407686874026\n",
      "Minibatch step 189/11718: mean batch inertia: 0.3348575699673038, ewa inertia: 0.36100016693353854\n",
      "Minibatch step 190/11718: mean batch inertia: 0.3504258908811721, ewa inertia: 0.36081970079279596\n",
      "Minibatch step 191/11718: mean batch inertia: 0.33465205402515796, ewa inertia: 0.36037311000955147\n",
      "Minibatch step 192/11718: mean batch inertia: 0.35288728261440394, ewa inertia: 0.3602453529533164\n",
      "Minibatch step 193/11718: mean batch inertia: 0.3342060566735939, ewa inertia: 0.3598009526668115\n",
      "Minibatch step 194/11718: mean batch inertia: 0.34379936365961156, ewa inertia: 0.35952786115685126\n",
      "Minibatch step 195/11718: mean batch inertia: 0.337058749594134, ewa inertia: 0.3591443915150946\n",
      "Minibatch step 196/11718: mean batch inertia: 0.3389347000794195, ewa inertia: 0.358799482322169\n",
      "Minibatch step 197/11718: mean batch inertia: 0.34053242641325304, ewa inertia: 0.3584877271659498\n",
      "Minibatch step 198/11718: mean batch inertia: 0.3348750020575415, ewa inertia: 0.3580847400156592\n",
      "Minibatch step 199/11718: mean batch inertia: 0.33268246114531735, ewa inertia: 0.3576512114023438\n",
      "Minibatch step 200/11718: mean batch inertia: 0.33762945514265913, ewa inertia: 0.3573095096096935\n",
      "Minibatch step 201/11718: mean batch inertia: 0.3377124797217039, ewa inertia: 0.3569750564200484\n",
      "Minibatch step 202/11718: mean batch inertia: 0.344236496924449, ewa inertia: 0.35675765348301464\n",
      "Minibatch step 203/11718: mean batch inertia: 0.3496478999322032, ewa inertia: 0.35663631470023727\n",
      "Minibatch step 204/11718: mean batch inertia: 0.33742514646557437, ewa inertia: 0.3563084468279313\n",
      "Minibatch step 205/11718: mean batch inertia: 0.3393596984409408, ewa inertia: 0.35601919059926185\n",
      "Minibatch step 206/11718: mean batch inertia: 0.3477489995355163, ewa inertia: 0.3558780471813024\n",
      "Minibatch step 207/11718: mean batch inertia: 0.34395975534063494, ewa inertia: 0.3556746433622535\n",
      "Minibatch step 208/11718: mean batch inertia: 0.33991689531784497, ewa inertia: 0.35540571337004556\n",
      "Minibatch step 209/11718: mean batch inertia: 0.3381753920009076, ewa inertia: 0.3551116516691931\n",
      "Minibatch step 210/11718: mean batch inertia: 0.3471823489534146, ewa inertia: 0.3549763260305575\n",
      "Minibatch step 211/11718: mean batch inertia: 0.3329050289956988, ewa inertia: 0.35459964570016533\n",
      "Minibatch step 212/11718: mean batch inertia: 0.34478595535996004, ewa inertia: 0.3544321601140724\n",
      "Minibatch step 213/11718: mean batch inertia: 0.3449937653797889, ewa inertia: 0.3542710795196123\n",
      "Minibatch step 214/11718: mean batch inertia: 0.338822564419543, ewa inertia: 0.3540074270590083\n",
      "Minibatch step 215/11718: mean batch inertia: 0.33952941828861616, ewa inertia: 0.3537603377684044\n",
      "Minibatch step 216/11718: mean batch inertia: 0.3401643296934668, ewa inertia: 0.35352830116423045\n",
      "Minibatch step 217/11718: mean batch inertia: 0.3401155507651419, ewa inertia: 0.3532993921316613\n",
      "Minibatch step 218/11718: mean batch inertia: 0.34647790013453883, ewa inertia: 0.35318297297173673\n",
      "Minibatch step 219/11718: mean batch inertia: 0.33470537060723027, ewa inertia: 0.35286762451928627\n",
      "Minibatch step 220/11718: mean batch inertia: 0.34813752977447643, ewa inertia: 0.35278689824169385\n",
      "Minibatch step 221/11718: mean batch inertia: 0.3374814809657739, ewa inertia: 0.35252568796360384\n",
      "Minibatch step 222/11718: mean batch inertia: 0.33828486207036695, ewa inertia: 0.35228264656037095\n",
      "Minibatch step 223/11718: mean batch inertia: 0.331493829038862, ewa inertia: 0.3519278536979444\n",
      "Minibatch step 224/11718: mean batch inertia: 0.333025976959985, ewa inertia: 0.3516052643565277\n",
      "Minibatch step 225/11718: mean batch inertia: 0.3324601442247835, ewa inertia: 0.3512785236957848\n",
      "Minibatch step 226/11718: mean batch inertia: 0.34205583807019097, ewa inertia: 0.35112112450610117\n",
      "Minibatch step 227/11718: mean batch inertia: 0.3328303249935375, ewa inertia: 0.35080896412908985\n",
      "Minibatch step 228/11718: mean batch inertia: 0.3304703423704394, ewa inertia: 0.3504618545436554\n",
      "Minibatch step 229/11718: mean batch inertia: 0.344294313212346, ewa inertia: 0.3503565960487552\n",
      "Minibatch step 230/11718: mean batch inertia: 0.34890788043301724, ewa inertia: 0.35033187150828443\n",
      "Minibatch step 231/11718: mean batch inertia: 0.34347209163986636, ewa inertia: 0.3502147989074684\n",
      "Minibatch step 232/11718: mean batch inertia: 0.34552250073056906, ewa inertia: 0.3501347176859262\n",
      "Minibatch step 233/11718: mean batch inertia: 0.3496841832674757, ewa inertia: 0.3501270286292601\n",
      "Minibatch step 234/11718: mean batch inertia: 0.32868396491574076, ewa inertia: 0.34976107005820417\n",
      "Minibatch step 235/11718: mean batch inertia: 0.3467353744392714, ewa inertia: 0.3497094319499586\n",
      "Minibatch step 236/11718: mean batch inertia: 0.33600816394083416, ewa inertia: 0.3494755989245448\n",
      "Minibatch step 237/11718: mean batch inertia: 0.33846336491008455, ewa inertia: 0.349287658363536\n",
      "Minibatch step 238/11718: mean batch inertia: 0.3375370735796779, ewa inertia: 0.34908711672107184\n",
      "Minibatch step 239/11718: mean batch inertia: 0.3447434749599097, ewa inertia: 0.3490129858527719\n",
      "Minibatch step 240/11718: mean batch inertia: 0.3418506439469366, ewa inertia: 0.348890749569548\n",
      "Minibatch step 241/11718: mean batch inertia: 0.3429145272845634, ewa inertia: 0.34878875622582883\n",
      "Minibatch step 242/11718: mean batch inertia: 0.3384794104641366, ewa inertia: 0.34861281152436846\n",
      "Minibatch step 243/11718: mean batch inertia: 0.34435720240406065, ewa inertia: 0.3485401830672857\n",
      "Minibatch step 244/11718: mean batch inertia: 0.34518955920661987, ewa inertia: 0.3484829995632595\n",
      "Minibatch step 245/11718: mean batch inertia: 0.3442055927842482, ewa inertia: 0.3484099990959016\n",
      "Minibatch step 246/11718: mean batch inertia: 0.3399790955867402, ewa inertia: 0.3482661128750637\n",
      "Minibatch step 247/11718: mean batch inertia: 0.3440116241984091, ewa inertia: 0.34819350354005996\n",
      "Minibatch step 248/11718: mean batch inertia: 0.33103926226199326, ewa inertia: 0.3479007402619416\n",
      "Minibatch step 249/11718: mean batch inertia: 0.33685655928193037, ewa inertia: 0.34771225447726434\n",
      "Minibatch step 250/11718: mean batch inertia: 0.3374743344923071, ewa inertia: 0.3475375287655687\n",
      "Minibatch step 251/11718: mean batch inertia: 0.3502623576157967, ewa inertia: 0.3475840321237513\n",
      "Minibatch step 252/11718: mean batch inertia: 0.34168563178609157, ewa inertia: 0.3474833669301985\n",
      "Minibatch step 253/11718: mean batch inertia: 0.3350499845413786, ewa inertia: 0.3472711723057178\n",
      "Minibatch step 254/11718: mean batch inertia: 0.33684095902469235, ewa inertia: 0.34709316481578406\n",
      "Minibatch step 255/11718: mean batch inertia: 0.3363102090943002, ewa inertia: 0.3469091372383672\n",
      "Minibatch step 256/11718: mean batch inertia: 0.32900576878754195, ewa inertia: 0.34660358896304205\n",
      "Minibatch step 257/11718: mean batch inertia: 0.33527254468328627, ewa inertia: 0.3464102074188471\n",
      "Minibatch step 258/11718: mean batch inertia: 0.3373598098343028, ewa inertia: 0.3462557485872278\n",
      "Minibatch step 259/11718: mean batch inertia: 0.338464821029092, ewa inertia: 0.3461227845316027\n",
      "Minibatch step 260/11718: mean batch inertia: 0.3360152974713969, ewa inertia: 0.34595028485660584\n",
      "Minibatch step 261/11718: mean batch inertia: 0.3422640419814793, ewa inertia: 0.34588737350246496\n",
      "Minibatch step 262/11718: mean batch inertia: 0.3413456318503913, ewa inertia: 0.3458098617575341\n",
      "Minibatch step 263/11718: mean batch inertia: 0.3402770390184465, ewa inertia: 0.3457154357030041\n",
      "Minibatch step 264/11718: mean batch inertia: 0.3360341588679376, ewa inertia: 0.3455502099552335\n",
      "Minibatch step 265/11718: mean batch inertia: 0.3409245365709795, ewa inertia: 0.34547126578734366\n",
      "Minibatch step 266/11718: mean batch inertia: 0.33658506452180864, ewa inertia: 0.3453196092162166\n",
      "Minibatch step 267/11718: mean batch inertia: 0.33849153705780166, ewa inertia: 0.3452030777558085\n",
      "Minibatch step 268/11718: mean batch inertia: 0.34083519633751597, ewa inertia: 0.3451285332008076\n",
      "Minibatch step 269/11718: mean batch inertia: 0.33776329076540645, ewa inertia: 0.34500283411073585\n",
      "Minibatch step 270/11718: mean batch inertia: 0.332735443289647, ewa inertia: 0.34479347238540364\n",
      "Minibatch step 271/11718: mean batch inertia: 0.3264188839768816, ewa inertia: 0.3444798820231512\n",
      "Minibatch step 272/11718: mean batch inertia: 0.3333520240176129, ewa inertia: 0.3442899681624722\n",
      "Minibatch step 273/11718: mean batch inertia: 0.33876725106291705, ewa inertia: 0.3441957145760864\n",
      "Minibatch step 274/11718: mean batch inertia: 0.3292588973943892, ewa inertia: 0.34394079502051506\n",
      "Minibatch step 275/11718: mean batch inertia: 0.3334988806150847, ewa inertia: 0.3437625878330556\n",
      "Minibatch step 276/11718: mean batch inertia: 0.3320715338694748, ewa inertia: 0.34356306217479093\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 277/11718: mean batch inertia: 0.33424932994611173, ewa inertia: 0.34340410913603014\n",
      "Minibatch step 278/11718: mean batch inertia: 0.33292588836155956, ewa inertia: 0.3432252823250359\n",
      "Minibatch step 279/11718: mean batch inertia: 0.34073557424082923, ewa inertia: 0.34318279166115434\n",
      "Minibatch step 280/11718: mean batch inertia: 0.3289947996709496, ewa inertia: 0.3429406519490191\n",
      "Minibatch step 281/11718: mean batch inertia: 0.33377212131970735, ewa inertia: 0.34278417699690344\n",
      "Minibatch step 282/11718: mean batch inertia: 0.33106318378942917, ewa inertia: 0.342584140379801\n",
      "Minibatch step 283/11718: mean batch inertia: 0.3426049443168748, ewa inertia: 0.34258449543070163\n",
      "Minibatch step 284/11718: mean batch inertia: 0.33378935024384365, ewa inertia: 0.34243439287036725\n",
      "Minibatch step 285/11718: mean batch inertia: 0.3253530634969718, ewa inertia: 0.3421428739450524\n",
      "Minibatch step 286/11718: mean batch inertia: 0.3362041452397539, ewa inertia: 0.34204152048642744\n",
      "Minibatch step 287/11718: mean batch inertia: 0.33907299228793686, ewa inertia: 0.34199085802736034\n",
      "Minibatch step 288/11718: mean batch inertia: 0.3328852761265036, ewa inertia: 0.3418354573912577\n",
      "Minibatch step 289/11718: mean batch inertia: 0.3350966888451293, ewa inertia: 0.34172045003313173\n",
      "Minibatch step 290/11718: mean batch inertia: 0.33250615736856315, ewa inertia: 0.34156319408212266\n",
      "Minibatch step 291/11718: mean batch inertia: 0.3356664624295649, ewa inertia: 0.341462557367225\n",
      "Minibatch step 292/11718: mean batch inertia: 0.3390860219085969, ewa inertia: 0.34142199816672436\n",
      "Minibatch step 293/11718: mean batch inertia: 0.33153653226779783, ewa inertia: 0.3412532876213039\n",
      "Minibatch step 294/11718: mean batch inertia: 0.3329718257035576, ewa inertia: 0.34111195184903914\n",
      "Minibatch step 295/11718: mean batch inertia: 0.3379178571407624, ewa inertia: 0.3410574397536187\n",
      "Minibatch step 296/11718: mean batch inertia: 0.3394653051459243, ewa inertia: 0.3410302675494158\n",
      "Minibatch step 297/11718: mean batch inertia: 0.33774508102505374, ewa inertia: 0.3409742008332893\n",
      "Minibatch step 298/11718: mean batch inertia: 0.33638687629315644, ewa inertia: 0.34089591114688506\n",
      "Minibatch step 299/11718: mean batch inertia: 0.32845403570227594, ewa inertia: 0.34068357157546014\n",
      "Minibatch step 300/11718: mean batch inertia: 0.3403916190859576, ewa inertia: 0.3406785889611611\n",
      "Minibatch step 301/11718: mean batch inertia: 0.33419052681301414, ewa inertia: 0.340567860289905\n",
      "Minibatch step 302/11718: mean batch inertia: 0.33537731631684475, ewa inertia: 0.34047927574430265\n",
      "Minibatch step 303/11718: mean batch inertia: 0.33909572727408577, ewa inertia: 0.340455663380514\n",
      "Minibatch step 304/11718: mean batch inertia: 0.3269780728316592, ewa inertia: 0.34022564775194375\n",
      "Minibatch step 305/11718: mean batch inertia: 0.32942703245928256, ewa inertia: 0.3400413529200726\n",
      "Minibatch step 306/11718: mean batch inertia: 0.33709583334160603, ewa inertia: 0.3399910831381816\n",
      "Minibatch step 307/11718: mean batch inertia: 0.3354196397783512, ewa inertia: 0.33991306448832925\n",
      "Minibatch step 308/11718: mean batch inertia: 0.3282056065015472, ewa inertia: 0.33971325887040166\n",
      "Minibatch step 309/11718: mean batch inertia: 0.34434050097828944, ewa inertia: 0.33979222981095175\n",
      "Minibatch step 310/11718: mean batch inertia: 0.33876014745673716, ewa inertia: 0.3397746157522236\n",
      "Minibatch step 311/11718: mean batch inertia: 0.33142771316684383, ewa inertia: 0.33963216313520495\n",
      "Minibatch step 312/11718: mean batch inertia: 0.3297304583600651, ewa inertia: 0.3394631754486066\n",
      "Minibatch step 313/11718: mean batch inertia: 0.33322958666936514, ewa inertia: 0.3393567897533217\n",
      "Minibatch step 314/11718: mean batch inertia: 0.33408843118849074, ewa inertia: 0.3392668771830866\n",
      "Minibatch step 315/11718: mean batch inertia: 0.3334267802948834, ewa inertia: 0.33916720702677927\n",
      "Minibatch step 316/11718: mean batch inertia: 0.33351155147783745, ewa inertia: 0.3390706846430972\n",
      "Minibatch step 317/11718: mean batch inertia: 0.323748722052237, ewa inertia: 0.33880919199398524\n",
      "Minibatch step 318/11718: mean batch inertia: 0.3326399576209842, ewa inertia: 0.3387039046047476\n",
      "Minibatch step 319/11718: mean batch inertia: 0.3282718724786132, ewa inertia: 0.3385258660734493\n",
      "Minibatch step 320/11718: mean batch inertia: 0.33927408722857266, ewa inertia: 0.33853863560808395\n",
      "Minibatch step 321/11718: mean batch inertia: 0.3285858312408074, ewa inertia: 0.33836877582904723\n",
      "Minibatch step 322/11718: mean batch inertia: 0.3346265941792559, ewa inertia: 0.33830490979444106\n",
      "Minibatch step 323/11718: mean batch inertia: 0.33049600453782524, ewa inertia: 0.33817163892198543\n",
      "Minibatch step 324/11718: mean batch inertia: 0.33899037987142894, ewa inertia: 0.3381856119844137\n",
      "Minibatch step 325/11718: mean batch inertia: 0.33256688856622935, ewa inertia: 0.338089719903844\n",
      "Minibatch step 326/11718: mean batch inertia: 0.33914288959500155, ewa inertia: 0.3381076938501235\n",
      "Minibatch step 327/11718: mean batch inertia: 0.3289773122624559, ewa inertia: 0.3379518699695596\n",
      "Minibatch step 328/11718: mean batch inertia: 0.34113719616748867, ewa inertia: 0.3380062324169839\n",
      "Minibatch step 329/11718: mean batch inertia: 0.3296969470946021, ewa inertia: 0.33786442179590376\n",
      "Minibatch step 330/11718: mean batch inertia: 0.3335137144263974, ewa inertia: 0.3377901703422263\n",
      "Minibatch step 331/11718: mean batch inertia: 0.32818363033690684, ewa inertia: 0.33762622009238763\n",
      "Minibatch step 332/11718: mean batch inertia: 0.32338726995263334, ewa inertia: 0.3373832107017474\n",
      "Minibatch step 333/11718: mean batch inertia: 0.3348080996348821, ewa inertia: 0.33733926250577456\n",
      "Minibatch step 334/11718: mean batch inertia: 0.3337000894848779, ewa inertia: 0.3372771544704516\n",
      "Minibatch step 335/11718: mean batch inertia: 0.33362287125795365, ewa inertia: 0.33721478855667425\n",
      "Minibatch step 336/11718: mean batch inertia: 0.3300461682949854, ewa inertia: 0.3370924451237367\n",
      "Minibatch step 337/11718: mean batch inertia: 0.3277189476736844, ewa inertia: 0.3369324721003643\n",
      "Minibatch step 338/11718: mean batch inertia: 0.33600499204168083, ewa inertia: 0.33691664323926995\n",
      "Minibatch step 339/11718: mean batch inertia: 0.33805866702951487, ewa inertia: 0.3369361336162036\n",
      "Minibatch step 340/11718: mean batch inertia: 0.3335989397383361, ewa inertia: 0.3368791793153072\n",
      "Minibatch step 341/11718: mean batch inertia: 0.3270887092883916, ewa inertia: 0.3367120900192586\n",
      "Minibatch step 342/11718: mean batch inertia: 0.32523201429920523, ewa inertia: 0.3365161650263446\n",
      "Minibatch step 343/11718: mean batch inertia: 0.3297360323756251, ewa inertia: 0.3364004517267165\n",
      "Minibatch step 344/11718: mean batch inertia: 0.3268989756293476, ewa inertia: 0.3362382945526312\n",
      "Minibatch step 345/11718: mean batch inertia: 0.3390867068413808, ewa inertia: 0.3362869070505884\n",
      "Minibatch step 346/11718: mean batch inertia: 0.3394843441985909, ewa inertia: 0.3363414761898381\n",
      "Minibatch step 347/11718: mean batch inertia: 0.33791080648174177, ewa inertia: 0.3363682592036282\n",
      "Minibatch step 348/11718: mean batch inertia: 0.3296090859899748, ewa inertia: 0.3362529036087451\n",
      "Minibatch step 349/11718: mean batch inertia: 0.3325729000081692, ewa inertia: 0.33619009873733585\n",
      "Minibatch step 350/11718: mean batch inertia: 0.326412643192511, ewa inertia: 0.336023231553264\n",
      "Minibatch step 351/11718: mean batch inertia: 0.33401556718660486, ewa inertia: 0.3359889677002718\n",
      "Minibatch step 352/11718: mean batch inertia: 0.3312750279087748, ewa inertia: 0.335908517131585\n",
      "Minibatch step 353/11718: mean batch inertia: 0.33759436392655995, ewa inertia: 0.335937288677123\n",
      "Minibatch step 354/11718: mean batch inertia: 0.3367708518527947, ewa inertia: 0.3359515147034376\n",
      "Minibatch step 355/11718: mean batch inertia: 0.3375495628876291, ewa inertia: 0.3359787878318384\n",
      "Minibatch step 356/11718: mean batch inertia: 0.3361204727791459, ewa inertia: 0.3359812059014552\n",
      "Minibatch step 357/11718: mean batch inertia: 0.33604355638462596, ewa inertia: 0.3359822700075004\n",
      "Minibatch step 358/11718: mean batch inertia: 0.32904537592499905, ewa inertia: 0.3358638813350647\n",
      "Minibatch step 359/11718: mean batch inertia: 0.3321463900842291, ewa inertia: 0.3358004366797559\n",
      "Minibatch step 360/11718: mean batch inertia: 0.32223616667039373, ewa inertia: 0.335568941734054\n",
      "Minibatch step 361/11718: mean batch inertia: 0.3370098291542414, ewa inertia: 0.33559353267443404\n",
      "Minibatch step 362/11718: mean batch inertia: 0.32820002382812785, ewa inertia: 0.3354673511749696\n",
      "Minibatch step 363/11718: mean batch inertia: 0.32560305917060833, ewa inertia: 0.3352990019943384\n",
      "Minibatch step 364/11718: mean batch inertia: 0.32682661144385544, ewa inertia: 0.3351544077338956\n",
      "Minibatch step 365/11718: mean batch inertia: 0.3352944303270906, ewa inertia: 0.3351567974329053\n",
      "Minibatch step 366/11718: mean batch inertia: 0.3347967652551584, ewa inertia: 0.3351506529349425\n",
      "Minibatch step 367/11718: mean batch inertia: 0.33450631100472716, ewa inertia: 0.3351396562576392\n",
      "Minibatch step 368/11718: mean batch inertia: 0.3300151823957264, ewa inertia: 0.3350521992992039\n",
      "Minibatch step 369/11718: mean batch inertia: 0.3325205448531821, ewa inertia: 0.335008992756713\n",
      "Minibatch step 370/11718: mean batch inertia: 0.3340576610945392, ewa inertia: 0.3349927568316446\n",
      "Minibatch step 371/11718: mean batch inertia: 0.3292000887706849, ewa inertia: 0.3348938961205768\n",
      "Minibatch step 372/11718: mean batch inertia: 0.3300431160495107, ewa inertia: 0.3348111101639136\n",
      "Minibatch step 373/11718: mean batch inertia: 0.326144290235909, ewa inertia: 0.3346631976697464\n",
      "Minibatch step 374/11718: mean batch inertia: 0.33385989691912765, ewa inertia: 0.3346494881178488\n",
      "Minibatch step 375/11718: mean batch inertia: 0.33968770912444046, ewa inertia: 0.33473547303982026\n",
      "Minibatch step 376/11718: mean batch inertia: 0.3338683649152694, ewa inertia: 0.3347206745178156\n",
      "Minibatch step 377/11718: mean batch inertia: 0.335523040512175, ewa inertia: 0.33473436811667273\n",
      "Minibatch step 378/11718: mean batch inertia: 0.33285889181716766, ewa inertia: 0.33470236025456007\n",
      "Minibatch step 379/11718: mean batch inertia: 0.33897602551920003, ewa inertia: 0.33477529686727153\n",
      "Minibatch step 380/11718: mean batch inertia: 0.330902983323597, ewa inertia: 0.33470920993351727\n",
      "Minibatch step 381/11718: mean batch inertia: 0.32912322017705375, ewa inertia: 0.3346138765027855\n",
      "Minibatch step 382/11718: mean batch inertia: 0.3269521562445911, ewa inertia: 0.33448311756670346\n",
      "Minibatch step 383/11718: mean batch inertia: 0.3268741804611454, ewa inertia: 0.33435325945558625\n",
      "Minibatch step 384/11718: mean batch inertia: 0.3299756948413364, ewa inertia: 0.33427854964208487\n",
      "Minibatch step 385/11718: mean batch inertia: 0.3337815643018853, ewa inertia: 0.3342700678296272\n",
      "Minibatch step 386/11718: mean batch inertia: 0.33073153389423954, ewa inertia: 0.3342096773537172\n",
      "Minibatch step 387/11718: mean batch inertia: 0.32826882661237766, ewa inertia: 0.33410828767931233\n",
      "Minibatch step 388/11718: mean batch inertia: 0.3347734176620167, ewa inertia: 0.3341196391364217\n",
      "Minibatch step 389/11718: mean batch inertia: 0.32527188702211796, ewa inertia: 0.33396863875867405\n",
      "Minibatch step 390/11718: mean batch inertia: 0.33576805200636717, ewa inertia: 0.3339993484888536\n",
      "Minibatch step 391/11718: mean batch inertia: 0.32733780311656235, ewa inertia: 0.3338856590619117\n",
      "Minibatch step 392/11718: mean batch inertia: 0.3298870563220771, ewa inertia: 0.333817416810504\n",
      "Minibatch step 393/11718: mean batch inertia: 0.33792233611279204, ewa inertia: 0.33388747351612386\n",
      "Minibatch step 394/11718: mean batch inertia: 0.3299201696416593, ewa inertia: 0.3338197654275671\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 395/11718: mean batch inertia: 0.3286616710324163, ewa inertia: 0.3337317346834794\n",
      "Minibatch step 396/11718: mean batch inertia: 0.33157683580859443, ewa inertia: 0.33369495804915333\n",
      "Minibatch step 397/11718: mean batch inertia: 0.3339102358830179, ewa inertia: 0.3336986320935676\n",
      "Minibatch step 398/11718: mean batch inertia: 0.33163254683878796, ewa inertia: 0.33366337119906014\n",
      "Minibatch step 399/11718: mean batch inertia: 0.32437139668181075, ewa inertia: 0.33350478948881335\n",
      "Minibatch step 400/11718: mean batch inertia: 0.33525101737502705, ewa inertia: 0.3335345915297211\n",
      "Minibatch step 401/11718: mean batch inertia: 0.3249115944406771, ewa inertia: 0.33338742693910633\n",
      "Minibatch step 402/11718: mean batch inertia: 0.3307595094616413, ewa inertia: 0.3333425775212361\n",
      "Minibatch step 403/11718: mean batch inertia: 0.33680848848304196, ewa inertia: 0.33340172857539213\n",
      "Minibatch step 404/11718: mean batch inertia: 0.3270010560191307, ewa inertia: 0.33329249134074224\n",
      "Minibatch step 405/11718: mean batch inertia: 0.3235014873083613, ewa inertia: 0.33312539293107635\n",
      "Minibatch step 406/11718: mean batch inertia: 0.32443154073794905, ewa inertia: 0.33297701909009564\n",
      "Minibatch step 407/11718: mean batch inertia: 0.328730076259139, ewa inertia: 0.33290453853645197\n",
      "Minibatch step 408/11718: mean batch inertia: 0.32905039762442545, ewa inertia: 0.33283876174635996\n",
      "Minibatch step 409/11718: mean batch inertia: 0.3338961412035041, ewa inertia: 0.3328568075387136\n",
      "Minibatch step 410/11718: mean batch inertia: 0.3238150389801894, ewa inertia: 0.33270249597457785\n",
      "Minibatch step 411/11718: mean batch inertia: 0.3283921035487929, ewa inertia: 0.332628932556873\n",
      "Minibatch step 412/11718: mean batch inertia: 0.3298858120032731, ewa inertia: 0.33258211702288765\n",
      "Minibatch step 413/11718: mean batch inertia: 0.33592357526496536, ewa inertia: 0.3326391441016601\n",
      "Minibatch step 414/11718: mean batch inertia: 0.3282941261813057, ewa inertia: 0.33256498974710574\n",
      "Minibatch step 415/11718: mean batch inertia: 0.32426976644686567, ewa inertia: 0.33242341911587026\n",
      "Minibatch step 416/11718: mean batch inertia: 0.3371229624735521, ewa inertia: 0.3325036239874674\n",
      "Minibatch step 417/11718: mean batch inertia: 0.3316976494894817, ewa inertia: 0.33248986880399495\n",
      "Minibatch step 418/11718: mean batch inertia: 0.33362127193525465, ewa inertia: 0.33250917792319246\n",
      "Minibatch step 419/11718: mean batch inertia: 0.33268781889039856, ewa inertia: 0.3325122267036263\n",
      "Minibatch step 420/11718: mean batch inertia: 0.33096062379858293, ewa inertia: 0.33248574623471744\n",
      "Minibatch step 421/11718: mean batch inertia: 0.3272069576029237, ewa inertia: 0.33239565565948964\n",
      "Minibatch step 422/11718: mean batch inertia: 0.32948421386669074, ewa inertia: 0.3323459674669608\n",
      "Minibatch step 423/11718: mean batch inertia: 0.3315777773011121, ewa inertia: 0.3323328571307165\n",
      "Minibatch step 424/11718: mean batch inertia: 0.3314317647003104, ewa inertia: 0.3323174786147252\n",
      "Minibatch step 425/11718: mean batch inertia: 0.3341246013245248, ewa inertia: 0.3323483199186283\n",
      "Minibatch step 426/11718: mean batch inertia: 0.32737445009793364, ewa inertia: 0.33226343324774404\n",
      "Minibatch step 427/11718: mean batch inertia: 0.32802713411707096, ewa inertia: 0.3321911343450714\n",
      "Minibatch step 428/11718: mean batch inertia: 0.3342745448302952, ewa inertia: 0.3322266909210478\n",
      "Minibatch step 429/11718: mean batch inertia: 0.32634195838654323, ewa inertia: 0.33212625898939163\n",
      "Minibatch step 430/11718: mean batch inertia: 0.3260995086295671, ewa inertia: 0.332023403307048\n",
      "Minibatch step 431/11718: mean batch inertia: 0.32729200390571017, ewa inertia: 0.33194265476350304\n",
      "Minibatch step 432/11718: mean batch inertia: 0.332192545642801, ewa inertia: 0.3319469195323033\n",
      "Minibatch step 433/11718: mean batch inertia: 0.3269880555961061, ewa inertia: 0.3318622889597137\n",
      "Minibatch step 434/11718: mean batch inertia: 0.33328900330776157, ewa inertia: 0.3318866380150116\n",
      "Minibatch step 435/11718: mean batch inertia: 0.32992265450528513, ewa inertia: 0.3318531196424321\n",
      "Minibatch step 436/11718: mean batch inertia: 0.3333752549088983, ewa inertia: 0.33187909720116676\n",
      "Minibatch step 437/11718: mean batch inertia: 0.3318388984705548, ewa inertia: 0.3318784111485481\n",
      "Minibatch step 438/11718: mean batch inertia: 0.33191624301766054, ewa inertia: 0.3318790568070671\n",
      "Minibatch step 439/11718: mean batch inertia: 0.33065714120518386, ewa inertia: 0.3318582029545771\n",
      "Minibatch step 440/11718: mean batch inertia: 0.33018772065515134, ewa inertia: 0.3318296936275779\n",
      "Minibatch step 441/11718: mean batch inertia: 0.33610275025286235, ewa inertia: 0.3319026198529309\n",
      "Minibatch step 442/11718: mean batch inertia: 0.32404052043681847, ewa inertia: 0.33176844114105186\n",
      "Minibatch step 443/11718: mean batch inertia: 0.32943502533102803, ewa inertia: 0.33172861784308827\n",
      "Minibatch step 444/11718: mean batch inertia: 0.32635615546303864, ewa inertia: 0.33163692858254595\n",
      "Minibatch step 445/11718: mean batch inertia: 0.3220821288678254, ewa inertia: 0.3314738613596416\n",
      "Minibatch step 446/11718: mean batch inertia: 0.32828088168416564, ewa inertia: 0.33141936829395563\n",
      "Minibatch step 447/11718: mean batch inertia: 0.3258121337207629, ewa inertia: 0.3313236722880398\n",
      "Minibatch step 448/11718: mean batch inertia: 0.33457530334588864, ewa inertia: 0.3313791663289768\n",
      "Minibatch step 449/11718: mean batch inertia: 0.32521974617975075, ewa inertia: 0.3312740464344291\n",
      "Minibatch step 450/11718: mean batch inertia: 0.3342846597386945, ewa inertia: 0.3313254271399827\n",
      "Minibatch step 451/11718: mean batch inertia: 0.3303542454206368, ewa inertia: 0.33130885244342834\n",
      "Minibatch step 452/11718: mean batch inertia: 0.32326361657105696, ewa inertia: 0.3311715482287417\n",
      "Minibatch step 453/11718: mean batch inertia: 0.3403155871208623, ewa inertia: 0.3313276051920258\n",
      "Minibatch step 454/11718: mean batch inertia: 0.3272734817008551, ewa inertia: 0.33125841539435813\n",
      "Minibatch step 455/11718: mean batch inertia: 0.32056455098803677, ewa inertia: 0.3310759082960494\n",
      "Minibatch step 456/11718: mean batch inertia: 0.3373748006680408, ewa inertia: 0.33118340849669636\n",
      "Minibatch step 457/11718: mean batch inertia: 0.32740006932557564, ewa inertia: 0.3311188400462463\n",
      "Minibatch step 458/11718: mean batch inertia: 0.3293411317536096, ewa inertia: 0.3310885007442128\n",
      "Minibatch step 459/11718: mean batch inertia: 0.326466683608344, ewa inertia: 0.33100962238908027\n",
      "Minibatch step 460/11718: mean batch inertia: 0.3274845155063908, ewa inertia: 0.33094946106629336\n",
      "Minibatch step 461/11718: mean batch inertia: 0.319100942000874, ewa inertia: 0.3307472480260189\n",
      "Minibatch step 462/11718: mean batch inertia: 0.32246521004983053, ewa inertia: 0.33060590242243865\n",
      "Minibatch step 463/11718: mean batch inertia: 0.33319490770141547, ewa inertia: 0.3306500877443222\n",
      "Minibatch step 464/11718: mean batch inertia: 0.32228742721105186, ewa inertia: 0.33050736619390064\n",
      "Minibatch step 465/11718: mean batch inertia: 0.3292723424364662, ewa inertia: 0.33048628863075347\n",
      "Minibatch step 466/11718: mean batch inertia: 0.3291986408113692, ewa inertia: 0.33046431295776657\n",
      "Minibatch step 467/11718: mean batch inertia: 0.3286375231935464, ewa inertia: 0.3304331360055985\n",
      "Minibatch step 468/11718: mean batch inertia: 0.3295802119118656, ewa inertia: 0.3304185795557025\n",
      "Minibatch step 469/11718: mean batch inertia: 0.32702680037689735, ewa inertia: 0.3303606936734332\n",
      "Minibatch step 470/11718: mean batch inertia: 0.33110620220882325, ewa inertia: 0.3303734169130768\n",
      "Minibatch step 471/11718: mean batch inertia: 0.3287318739534674, ewa inertia: 0.33034540148002806\n",
      "Minibatch step 472/11718: mean batch inertia: 0.32709550790193087, ewa inertia: 0.3302899370918318\n",
      "Minibatch step 473/11718: mean batch inertia: 0.32573433524359485, ewa inertia: 0.3302121888015243\n",
      "Minibatch step 474/11718: mean batch inertia: 0.32877913882825804, ewa inertia: 0.33018773161912374\n",
      "Minibatch step 475/11718: mean batch inertia: 0.32875202644098744, ewa inertia: 0.33016322912160434\n",
      "Minibatch step 476/11718: mean batch inertia: 0.33044931126210764, ewa inertia: 0.3301681115494487\n",
      "Minibatch step 477/11718: mean batch inertia: 0.327912230334461, ewa inertia: 0.3301296114975467\n",
      "Minibatch step 478/11718: mean batch inertia: 0.3300452579367701, ewa inertia: 0.33012817187543964\n",
      "Minibatch step 479/11718: mean batch inertia: 0.3292378640342858, ewa inertia: 0.3301129774149045\n",
      "Minibatch step 480/11718: mean batch inertia: 0.31917712694678435, ewa inertia: 0.3299263404555566\n",
      "Minibatch step 481/11718: mean batch inertia: 0.3241905708320082, ewa inertia: 0.3298284508030618\n",
      "Minibatch step 482/11718: mean batch inertia: 0.32857647634510234, ewa inertia: 0.32980708395036973\n",
      "Minibatch step 483/11718: mean batch inertia: 0.33219673205374695, ewa inertia: 0.32984786693814244\n",
      "Minibatch step 484/11718: mean batch inertia: 0.32332609664294193, ewa inertia: 0.32973656298597065\n",
      "Minibatch step 485/11718: mean batch inertia: 0.328715925385821, ewa inertia: 0.32971914424941756\n",
      "Minibatch step 486/11718: mean batch inertia: 0.33305059457619285, ewa inertia: 0.3297760005278589\n",
      "Minibatch step 487/11718: mean batch inertia: 0.32903490558225595, ewa inertia: 0.3297633526128532\n",
      "Minibatch step 488/11718: mean batch inertia: 0.33022990132015473, ewa inertia: 0.32977131497777146\n",
      "Minibatch step 489/11718: mean batch inertia: 0.3248831239391373, ewa inertia: 0.3296878905459157\n",
      "Minibatch step 490/11718: mean batch inertia: 0.330515198048214, ewa inertia: 0.3297020098096277\n",
      "Minibatch step 491/11718: mean batch inertia: 0.32925784533527264, ewa inertia: 0.3296944294657682\n",
      "Minibatch step 492/11718: mean batch inertia: 0.3281929524957114, ewa inertia: 0.3296688044723542\n",
      "Minibatch step 493/11718: mean batch inertia: 0.3292012012016374, ewa inertia: 0.32966082410970365\n",
      "Minibatch step 494/11718: mean batch inertia: 0.33711838111580095, ewa inertia: 0.3297880986886529\n",
      "Minibatch step 495/11718: mean batch inertia: 0.33015313772534577, ewa inertia: 0.3297943286362962\n",
      "Minibatch step 496/11718: mean batch inertia: 0.3258469024700512, ewa inertia: 0.3297269597911327\n",
      "Minibatch step 497/11718: mean batch inertia: 0.324069023338834, ewa inertia: 0.32963039848035774\n",
      "Minibatch step 498/11718: mean batch inertia: 0.3273352576339805, ewa inertia: 0.3295912284029969\n",
      "Minibatch step 499/11718: mean batch inertia: 0.3316184477878537, ewa inertia: 0.32962582599218526\n",
      "Minibatch step 500/11718: mean batch inertia: 0.32243921482746635, ewa inertia: 0.3295031755170614\n",
      "Minibatch step 501/11718: mean batch inertia: 0.3252736354545689, ewa inertia: 0.3294309919681911\n",
      "Minibatch step 502/11718: mean batch inertia: 0.3288894053956257, ewa inertia: 0.329421748967711\n",
      "Minibatch step 503/11718: mean batch inertia: 0.3281837009005012, ewa inertia: 0.3294006197901071\n",
      "Minibatch step 504/11718: mean batch inertia: 0.32510796689229166, ewa inertia: 0.3293273591244899\n",
      "Minibatch step 505/11718: mean batch inertia: 0.3245377344725582, ewa inertia: 0.32924561687828235\n",
      "Minibatch step 506/11718: mean batch inertia: 0.3280928937841368, ewa inertia: 0.3292259439014171\n",
      "Minibatch step 507/11718: mean batch inertia: 0.3283323388555852, ewa inertia: 0.32921069316905766\n",
      "Minibatch step 508/11718: mean batch inertia: 0.33081452597833894, ewa inertia: 0.32923806502090397\n",
      "Minibatch step 509/11718: mean batch inertia: 0.32228893459656727, ewa inertia: 0.3291194675166412\n",
      "Minibatch step 510/11718: mean batch inertia: 0.33043403130977667, ewa inertia: 0.329141902551752\n",
      "Minibatch step 511/11718: mean batch inertia: 0.32786158863510817, ewa inertia: 0.32912005204299555\n",
      "Minibatch step 512/11718: mean batch inertia: 0.33241042439081253, ewa inertia: 0.32917620726310476\n",
      "[MiniBatchKMeans] Reassigning 512 cluster centers.\n",
      "Minibatch step 513/11718: mean batch inertia: 0.3337553551812154, ewa inertia: 0.32925435740298936\n",
      "Minibatch step 514/11718: mean batch inertia: 0.3260259862794188, ewa inertia: 0.32919926032828933\n",
      "Minibatch step 515/11718: mean batch inertia: 0.32833182639965247, ewa inertia: 0.3291844562459413\n",
      "Minibatch step 516/11718: mean batch inertia: 0.3286875508265523, ewa inertia: 0.3291759757974541\n",
      "Minibatch step 517/11718: mean batch inertia: 0.32659355976476934, ewa inertia: 0.32913190293110356\n",
      "Minibatch step 518/11718: mean batch inertia: 0.3241668663209013, ewa inertia: 0.3290471670124221\n",
      "Minibatch step 519/11718: mean batch inertia: 0.3336372185016532, ewa inertia: 0.32912550323836975\n",
      "Minibatch step 520/11718: mean batch inertia: 0.32909546194710115, ewa inertia: 0.3291249905379379\n",
      "Minibatch step 521/11718: mean batch inertia: 0.33093556400317276, ewa inertia: 0.32915589073424295\n",
      "Minibatch step 522/11718: mean batch inertia: 0.33162165014631484, ewa inertia: 0.32919797267752615\n",
      "Minibatch step 523/11718: mean batch inertia: 0.3293631882969547, ewa inertia: 0.3292007923339339\n",
      "Minibatch step 524/11718: mean batch inertia: 0.32150663644889743, ewa inertia: 0.32906947983443346\n",
      "Minibatch step 525/11718: mean batch inertia: 0.32459080976607696, ewa inertia: 0.32899304450222794\n",
      "Minibatch step 526/11718: mean batch inertia: 0.32464480067423496, ewa inertia: 0.3289188350926419\n",
      "Minibatch step 527/11718: mean batch inertia: 0.3283768262496542, ewa inertia: 0.32890958488547334\n",
      "Minibatch step 528/11718: mean batch inertia: 0.3196204188478635, ewa inertia: 0.328751051106213\n",
      "Minibatch step 529/11718: mean batch inertia: 0.32400701056920495, ewa inertia: 0.32867008682241705\n",
      "Minibatch step 530/11718: mean batch inertia: 0.32981530926505664, ewa inertia: 0.32868963178923005\n",
      "Minibatch step 531/11718: mean batch inertia: 0.32103335936948957, ewa inertia: 0.32855896582881616\n",
      "Minibatch step 532/11718: mean batch inertia: 0.3330306349936815, ewa inertia: 0.3286352816799311\n",
      "Minibatch step 533/11718: mean batch inertia: 0.3315982421834652, ewa inertia: 0.3286858491177961\n",
      "Minibatch step 534/11718: mean batch inertia: 0.331786249887983, ewa inertia: 0.32873876218333176\n",
      "Minibatch step 535/11718: mean batch inertia: 0.3340182136230738, ewa inertia: 0.32882886407038764\n",
      "Minibatch step 536/11718: mean batch inertia: 0.32174845661016827, ewa inertia: 0.3287080261233828\n",
      "Minibatch step 537/11718: mean batch inertia: 0.3272674727581935, ewa inertia: 0.32868344088416057\n",
      "Minibatch step 538/11718: mean batch inertia: 0.32824982407566256, ewa inertia: 0.3286760405522983\n",
      "Minibatch step 539/11718: mean batch inertia: 0.33573394600018874, ewa inertia: 0.32879649446815973\n",
      "Minibatch step 540/11718: mean batch inertia: 0.3196791264715877, ewa inertia: 0.3286408926843664\n",
      "Minibatch step 541/11718: mean batch inertia: 0.33103986061516644, ewa inertia: 0.32868183472920165\n",
      "Converged (lack of improvement in inertia) at step 541/11718\n",
      "\n",
      "Clustering complete\n",
      "Valid clusters (size 2-10): 6,209\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "embeddings_normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Determine optimal number of clusters\n",
    "n_clusters = min(CONFIG[\"n_clusters\"], len(terms) // 2)\n",
    "print(f\"Clustering {len(terms):,} terms into {n_clusters:,} clusters...\")\n",
    "\n",
    "# Apply K-means\n",
    "kmeans = MiniBatchKMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    batch_size=1024,\n",
    "    n_init=3,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "cluster_labels = kmeans.fit_predict(embeddings_normalized)\n",
    "print(f\"\\nClustering complete\")\n",
    "\n",
    "# Group terms by cluster\n",
    "clusters = defaultdict(list)\n",
    "for i, label in enumerate(cluster_labels):\n",
    "    clusters[label].append((terms[i], i))\n",
    "\n",
    "# Filter clusters by size\n",
    "valid_clusters = {\n",
    "    label: terms_list \n",
    "    for label, terms_list in clusters.items()\n",
    "    if CONFIG[\"min_cluster_size\"] <= len(terms_list) <= CONFIG[\"max_cluster_size\"]\n",
    "}\n",
    "print(f\"Valid clusters (size {CONFIG['min_cluster_size']}-{CONFIG['max_cluster_size']}): {len(valid_clusters):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract Synonym Pairs from Clusters\n",
    "\n",
    "For each cluster, compute pairwise cosine similarity and extract high-quality synonym pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting synonym pairs (similarity >= 0.75)...\n",
      "Extracted 61,492 synonym pairs from clusters\n"
     ]
    }
   ],
   "source": [
    "def extract_synonym_pairs_from_clusters(\n",
    "    valid_clusters: Dict[int, List[Tuple[str, int]]],\n",
    "    embeddings_normalized: np.ndarray,\n",
    "    similarity_threshold: float = 0.7\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Extract synonym pairs from clusters based on cosine similarity.\"\"\"\n",
    "    synonym_pairs = []\n",
    "    \n",
    "    for cluster_id, terms_list in valid_clusters.items():\n",
    "        if len(terms_list) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Get embeddings for this cluster\n",
    "        cluster_terms = [t[0] for t in terms_list]\n",
    "        cluster_indices = [t[1] for t in terms_list]\n",
    "        cluster_embeddings = embeddings_normalized[cluster_indices]\n",
    "        \n",
    "        # Compute pairwise similarities\n",
    "        similarities = cosine_similarity(cluster_embeddings)\n",
    "        \n",
    "        # Extract pairs above threshold\n",
    "        for i in range(len(cluster_terms)):\n",
    "            for j in range(i + 1, len(cluster_terms)):\n",
    "                sim = similarities[i][j]\n",
    "                if sim >= similarity_threshold:\n",
    "                    # Bidirectional pairs\n",
    "                    synonym_pairs.append({\n",
    "                        \"source\": cluster_terms[i],\n",
    "                        \"target\": cluster_terms[j],\n",
    "                        \"similarity\": float(sim),\n",
    "                        \"relation\": \"synonym\",\n",
    "                        \"category\": \"cluster\"\n",
    "                    })\n",
    "                    synonym_pairs.append({\n",
    "                        \"source\": cluster_terms[j],\n",
    "                        \"target\": cluster_terms[i],\n",
    "                        \"similarity\": float(sim),\n",
    "                        \"relation\": \"synonym\",\n",
    "                        \"category\": \"cluster\"\n",
    "                    })\n",
    "    \n",
    "    return synonym_pairs\n",
    "\n",
    "print(f\"Extracting synonym pairs (similarity >= {CONFIG['similarity_threshold']})...\")\n",
    "cluster_synonym_pairs = extract_synonym_pairs_from_clusters(\n",
    "    valid_clusters, \n",
    "    embeddings_normalized,\n",
    "    CONFIG[\"similarity_threshold\"]\n",
    ")\n",
    "print(f\"Extracted {len(cluster_synonym_pairs):,} synonym pairs from clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Handle OOV Words with BPE\n",
    "\n",
    "For terms not in the tokenizer vocabulary, use BPE subword decomposition\n",
    "to map them to known vocabulary terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting BPE expansion pairs...\n",
      "OOV terms (multi-token): 108,668\n",
      "Extracted 73,278 BPE expansion pairs\n"
     ]
    }
   ],
   "source": [
    "def get_bpe_decomposition(term: str, tokenizer) -> List[str]:\n",
    "    \"\"\"Get BPE subword decomposition for a term.\"\"\"\n",
    "    tokens = tokenizer.tokenize(term)\n",
    "    # Clean subword markers\n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        clean_token = token.replace(\"##\", \"\").replace(\"â–\", \"\").strip()\n",
    "        if clean_token and len(clean_token) >= 2:\n",
    "            clean_tokens.append(clean_token)\n",
    "    return clean_tokens\n",
    "\n",
    "def create_bpe_expansion_pairs(\n",
    "    terms: List[str],\n",
    "    tokenizer,\n",
    "    embeddings_normalized: np.ndarray,\n",
    "    term_to_idx: Dict[str, int]\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Create expansion pairs for OOV terms using BPE decomposition.\"\"\"\n",
    "    bpe_pairs = []\n",
    "    oov_count = 0\n",
    "    \n",
    "    for term in terms:\n",
    "        # Check if term is in vocabulary as a single token\n",
    "        token_ids = tokenizer.encode(term, add_special_tokens=False)\n",
    "        \n",
    "        # If it's tokenized into multiple subwords\n",
    "        if len(token_ids) > 1:\n",
    "            oov_count += 1\n",
    "            subwords = get_bpe_decomposition(term, tokenizer)\n",
    "            \n",
    "            # Create pairs from term to each meaningful subword\n",
    "            for subword in subwords:\n",
    "                if subword in term_to_idx and subword != term:\n",
    "                    # Check semantic similarity\n",
    "                    term_idx = term_to_idx.get(term)\n",
    "                    subword_idx = term_to_idx.get(subword)\n",
    "                    \n",
    "                    if term_idx is not None and subword_idx is not None:\n",
    "                        sim = np.dot(\n",
    "                            embeddings_normalized[term_idx],\n",
    "                            embeddings_normalized[subword_idx]\n",
    "                        )\n",
    "                        if sim >= 0.5:  # Lower threshold for BPE pairs\n",
    "                            bpe_pairs.append({\n",
    "                                \"source\": term,\n",
    "                                \"target\": subword,\n",
    "                                \"similarity\": float(sim),\n",
    "                                \"relation\": \"bpe_expansion\",\n",
    "                                \"category\": \"BPE\"\n",
    "                            })\n",
    "    \n",
    "    print(f\"OOV terms (multi-token): {oov_count:,}\")\n",
    "    return bpe_pairs\n",
    "\n",
    "# Create term to index mapping\n",
    "term_to_idx = {term: i for i, term in enumerate(terms)}\n",
    "\n",
    "# Extract BPE expansion pairs\n",
    "print(\"Extracting BPE expansion pairs...\")\n",
    "bpe_pairs = create_bpe_expansion_pairs(\n",
    "    terms, tokenizer, embeddings_normalized, term_to_idx\n",
    ")\n",
    "print(f\"Extracted {len(bpe_pairs):,} BPE expansion pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Sample Clusters\n",
    "\n",
    "Inspect some example clusters to verify quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample clusters with synonym potential:\n",
      "\n",
      "Cluster 2715 (avg_sim=0.714):\n",
      "  Terms: ì´ì „, ì´ì „ì‹œëŒ€, ì´ì „ëª¨ë¸, ì´ì „ë‹¨ê³„, ì´ì „ëŒ€íšŒ, ì´ì „ì˜ë¬´, ì´ì „ê¸°ë¡, ì´ì „ì„¸ëŒ€, ì´ì „ìíšŒì‚¬, ì´ì „ìƒíƒœ\n",
      "\n",
      "Cluster 9375 (avg_sim=0.744):\n",
      "  Terms: ê¸°ì¬, ê¸°ì¬ë‚´ìš©, ê¸°ì¬ì‚¬í•­, ê¸°ì¬ë¶€ë¶„, ê¸°ì¬ìì²´, ê¸°ì¬ê±´ë¬¼, ê¸°ì¬ì‚¬ì‹¤, ê¸°ì¬ìš”ê±´, ê¸°ì¬ì£¼ê¶Œ, ê¸°ì¬ëˆ„ë½\n",
      "\n",
      "Cluster 11157 (avg_sim=0.746):\n",
      "  Terms: êµ¬ì„±, ì„±ë¶„, êµ¬ì„±ìš”ì†Œ, êµ¬ì„±ë¶€ë¶„, êµ¬ì„±ì„±ë¶„, êµ¬ì„±ë¬¼, êµ¬ì„±ë©´, Component, êµ¬ì„±ë¶€ë¶„ì „ì²´, component\n",
      "\n",
      "Cluster 188 (avg_sim=0.719):\n",
      "  Terms: ì„¤ëª…, í•´ì„¤, í•´ëª…, ì„¤ëª…ì„œ, í•´ì„¤ì„œ, description, Description, ì„¤ëª…ë¬¸, í•´ëª…ìë£Œ, documentation\n",
      "\n",
      "Cluster 1023 (avg_sim=0.710):\n",
      "  Terms: ìœ ì§€, ìœ ì§€ê´‘, ìœ ì§€íƒœ, ìœ ì§€ì›, ìœ ì§€ì¬ë‹¨, ìœ ì§€ë°©, ìœ ì§€ì¸, ìœ ì§€ì˜, ìœ ì§€ê²½ì˜, ìœ ì§€ì—°\n",
      "\n",
      "Cluster 9971 (avg_sim=0.645):\n",
      "  Terms: ë‹¤ì–‘, ë‹¤ì–‘ì²´, ì²œì°¨ë§Œë³„, ê°ì–‘ê°ìƒ‰, ê°€ì§€ê°ìƒ‰, ë‹¤ì–‘ì²´ìœ„, Various, í˜•í˜•ìƒ‰ìƒ‰, Variations, Diversity\n",
      "\n",
      "Cluster 4511 (avg_sim=0.747):\n",
      "  Terms: of, de, ì˜¤ë¸Œ, Of, von, des, De, Des, Von, Den\n",
      "\n",
      "Cluster 1601 (avg_sim=0.689):\n",
      "  Terms: êµìˆ˜, êµìˆ˜í˜•, êµìˆ˜ì§„, êµìˆ˜ì§, êµìˆ˜ë²•, Professor, Faculty, êµìˆ˜ìƒí™œ, êµìˆ˜ìê²©, ëŒ€í•™ê°•ì‚¬\n",
      "\n",
      "Cluster 3232 (avg_sim=0.713):\n",
      "  Terms: ë™ì‹œ, ì‹œì¢…ì¼ê´€, ë™ì‹œë‹¤ë°œ, ë™ì‹œë°œë§¤, ë™ì‹œì‹¬, ë™ì‹œì´í–‰í•­ë³€, ë™ì‹œë°©ì†¡, ë™ì‹œë²”, ë™ì‹œìˆ˜ìƒ, ë™ì‹œí†µì—­\n",
      "\n",
      "Cluster 2798 (avg_sim=0.742):\n",
      "  Terms: ì‹œë¦¬ì¦ˆ, Series, series, ì‹œë¦¬ì¦ˆë¬¼, ì‹œë¦¬ì¦ˆì „ì , Serial, ì‹œë¦¬ì¦ˆêµ¬ì„±, ì‹œë¦¬ì¦ˆëŒ€íšŒ, ì‹œë¦¬ì¦ˆì œì‘, ì‹œë¦¬ì¦ˆìš°ìŠ¹\n",
      "\n",
      "Cluster 4313 (avg_sim=0.686):\n",
      "  Terms: ì´ì‚¬, ì´ì‚¬ë²¨, ì´ì‚¬êµ­, ì´ì‚¬ì•¼, ì´ì‚¬ì§, ì´ì‚¬ê´€, ì´ì‚¬ì˜¤, ì´ì‚¬ë¶€, ì´ì‚¬ì „ì›, ì´ì‚¬ì² \n",
      "\n",
      "Cluster 981 (avg_sim=0.691):\n",
      "  Terms: ë³€í™”, ì „í™˜, ì „í™˜ì , ì „í™˜ê¸°, ë³€í™”êµ¬, Transformation, transition, ë³€í™”ê³¼ì •, Transition, transformation\n",
      "\n",
      "Cluster 2651 (avg_sim=0.778):\n",
      "  Terms: ì„¤ì •, ì„¤ì •ë“±ê¸°, ì„¤ì •ê³„ì•½, ì„¤ì •ê³„ì•½ì„œ, ì„¤ì •ì, ì„¤ì •ë“±ë¡, ì„¤ì •í–‰ìœ„, ì„¤ì •ë“±ê¸°ì ˆì°¨, ì„¤ì •ë“±ê¸°ì²­êµ¬, ì„¤ì •ê°’\n",
      "\n",
      "Cluster 4992 (avg_sim=0.673):\n",
      "  Terms: ê³µê°œ, ëŒ€ì¤‘, public, Public, í¼ë¸”ë¦­, ë¦¬í¼ë¸”ë¦­, í¼ë¸”ë¦¬ì‹œí‹°, í¼ë¸”ë¦­ë„ë©”ì¸, PUBLIC, publica\n",
      "\n",
      "Cluster 324 (avg_sim=0.830):\n",
      "  Terms: ì‹±ê¸€, Single, ì‹±ê¸€ì•¨ë²”, single, ì‹±ê¸€ê³¡, ì‹±ê¸€ìŒë°˜, ì‹±ê¸€ì¦ˆ, ì‹±ê¸€ë°œë§¤, ì‹±ê¸€ë²„ì „, Singles\n",
      "\n",
      "Cluster 4119 (avg_sim=0.714):\n",
      "  Terms: í•­ê³ , í•­ê³ ì¸, í•­ê³ ì†Œì†¡, í•­ê±°, í•­ê³ ì¥, í•­ê³ ë…¼ì§€, í•­ê³ ì‹¬, í•­ê³ ì‚¬ê±´, í•­ê³ ê¸°ê°„, í•­ê³ ì ˆì°¨\n",
      "\n",
      "Cluster 5583 (avg_sim=0.694):\n",
      "  Terms: íŠ¹ì§•, íŠ¹ì„±, ì¥ì , íŠ¹ì´ì , Feature, íŠ¹ì„±ë¥˜, íŠ¹ì§•ë†’ì´, features, íŠ¹ì§•ë³„, feature\n",
      "\n",
      "Cluster 1578 (avg_sim=0.772):\n",
      "  Terms: ì›íŒê²°, ì›íŒê²°íŒë‹¨, ì›íŒê²°ì„¤, ì›íŒê²°ê³¼ê¸°ë¡, ì›íŒê²°ì£¼ë¬¸, ì›íŒê²°ê²°ê³¼, ì›íŒê²°ì ì‹œ, ì›ì‹¬ì‚¬ì‹¤ì¸ì •, ì›íŒê²°ì¸ì •, ì›íŒê²°ì¸ì •ì‚¬ì‹¤\n",
      "\n",
      "Cluster 8912 (avg_sim=0.765):\n",
      "  Terms: ì¸ìš©, ì¸ìš©ìƒí‘œ, ì¸ìš©ë°œëª…, ì¸ìš©ë¬¸, ì¸ìš©ê³ ì•ˆ, ì¸ìš©ì¦ê±°, ì¸ìš©êµ¬, ì¸ìš©í‘œì¥, ì¸ìš©ì˜ì¥, ì¸ìš©ê²°ì •\n",
      "\n",
      "Cluster 3864 (avg_sim=0.730):\n",
      "  Terms: ë³€ì œ, ë³€ì œê¸°ì¼, ë³€ì œì¶©ë‹¹, ë³€ì œê¸°í•œ, ë³€ìƒì±…ì„, ë³€ì œì œê³µ, ë³€ì œì±…ì„, ë³€ì œì¦ì„œ, ë³€ì œí•­ë³€, ë³€ì œë°©ë²•\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display sample clusters\n",
    "print(\"Sample clusters with synonym potential:\\n\")\n",
    "\n",
    "sample_count = 0\n",
    "for cluster_id, terms_list in sorted(valid_clusters.items(), key=lambda x: -len(x[1])):\n",
    "    if sample_count >= 20:\n",
    "        break\n",
    "    \n",
    "    cluster_terms = [t[0] for t in terms_list]\n",
    "    cluster_indices = [t[1] for t in terms_list]\n",
    "    \n",
    "    # Compute average similarity within cluster\n",
    "    if len(cluster_terms) >= 2:\n",
    "        cluster_embeddings = embeddings_normalized[cluster_indices]\n",
    "        sims = cosine_similarity(cluster_embeddings)\n",
    "        avg_sim = (sims.sum() - len(cluster_terms)) / (len(cluster_terms) * (len(cluster_terms) - 1))\n",
    "        \n",
    "        if avg_sim >= 0.6:  # Only show high-quality clusters\n",
    "            print(f\"Cluster {cluster_id} (avg_sim={avg_sim:.3f}):\")\n",
    "            print(f\"  Terms: {', '.join(cluster_terms)}\")\n",
    "            print()\n",
    "            sample_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Merge and Save All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total pairs collected:\n",
      "  Cluster synonyms: 61,492\n",
      "  BPE expansions:   73,278\n",
      "  ------------------------------\n",
      "  Total:            134,770\n"
     ]
    }
   ],
   "source": [
    "# Merge all pairs\n",
    "all_pairs = cluster_synonym_pairs + bpe_pairs\n",
    "\n",
    "print(f\"\\nTotal pairs collected:\")\n",
    "print(f\"  Cluster synonyms: {len(cluster_synonym_pairs):,}\")\n",
    "print(f\"  BPE expansions:   {len(bpe_pairs):,}\")\n",
    "print(f\"  \" + \"-\" * 30)\n",
    "print(f\"  Total:            {len(all_pairs):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique pairs after deduplication: 131,166\n",
      "Saved to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.2_korean_legal_medical/korean_synonym_pairs.jsonl\n",
      "Total pairs: 131,166\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates and self-pairs\n",
    "seen = set()\n",
    "unique_pairs = []\n",
    "\n",
    "for pair in all_pairs:\n",
    "    # Skip self-pairs\n",
    "    if pair[\"source\"] == pair[\"target\"]:\n",
    "        continue\n",
    "    \n",
    "    key = (pair[\"source\"], pair[\"target\"])\n",
    "    if key not in seen:\n",
    "        seen.add(key)\n",
    "        unique_pairs.append(pair)\n",
    "\n",
    "print(f\"Unique pairs after deduplication: {len(unique_pairs):,}\")\n",
    "\n",
    "# Sort by similarity (highest first)\n",
    "unique_pairs.sort(key=lambda x: -x.get(\"similarity\", 0))\n",
    "\n",
    "# Save to JSONL\n",
    "output_path = OUTPUT_DIR / \"korean_synonym_pairs.jsonl\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for pair in unique_pairs:\n",
    "        f.write(json.dumps(pair, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved to: {output_path}\")\n",
    "print(f\"Total pairs: {len(unique_pairs):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "By Category:\n",
      "  BPE: 69,674\n",
      "  cluster: 61,492\n",
      "\n",
      "By Relation:\n",
      "  bpe_expansion: 69,674\n",
      "  synonym: 61,492\n",
      "\n",
      "Similarity statistics:\n",
      "  Min: 0.500\n",
      "  Max: 1.000\n",
      "  Mean: 0.757\n",
      "  Median: 0.776\n"
     ]
    }
   ],
   "source": [
    "# Statistics by category and relation\n",
    "categories = Counter(p[\"category\"] for p in unique_pairs)\n",
    "relations = Counter(p[\"relation\"] for p in unique_pairs)\n",
    "\n",
    "print(\"\\nBy Category:\")\n",
    "for cat, count in categories.most_common():\n",
    "    print(f\"  {cat}: {count:,}\")\n",
    "\n",
    "print(\"\\nBy Relation:\")\n",
    "for rel, count in relations.most_common():\n",
    "    print(f\"  {rel}: {count:,}\")\n",
    "\n",
    "# Similarity distribution\n",
    "if unique_pairs:\n",
    "    similarities = [p.get(\"similarity\", 0) for p in unique_pairs]\n",
    "    print(f\"\\nSimilarity statistics:\")\n",
    "    print(f\"  Min: {min(similarities):.3f}\")\n",
    "    print(f\"  Max: {max(similarities):.3f}\")\n",
    "    print(f\"  Mean: {np.mean(similarities):.3f}\")\n",
    "    print(f\"  Median: {np.median(similarities):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 30 synonym pairs (highest similarity):\n",
      "  ê³—ë‚  -> ì˜œë‚  (sim=1.000, synonym)\n",
      "  ì˜œë‚  -> ê³—ë‚  (sim=1.000, synonym)\n",
      "  í—ë¦¬ìš°ë“œì˜í™” -> í—ë¦¬ì›ƒì˜í™” (sim=0.991, synonym)\n",
      "  í—ë¦¬ì›ƒì˜í™” -> í—ë¦¬ìš°ë“œì˜í™” (sim=0.991, synonym)\n",
      "  í•˜ë²„ë“œëŒ€í•™êµ -> í•˜ë²„ë“œëŒ€í•™ (sim=0.991, synonym)\n",
      "  í•˜ë²„ë“œëŒ€í•™ -> í•˜ë²„ë“œëŒ€í•™êµ (sim=0.991, synonym)\n",
      "  í€„ë¦¬í‹° -> í€„ëŸ¬í‹° (sim=0.988, synonym)\n",
      "  í€„ëŸ¬í‹° -> í€„ë¦¬í‹° (sim=0.988, synonym)\n",
      "  êµ­ê°€ì‚¬íšŒì£¼ì˜ë…ì¼ ë…¸ë™ìë‹¹ -> êµ­ê°€ì‚¬íšŒì£¼ì˜ë…ì¼ë…¸ë™ìë‹¹ (sim=0.988, synonym)\n",
      "  êµ­ê°€ì‚¬íšŒì£¼ì˜ë…ì¼ë…¸ë™ìë‹¹ -> êµ­ê°€ì‚¬íšŒì£¼ì˜ë…ì¼ ë…¸ë™ìë‹¹ (sim=0.988, synonym)\n",
      "  ì»¨íŠ¸ë¡¤ -> ì½˜íŠ¸ë¡¤ (sim=0.988, synonym)\n",
      "  ì½˜íŠ¸ë¡¤ -> ì»¨íŠ¸ë¡¤ (sim=0.988, synonym)\n",
      "  ì•…ì„¸ì‚¬ë¦¬ -> ì•…ì„¸ì„œë¦¬ (sim=0.988, synonym)\n",
      "  ì•…ì„¸ì„œë¦¬ -> ì•…ì„¸ì‚¬ë¦¬ (sim=0.988, synonym)\n",
      "  ì¸í„°ë ‰ì…˜ ë””ìì¸ -> ì¸í„°ë™ì…˜ ë””ìì¸ (sim=0.988, synonym)\n",
      "  ì¸í„°ë™ì…˜ ë””ìì¸ -> ì¸í„°ë ‰ì…˜ ë””ìì¸ (sim=0.988, synonym)\n",
      "  ì–¼ë§ˆê°„ -> ì–¼ë§ˆë™ì•ˆ (sim=0.988, synonym)\n",
      "  ì–¼ë§ˆë™ì•ˆ -> ì–¼ë§ˆê°„ (sim=0.988, synonym)\n",
      "  ì—˜ë¦¬ë² ì´í„° -> ì—˜ë ˆë² ì´í„° (sim=0.987, synonym)\n",
      "  ì—˜ë ˆë² ì´í„° -> ì—˜ë¦¬ë² ì´í„° (sim=0.987, synonym)\n",
      "  ê³ ì†ë„ë¡œë‚˜ë“¤ëª©ì¶©ì²­ë‚¨ë„ -> ê³ ì†ë„ë¡œë‚˜ë“¤ëª©ì¶©ì²­ë¶ë„ (sim=0.987, synonym)\n",
      "  ê³ ì†ë„ë¡œë‚˜ë“¤ëª©ì¶©ì²­ë¶ë„ -> ê³ ì†ë„ë¡œë‚˜ë“¤ëª©ì¶©ì²­ë‚¨ë„ (sim=0.987, synonym)\n",
      "  ì„ì°¨ë³´ì¦ê¸ˆë°˜í™˜ì±„ê¶Œ -> ì„ëŒ€ì°¨ë³´ì¦ê¸ˆë°˜í™˜ì±„ê¶Œ (sim=0.987, synonym)\n",
      "  ì„ëŒ€ì°¨ë³´ì¦ê¸ˆë°˜í™˜ì±„ê¶Œ -> ì„ì°¨ë³´ì¦ê¸ˆë°˜í™˜ì±„ê¶Œ (sim=0.987, synonym)\n",
      "  í”„ë¦°ìŠ¤í„´ëŒ€í•™êµ -> í”„ë¦°ìŠ¤í„´ëŒ€í•™ (sim=0.986, synonym)\n",
      "  í”„ë¦°ìŠ¤í„´ëŒ€í•™ -> í”„ë¦°ìŠ¤í„´ëŒ€í•™êµ (sim=0.986, synonym)\n",
      "  ë¡œë§¨í‹±ì½”ë¯¸ë”” -> ë¡œë§¨í‹±ì½”ë©”ë”” (sim=0.986, synonym)\n",
      "  ë¡œë§¨í‹±ì½”ë©”ë”” -> ë¡œë§¨í‹±ì½”ë¯¸ë”” (sim=0.986, synonym)\n",
      "  U.S. -> U.S (sim=0.986, synonym)\n",
      "  U.S -> U.S. (sim=0.986, synonym)\n"
     ]
    }
   ],
   "source": [
    "# Sample high-quality synonym pairs\n",
    "print(\"\\nTop 30 synonym pairs (highest similarity):\")\n",
    "for p in unique_pairs[:30]:\n",
    "    print(f\"  {p['source']} -> {p['target']} (sim={p.get('similarity', 0):.3f}, {p['relation']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create SPLADE Training Dataset with Hard Negatives\n",
    "\n",
    "Generate triplet dataset (anchor, positive, negative) using Hard Negative Mining.\n",
    "\n",
    "**Hard Negative**: ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•˜ì§€ë§Œ ì •ë‹µì´ ì•„ë‹Œ ë¬¸ì„œ\n",
    "- BGE-M3 ì„ë² ë”©ìœ¼ë¡œ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "- Top-K ìœ ì‚¬ ìš©ì–´ ì¤‘ ë™ì˜ì–´ê°€ ì•„ë‹Œ ê²ƒì„ Hard Negativeë¡œ ì„ íƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating synonym lookup table...\n",
      "Lookup table size: 79,011 terms\n",
      "\n",
      "Generating triplet dataset with hard negatives...\n",
      "Processing pair 0/131,166...\n",
      "Processing pair 10,000/131,166...\n",
      "Processing pair 20,000/131,166...\n",
      "Processing pair 30,000/131,166...\n",
      "Processing pair 40,000/131,166...\n",
      "Processing pair 50,000/131,166...\n",
      "Processing pair 60,000/131,166...\n",
      "Processing pair 70,000/131,166...\n",
      "Processing pair 80,000/131,166...\n",
      "Processing pair 90,000/131,166...\n",
      "Processing pair 100,000/131,166...\n",
      "Processing pair 110,000/131,166...\n",
      "Processing pair 120,000/131,166...\n",
      "Processing pair 130,000/131,166...\n",
      "\n",
      "Generated 131,166 triplets\n",
      "Dataset features: {'anchor': Value(dtype='string', id=None), 'positive': Value(dtype='string', id=None), 'negative': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "def create_synonym_lookup(unique_pairs: List[Dict]) -> Dict[str, Set[str]]:\n",
    "    \"\"\"Create a lookup table: source -> set of synonyms.\"\"\"\n",
    "    synonym_lookup = defaultdict(set)\n",
    "    for pair in unique_pairs:\n",
    "        synonym_lookup[pair[\"source\"]].add(pair[\"target\"])\n",
    "        synonym_lookup[pair[\"target\"]].add(pair[\"source\"])\n",
    "    return synonym_lookup\n",
    "\n",
    "def find_hard_negatives(\n",
    "    term: str,\n",
    "    term_to_idx: Dict[str, int],\n",
    "    embeddings_normalized: np.ndarray,\n",
    "    synonym_lookup: Dict[str, Set[str]],\n",
    "    terms: List[str],\n",
    "    top_k: int = 50,\n",
    "    n_negatives: int = 3\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Find hard negatives for a term using embedding similarity.\n",
    "    \n",
    "    Hard Negative: Similar in embedding space but NOT a synonym.\n",
    "    \"\"\"\n",
    "    if term not in term_to_idx:\n",
    "        return []\n",
    "    \n",
    "    term_idx = term_to_idx[term]\n",
    "    term_embedding = embeddings_normalized[term_idx]\n",
    "    \n",
    "    # Compute similarities with all terms\n",
    "    similarities = np.dot(embeddings_normalized, term_embedding)\n",
    "    \n",
    "    # Get top-k similar terms (excluding self)\n",
    "    top_indices = np.argsort(similarities)[::-1][1:top_k+1]\n",
    "    \n",
    "    # Filter: similar but NOT synonym\n",
    "    synonyms = synonym_lookup.get(term, set())\n",
    "    hard_negatives = []\n",
    "    \n",
    "    for idx in top_indices:\n",
    "        candidate = terms[idx]\n",
    "        # Hard negative: similar (top-k) but not a synonym\n",
    "        if candidate not in synonyms and candidate != term:\n",
    "            hard_negatives.append(candidate)\n",
    "            if len(hard_negatives) >= n_negatives:\n",
    "                break\n",
    "    \n",
    "    return hard_negatives\n",
    "\n",
    "# Create synonym lookup\n",
    "print(\"Creating synonym lookup table...\")\n",
    "synonym_lookup = create_synonym_lookup(unique_pairs)\n",
    "print(f\"Lookup table size: {len(synonym_lookup):,} terms\")\n",
    "\n",
    "# Generate triplet dataset\n",
    "print(\"\\nGenerating triplet dataset with hard negatives...\")\n",
    "triplets = []\n",
    "\n",
    "for i, pair in enumerate(unique_pairs):\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"Processing pair {i:,}/{len(unique_pairs):,}...\")\n",
    "    \n",
    "    anchor = pair[\"source\"]\n",
    "    positive = pair[\"target\"]\n",
    "    \n",
    "    # Find hard negatives\n",
    "    hard_negs = find_hard_negatives(\n",
    "        anchor, term_to_idx, embeddings_normalized, \n",
    "        synonym_lookup, terms, top_k=50, n_negatives=1\n",
    "    )\n",
    "    \n",
    "    if hard_negs:\n",
    "        triplets.append({\n",
    "            \"anchor\": anchor,\n",
    "            \"positive\": positive,\n",
    "            \"negative\": hard_negs[0]  # Use first hard negative\n",
    "        })\n",
    "\n",
    "print(f\"\\nGenerated {len(triplets):,} triplets\")\n",
    "\n",
    "# Create HuggingFace Dataset\n",
    "triplet_dataset = Dataset.from_dict({\n",
    "    \"anchor\": [t[\"anchor\"] for t in triplets],\n",
    "    \"positive\": [t[\"positive\"] for t in triplets],\n",
    "    \"negative\": [t[\"negative\"] for t in triplets],\n",
    "})\n",
    "print(f\"Dataset features: {triplet_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa7f964486b426480bf194bd86a2cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/131166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved triplet dataset to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.2_korean_legal_medical/splade_triplet_dataset\n",
      "Saved JSONL to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.2_korean_legal_medical/splade_triplet_dataset.jsonl\n",
      "\n",
      "================================================================================\n",
      "Sample Triplets (Anchor -> Positive vs Negative)\n",
      "================================================================================\n",
      "  Anchor:   ê³—ë‚                   \n",
      "  Positive: ì˜œë‚                    (synonym)\n",
      "  Negative: ì‚¼ì§‡ë‚                   (hard negative)\n",
      "\n",
      "  Anchor:   ì˜œë‚                   \n",
      "  Positive: ê³—ë‚                    (synonym)\n",
      "  Negative: ì‚¼ì§‡ë‚                   (hard negative)\n",
      "\n",
      "  Anchor:   í—ë¦¬ìš°ë“œì˜í™”              \n",
      "  Positive: í—ë¦¬ì›ƒì˜í™”                (synonym)\n",
      "  Negative: í…”ë ˆë¹„ì „ì˜í™”               (hard negative)\n",
      "\n",
      "  Anchor:   í—ë¦¬ì›ƒì˜í™”               \n",
      "  Positive: í—ë¦¬ìš°ë“œì˜í™”               (synonym)\n",
      "  Negative: í…”ë ˆë¹„ì „ì˜í™”               (hard negative)\n",
      "\n",
      "  Anchor:   í•˜ë²„ë“œëŒ€í•™êµ              \n",
      "  Positive: í•˜ë²„ë“œëŒ€í•™                (synonym)\n",
      "  Negative: í•˜ì›Œë“œëŒ€í•™êµ               (hard negative)\n",
      "\n",
      "  Anchor:   í•˜ë²„ë“œëŒ€í•™               \n",
      "  Positive: í•˜ë²„ë“œëŒ€í•™êµ               (synonym)\n",
      "  Negative: í•˜ì´ë¸ë² ë¥´í¬ëŒ€í•™             (hard negative)\n",
      "\n",
      "  Anchor:   í€„ë¦¬í‹°                 \n",
      "  Positive: í€„ëŸ¬í‹°                  (synonym)\n",
      "  Negative: ì„œë¹„ìŠ¤í’ˆì§ˆ                (hard negative)\n",
      "\n",
      "  Anchor:   í€„ëŸ¬í‹°                 \n",
      "  Positive: í€„ë¦¬í‹°                  (synonym)\n",
      "  Negative: ì„œë¹„ìŠ¤í’ˆì§ˆ                (hard negative)\n",
      "\n",
      "  Anchor:   êµ­ê°€ì‚¬íšŒì£¼ì˜ë…ì¼ ë…¸ë™ìë‹¹       \n",
      "  Positive: êµ­ê°€ì‚¬íšŒì£¼ì˜ë…ì¼ë…¸ë™ìë‹¹         (synonym)\n",
      "  Negative: ë…ì¼ê³µì‚°ë‹¹                (hard negative)\n",
      "\n",
      "  Anchor:   êµ­ê°€ì‚¬íšŒì£¼ì˜ë…ì¼ë…¸ë™ìë‹¹        \n",
      "  Positive: êµ­ê°€ì‚¬íšŒì£¼ì˜ë…ì¼ ë…¸ë™ìë‹¹        (synonym)\n",
      "  Negative: ì‚¬íšŒì£¼ì˜ë…¸ë™ìë‹¹             (hard negative)\n",
      "\n",
      "  Anchor:   ì»¨íŠ¸ë¡¤                 \n",
      "  Positive: ì½˜íŠ¸ë¡¤                  (synonym)\n",
      "  Negative: ì»¨íŠ¸ë¡¤ëŸ¬                 (hard negative)\n",
      "\n",
      "  Anchor:   ì½˜íŠ¸ë¡¤                 \n",
      "  Positive: ì»¨íŠ¸ë¡¤                  (synonym)\n",
      "  Negative: ì»¨íŠ¸ë¡¤ëŸ¬                 (hard negative)\n",
      "\n",
      "  Anchor:   ì•…ì„¸ì‚¬ë¦¬                \n",
      "  Positive: ì•…ì„¸ì„œë¦¬                 (synonym)\n",
      "  Negative: ë³´ì„                   (hard negative)\n",
      "\n",
      "  Anchor:   ì•…ì„¸ì„œë¦¬                \n",
      "  Positive: ì•…ì„¸ì‚¬ë¦¬                 (synonym)\n",
      "  Negative: ê·€ê±¸ì´                  (hard negative)\n",
      "\n",
      "  Anchor:   ì¸í„°ë ‰ì…˜ ë””ìì¸            \n",
      "  Positive: ì¸í„°ë™ì…˜ ë””ìì¸             (synonym)\n",
      "  Negative: ì¸í„°ë™ì…˜                 (hard negative)\n",
      "\n",
      "  Anchor:   ì¸í„°ë™ì…˜ ë””ìì¸            \n",
      "  Positive: ì¸í„°ë ‰ì…˜ ë””ìì¸             (synonym)\n",
      "  Negative: ì¸í„°ë™ì…˜                 (hard negative)\n",
      "\n",
      "  Anchor:   ì–¼ë§ˆê°„                 \n",
      "  Positive: ì–¼ë§ˆë™ì•ˆ                 (synonym)\n",
      "  Negative: ìˆ˜ë…„ë™ì•ˆ                 (hard negative)\n",
      "\n",
      "  Anchor:   ì–¼ë§ˆë™ì•ˆ                \n",
      "  Positive: ì–¼ë§ˆê°„                  (synonym)\n",
      "  Negative: ìˆ˜ë…„ë™ì•ˆ                 (hard negative)\n",
      "\n",
      "  Anchor:   ì—˜ë¦¬ë² ì´í„°               \n",
      "  Positive: ì—˜ë ˆë² ì´í„°                (synonym)\n",
      "  Negative: í˜„ëŒ€ì—˜ë¦¬ë² ì´í„°              (hard negative)\n",
      "\n",
      "  Anchor:   ì—˜ë ˆë² ì´í„°               \n",
      "  Positive: ì—˜ë¦¬ë² ì´í„°                (synonym)\n",
      "  Negative: í˜„ëŒ€ì—˜ë¦¬ë² ì´í„°              (hard negative)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save triplet dataset\n",
    "triplet_dataset_path = OUTPUT_DIR / \"splade_triplet_dataset\"\n",
    "triplet_dataset.save_to_disk(str(triplet_dataset_path))\n",
    "print(f\"Saved triplet dataset to: {triplet_dataset_path}\")\n",
    "\n",
    "# Also save as JSONL\n",
    "triplet_jsonl_path = OUTPUT_DIR / \"splade_triplet_dataset.jsonl\"\n",
    "with open(triplet_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for t in triplets:\n",
    "        f.write(json.dumps(t, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved JSONL to: {triplet_jsonl_path}\")\n",
    "\n",
    "# Display sample triplets\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Sample Triplets (Anchor -> Positive vs Negative)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for t in triplets[:20]:\n",
    "    print(f\"  Anchor:   {t['anchor']:20}\")\n",
    "    print(f\"  Positive: {t['positive']:20} (synonym)\")\n",
    "    print(f\"  Negative: {t['negative']:20} (hard negative)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Split Dataset into Train/Test (7:3)\n",
    "\n",
    "Split the triplet dataset into training (70%) and test (30%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset into train (70%) and test (30%)...\n",
      "\n",
      "Dataset split:\n",
      "  Train: 91,816 samples (70%)\n",
      "  Test:  39,350 samples (30%)\n",
      "  Total: 131,166 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2deee2cb8342b9943d9371fe09110c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/91816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f933cf366084b43b4c411a6365703ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/39350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved train dataset to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.2_korean_legal_medical/train_dataset\n",
      "Saved test dataset to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.2_korean_legal_medical/test_dataset\n",
      "Saved train JSONL to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.2_korean_legal_medical/train_triplets.jsonl\n",
      "Saved test JSONL to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.2_korean_legal_medical/test_triplets.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Split into train (70%) and test (30%)\n",
    "print(\"Splitting dataset into train (70%) and test (30%)...\")\n",
    "dataset_split = triplet_dataset.train_test_split(test_size=0.3, seed=42)\n",
    "\n",
    "train_dataset = dataset_split[\"train\"]\n",
    "test_dataset = dataset_split[\"test\"]\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"  Train: {len(train_dataset):,} samples (70%)\")\n",
    "print(f\"  Test:  {len(test_dataset):,} samples (30%)\")\n",
    "print(f\"  Total: {len(train_dataset) + len(test_dataset):,} samples\")\n",
    "\n",
    "# Save train and test datasets separately\n",
    "train_path = OUTPUT_DIR / \"train_dataset\"\n",
    "test_path = OUTPUT_DIR / \"test_dataset\"\n",
    "\n",
    "train_dataset.save_to_disk(str(train_path))\n",
    "test_dataset.save_to_disk(str(test_path))\n",
    "\n",
    "print(f\"\\nSaved train dataset to: {train_path}\")\n",
    "print(f\"Saved test dataset to: {test_path}\")\n",
    "\n",
    "# Also save as JSONL for easy inspection\n",
    "train_jsonl_path = OUTPUT_DIR / \"train_triplets.jsonl\"\n",
    "test_jsonl_path = OUTPUT_DIR / \"test_triplets.jsonl\"\n",
    "\n",
    "with open(train_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in train_dataset:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(test_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in test_dataset:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved train JSONL to: {train_jsonl_path}\")\n",
    "print(f\"Saved test JSONL to: {test_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "ML-based data collection with diverse domains complete.\n",
    "\n",
    "### Data Pipeline\n",
    "1. **Data Sources**: ë‹¤ì–‘í•œ ë„ë©”ì¸ì˜ í•œêµ­ì–´ ë°ì´í„°ì…‹\n",
    "2. **Term Extraction**: Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° (ê³ ìœ ëª…ì‚¬/ë³µí•©ëª…ì‚¬)\n",
    "3. **Embeddings**: BGE-M3 (Teacher model)\n",
    "4. **Clustering**: K-means â†’ ë™ì˜ì–´ ì¶”ì¶œ\n",
    "5. **Hard Negative Mining**: BGE-M3 ìœ ì‚¬ë„ ê¸°ë°˜\n",
    "6. **Train/Test Split**: 7:3 ë¹„ìœ¨ (seed=42)\n",
    "\n",
    "### Data Sources (Diverse Domains)\n",
    "\n",
    "| Domain | Dataset | Content |\n",
    "|--------|---------|---------|\n",
    "| ë°±ê³¼ì‚¬ì „ | Wikipedia | ì¼ë°˜ ì§€ì‹, ì—­ì‚¬, ê³¼í•™, ë¬¸í™” (100K) |\n",
    "| ë‰´ìŠ¤/QA | KLUE-MRC | ë‰´ìŠ¤ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ |\n",
    "| QA | KorQuAD | í•œêµ­ì–´ ì§ˆì˜ì‘ë‹µ |\n",
    "| ë¦¬ë·° | NSMC | ì˜í™” ë¦¬ë·° (150K) |\n",
    "| ëŒ€í™” | KorHate | ì˜¨ë¼ì¸ ëŒ€í™” |\n",
    "| ë‰´ìŠ¤ | KLUE-YNAT | ë‰´ìŠ¤ ì œëª© ë¶„ë¥˜ |\n",
    "| ìœ ì‚¬ë„ | KLUE-STS | ë¬¸ì¥ ìœ ì‚¬ë„ |\n",
    "| NLI | KLUE-NLI | ìì—°ì–´ ì¶”ë¡  |\n",
    "| ì§€ì‹œ | KoAlpaca | ë‹¤ì–‘í•œ ì§€ì‹œë¬¸ (50K) |\n",
    "| QA | KorQuAD-Chat | ëŒ€í™”í˜• QA (30K) |\n",
    "\n",
    "### Output Files\n",
    "\n",
    "| File | Format | Description |\n",
    "|------|--------|-------------|\n",
    "| `korean_synonym_pairs.jsonl` | `{source, target, similarity}` | Raw pairs |\n",
    "| `splade_triplet_dataset/` | `{anchor, positive, negative}` | Full triplet dataset |\n",
    "| `train_dataset/` | HuggingFace Dataset | **Train set (70%)** |\n",
    "| `test_dataset/` | HuggingFace Dataset | **Test set (30%)** |\n",
    "| `train_triplets.jsonl` | JSONL | Train backup |\n",
    "| `test_triplets.jsonl` | JSONL | Test backup |\n",
    "\n",
    "### Dataset Format\n",
    "\n",
    "```python\n",
    "# Triplet format for SparseTripletLoss\n",
    "{\n",
    "    \"anchor\": \"ì¶”ì²œ\",              # Query\n",
    "    \"positive\": \"ê¶Œì¥\",            # Synonym (ì •ë‹µ)\n",
    "    \"negative\": \"ì œì•ˆ\"             # Hard negative (ìœ ì‚¬í•˜ì§€ë§Œ ì˜¤ë‹µ)\n",
    "}\n",
    "```\n",
    "\n",
    "### Usage with Sentence Transformers v5\n",
    "\n",
    "```python\n",
    "from datasets import load_from_disk\n",
    "from sentence_transformers.sparse_encoder import SparseEncoder\n",
    "from sentence_transformers.sparse_encoder.losses import SpladeLoss, SparseTripletLoss\n",
    "\n",
    "# Load train/test datasets\n",
    "train_dataset = load_from_disk(\"dataset/v21.1_korean_general/train_dataset\")\n",
    "test_dataset = load_from_disk(\"dataset/v21.1_korean_general/test_dataset\")\n",
    "\n",
    "# SparseTripletLoss with hard negatives\n",
    "loss = SpladeLoss(\n",
    "    model=model, \n",
    "    loss=SparseTripletLoss(model=model)\n",
    ")\n",
    "```\n",
    "\n",
    "Next step: Run `02_training.ipynb` with train/test datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

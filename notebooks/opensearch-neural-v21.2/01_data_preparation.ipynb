{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v21.1 Data Preparation - Quality Verification (Optional)\n",
    "\n",
    "This notebook is **optional** and used only for data quality verification.\n",
    "\n",
    "## Main Data Flow\n",
    "\n",
    "The main data processing flow is now integrated in `00_data_ingestion.ipynb`:\n",
    "1. Load Korean data from HuggingFace (Wikipedia, hate speech)\n",
    "2. Extract terms using Kiwi morphological analyzer\n",
    "3. Compute BGE-M3 embeddings\n",
    "4. K-means clustering for synonym extraction\n",
    "5. Hard negative mining for triplet dataset\n",
    "6. **Train/Test split (7:3)**\n",
    "\n",
    "## This Notebook's Purpose\n",
    "\n",
    "- Verify data quality of the triplet dataset\n",
    "- Analyze similarity distributions\n",
    "- Optional: Re-process with different similarity thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train\n",
      "PyTorch version: 2.10.0.dev20251109+cu130\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.1_korean_enhanced/korean_synonym_pairs.jsonl\n",
      "Output: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.1_korean_enhanced/term_mappings.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "INPUT_DIR = PROJECT_ROOT / \"dataset\" / \"v21.1_korean_enhanced\"\n",
    "OUTPUT_DIR = INPUT_DIR\n",
    "\n",
    "INPUT_FILE = INPUT_DIR / \"korean_synonym_pairs.jsonl\"\n",
    "OUTPUT_FILE = OUTPUT_DIR / \"term_mappings.jsonl\"\n",
    "\n",
    "print(f\"Input: {INPUT_FILE}\")\n",
    "print(f\"Output: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Korean Synonym Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50,534 pairs\n",
      "\n",
      "Sample pairs:\n",
      "  Oh -> oh (synonym)\n",
      "  oh -> Oh (synonym)\n",
      "  로스앤젤레스 -> 로스엔젤레스 (synonym)\n",
      "  로스엔젤레스 -> 로스앤젤레스 (synonym)\n",
      "  하버드대학교 -> 하버드대학 (synonym)\n"
     ]
    }
   ],
   "source": [
    "# Load pairs\n",
    "pairs = []\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        pairs.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(pairs):,} pairs\")\n",
    "\n",
    "# Sample\n",
    "print(\"\\nSample pairs:\")\n",
    "for p in pairs[:5]:\n",
    "    print(f\"  {p['source']} -> {p['target']} ({p['relation']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Sentence Encoder for Similarity Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading encoder: BAAI/bge-m3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/west/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  queued_call()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Use a Korean-optimized sentence encoder\n",
    "# Options: \n",
    "# - \"jhgan/ko-sroberta-multitask\" (Korean-specific)\n",
    "# - \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" (multilingual)\n",
    "# - \"BAAI/bge-m3\" (multilingual, high quality)\n",
    "\n",
    "ENCODER_MODEL = \"BAAI/bge-m3\"\n",
    "\n",
    "print(f\"Loading encoder: {ENCODER_MODEL}\")\n",
    "encoder = SentenceTransformer(ENCODER_MODEL)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = encoder.to(device)\n",
    "print(f\"Encoder loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute Similarity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding sources...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3deb001124c242f89514954524d5e166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding targets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ea554317b84f2e939334a89c79caa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing similarities...\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity_batch(\n",
    "    pairs: List[Dict],\n",
    "    encoder: SentenceTransformer,\n",
    "    batch_size: int = 64\n",
    ") -> List[float]:\n",
    "    \"\"\"Compute cosine similarity for pairs in batches.\"\"\"\n",
    "    sources = [p[\"source\"] for p in pairs]\n",
    "    targets = [p[\"target\"] for p in pairs]\n",
    "    \n",
    "    # Encode in batches\n",
    "    print(\"Encoding sources...\")\n",
    "    source_embeddings = encoder.encode(\n",
    "        sources, \n",
    "        batch_size=batch_size, \n",
    "        show_progress_bar=True,\n",
    "        convert_to_tensor=True\n",
    "    )\n",
    "    \n",
    "    print(\"Encoding targets...\")\n",
    "    target_embeddings = encoder.encode(\n",
    "        targets,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_tensor=True\n",
    "    )\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    print(\"Computing similarities...\")\n",
    "    similarities = torch.nn.functional.cosine_similarity(\n",
    "        source_embeddings, target_embeddings\n",
    "    )\n",
    "    \n",
    "    return similarities.cpu().tolist()\n",
    "\n",
    "# Compute similarities\n",
    "similarities = compute_similarity_batch(pairs, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity Statistics:\n",
      "  Min: 0.4992\n",
      "  Max: 0.9917\n",
      "  Mean: 0.7509\n",
      "  Median: 0.7575\n",
      "  Std: 0.0937\n"
     ]
    }
   ],
   "source": [
    "# Add similarity scores to pairs\n",
    "for pair, sim in zip(pairs, similarities):\n",
    "    pair[\"similarity\"] = sim\n",
    "\n",
    "# Statistics\n",
    "import numpy as np\n",
    "\n",
    "sim_array = np.array(similarities)\n",
    "print(f\"\\nSimilarity Statistics:\")\n",
    "print(f\"  Min: {sim_array.min():.4f}\")\n",
    "print(f\"  Max: {sim_array.max():.4f}\")\n",
    "print(f\"  Mean: {sim_array.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(sim_array):.4f}\")\n",
    "print(f\"  Std: {sim_array.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity by Relation Type:\n",
      "  bpe_expansion   | mean=0.691 | std=0.102 | n=19980\n",
      "  synonym         | mean=0.790 | std=0.062 | n=30554\n"
     ]
    }
   ],
   "source": [
    "# Distribution by relation type\n",
    "from collections import defaultdict\n",
    "\n",
    "relation_sims = defaultdict(list)\n",
    "for pair in pairs:\n",
    "    relation_sims[pair[\"relation\"]].append(pair[\"similarity\"])\n",
    "\n",
    "print(\"\\nSimilarity by Relation Type:\")\n",
    "for relation, sims in sorted(relation_sims.items()):\n",
    "    arr = np.array(sims)\n",
    "    print(f\"  {relation:15} | mean={arr.mean():.3f} | std={arr.std():.3f} | n={len(sims)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filter by Quality Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original pairs: 50,534\n",
      "After filtering: 50,534\n",
      "Removed: 0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Different thresholds for different relation types\n",
    "THRESHOLDS = {\n",
    "    \"synonym\": 0.3,      # Lower threshold for synonyms (they should be similar)\n",
    "    \"redirect\": 0.3,     # Wiki redirects\n",
    "    \"alias\": 0.3,        # Wikidata aliases\n",
    "    \"morphological\": 0.2, # Morphological variations (can be quite different)\n",
    "    \"hypernym\": 0.2,     # Hypernym relations\n",
    "    \"hyponym\": 0.2,      # Hyponym relations\n",
    "    \"sibling\": 0.2,      # Sibling terms\n",
    "}\n",
    "\n",
    "DEFAULT_THRESHOLD = 0.3\n",
    "\n",
    "filtered_pairs = []\n",
    "for pair in pairs:\n",
    "    threshold = THRESHOLDS.get(pair[\"relation\"], DEFAULT_THRESHOLD)\n",
    "    if pair[\"similarity\"] >= threshold:\n",
    "        filtered_pairs.append(pair)\n",
    "\n",
    "print(f\"Original pairs: {len(pairs):,}\")\n",
    "print(f\"After filtering: {len(filtered_pairs):,}\")\n",
    "print(f\"Removed: {len(pairs) - len(filtered_pairs):,} ({(len(pairs) - len(filtered_pairs)) / len(pairs) * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered pairs by relation:\n",
      "  bpe_expansion: 19,980\n",
      "  synonym: 30,554\n"
     ]
    }
   ],
   "source": [
    "# Show filtered distribution\n",
    "filtered_relation_counts = defaultdict(int)\n",
    "for pair in filtered_pairs:\n",
    "    filtered_relation_counts[pair[\"relation\"]] += 1\n",
    "\n",
    "print(\"\\nFiltered pairs by relation:\")\n",
    "for relation, count in sorted(filtered_relation_counts.items()):\n",
    "    print(f\"  {relation}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Group by Source Term (1:N Mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique source terms: 24,954\n"
     ]
    }
   ],
   "source": [
    "# Group pairs by source term\n",
    "term_mappings = defaultdict(list)\n",
    "\n",
    "for pair in filtered_pairs:\n",
    "    source = pair[\"source\"]\n",
    "    target_info = {\n",
    "        \"target\": pair[\"target\"],\n",
    "        \"similarity\": pair[\"similarity\"],\n",
    "        \"relation\": pair[\"relation\"],\n",
    "        \"category\": pair[\"category\"]\n",
    "    }\n",
    "    term_mappings[source].append(target_info)\n",
    "\n",
    "print(f\"Unique source terms: {len(term_mappings):,}\")\n",
    "\n",
    "# Sort targets by similarity within each source\n",
    "for source in term_mappings:\n",
    "    term_mappings[source] = sorted(\n",
    "        term_mappings[source],\n",
    "        key=lambda x: x[\"similarity\"],\n",
    "        reverse=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mappings per source term:\n",
      "  Min: 1\n",
      "  Max: 11\n",
      "  Mean: 2.03\n",
      "  Median: 1\n"
     ]
    }
   ],
   "source": [
    "# Statistics on mappings per term\n",
    "mapping_counts = [len(v) for v in term_mappings.values()]\n",
    "mapping_arr = np.array(mapping_counts)\n",
    "\n",
    "print(f\"\\nMappings per source term:\")\n",
    "print(f\"  Min: {mapping_arr.min()}\")\n",
    "print(f\"  Max: {mapping_arr.max()}\")\n",
    "print(f\"  Mean: {mapping_arr.mean():.2f}\")\n",
    "print(f\"  Median: {np.median(mapping_arr):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample mappings:\n",
      "\n",
      "검색:\n",
      "  -> Search (sim=0.903, synonym)\n",
      "  -> 검색어 (sim=0.896, synonym)\n",
      "  -> search (sim=0.884, synonym)\n",
      "  -> 검색결과 (sim=0.838, synonym)\n",
      "  -> 검색엔진 (sim=0.803, synonym)\n"
     ]
    }
   ],
   "source": [
    "# Sample mappings\n",
    "print(\"\\nSample mappings:\")\n",
    "sample_terms = [\"인공지능\", \"검색\", \"추천\", \"데이터베이스\", \"보안\"]\n",
    "\n",
    "for term in sample_terms:\n",
    "    if term in term_mappings:\n",
    "        targets = term_mappings[term][:5]  # Top 5\n",
    "        print(f\"\\n{term}:\")\n",
    "        for t in targets:\n",
    "            print(f\"  -> {t['target']} (sim={t['similarity']:.3f}, {t['relation']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.1_korean_enhanced/term_mappings.jsonl\n",
      "Total source terms: 24,954\n",
      "Total target mappings: 50,534\n"
     ]
    }
   ],
   "source": [
    "# Convert to list format for saving\n",
    "output_data = []\n",
    "\n",
    "for source, targets in term_mappings.items():\n",
    "    output_data.append({\n",
    "        \"source\": source,\n",
    "        \"targets\": targets\n",
    "    })\n",
    "\n",
    "# Save to JSONL\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in output_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved to: {OUTPUT_FILE}\")\n",
    "print(f\"Total source terms: {len(output_data):,}\")\n",
    "print(f\"Total target mappings: {sum(len(d['targets']) for d in output_data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved filtered pairs to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.1_korean_enhanced/filtered_pairs.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Save filtered pairs as well (for reference)\n",
    "filtered_pairs_path = OUTPUT_DIR / \"filtered_pairs.jsonl\"\n",
    "\n",
    "with open(filtered_pairs_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for pair in filtered_pairs:\n",
    "        f.write(json.dumps(pair, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved filtered pairs to: {filtered_pairs_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High similarity pairs (>=0.8): 15494\n",
      "\n",
      "Top 10 most similar pairs:\n",
      "  Oh <-> oh (sim=0.9917)\n",
      "  oh <-> Oh (sim=0.9917)\n",
      "  로스앤젤레스 <-> 로스엔젤레스 (sim=0.9909)\n",
      "  로스엔젤레스 <-> 로스앤젤레스 (sim=0.9909)\n",
      "  하버드대학교 <-> 하버드대학 (sim=0.9906)\n",
      "  하버드대학 <-> 하버드대학교 (sim=0.9906)\n",
      "  Video <-> video (sim=0.9873)\n",
      "  video <-> Video (sim=0.9873)\n",
      "  콤플렉스 <-> 컴플렉스 (sim=0.9860)\n",
      "  컴플렉스 <-> 콤플렉스 (sim=0.9860)\n"
     ]
    }
   ],
   "source": [
    "# High similarity pairs (very good synonyms)\n",
    "high_sim_pairs = [p for p in filtered_pairs if p[\"similarity\"] >= 0.8]\n",
    "print(f\"High similarity pairs (>=0.8): {len(high_sim_pairs)}\")\n",
    "\n",
    "print(\"\\nTop 10 most similar pairs:\")\n",
    "sorted_pairs = sorted(filtered_pairs, key=lambda x: x[\"similarity\"], reverse=True)\n",
    "for p in sorted_pairs[:10]:\n",
    "    print(f\"  {p['source']} <-> {p['target']} (sim={p['similarity']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Terms with most synonyms:\n",
      "  한국프로축구연맹: 11 synonyms\n",
      "  사회운동가: 11 synonyms\n",
      "  인권운동가: 11 synonyms\n",
      "  노동운동가: 11 synonyms\n",
      "  한국프로농구: 10 synonyms\n",
      "  마쓰모토시: 10 synonyms\n",
      "  후쿠시마: 10 synonyms\n",
      "  후쿠야마: 10 synonyms\n",
      "  공산주의: 10 synonyms\n",
      "  환경운동가: 10 synonyms\n"
     ]
    }
   ],
   "source": [
    "# Terms with most synonyms\n",
    "sorted_terms = sorted(term_mappings.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "print(\"\\nTerms with most synonyms:\")\n",
    "for term, targets in sorted_terms[:10]:\n",
    "    print(f\"  {term}: {len(targets)} synonyms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Data preparation complete. The processed data is saved to:\n",
    "- `dataset/v21.1_korean_enhanced/term_mappings.jsonl` - 1:N term mappings\n",
    "- `dataset/v21.1_korean_enhanced/filtered_pairs.jsonl` - Filtered pairs with similarity\n",
    "\n",
    "Next step: Run `02_training.ipynb` to train the Korean-enhanced SPLADE model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

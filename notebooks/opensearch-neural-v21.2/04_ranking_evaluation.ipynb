{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# v21.2 Ranking-Based Evaluation\n",
        "\n",
        "This notebook demonstrates comprehensive ranking-based evaluation metrics for SPLADE synonym expansion models.\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "Current metrics (source preservation rate, synonym activation rate) saturate at 100% from epoch 1,\n",
        "making it impossible to differentiate between model quality improvements across training.\n",
        "\n",
        "## Solution: Ranking Metrics\n",
        "\n",
        "We implement three ranking-aware metrics with multi-grade relevance:\n",
        "\n",
        "1. **Recall@K** (K=5, 10, 20): Fraction of relevant items in top-K\n",
        "2. **MRR (Mean Reciprocal Rank)**: How early the first relevant item appears\n",
        "3. **nDCG@K**: Quality of ranking with graded relevance\n",
        "\n",
        "### Relevance Grades\n",
        "\n",
        "| Grade | Description | Example for \"손해배상\" |\n",
        "|-------|-------------|----------------------|\n",
        "| 3 | Exact synonym | 배상, 보상 |\n",
        "| 2 | Partial match | 손해, 피해 |\n",
        "| 1 | Related term | 사고, 책임 |\n",
        "| 0 | Not relevant | (implicit) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def find_project_root():\n",
        "    current = Path.cwd()\n",
        "    for parent in [current] + list(current.parents):\n",
        "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
        "            return parent\n",
        "    return Path.cwd().parent.parent\n",
        "\n",
        "PROJECT_ROOT = find_project_root()\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Import our evaluation module\n",
        "from src.evaluation import (\n",
        "    RankingMetrics,\n",
        "    EvaluationDataset,\n",
        "    GradedRelevance,\n",
        "    ModelComparison,\n",
        "    create_korean_legal_medical_eval_dataset,\n",
        ")\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Mathematical Formulations\n",
        "\n",
        "### 1.1 Recall@K\n",
        "\n",
        "Measures the fraction of relevant items retrieved in top-K positions.\n",
        "\n",
        "$$\\text{Recall@K} = \\frac{|\\{\\text{relevant items in top-K}\\}|}{|\\{\\text{all relevant items}\\}|}$$\n",
        "\n",
        "For graded relevance with threshold:\n",
        "\n",
        "$$\\text{Recall@K}(\\theta) = \\frac{|\\{i \\in \\text{top-K} : \\text{rel}_i \\geq \\theta\\}|}{|\\{j : \\text{rel}_j \\geq \\theta\\}|}$$\n",
        "\n",
        "### 1.2 Mean Reciprocal Rank (MRR)\n",
        "\n",
        "Measures how early the first relevant item appears in the ranking.\n",
        "\n",
        "$$\\text{MRR} = \\frac{1}{|Q|} \\sum_{q \\in Q} \\frac{1}{\\text{rank}_q}$$\n",
        "\n",
        "where $\\text{rank}_q$ is the position of the first relevant item for query $q$.\n",
        "\n",
        "### 1.3 Normalized Discounted Cumulative Gain (nDCG@K)\n",
        "\n",
        "Measures ranking quality with graded relevance and position discount.\n",
        "\n",
        "$$\\text{DCG@K} = \\sum_{i=1}^{K} \\frac{2^{\\text{rel}_i} - 1}{\\log_2(i + 1)}$$\n",
        "\n",
        "$$\\text{IDCG@K} = \\text{DCG@K of ideal ranking}$$\n",
        "\n",
        "$$\\text{nDCG@K} = \\frac{\\text{DCG@K}}{\\text{IDCG@K}}$$\n",
        "\n",
        "Key properties:\n",
        "- Grade 3 (exact synonym) contributes $2^3 - 1 = 7$ to gain\n",
        "- Grade 2 (partial match) contributes $2^2 - 1 = 3$ to gain\n",
        "- Grade 1 (related term) contributes $2^1 - 1 = 1$ to gain\n",
        "- Higher positions have larger logarithmic discount factors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Model and Create Evaluation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SPLADE Model definition (same as training)\n",
        "class SPLADEModel(nn.Module):\n",
        "    \"\"\"SPLADE model for Korean sparse retrieval.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"skt/A.X-Encoder-base\"):\n",
        "        super().__init__()\n",
        "        self.model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "        self.config = self.model.config\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        token_scores = torch.log1p(self.relu(logits))\n",
        "        mask = attention_mask.unsqueeze(-1).float()\n",
        "        token_scores = token_scores * mask\n",
        "        sparse_repr, _ = token_scores.max(dim=1)\n",
        "        token_weights = token_scores.max(dim=-1).values\n",
        "        return sparse_repr, token_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"skt/A.X-Encoder-base\"\n",
        "MODEL_PATH = PROJECT_ROOT / \"outputs\" / \"v21.2_korean_legal_medical\" / \"best_model.pt\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(f\"Tokenizer vocab size: {tokenizer.vocab_size:,}\")\n",
        "\n",
        "# Load model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SPLADEModel(MODEL_NAME)\n",
        "\n",
        "# Load trained weights if available\n",
        "if MODEL_PATH.exists():\n",
        "    checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    print(f\"Loaded model from: {MODEL_PATH}\")\n",
        "else:\n",
        "    print(f\"No trained model found at {MODEL_PATH}, using pretrained weights\")\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "print(f\"Model loaded on {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create evaluation dataset\n",
        "eval_dataset = create_korean_legal_medical_eval_dataset()\n",
        "\n",
        "print(\"Evaluation Dataset Statistics:\")\n",
        "stats = eval_dataset.statistics()\n",
        "for key, value in stats.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show sample queries with graded relevance\n",
        "print(\"\\nSample Queries with Graded Relevance:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, query in enumerate(eval_dataset[:3]):\n",
        "    print(f\"\\nQuery {i+1}: '{query.query}' (domain: {query.domain})\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for grade in [3, 2, 1]:\n",
        "        tokens = query.get_tokens_by_grade(grade)\n",
        "        grade_names = {3: \"Exact synonym\", 2: \"Partial match\", 1: \"Related term\"}\n",
        "        if tokens:\n",
        "            print(f\"  Grade {grade} ({grade_names[grade]}): {', '.join(sorted(tokens))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize Ranking Metrics and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize ranking metrics calculator\n",
        "ranking_metrics = RankingMetrics(\n",
        "    tokenizer=tokenizer,\n",
        "    k_values=[5, 10, 20, 50],\n",
        ")\n",
        "\n",
        "print(f\"Configured K values: {ranking_metrics.k_values}\")\n",
        "print(f\"Special token IDs excluded: {ranking_metrics.special_token_ids}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model\n",
        "print(\"Evaluating model...\")\n",
        "result = ranking_metrics.evaluate(\n",
        "    model=model,\n",
        "    eval_dataset=eval_dataset,\n",
        "    batch_size=16,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "print(result.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed per-query analysis\n",
        "print(\"\\nPer-Query Metrics (first 5 queries):\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, metrics in enumerate(result.per_query_metrics[:5]):\n",
        "    print(f\"\\nQuery: '{metrics['query']}' (domain: {metrics['domain']})\")\n",
        "    print(f\"  MRR: {metrics['mrr']:.4f}\")\n",
        "    print(f\"  First relevant rank: {metrics['first_relevant_rank']}\")\n",
        "    print(f\"  Recall@5: {metrics['recall@5']:.4f}, Recall@10: {metrics['recall@10']:.4f}\")\n",
        "    print(f\"  nDCG@5: {metrics['ndcg@5']:.4f}, nDCG@10: {metrics['ndcg@10']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot metrics across K values\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "k_values = sorted(result.recall_at_k.keys())\n",
        "\n",
        "# Recall@K\n",
        "recall_values = [result.recall_at_k[k] for k in k_values]\n",
        "axes[0].bar(range(len(k_values)), recall_values, color='#3498db', alpha=0.8)\n",
        "axes[0].set_xticks(range(len(k_values)))\n",
        "axes[0].set_xticklabels([f'@{k}' for k in k_values])\n",
        "axes[0].set_ylabel('Recall')\n",
        "axes[0].set_title('Recall@K')\n",
        "axes[0].set_ylim(0, 1)\n",
        "for i, v in enumerate(recall_values):\n",
        "    axes[0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=10)\n",
        "\n",
        "# nDCG@K\n",
        "ndcg_values = [result.ndcg_at_k[k] for k in k_values]\n",
        "axes[1].bar(range(len(k_values)), ndcg_values, color='#2ecc71', alpha=0.8)\n",
        "axes[1].set_xticks(range(len(k_values)))\n",
        "axes[1].set_xticklabels([f'@{k}' for k in k_values])\n",
        "axes[1].set_ylabel('nDCG')\n",
        "axes[1].set_title('nDCG@K')\n",
        "axes[1].set_ylim(0, 1)\n",
        "for i, v in enumerate(ndcg_values):\n",
        "    axes[1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=10)\n",
        "\n",
        "# MRR and First Relevant Rank\n",
        "metrics_names = ['MRR', 'Mean 1st Rank', 'Median 1st Rank']\n",
        "metrics_values = [\n",
        "    result.mrr,\n",
        "    result.mean_first_relevant_rank / 100 if result.mean_first_relevant_rank else 0,\n",
        "    result.median_first_relevant_rank / 100 if result.median_first_relevant_rank else 0,\n",
        "]\n",
        "colors = ['#e74c3c', '#9b59b6', '#f39c12']\n",
        "axes[2].bar(range(len(metrics_names)), metrics_values, color=colors, alpha=0.8)\n",
        "axes[2].set_xticks(range(len(metrics_names)))\n",
        "axes[2].set_xticklabels(metrics_names, rotation=15)\n",
        "axes[2].set_title('MRR and First Relevant Rank')\n",
        "axes[2].set_ylim(0, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(PROJECT_ROOT / \"outputs\" / \"v21.2_korean_legal_medical\" / \"ranking_metrics.png\", dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-domain comparison\n",
        "if result.domain_metrics:\n",
        "    domains = list(result.domain_metrics.keys())\n",
        "    n_domains = len(domains)\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    \n",
        "    x = np.arange(n_domains)\n",
        "    width = 0.25\n",
        "    \n",
        "    mrr_values = [result.domain_metrics[d]['mrr'] for d in domains]\n",
        "    ndcg10_values = [result.domain_metrics[d]['ndcg_at_k'].get(10, 0) for d in domains]\n",
        "    recall10_values = [result.domain_metrics[d]['recall_at_k'].get(10, 0) for d in domains]\n",
        "    \n",
        "    ax.bar(x - width, mrr_values, width, label='MRR', color='#3498db')\n",
        "    ax.bar(x, ndcg10_values, width, label='nDCG@10', color='#2ecc71')\n",
        "    ax.bar(x + width, recall10_values, width, label='Recall@10', color='#e74c3c')\n",
        "    \n",
        "    ax.set_xlabel('Domain')\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Metrics by Domain')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(domains)\n",
        "    ax.legend()\n",
        "    ax.set_ylim(0, 1)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(PROJECT_ROOT / \"outputs\" / \"v21.2_korean_legal_medical\" / \"domain_metrics.png\", dpi=150)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Analyze Individual Query Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_query(\n",
        "    query: str,\n",
        "    model: nn.Module,\n",
        "    tokenizer,\n",
        "    ground_truth: GradedRelevance,\n",
        "    top_k: int = 20,\n",
        "    device=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Analyze a single query showing ranking vs ground truth.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = next(model.parameters()).device\n",
        "    \n",
        "    # Encode query\n",
        "    inputs = tokenizer(\n",
        "        query,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=64,\n",
        "    )\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        sparse_repr, _ = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "    \n",
        "    # Get special token IDs\n",
        "    special_ids = {tokenizer.pad_token_id, tokenizer.cls_token_id,\n",
        "                   tokenizer.sep_token_id, tokenizer.unk_token_id}\n",
        "    special_ids = {t for t in special_ids if t is not None}\n",
        "    \n",
        "    # Mask special tokens\n",
        "    weights = sparse_repr[0].clone()\n",
        "    for tid in special_ids:\n",
        "        weights[tid] = -float('inf')\n",
        "    \n",
        "    # Get top-k\n",
        "    top_values, top_indices = torch.topk(weights, k=top_k)\n",
        "    \n",
        "    # Build relevance map\n",
        "    relevance_map = {}\n",
        "    for token, grade in ground_truth.relevance_judgments.items():\n",
        "        ids = tokenizer.encode(token, add_special_tokens=False)\n",
        "        if len(ids) == 1:\n",
        "            relevance_map[ids[0]] = (token, grade)\n",
        "    \n",
        "    print(f\"\\nQuery: '{query}'\")\n",
        "    print(f\"Domain: {ground_truth.domain}\")\n",
        "    print(f\"Ground Truth: {len(ground_truth.relevance_judgments)} judged tokens\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"{'Rank':<6}{'Token':<15}{'Weight':<12}{'Relevance':<12}{'Grade'}\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    for rank, (idx, weight) in enumerate(zip(top_indices.tolist(), top_values.tolist()), 1):\n",
        "        token = tokenizer.decode([idx]).strip()\n",
        "        if idx in relevance_map:\n",
        "            gt_token, grade = relevance_map[idx]\n",
        "            grade_labels = {3: \"Exact\", 2: \"Partial\", 1: \"Related\"}\n",
        "            rel_label = grade_labels.get(grade, \"?\")\n",
        "            marker = \"***\" if grade == 3 else (\"**\" if grade == 2 else \"*\")\n",
        "        else:\n",
        "            grade = 0\n",
        "            rel_label = \"-\"\n",
        "            marker = \"\"\n",
        "        \n",
        "        print(f\"{rank:<6}{token:<15}{weight:<12.4f}{rel_label:<12}{marker}\")\n",
        "    \n",
        "    # Compute metrics for this query\n",
        "    ranking = [(idx.item(), val.item()) for idx, val in zip(top_indices, top_values) if val > 0]\n",
        "    metrics = RankingMetrics(tokenizer)\n",
        "    \n",
        "    print(\"\\nMetrics:\")\n",
        "    print(f\"  MRR: {metrics.compute_mrr(ranking, ground_truth):.4f}\")\n",
        "    for k in [5, 10, 20]:\n",
        "        recall = metrics.compute_recall_at_k(ranking, ground_truth, k)\n",
        "        ndcg = metrics.compute_ndcg(ranking, ground_truth, k)\n",
        "        print(f\"  Recall@{k}: {recall:.4f}, nDCG@{k}: {ndcg:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze sample queries\n",
        "for query_data in eval_dataset[:3]:\n",
        "    analyze_query(\n",
        "        query=query_data.query,\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        ground_truth=query_data,\n",
        "        top_k=20,\n",
        "        device=device,\n",
        "    )\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Compare Models Across Epochs\n",
        "\n",
        "This section demonstrates how to compare model quality across training epochs using statistical significance testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to load model from checkpoint\n",
        "def load_model_from_checkpoint(checkpoint_path, model_name, device):\n",
        "    \"\"\"Load model from checkpoint file.\"\"\"\n",
        "    model = SPLADEModel(model_name)\n",
        "    if checkpoint_path.exists():\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Example: Compare epoch 5 vs epoch 25\n",
        "output_dir = PROJECT_ROOT / \"outputs\" / \"v21.2_korean_legal_medical\"\n",
        "epoch5_path = output_dir / \"checkpoint_epoch5.pt\"\n",
        "epoch25_path = output_dir / \"checkpoint_epoch25.pt\"\n",
        "\n",
        "if epoch5_path.exists() and epoch25_path.exists():\n",
        "    print(\"Loading models from epochs 5 and 25...\")\n",
        "    \n",
        "    model_epoch5 = load_model_from_checkpoint(epoch5_path, MODEL_NAME, device)\n",
        "    model_epoch25 = load_model_from_checkpoint(epoch25_path, MODEL_NAME, device)\n",
        "    \n",
        "    # Evaluate both models\n",
        "    print(\"Evaluating epoch 5 model...\")\n",
        "    result_epoch5 = ranking_metrics.evaluate(model_epoch5, eval_dataset, device=device)\n",
        "    \n",
        "    print(\"Evaluating epoch 25 model...\")\n",
        "    result_epoch25 = ranking_metrics.evaluate(model_epoch25, eval_dataset, device=device)\n",
        "    \n",
        "    # Statistical comparison\n",
        "    comparison = ModelComparison.compare_models(\n",
        "        result_epoch5,\n",
        "        result_epoch25,\n",
        "        model_a_name=\"Epoch 5\",\n",
        "        model_b_name=\"Epoch 25\",\n",
        "    )\n",
        "    \n",
        "    print(comparison.summary())\n",
        "else:\n",
        "    print(f\"Checkpoint files not found. Expected:\")\n",
        "    print(f\"  - {epoch5_path}\")\n",
        "    print(f\"  - {epoch25_path}\")\n",
        "    print(\"\\nRun training first to generate checkpoints.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Creating Custom Evaluation Datasets\n",
        "\n",
        "This section shows how to create your own evaluation dataset with graded relevance judgments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 1: From synonym pairs (convenient format)\n",
        "custom_synonym_data = [\n",
        "    {\n",
        "        \"source\": \"기업\",\n",
        "        \"synonyms\": {\n",
        "            \"exact\": [\"회사\", \"법인\", \"사업체\"],\n",
        "            \"partial\": [\"기업체\", \"업체\", \"상사\"],\n",
        "            \"related\": [\"사업\", \"경영\", \"산업\"],\n",
        "        },\n",
        "        \"domain\": \"business\",\n",
        "    },\n",
        "    {\n",
        "        \"source\": \"투자\",\n",
        "        \"synonyms\": {\n",
        "            \"exact\": [\"출자\", \"투입\", \"자본투자\"],\n",
        "            \"partial\": [\"투자금\", \"자금\", \"투자액\"],\n",
        "            \"related\": [\"수익\", \"이익\", \"자산\"],\n",
        "        },\n",
        "        \"domain\": \"finance\",\n",
        "    },\n",
        "]\n",
        "\n",
        "custom_dataset = EvaluationDataset.from_synonym_pairs(\n",
        "    custom_synonym_data,\n",
        "    name=\"custom_business_finance\",\n",
        ")\n",
        "\n",
        "print(\"Custom Dataset Statistics:\")\n",
        "print(custom_dataset.statistics())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 2: Direct GradedRelevance construction (full control)\n",
        "manual_queries = [\n",
        "    GradedRelevance(\n",
        "        query=\"프로그래밍\",\n",
        "        relevance_judgments={\n",
        "            \"코딩\": 3,\n",
        "            \"개발\": 3,\n",
        "            \"프로그램\": 2,\n",
        "            \"소프트웨어\": 2,\n",
        "            \"컴퓨터\": 1,\n",
        "            \"언어\": 1,\n",
        "        },\n",
        "        domain=\"tech\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "manual_dataset = EvaluationDataset(\n",
        "    queries=manual_queries,\n",
        "    name=\"manual_tech_dataset\",\n",
        ")\n",
        "\n",
        "print(\"\\nManual Dataset:\")\n",
        "print(f\"  Queries: {len(manual_dataset)}\")\n",
        "print(f\"  First query ideal ranking: {manual_dataset[0].ideal_ranking()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save and load datasets\n",
        "save_path = PROJECT_ROOT / \"dataset\" / \"eval_datasets\" / \"custom_eval.json\"\n",
        "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "custom_dataset.save(save_path)\n",
        "print(f\"Saved dataset to: {save_path}\")\n",
        "\n",
        "# Reload\n",
        "reloaded = EvaluationDataset.load(save_path)\n",
        "print(f\"Reloaded dataset: {len(reloaded)} queries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary and Recommendations\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **nDCG@K is the primary metric** for synonym expansion quality:\n",
        "   - Captures graded relevance (exact > partial > related)\n",
        "   - Rewards putting exact synonyms at higher ranks\n",
        "   - Normalized to [0, 1] for easy interpretation\n",
        "\n",
        "2. **MRR complements nDCG**:\n",
        "   - Measures how quickly the model finds ANY relevant synonym\n",
        "   - Useful for applications where only the first result matters\n",
        "\n",
        "3. **Recall@K for coverage**:\n",
        "   - Measures completeness of synonym retrieval\n",
        "   - Use multiple K values (5, 10, 20) to understand recall curve\n",
        "\n",
        "### Recommended Evaluation Protocol\n",
        "\n",
        "1. Create domain-specific evaluation datasets with 3-grade relevance\n",
        "2. Report nDCG@10 as primary metric\n",
        "3. Use MRR and Recall@10 as secondary metrics\n",
        "4. Use paired t-test or bootstrap CI for model comparison\n",
        "5. Report per-domain metrics for specialized applications\n",
        "\n",
        "### Integration with Training\n",
        "\n",
        "Use these ranking metrics during training validation to:\n",
        "- Detect when model quality actually improves (not just loss decreases)\n",
        "- Enable early stopping based on nDCG@K plateau\n",
        "- Compare different hyperparameter configurations objectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final results\n",
        "import json\n",
        "\n",
        "results_dict = result.to_dict()\n",
        "results_path = PROJECT_ROOT / \"outputs\" / \"v21.2_korean_legal_medical\" / \"ranking_evaluation_results.json\"\n",
        "\n",
        "with open(results_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results_dict, f, ensure_ascii=False, indent=2, default=str)\n",
        "\n",
        "print(f\"Results saved to: {results_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

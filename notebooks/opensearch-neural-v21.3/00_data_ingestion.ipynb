{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v21.3 Data Ingestion - Enhanced with Medical Data\n",
    "\n",
    "This notebook collects Korean text data from diverse domains including properly loaded medical data.\n",
    "\n",
    "## Changes from v21.2\n",
    "\n",
    "| Feature | v21.2 | v21.3 |\n",
    "|---------|-------|-------|\n",
    "| Medical Data | Failed to load | **All 4 configs loaded** |\n",
    "| KorMedMCQA | 0 texts | **50K+ texts** |\n",
    "| Total Corpus | 643K | **700K+ texts** |\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "| Domain | Dataset | Description |\n",
    "|--------|---------|-------------|\n",
    "| ë°±ê³¼ì‚¬ì „ | Wikipedia | ì¼ë°˜ ì§€ì‹, ì—­ì‚¬, ê³¼í•™ |\n",
    "| ë‰´ìŠ¤/QA | KLUE-MRC, KorQuAD | ë‰´ìŠ¤ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ |\n",
    "| ë²•ë¥  | Korean Law Precedents | ë²•ë¥  ìš©ì–´, íŒë¡€ |\n",
    "| **ì˜ë£Œ** | **KorMedMCQA (4 configs)** | **ì˜ë£Œ ìžê²©ì‹œí—˜ QA** |\n",
    "| ëŒ€í™” | KorHate, Open Korean Inst | ì¼ìƒ ëŒ€í™” |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Set, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory - v21.3\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"dataset\" / \"v21.3_filtered_enhanced\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"min_term_freq\": 3,\n",
    "    \"max_terms\": 150000,  # Increased for more coverage\n",
    "    \"embedding_batch_size\": 64,\n",
    "    \"n_clusters\": 15000,  # More clusters for diversity\n",
    "    \"min_cluster_size\": 2,\n",
    "    \"max_cluster_size\": 10,\n",
    "    \"similarity_threshold\": 0.75,\n",
    "}\n",
    "print(f\"Config: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Load Diverse Korean Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "def load_diverse_korean_datasets() -> List[str]:\n",
    "    \"\"\"Load diverse Korean text data from multiple domains.\n",
    "    \n",
    "    v21.3: Fixed medical data loading with proper config names.\n",
    "    \"\"\"\n",
    "    all_texts = []\n",
    "    domain_stats = {}\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 1. Wikipedia (ë°±ê³¼ì‚¬ì „)\n",
    "    # ========================================================================\n",
    "    print(\"=\" * 60)\n",
    "    print(\"[1/14] Loading Korean Wikipedia...\")\n",
    "    try:\n",
    "        wiki_dataset = load_dataset(\n",
    "            \"wikimedia/wikipedia\", \n",
    "            \"20231101.ko\",\n",
    "            split=\"train\",\n",
    "            streaming=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        wiki_texts = []\n",
    "        for i, item in enumerate(wiki_dataset):\n",
    "            if i >= 100000:\n",
    "                break\n",
    "            text = item.get(\"text\", \"\")\n",
    "            if text and len(text) > 100:\n",
    "                wiki_texts.append(text[:3000])\n",
    "        all_texts.extend(wiki_texts)\n",
    "        domain_stats[\"Wikipedia\"] = len(wiki_texts)\n",
    "        print(f\"  âœ“ Wikipedia: {len(wiki_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Wikipedia failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 2. KLUE-MRC (ë‰´ìŠ¤ QA)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[2/14] Loading KLUE-MRC...\")\n",
    "    try:\n",
    "        klue_dataset = load_dataset(\"klue\", \"mrc\", split=\"train\", trust_remote_code=True)\n",
    "        klue_texts = []\n",
    "        for item in klue_dataset:\n",
    "            context = item.get(\"context\", \"\")\n",
    "            question = item.get(\"question\", \"\")\n",
    "            if context and len(context) > 50:\n",
    "                klue_texts.append(context[:2000])\n",
    "            if question:\n",
    "                klue_texts.append(question)\n",
    "        all_texts.extend(klue_texts)\n",
    "        domain_stats[\"KLUE-MRC\"] = len(klue_texts)\n",
    "        print(f\"  âœ“ KLUE-MRC: {len(klue_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— KLUE-MRC failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 3. KorQuAD (QA)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[3/14] Loading KorQuAD...\")\n",
    "    try:\n",
    "        korquad_dataset = load_dataset(\"squad_kor_v1\", split=\"train\", trust_remote_code=True)\n",
    "        korquad_texts = []\n",
    "        for item in korquad_dataset:\n",
    "            context = item.get(\"context\", \"\")\n",
    "            question = item.get(\"question\", \"\")\n",
    "            if context and len(context) > 50:\n",
    "                korquad_texts.append(context[:2000])\n",
    "            if question:\n",
    "                korquad_texts.append(question)\n",
    "        all_texts.extend(korquad_texts)\n",
    "        domain_stats[\"KorQuAD\"] = len(korquad_texts)\n",
    "        print(f\"  âœ“ KorQuAD: {len(korquad_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— KorQuAD failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 4. NSMC (ë¦¬ë·°)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[4/14] Loading NSMC...\")\n",
    "    try:\n",
    "        nsmc_dataset = load_dataset(\"nsmc\", split=\"train\", trust_remote_code=True)\n",
    "        nsmc_texts = [item.get(\"document\", \"\") for item in nsmc_dataset \n",
    "                     if item.get(\"document\") and len(item.get(\"document\", \"\")) > 10]\n",
    "        all_texts.extend(nsmc_texts)\n",
    "        domain_stats[\"NSMC\"] = len(nsmc_texts)\n",
    "        print(f\"  âœ“ NSMC: {len(nsmc_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— NSMC failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 5-8. KLUE Tasks (NLI, STS, YNAT)\n",
    "    # ========================================================================\n",
    "    klue_tasks = [\n",
    "        (\"nli\", [\"premise\", \"hypothesis\"]),\n",
    "        (\"sts\", [\"sentence1\", \"sentence2\"]),\n",
    "        (\"ynat\", [\"title\"]),\n",
    "    ]\n",
    "    for idx, (task, fields) in enumerate(klue_tasks, 5):\n",
    "        print(f\"\\n[{idx}/14] Loading KLUE-{task.upper()}...\")\n",
    "        try:\n",
    "            dataset = load_dataset(\"klue\", task, split=\"train\", trust_remote_code=True)\n",
    "            task_texts = []\n",
    "            for item in dataset:\n",
    "                for field in fields:\n",
    "                    if item.get(field):\n",
    "                        task_texts.append(item[field])\n",
    "            all_texts.extend(task_texts)\n",
    "            domain_stats[f\"KLUE-{task.upper()}\"] = len(task_texts)\n",
    "            print(f\"  âœ“ KLUE-{task.upper()}: {len(task_texts):,} texts\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— KLUE-{task.upper()} failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 9. KoAlpaca (ì§€ì‹œë¬¸)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[8/14] Loading KoAlpaca...\")\n",
    "    try:\n",
    "        alpaca_dataset = load_dataset(\"Bingsu/ko_alpaca_data\", split=\"train\", trust_remote_code=True)\n",
    "        alpaca_texts = []\n",
    "        for item in alpaca_dataset:\n",
    "            if item.get(\"instruction\"): \n",
    "                alpaca_texts.append(item[\"instruction\"])\n",
    "            if item.get(\"output\") and len(item.get(\"output\", \"\")) > 20:\n",
    "                alpaca_texts.append(item[\"output\"][:1000])\n",
    "        alpaca_texts = alpaca_texts[:50000]\n",
    "        all_texts.extend(alpaca_texts)\n",
    "        domain_stats[\"KoAlpaca\"] = len(alpaca_texts)\n",
    "        print(f\"  âœ“ KoAlpaca: {len(alpaca_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— KoAlpaca failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 10. Korean Law Precedents (ë²•ë¥ )\n",
    "    # ========================================================================\n",
    "    print(\"\\n[9/14] Loading Korean Law Precedents...\")\n",
    "    try:\n",
    "        law_precedents = load_dataset(\n",
    "            \"joonhok-exo-ai/korean_law_open_data_precedents\",\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        law_texts = []\n",
    "        for item in law_precedents:\n",
    "            for field in [\"íŒì‹œì‚¬í•­\", \"íŒê²°ìš”ì§€\", \"ì „ë¬¸\", \"ì‚¬ê±´ëª…\", \"ì‚¬ê±´ê°œìš”\"]:\n",
    "                text = item.get(field, \"\")\n",
    "                if text and len(text) > 30:\n",
    "                    law_texts.append(text[:2000])\n",
    "        law_texts = law_texts[:80000]\n",
    "        all_texts.extend(law_texts)\n",
    "        domain_stats[\"LawPrecedents\"] = len(law_texts)\n",
    "        print(f\"  âœ“ Law Precedents: {len(law_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Law Precedents failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 11-14. KorMedMCQA - ALL 4 CONFIGS (ì˜ë£Œ) - FIXED!\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"[10-13/14] Loading KorMedMCQA (Medical) - ALL CONFIGS...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    medical_configs = [\"dentist\", \"doctor\", \"nurse\", \"pharm\"]\n",
    "    total_medical = 0\n",
    "    \n",
    "    for config in medical_configs:\n",
    "        print(f\"\\n  Loading KorMedMCQA/{config}...\")\n",
    "        try:\n",
    "            med_dataset = load_dataset(\n",
    "                \"sean0042/KorMedMCQA\",\n",
    "                config,  # Specify config name!\n",
    "                split=\"train\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            med_texts = []\n",
    "            for item in med_dataset:\n",
    "                # Extract question\n",
    "                if item.get(\"question\"):\n",
    "                    med_texts.append(item[\"question\"])\n",
    "                # Extract options (list of choices)\n",
    "                if item.get(\"options\"):\n",
    "                    options = item[\"options\"]\n",
    "                    if isinstance(options, list):\n",
    "                        for opt in options:\n",
    "                            if isinstance(opt, str) and len(opt) > 5:\n",
    "                                med_texts.append(opt)\n",
    "                # Extract answer/explanation\n",
    "                for field in [\"answer\", \"explanation\"]:\n",
    "                    text = item.get(field, \"\")\n",
    "                    if isinstance(text, str) and len(text) > 10:\n",
    "                        med_texts.append(text[:1000])\n",
    "            \n",
    "            all_texts.extend(med_texts)\n",
    "            domain_stats[f\"KorMedMCQA-{config}\"] = len(med_texts)\n",
    "            total_medical += len(med_texts)\n",
    "            print(f\"    âœ“ {config}: {len(med_texts):,} texts\")\n",
    "        except Exception as e:\n",
    "            print(f\"    âœ— {config} failed: {e}\")\n",
    "    \n",
    "    print(f\"\\n  Total Medical: {total_medical:,} texts\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 15. KorHate (ëŒ€í™”)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[14/14] Loading KorHate...\")\n",
    "    try:\n",
    "        hate_dataset = load_dataset(\"kor_hate\", split=\"train\", trust_remote_code=True)\n",
    "        hate_texts = [item.get(\"comments\", \"\") for item in hate_dataset if item.get(\"comments\")]\n",
    "        all_texts.extend(hate_texts)\n",
    "        domain_stats[\"KorHate\"] = len(hate_texts)\n",
    "        print(f\"  âœ“ KorHate: {len(hate_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— KorHate failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Summary\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Data Collection Summary (v21.3 - Medical Fixed)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total = 0\n",
    "    for domain, count in sorted(domain_stats.items(), key=lambda x: -x[1]):\n",
    "        pct = count / sum(domain_stats.values()) * 100 if sum(domain_stats.values()) > 0 else 0\n",
    "        marker = \"ðŸ“š\" if \"Law\" in domain else \"ðŸ¥\" if \"Med\" in domain else \"  \"\n",
    "        print(f\"{marker} {domain:25} {count:>10,} texts ({pct:5.1f}%)\")\n",
    "        total += count\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"   {'TOTAL':25} {total:>10,} texts\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return all_texts\n",
    "\n",
    "texts = load_diverse_korean_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Extract Korean Terms with Kiwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize Kiwi\n",
    "print(\"Loading Kiwi morphological analyzer...\")\n",
    "kiwi = Kiwi()\n",
    "print(\"Kiwi loaded successfully\")\n",
    "\n",
    "# Load tokenizer\n",
    "MODEL_NAME = \"skt/A.X-Encoder-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer: {MODEL_NAME}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "# POS tags to keep\n",
    "VALID_POS_TAGS = {'NNG', 'NNP', 'NNB', 'SL', 'SH'}\n",
    "\n",
    "def extract_nouns_with_kiwi(text: str, kiwi_instance: Kiwi) -> List[str]:\n",
    "    \"\"\"Extract nouns using Kiwi.\"\"\"\n",
    "    try:\n",
    "        result = kiwi_instance.tokenize(text)\n",
    "        nouns = []\n",
    "        for token in result:\n",
    "            if token.tag in VALID_POS_TAGS:\n",
    "                word = token.form.strip()\n",
    "                if 2 <= len(word) <= 15:\n",
    "                    nouns.append(word)\n",
    "        return nouns\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def extract_compound_nouns(text: str, kiwi_instance: Kiwi) -> List[str]:\n",
    "    \"\"\"Extract compound nouns.\"\"\"\n",
    "    try:\n",
    "        result = kiwi_instance.tokenize(text)\n",
    "        compounds = []\n",
    "        current_compound = []\n",
    "        \n",
    "        for token in result:\n",
    "            if token.tag in {'NNG', 'NNP', 'SL', 'SH'}:\n",
    "                current_compound.append(token.form)\n",
    "            else:\n",
    "                if len(current_compound) >= 2:\n",
    "                    compound = ''.join(current_compound)\n",
    "                    if 2 <= len(compound) <= 15:\n",
    "                        compounds.append(compound)\n",
    "                current_compound = []\n",
    "        \n",
    "        if len(current_compound) >= 2:\n",
    "            compound = ''.join(current_compound)\n",
    "            if 2 <= len(compound) <= 15:\n",
    "                compounds.append(compound)\n",
    "        \n",
    "        return compounds\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# Test\n",
    "test_text = \"ë‹¹ë‡¨ë³‘ í™˜ìžì˜ í˜ˆë‹¹ ì¡°ì ˆì„ ìœ„í•œ ì¸ìŠë¦° íˆ¬ì—¬ ë°©ë²•\"\n",
    "print(f\"\\nTest text: {test_text}\")\n",
    "print(f\"Extracted nouns: {extract_nouns_with_kiwi(test_text, kiwi)}\")\n",
    "print(f\"Compound nouns: {extract_compound_nouns(test_text, kiwi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_terms(texts: List[str], kiwi_instance: Kiwi, min_freq: int = 3) -> List[Tuple[str, int]]:\n",
    "    \"\"\"Extract Korean terms using morphological analysis.\"\"\"\n",
    "    term_freq = Counter()\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        if i % 20000 == 0:\n",
    "            print(f\"Processing text {i:,}/{len(texts):,}...\")\n",
    "        \n",
    "        nouns = extract_nouns_with_kiwi(text, kiwi_instance)\n",
    "        for noun in nouns:\n",
    "            term_freq[noun] += 1\n",
    "        \n",
    "        compounds = extract_compound_nouns(text, kiwi_instance)\n",
    "        for compound in compounds:\n",
    "            term_freq[compound] += 1\n",
    "    \n",
    "    filtered_terms = [\n",
    "        (term, freq) for term, freq in term_freq.items() \n",
    "        if freq >= min_freq\n",
    "    ]\n",
    "    filtered_terms.sort(key=lambda x: -x[1])\n",
    "    \n",
    "    return filtered_terms\n",
    "\n",
    "print(f\"\\nExtracting terms from {len(texts):,} texts...\")\n",
    "terms_with_freq = extract_terms(texts, kiwi, CONFIG[\"min_term_freq\"])\n",
    "\n",
    "print(f\"\\nExtracted {len(terms_with_freq):,} unique terms\")\n",
    "print(f\"\\nTop 30 terms:\")\n",
    "for term, freq in terms_with_freq[:30]:\n",
    "    print(f\"  {term}: {freq:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Compute Embeddings with BGE-M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import torch\n",
    "\n",
    "# Limit terms\n",
    "terms = [t[0] for t in terms_with_freq[:CONFIG[\"max_terms\"]]]\n",
    "print(f\"Processing {len(terms):,} terms for embeddings\")\n",
    "\n",
    "# Load BGE-M3\n",
    "print(\"\\nLoading BGE-M3 model...\")\n",
    "bge_model = BGEM3FlagModel(\n",
    "    \"BAAI/bge-m3\",\n",
    "    use_fp16=True,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"BGE-M3 loaded on {bge_model.device}\")\n",
    "\n",
    "def compute_embeddings(terms: List[str], model, batch_size: int = 64) -> np.ndarray:\n",
    "    \"\"\"Compute BGE-M3 embeddings.\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(terms), batch_size):\n",
    "        batch = terms[i:i + batch_size]\n",
    "        if i % 5000 == 0:\n",
    "            print(f\"Embedding batch {i:,}/{len(terms):,}...\")\n",
    "        \n",
    "        output = model.encode(\n",
    "            batch,\n",
    "            return_dense=True,\n",
    "            return_sparse=False,\n",
    "            return_colbert_vecs=False\n",
    "        )\n",
    "        embeddings = output[\"dense_vecs\"]\n",
    "        all_embeddings.append(embeddings)\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "embeddings = compute_embeddings(terms, bge_model, CONFIG[\"embedding_batch_size\"])\n",
    "print(f\"\\nEmbeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Normalize\n",
    "embeddings_normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Clustering\n",
    "n_clusters = min(CONFIG[\"n_clusters\"], len(terms) // 2)\n",
    "print(f\"Clustering {len(terms):,} terms into {n_clusters:,} clusters...\")\n",
    "\n",
    "kmeans = MiniBatchKMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    batch_size=1024,\n",
    "    n_init=3,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "cluster_labels = kmeans.fit_predict(embeddings_normalized)\n",
    "print(f\"\\nClustering complete\")\n",
    "\n",
    "# Group by cluster\n",
    "clusters = defaultdict(list)\n",
    "for i, label in enumerate(cluster_labels):\n",
    "    clusters[label].append((terms[i], i))\n",
    "\n",
    "# Filter\n",
    "valid_clusters = {\n",
    "    label: terms_list \n",
    "    for label, terms_list in clusters.items()\n",
    "    if CONFIG[\"min_cluster_size\"] <= len(terms_list) <= CONFIG[\"max_cluster_size\"]\n",
    "}\n",
    "print(f\"Valid clusters: {len(valid_clusters):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Extract Synonym Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_synonym_pairs_from_clusters(\n",
    "    valid_clusters: Dict[int, List[Tuple[str, int]]],\n",
    "    embeddings_normalized: np.ndarray,\n",
    "    similarity_threshold: float = 0.75\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Extract synonym pairs from clusters.\"\"\"\n",
    "    synonym_pairs = []\n",
    "    \n",
    "    for cluster_id, terms_list in valid_clusters.items():\n",
    "        if len(terms_list) < 2:\n",
    "            continue\n",
    "        \n",
    "        cluster_terms = [t[0] for t in terms_list]\n",
    "        cluster_indices = [t[1] for t in terms_list]\n",
    "        cluster_embeddings = embeddings_normalized[cluster_indices]\n",
    "        \n",
    "        similarities = cosine_similarity(cluster_embeddings)\n",
    "        \n",
    "        for i in range(len(cluster_terms)):\n",
    "            for j in range(i + 1, len(cluster_terms)):\n",
    "                sim = similarities[i][j]\n",
    "                if sim >= similarity_threshold:\n",
    "                    synonym_pairs.append({\n",
    "                        \"source\": cluster_terms[i],\n",
    "                        \"target\": cluster_terms[j],\n",
    "                        \"similarity\": float(sim),\n",
    "                        \"relation\": \"synonym\",\n",
    "                        \"category\": \"cluster\"\n",
    "                    })\n",
    "                    synonym_pairs.append({\n",
    "                        \"source\": cluster_terms[j],\n",
    "                        \"target\": cluster_terms[i],\n",
    "                        \"similarity\": float(sim),\n",
    "                        \"relation\": \"synonym\",\n",
    "                        \"category\": \"cluster\"\n",
    "                    })\n",
    "    \n",
    "    return synonym_pairs\n",
    "\n",
    "print(f\"Extracting synonym pairs (similarity >= {CONFIG['similarity_threshold']})...\")\n",
    "cluster_synonym_pairs = extract_synonym_pairs_from_clusters(\n",
    "    valid_clusters, \n",
    "    embeddings_normalized,\n",
    "    CONFIG[\"similarity_threshold\"]\n",
    ")\n",
    "print(f\"Extracted {len(cluster_synonym_pairs):,} synonym pairs from clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Save Raw Data (Before Filtering)\n",
    "\n",
    "Save the raw synonym pairs and corpus for the filtering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "seen = set()\n",
    "unique_pairs = []\n",
    "for pair in cluster_synonym_pairs:\n",
    "    if pair[\"source\"] == pair[\"target\"]:\n",
    "        continue\n",
    "    key = (pair[\"source\"], pair[\"target\"])\n",
    "    if key not in seen:\n",
    "        seen.add(key)\n",
    "        unique_pairs.append(pair)\n",
    "\n",
    "print(f\"Unique pairs after deduplication: {len(unique_pairs):,}\")\n",
    "\n",
    "# Sort by similarity\n",
    "unique_pairs.sort(key=lambda x: -x.get(\"similarity\", 0))\n",
    "\n",
    "# Save raw pairs (before IG/PMI filtering)\n",
    "raw_pairs_path = OUTPUT_DIR / \"raw_synonym_pairs.jsonl\"\n",
    "with open(raw_pairs_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for pair in unique_pairs:\n",
    "        f.write(json.dumps(pair, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved raw pairs to: {raw_pairs_path}\")\n",
    "\n",
    "# Save corpus texts for PMI calculation\n",
    "corpus_path = OUTPUT_DIR / \"corpus_texts.jsonl\"\n",
    "with open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for text in texts:\n",
    "        f.write(json.dumps({\"text\": text}, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved corpus to: {corpus_path}\")\n",
    "\n",
    "# Save embeddings\n",
    "np.save(OUTPUT_DIR / \"term_embeddings.npy\", embeddings_normalized)\n",
    "print(f\"Saved embeddings: {embeddings_normalized.shape}\")\n",
    "\n",
    "# Save term list\n",
    "with open(OUTPUT_DIR / \"term_list.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(terms, f, ensure_ascii=False)\n",
    "print(f\"Saved term list: {len(terms):,} terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Data collection complete. Next step: `01_noise_filtering.ipynb` to apply:\n",
    "- Information Gain filtering\n",
    "- PMI filtering\n",
    "- Cross-encoder reranking\n",
    "\n",
    "### Output Files\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `raw_synonym_pairs.jsonl` | Raw synonym pairs (before filtering) |\n",
    "| `corpus_texts.jsonl` | Corpus for PMI calculation |\n",
    "| `term_embeddings.npy` | Normalized BGE-M3 embeddings |\n",
    "| `term_list.json` | List of terms |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v21.3 Data Ingestion - Enhanced with Medical Data\n",
    "\n",
    "This notebook collects Korean text data from diverse domains including properly loaded medical data.\n",
    "\n",
    "## Changes from v21.2\n",
    "\n",
    "| Feature | v21.2 | v21.3 |\n",
    "|---------|-------|-------|\n",
    "| Medical Data | Failed to load | **All 4 configs loaded** |\n",
    "| KorMedMCQA | 0 texts | **50K+ texts** |\n",
    "| Total Corpus | 643K | **700K+ texts** |\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "| Domain | Dataset | Description |\n",
    "|--------|---------|-------------|\n",
    "| Î∞±Í≥ºÏÇ¨Ï†Ñ | Wikipedia | ÏùºÎ∞ò ÏßÄÏãù, Ïó≠ÏÇ¨, Í≥ºÌïô |\n",
    "| Îâ¥Ïä§/QA | KLUE-MRC, KorQuAD | Îâ¥Ïä§ Í∏∞Î∞ò ÏßàÏùòÏùëÎãµ |\n",
    "| Î≤ïÎ•† | Korean Law Precedents | Î≤ïÎ•† Ïö©Ïñ¥, ÌåêÎ°Ä |\n",
    "| **ÏùòÎ£å** | **KorMedMCQA (4 configs)** | **ÏùòÎ£å ÏûêÍ≤©ÏãúÌóò QA** |\n",
    "| ÎåÄÌôî | KorHate, Open Korean Inst | ÏùºÏÉÅ ÎåÄÌôî |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Set, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.3_filtered_enhanced\n",
      "Config: {'min_term_freq': 3, 'max_terms': 150000, 'embedding_batch_size': 64, 'n_clusters': 15000, 'min_cluster_size': 2, 'max_cluster_size': 10, 'similarity_threshold': 0.75}\n"
     ]
    }
   ],
   "source": [
    "# Output directory - v21.3\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"dataset\" / \"v21.3_filtered_enhanced\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"min_term_freq\": 3,\n",
    "    \"max_terms\": 150000,  # Increased for more coverage\n",
    "    \"embedding_batch_size\": 64,\n",
    "    \"n_clusters\": 15000,  # More clusters for diversity\n",
    "    \"min_cluster_size\": 2,\n",
    "    \"max_cluster_size\": 10,\n",
    "    \"similarity_threshold\": 0.75,\n",
    "}\n",
    "print(f\"Config: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Load Diverse Korean Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[1/14] Loading Korean Wikipedia...\n",
      "  ‚úì Wikipedia: 96,359 texts\n",
      "\n",
      "[2/14] Loading KLUE-MRC...\n",
      "  ‚úì KLUE-MRC: 35,108 texts\n",
      "\n",
      "[3/14] Loading KorQuAD...\n",
      "  ‚úì KorQuAD: 120,814 texts\n",
      "\n",
      "[4/14] Loading NSMC...\n",
      "  ‚úì NSMC: 134,112 texts\n",
      "\n",
      "[5/14] Loading KLUE-NLI...\n",
      "  ‚úì KLUE-NLI: 49,996 texts\n",
      "\n",
      "[6/14] Loading KLUE-STS...\n",
      "  ‚úì KLUE-STS: 23,336 texts\n",
      "\n",
      "[7/14] Loading KLUE-YNAT...\n",
      "  ‚úì KLUE-YNAT: 45,678 texts\n",
      "\n",
      "[8/14] Loading KoAlpaca...\n",
      "  ‚úì KoAlpaca: 50,000 texts\n",
      "\n",
      "[9/14] Loading Korean Law Precedents...\n",
      "  ‚úì Law Precedents: 80,000 texts\n",
      "\n",
      "============================================================\n",
      "[10-13/14] Loading KorMedMCQA (Medical) - ALL CONFIGS...\n",
      "============================================================\n",
      "\n",
      "  Loading KorMedMCQA/dentist...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473448a58df3435a83175978030dc763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/79.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2962584c844634bd6c78b8153a86ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/81.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab1f62581bb4ffca0c6a7cc5c40b80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/200k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99976361abed41a8aaef9510c9f9161b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/13.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee06dceba844b66a8fed324b9362449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/297 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658a0f1bf2a44c9b8c8d4ed986b30d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/304 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61965e125cdd41e49868896313bc4386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec16ad5574e47d59bc21eb0bfc1362c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating fewshot split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úì dentist: 297 texts\n",
      "\n",
      "  Loading KorMedMCQA/doctor...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bc736228744734ab1f2cd3e6f11da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/607k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9adc6085ee40968ccf1909ae689ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/71.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3d206fa8cb457792c2f3e69d79fdbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/174k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9716c9a12544a6e8cdfa5ec76cd14f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/19.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaefb1e0cc104a31a45ee53c8bbc1dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1890 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218555de15964c82a8b652978f215aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/164 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4928aaafe7324da6a77a38fdf25f67b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/435 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ee4130af544e5d8e8a8be1298ab78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating fewshot split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úì doctor: 1,890 texts\n",
      "\n",
      "  Loading KorMedMCQA/nurse...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0731fabe90a9456fbcb7ac26ee0ed09d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/135k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09ad50e294143e88f439c9704361da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/74.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d296fa014e3f4ac59701c4414c4f7b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/195k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4c64126ae24590a169acb607bed4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/16.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c093f75b564a5db7ee4573a2e440b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977f60c329e04854bfeeb34faa51ed5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/291 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0741a521fd4d06bdd075b7bfbf6412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/878 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8d50875bf642c3b0e03240ec9e88d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating fewshot split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úì nurse: 582 texts\n",
      "\n",
      "  Loading KorMedMCQA/pharm...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9029361f772e464198a3b39d9b0cd7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/161k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9ddd9faca6498a8624beb0632b635b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/89.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300bc94f23714c8a88d3592e13077b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/238k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046bfd0af86a4101a0d476bfa02728d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/16.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4a403550f34da8a8008600c08943ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/632 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f6dabc701f4ab3975f50a7f9c84a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644cd035133144419e1b46d383b83afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/885 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e021cdbd1554523840ddc9127491880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating fewshot split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úì pharm: 632 texts\n",
      "\n",
      "  Total Medical: 3,401 texts\n",
      "\n",
      "[14/14] Loading KorHate...\n",
      "  ‚úì KorHate: 7,896 texts\n",
      "\n",
      "============================================================\n",
      "Data Collection Summary (v21.3 - Medical Fixed)\n",
      "============================================================\n",
      "   NSMC                         134,112 texts ( 20.7%)\n",
      "   KorQuAD                      120,814 texts ( 18.7%)\n",
      "   Wikipedia                     96,359 texts ( 14.9%)\n",
      "üìö LawPrecedents                 80,000 texts ( 12.4%)\n",
      "   KoAlpaca                      50,000 texts (  7.7%)\n",
      "   KLUE-NLI                      49,996 texts (  7.7%)\n",
      "   KLUE-YNAT                     45,678 texts (  7.1%)\n",
      "   KLUE-MRC                      35,108 texts (  5.4%)\n",
      "   KLUE-STS                      23,336 texts (  3.6%)\n",
      "   KorHate                        7,896 texts (  1.2%)\n",
      "üè• KorMedMCQA-doctor              1,890 texts (  0.3%)\n",
      "üè• KorMedMCQA-pharm                 632 texts (  0.1%)\n",
      "üè• KorMedMCQA-nurse                 582 texts (  0.1%)\n",
      "üè• KorMedMCQA-dentist               297 texts (  0.0%)\n",
      "------------------------------------------------------------\n",
      "   TOTAL                        646,700 texts\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "def load_diverse_korean_datasets() -> List[str]:\n",
    "    \"\"\"Load diverse Korean text data from multiple domains.\n",
    "    \n",
    "    v21.3: Fixed medical data loading with proper config names.\n",
    "    \"\"\"\n",
    "    all_texts = []\n",
    "    domain_stats = {}\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 1. Wikipedia (Î∞±Í≥ºÏÇ¨Ï†Ñ)\n",
    "    # ========================================================================\n",
    "    print(\"=\" * 60)\n",
    "    print(\"[1/14] Loading Korean Wikipedia...\")\n",
    "    try:\n",
    "        wiki_dataset = load_dataset(\n",
    "            \"wikimedia/wikipedia\", \n",
    "            \"20231101.ko\",\n",
    "            split=\"train\",\n",
    "            streaming=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        wiki_texts = []\n",
    "        for i, item in enumerate(wiki_dataset):\n",
    "            if i >= 100000:\n",
    "                break\n",
    "            text = item.get(\"text\", \"\")\n",
    "            if text and len(text) > 100:\n",
    "                wiki_texts.append(text[:3000])\n",
    "        all_texts.extend(wiki_texts)\n",
    "        domain_stats[\"Wikipedia\"] = len(wiki_texts)\n",
    "        print(f\"  ‚úì Wikipedia: {len(wiki_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Wikipedia failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 2. KLUE-MRC (Îâ¥Ïä§ QA)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[2/14] Loading KLUE-MRC...\")\n",
    "    try:\n",
    "        klue_dataset = load_dataset(\"klue\", \"mrc\", split=\"train\", trust_remote_code=True)\n",
    "        klue_texts = []\n",
    "        for item in klue_dataset:\n",
    "            context = item.get(\"context\", \"\")\n",
    "            question = item.get(\"question\", \"\")\n",
    "            if context and len(context) > 50:\n",
    "                klue_texts.append(context[:2000])\n",
    "            if question:\n",
    "                klue_texts.append(question)\n",
    "        all_texts.extend(klue_texts)\n",
    "        domain_stats[\"KLUE-MRC\"] = len(klue_texts)\n",
    "        print(f\"  ‚úì KLUE-MRC: {len(klue_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó KLUE-MRC failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 3. KorQuAD (QA)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[3/14] Loading KorQuAD...\")\n",
    "    try:\n",
    "        korquad_dataset = load_dataset(\"squad_kor_v1\", split=\"train\", trust_remote_code=True)\n",
    "        korquad_texts = []\n",
    "        for item in korquad_dataset:\n",
    "            context = item.get(\"context\", \"\")\n",
    "            question = item.get(\"question\", \"\")\n",
    "            if context and len(context) > 50:\n",
    "                korquad_texts.append(context[:2000])\n",
    "            if question:\n",
    "                korquad_texts.append(question)\n",
    "        all_texts.extend(korquad_texts)\n",
    "        domain_stats[\"KorQuAD\"] = len(korquad_texts)\n",
    "        print(f\"  ‚úì KorQuAD: {len(korquad_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó KorQuAD failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 4. NSMC (Î¶¨Î∑∞)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[4/14] Loading NSMC...\")\n",
    "    try:\n",
    "        nsmc_dataset = load_dataset(\"nsmc\", split=\"train\", trust_remote_code=True)\n",
    "        nsmc_texts = [item.get(\"document\", \"\") for item in nsmc_dataset \n",
    "                     if item.get(\"document\") and len(item.get(\"document\", \"\")) > 10]\n",
    "        all_texts.extend(nsmc_texts)\n",
    "        domain_stats[\"NSMC\"] = len(nsmc_texts)\n",
    "        print(f\"  ‚úì NSMC: {len(nsmc_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó NSMC failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 5-8. KLUE Tasks (NLI, STS, YNAT)\n",
    "    # ========================================================================\n",
    "    klue_tasks = [\n",
    "        (\"nli\", [\"premise\", \"hypothesis\"]),\n",
    "        (\"sts\", [\"sentence1\", \"sentence2\"]),\n",
    "        (\"ynat\", [\"title\"]),\n",
    "    ]\n",
    "    for idx, (task, fields) in enumerate(klue_tasks, 5):\n",
    "        print(f\"\\n[{idx}/14] Loading KLUE-{task.upper()}...\")\n",
    "        try:\n",
    "            dataset = load_dataset(\"klue\", task, split=\"train\", trust_remote_code=True)\n",
    "            task_texts = []\n",
    "            for item in dataset:\n",
    "                for field in fields:\n",
    "                    if item.get(field):\n",
    "                        task_texts.append(item[field])\n",
    "            all_texts.extend(task_texts)\n",
    "            domain_stats[f\"KLUE-{task.upper()}\"] = len(task_texts)\n",
    "            print(f\"  ‚úì KLUE-{task.upper()}: {len(task_texts):,} texts\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó KLUE-{task.upper()} failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 9. KoAlpaca (ÏßÄÏãúÎ¨∏)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[8/14] Loading KoAlpaca...\")\n",
    "    try:\n",
    "        alpaca_dataset = load_dataset(\"Bingsu/ko_alpaca_data\", split=\"train\", trust_remote_code=True)\n",
    "        alpaca_texts = []\n",
    "        for item in alpaca_dataset:\n",
    "            if item.get(\"instruction\"): \n",
    "                alpaca_texts.append(item[\"instruction\"])\n",
    "            if item.get(\"output\") and len(item.get(\"output\", \"\")) > 20:\n",
    "                alpaca_texts.append(item[\"output\"][:1000])\n",
    "        alpaca_texts = alpaca_texts[:50000]\n",
    "        all_texts.extend(alpaca_texts)\n",
    "        domain_stats[\"KoAlpaca\"] = len(alpaca_texts)\n",
    "        print(f\"  ‚úì KoAlpaca: {len(alpaca_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó KoAlpaca failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 10. Korean Law Precedents (Î≤ïÎ•†)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[9/14] Loading Korean Law Precedents...\")\n",
    "    try:\n",
    "        law_precedents = load_dataset(\n",
    "            \"joonhok-exo-ai/korean_law_open_data_precedents\",\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        law_texts = []\n",
    "        for item in law_precedents:\n",
    "            for field in [\"ÌåêÏãúÏÇ¨Ìï≠\", \"ÌåêÍ≤∞ÏöîÏßÄ\", \"Ï†ÑÎ¨∏\", \"ÏÇ¨Í±¥Î™Ö\", \"ÏÇ¨Í±¥Í∞úÏöî\"]:\n",
    "                text = item.get(field, \"\")\n",
    "                if text and len(text) > 30:\n",
    "                    law_texts.append(text[:2000])\n",
    "        law_texts = law_texts[:80000]\n",
    "        all_texts.extend(law_texts)\n",
    "        domain_stats[\"LawPrecedents\"] = len(law_texts)\n",
    "        print(f\"  ‚úì Law Precedents: {len(law_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Law Precedents failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 11-14. KorMedMCQA - ALL 4 CONFIGS (ÏùòÎ£å) - FIXED!\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"[10-13/14] Loading KorMedMCQA (Medical) - ALL CONFIGS...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    medical_configs = [\"dentist\", \"doctor\", \"nurse\", \"pharm\"]\n",
    "    total_medical = 0\n",
    "    \n",
    "    for config in medical_configs:\n",
    "        print(f\"\\n  Loading KorMedMCQA/{config}...\")\n",
    "        try:\n",
    "            med_dataset = load_dataset(\n",
    "                \"sean0042/KorMedMCQA\",\n",
    "                config,  # Specify config name!\n",
    "                split=\"train\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            med_texts = []\n",
    "            for item in med_dataset:\n",
    "                # Extract question\n",
    "                if item.get(\"question\"):\n",
    "                    med_texts.append(item[\"question\"])\n",
    "                # Extract options (list of choices)\n",
    "                if item.get(\"options\"):\n",
    "                    options = item[\"options\"]\n",
    "                    if isinstance(options, list):\n",
    "                        for opt in options:\n",
    "                            if isinstance(opt, str) and len(opt) > 5:\n",
    "                                med_texts.append(opt)\n",
    "                # Extract answer/explanation\n",
    "                for field in [\"answer\", \"explanation\"]:\n",
    "                    text = item.get(field, \"\")\n",
    "                    if isinstance(text, str) and len(text) > 10:\n",
    "                        med_texts.append(text[:1000])\n",
    "            \n",
    "            all_texts.extend(med_texts)\n",
    "            domain_stats[f\"KorMedMCQA-{config}\"] = len(med_texts)\n",
    "            total_medical += len(med_texts)\n",
    "            print(f\"    ‚úì {config}: {len(med_texts):,} texts\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚úó {config} failed: {e}\")\n",
    "    \n",
    "    print(f\"\\n  Total Medical: {total_medical:,} texts\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 15. KorHate (ÎåÄÌôî)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[14/14] Loading KorHate...\")\n",
    "    try:\n",
    "        hate_dataset = load_dataset(\"kor_hate\", split=\"train\", trust_remote_code=True)\n",
    "        hate_texts = [item.get(\"comments\", \"\") for item in hate_dataset if item.get(\"comments\")]\n",
    "        all_texts.extend(hate_texts)\n",
    "        domain_stats[\"KorHate\"] = len(hate_texts)\n",
    "        print(f\"  ‚úì KorHate: {len(hate_texts):,} texts\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó KorHate failed: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Summary\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Data Collection Summary (v21.3 - Medical Fixed)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total = 0\n",
    "    for domain, count in sorted(domain_stats.items(), key=lambda x: -x[1]):\n",
    "        pct = count / sum(domain_stats.values()) * 100 if sum(domain_stats.values()) > 0 else 0\n",
    "        marker = \"üìö\" if \"Law\" in domain else \"üè•\" if \"Med\" in domain else \"  \"\n",
    "        print(f\"{marker} {domain:25} {count:>10,} texts ({pct:5.1f}%)\")\n",
    "        total += count\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"   {'TOTAL':25} {total:>10,} texts\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return all_texts\n",
    "\n",
    "texts = load_diverse_korean_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Extract Korean Terms with Kiwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Kiwi morphological analyzer...\n",
      "Kiwi loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantization is not supported for ArchType::neon. Fall back to non-quantized model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: skt/A.X-Encoder-base\n",
      "Vocab size: 49,999\n",
      "\n",
      "Test text: ÎãπÎá®Î≥ë ÌôòÏûêÏùò ÌòàÎãπ Ï°∞Ï†àÏùÑ ÏúÑÌïú Ïù∏ÏäêÎ¶∞ Ìà¨Ïó¨ Î∞©Î≤ï\n",
      "Extracted nouns: ['ÎãπÎá®Î≥ë', 'ÌôòÏûê', 'ÌòàÎãπ', 'Ï°∞Ï†à', 'Ïù∏ÏäêÎ¶∞', 'Ìà¨Ïó¨', 'Î∞©Î≤ï']\n",
      "Compound nouns: ['ÎãπÎá®Î≥ëÌôòÏûê', 'ÌòàÎãπÏ°∞Ï†à', 'Ïù∏ÏäêÎ¶∞Ìà¨Ïó¨Î∞©Î≤ï']\n"
     ]
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize Kiwi\n",
    "print(\"Loading Kiwi morphological analyzer...\")\n",
    "kiwi = Kiwi()\n",
    "print(\"Kiwi loaded successfully\")\n",
    "\n",
    "# Load tokenizer\n",
    "MODEL_NAME = \"skt/A.X-Encoder-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer: {MODEL_NAME}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "# POS tags to keep\n",
    "VALID_POS_TAGS = {'NNG', 'NNP', 'NNB', 'SL', 'SH'}\n",
    "\n",
    "def extract_nouns_with_kiwi(text: str, kiwi_instance: Kiwi) -> List[str]:\n",
    "    \"\"\"Extract nouns using Kiwi.\"\"\"\n",
    "    try:\n",
    "        result = kiwi_instance.tokenize(text)\n",
    "        nouns = []\n",
    "        for token in result:\n",
    "            if token.tag in VALID_POS_TAGS:\n",
    "                word = token.form.strip()\n",
    "                if 2 <= len(word) <= 15:\n",
    "                    nouns.append(word)\n",
    "        return nouns\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def extract_compound_nouns(text: str, kiwi_instance: Kiwi) -> List[str]:\n",
    "    \"\"\"Extract compound nouns.\"\"\"\n",
    "    try:\n",
    "        result = kiwi_instance.tokenize(text)\n",
    "        compounds = []\n",
    "        current_compound = []\n",
    "        \n",
    "        for token in result:\n",
    "            if token.tag in {'NNG', 'NNP', 'SL', 'SH'}:\n",
    "                current_compound.append(token.form)\n",
    "            else:\n",
    "                if len(current_compound) >= 2:\n",
    "                    compound = ''.join(current_compound)\n",
    "                    if 2 <= len(compound) <= 15:\n",
    "                        compounds.append(compound)\n",
    "                current_compound = []\n",
    "        \n",
    "        if len(current_compound) >= 2:\n",
    "            compound = ''.join(current_compound)\n",
    "            if 2 <= len(compound) <= 15:\n",
    "                compounds.append(compound)\n",
    "        \n",
    "        return compounds\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# Test\n",
    "test_text = \"ÎãπÎá®Î≥ë ÌôòÏûêÏùò ÌòàÎãπ Ï°∞Ï†àÏùÑ ÏúÑÌïú Ïù∏ÏäêÎ¶∞ Ìà¨Ïó¨ Î∞©Î≤ï\"\n",
    "print(f\"\\nTest text: {test_text}\")\n",
    "print(f\"Extracted nouns: {extract_nouns_with_kiwi(test_text, kiwi)}\")\n",
    "print(f\"Compound nouns: {extract_compound_nouns(test_text, kiwi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting terms from 646,700 texts...\n",
      "Processing text 0/646,700...\n",
      "Processing text 20,000/646,700...\n",
      "Processing text 40,000/646,700...\n",
      "Processing text 60,000/646,700...\n",
      "Processing text 80,000/646,700...\n",
      "Processing text 100,000/646,700...\n",
      "Processing text 120,000/646,700...\n",
      "Processing text 140,000/646,700...\n",
      "Processing text 160,000/646,700...\n",
      "Processing text 180,000/646,700...\n"
     ]
    }
   ],
   "source": [
    "def extract_terms(texts: List[str], kiwi_instance: Kiwi, min_freq: int = 3) -> List[Tuple[str, int]]:\n",
    "    \"\"\"Extract Korean terms using morphological analysis.\"\"\"\n",
    "    term_freq = Counter()\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        if i % 20000 == 0:\n",
    "            print(f\"Processing text {i:,}/{len(texts):,}...\")\n",
    "        \n",
    "        nouns = extract_nouns_with_kiwi(text, kiwi_instance)\n",
    "        for noun in nouns:\n",
    "            term_freq[noun] += 1\n",
    "        \n",
    "        compounds = extract_compound_nouns(text, kiwi_instance)\n",
    "        for compound in compounds:\n",
    "            term_freq[compound] += 1\n",
    "    \n",
    "    filtered_terms = [\n",
    "        (term, freq) for term, freq in term_freq.items() \n",
    "        if freq >= min_freq\n",
    "    ]\n",
    "    filtered_terms.sort(key=lambda x: -x[1])\n",
    "    \n",
    "    return filtered_terms\n",
    "\n",
    "print(f\"\\nExtracting terms from {len(texts):,} texts...\")\n",
    "terms_with_freq = extract_terms(texts, kiwi, CONFIG[\"min_term_freq\"])\n",
    "\n",
    "print(f\"\\nExtracted {len(terms_with_freq):,} unique terms\")\n",
    "print(f\"\\nTop 30 terms:\")\n",
    "for term, freq in terms_with_freq[:30]:\n",
    "    print(f\"  {term}: {freq:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Compute Embeddings with BGE-M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import torch\n",
    "\n",
    "# Limit terms\n",
    "terms = [t[0] for t in terms_with_freq[:CONFIG[\"max_terms\"]]]\n",
    "print(f\"Processing {len(terms):,} terms for embeddings\")\n",
    "\n",
    "# Load BGE-M3\n",
    "print(\"\\nLoading BGE-M3 model...\")\n",
    "bge_model = BGEM3FlagModel(\n",
    "    \"BAAI/bge-m3\",\n",
    "    use_fp16=True,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"BGE-M3 loaded on {bge_model.device}\")\n",
    "\n",
    "def compute_embeddings(terms: List[str], model, batch_size: int = 64) -> np.ndarray:\n",
    "    \"\"\"Compute BGE-M3 embeddings.\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(terms), batch_size):\n",
    "        batch = terms[i:i + batch_size]\n",
    "        if i % 5000 == 0:\n",
    "            print(f\"Embedding batch {i:,}/{len(terms):,}...\")\n",
    "        \n",
    "        output = model.encode(\n",
    "            batch,\n",
    "            return_dense=True,\n",
    "            return_sparse=False,\n",
    "            return_colbert_vecs=False\n",
    "        )\n",
    "        embeddings = output[\"dense_vecs\"]\n",
    "        all_embeddings.append(embeddings)\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "embeddings = compute_embeddings(terms, bge_model, CONFIG[\"embedding_batch_size\"])\n",
    "print(f\"\\nEmbeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Normalize\n",
    "embeddings_normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Clustering\n",
    "n_clusters = min(CONFIG[\"n_clusters\"], len(terms) // 2)\n",
    "print(f\"Clustering {len(terms):,} terms into {n_clusters:,} clusters...\")\n",
    "\n",
    "kmeans = MiniBatchKMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    batch_size=1024,\n",
    "    n_init=3,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "cluster_labels = kmeans.fit_predict(embeddings_normalized)\n",
    "print(f\"\\nClustering complete\")\n",
    "\n",
    "# Group by cluster\n",
    "clusters = defaultdict(list)\n",
    "for i, label in enumerate(cluster_labels):\n",
    "    clusters[label].append((terms[i], i))\n",
    "\n",
    "# Filter\n",
    "valid_clusters = {\n",
    "    label: terms_list \n",
    "    for label, terms_list in clusters.items()\n",
    "    if CONFIG[\"min_cluster_size\"] <= len(terms_list) <= CONFIG[\"max_cluster_size\"]\n",
    "}\n",
    "print(f\"Valid clusters: {len(valid_clusters):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Extract Synonym Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_synonym_pairs_from_clusters(\n",
    "    valid_clusters: Dict[int, List[Tuple[str, int]]],\n",
    "    embeddings_normalized: np.ndarray,\n",
    "    similarity_threshold: float = 0.75\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Extract synonym pairs from clusters.\"\"\"\n",
    "    synonym_pairs = []\n",
    "    \n",
    "    for cluster_id, terms_list in valid_clusters.items():\n",
    "        if len(terms_list) < 2:\n",
    "            continue\n",
    "        \n",
    "        cluster_terms = [t[0] for t in terms_list]\n",
    "        cluster_indices = [t[1] for t in terms_list]\n",
    "        cluster_embeddings = embeddings_normalized[cluster_indices]\n",
    "        \n",
    "        similarities = cosine_similarity(cluster_embeddings)\n",
    "        \n",
    "        for i in range(len(cluster_terms)):\n",
    "            for j in range(i + 1, len(cluster_terms)):\n",
    "                sim = similarities[i][j]\n",
    "                if sim >= similarity_threshold:\n",
    "                    synonym_pairs.append({\n",
    "                        \"source\": cluster_terms[i],\n",
    "                        \"target\": cluster_terms[j],\n",
    "                        \"similarity\": float(sim),\n",
    "                        \"relation\": \"synonym\",\n",
    "                        \"category\": \"cluster\"\n",
    "                    })\n",
    "                    synonym_pairs.append({\n",
    "                        \"source\": cluster_terms[j],\n",
    "                        \"target\": cluster_terms[i],\n",
    "                        \"similarity\": float(sim),\n",
    "                        \"relation\": \"synonym\",\n",
    "                        \"category\": \"cluster\"\n",
    "                    })\n",
    "    \n",
    "    return synonym_pairs\n",
    "\n",
    "print(f\"Extracting synonym pairs (similarity >= {CONFIG['similarity_threshold']})...\")\n",
    "cluster_synonym_pairs = extract_synonym_pairs_from_clusters(\n",
    "    valid_clusters, \n",
    "    embeddings_normalized,\n",
    "    CONFIG[\"similarity_threshold\"]\n",
    ")\n",
    "print(f\"Extracted {len(cluster_synonym_pairs):,} synonym pairs from clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Save Raw Data (Before Filtering)\n",
    "\n",
    "Save the raw synonym pairs and corpus for the filtering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "seen = set()\n",
    "unique_pairs = []\n",
    "for pair in cluster_synonym_pairs:\n",
    "    if pair[\"source\"] == pair[\"target\"]:\n",
    "        continue\n",
    "    key = (pair[\"source\"], pair[\"target\"])\n",
    "    if key not in seen:\n",
    "        seen.add(key)\n",
    "        unique_pairs.append(pair)\n",
    "\n",
    "print(f\"Unique pairs after deduplication: {len(unique_pairs):,}\")\n",
    "\n",
    "# Sort by similarity\n",
    "unique_pairs.sort(key=lambda x: -x.get(\"similarity\", 0))\n",
    "\n",
    "# Save raw pairs (before IG/PMI filtering)\n",
    "raw_pairs_path = OUTPUT_DIR / \"raw_synonym_pairs.jsonl\"\n",
    "with open(raw_pairs_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for pair in unique_pairs:\n",
    "        f.write(json.dumps(pair, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved raw pairs to: {raw_pairs_path}\")\n",
    "\n",
    "# Save corpus texts for PMI calculation\n",
    "corpus_path = OUTPUT_DIR / \"corpus_texts.jsonl\"\n",
    "with open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for text in texts:\n",
    "        f.write(json.dumps({\"text\": text}, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved corpus to: {corpus_path}\")\n",
    "\n",
    "# Save embeddings\n",
    "np.save(OUTPUT_DIR / \"term_embeddings.npy\", embeddings_normalized)\n",
    "print(f\"Saved embeddings: {embeddings_normalized.shape}\")\n",
    "\n",
    "# Save term list\n",
    "with open(OUTPUT_DIR / \"term_list.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(terms, f, ensure_ascii=False)\n",
    "print(f\"Saved term list: {len(terms):,} terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Data collection complete. Next step: `01_noise_filtering.ipynb` to apply:\n",
    "- Information Gain filtering\n",
    "- PMI filtering\n",
    "- Cross-encoder reranking\n",
    "\n",
    "### Output Files\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `raw_synonym_pairs.jsonl` | Raw synonym pairs (before filtering) |\n",
    "| `corpus_texts.jsonl` | Corpus for PMI calculation |\n",
    "| `term_embeddings.npy` | Normalized BGE-M3 embeddings |\n",
    "| `term_list.json` | List of terms |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

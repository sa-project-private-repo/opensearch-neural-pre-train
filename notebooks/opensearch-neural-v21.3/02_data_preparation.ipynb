{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v21.3 Data Preparation - Triplet Dataset with Hard Negatives\n",
    "\n",
    "This notebook creates the training dataset from filtered synonym pairs.\n",
    "\n",
    "## Features\n",
    "\n",
    "1. **Hard Negative Mining**: Balanced difficulty sampling (Easy/Medium/Hard)\n",
    "2. **Triplet Format**: (anchor, positive, negative) for contrastive learning\n",
    "3. **HuggingFace Dataset**: Save in standard format for training\n",
    "\n",
    "## Hard Negative Strategy\n",
    "\n",
    "| Difficulty | Similarity Range | Ratio |\n",
    "|------------|------------------|-------|\n",
    "| Easy | 0.3 - 0.5 | 33% |\n",
    "| Medium | 0.5 - 0.7 | 33% |\n",
    "| Hard | 0.7 - 0.9 | 33% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Configuration:\n",
      "  Easy range: (0.3, 0.5)\n",
      "  Medium range: (0.5, 0.7)\n",
      "  Hard range: (0.7, 0.9)\n",
      "  Negatives per anchor: 5\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Configuration for data preparation.\"\"\"\n",
    "    # Hard negative difficulty ranges (cosine similarity)\n",
    "    easy_range: Tuple[float, float] = (0.3, 0.5)\n",
    "    medium_range: Tuple[float, float] = (0.5, 0.7)\n",
    "    hard_range: Tuple[float, float] = (0.7, 0.9)\n",
    "    \n",
    "    # Sampling ratios\n",
    "    easy_ratio: float = 0.33\n",
    "    medium_ratio: float = 0.33\n",
    "    hard_ratio: float = 0.34\n",
    "    \n",
    "    # Number of negatives per anchor\n",
    "    negatives_per_anchor: int = 5\n",
    "    \n",
    "    # Dataset split\n",
    "    train_ratio: float = 0.9\n",
    "    val_ratio: float = 0.1\n",
    "    \n",
    "    # Batch size for embedding computation\n",
    "    batch_size: int = 128\n",
    "    \n",
    "config = DataConfig()\n",
    "print(f\"Data Configuration:\")\n",
    "print(f\"  Easy range: {config.easy_range}\")\n",
    "print(f\"  Medium range: {config.medium_range}\")\n",
    "print(f\"  Hard range: {config.hard_range}\")\n",
    "print(f\"  Negatives per anchor: {config.negatives_per_anchor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.3_filtered_enhanced\n",
      "Output directory: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.3_filtered_enhanced\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "DATA_DIR = PROJECT_ROOT / \"dataset\" / \"v21.3_filtered_enhanced\"\n",
    "OUTPUT_DIR = DATA_DIR\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Filtered Synonym Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 66,070 filtered synonym pairs\n",
      "\n",
      "Sample pairs:\n",
      "  李滉 -> 李穡 (sim=1.0000)\n",
      "  李穡 -> 李滉 (sim=1.0000)\n",
      "  李滉 -> 李塏 (sim=1.0000)\n",
      "  李塏 -> 李滉 (sim=1.0000)\n",
      "  李滉 -> 李芑 (sim=1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Load filtered synonym pairs from 01_noise_filtering.ipynb\n",
    "filtered_pairs_path = DATA_DIR / \"filtered_synonym_pairs.jsonl\"\n",
    "\n",
    "if not filtered_pairs_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Filtered pairs file not found: {filtered_pairs_path}\\n\"\n",
    "        \"Please run 01_noise_filtering.ipynb first.\"\n",
    "    )\n",
    "\n",
    "synonym_pairs = []\n",
    "with open(filtered_pairs_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            synonym_pairs.append(json.loads(line.strip()))\n",
    "\n",
    "print(f\"Loaded {len(synonym_pairs):,} filtered synonym pairs\")\n",
    "\n",
    "if len(synonym_pairs) == 0:\n",
    "    raise ValueError(\n",
    "        \"filtered_synonym_pairs.jsonl is empty!\\n\"\n",
    "        \"Please re-run cells 30-31 in 01_noise_filtering.ipynb to save the filtered pairs.\"\n",
    "    )\n",
    "\n",
    "# Sample\n",
    "print(\"\\nSample pairs:\")\n",
    "for pair in synonym_pairs[:5]:\n",
    "    print(f\"  {pair['source']} -> {pair['target']} (sim={pair['similarity']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (150000, 1024)\n",
      "Terms count: 150,000\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings and terms\n",
    "embeddings = np.load(DATA_DIR / \"term_embeddings.npy\")\n",
    "with open(DATA_DIR / \"term_list.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    terms = json.load(f)\n",
    "\n",
    "term_to_idx = {term: idx for idx, term in enumerate(terms)}\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Terms count: {len(terms):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Anchor-Positive Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique anchors: 28,371\n",
      "\n",
      "Positives per anchor:\n",
      "  Mean: 2.47\n",
      "  Min: 1\n",
      "  Max: 9\n",
      "  Median: 2\n"
     ]
    }
   ],
   "source": [
    "# Build anchor to positives mapping\n",
    "anchor_to_positives = defaultdict(set)\n",
    "\n",
    "for pair in synonym_pairs:\n",
    "    source, target = pair[\"source\"], pair[\"target\"]\n",
    "    \n",
    "    # Skip if not in vocabulary\n",
    "    if source not in term_to_idx or target not in term_to_idx:\n",
    "        continue\n",
    "    \n",
    "    anchor_to_positives[source].add(target)\n",
    "    anchor_to_positives[target].add(source)  # Bidirectional\n",
    "\n",
    "print(f\"Unique anchors: {len(anchor_to_positives):,}\")\n",
    "\n",
    "# Check if we have data\n",
    "if len(anchor_to_positives) == 0:\n",
    "    raise ValueError(\n",
    "        \"No anchors found! Please run 01_noise_filtering.ipynb first to generate \"\n",
    "        \"filtered_synonym_pairs.jsonl\"\n",
    "    )\n",
    "\n",
    "# Distribution of positives per anchor\n",
    "positive_counts = [len(v) for v in anchor_to_positives.values()]\n",
    "print(f\"\\nPositives per anchor:\")\n",
    "print(f\"  Mean: {np.mean(positive_counts):.2f}\")\n",
    "print(f\"  Min: {np.min(positive_counts)}\")\n",
    "print(f\"  Max: {np.max(positive_counts)}\")\n",
    "print(f\"  Median: {np.median(positive_counts):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hard Negative Mining\n",
    "\n",
    "Sample negatives based on similarity difficulty levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test anchor: 李滉\n",
      "Positives: ['李傕', '李穡', '李塏', '李芑']\n",
      "Easy negatives: 65220\n",
      "Medium negatives: 102\n",
      "Hard negatives: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def mine_hard_negatives(\n",
    "    anchor: str,\n",
    "    positives: set,\n",
    "    embeddings: np.ndarray,\n",
    "    term_to_idx: Dict[str, int],\n",
    "    terms: List[str],\n",
    "    config: DataConfig,\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Mine hard negatives for an anchor at different difficulty levels.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with keys 'easy', 'medium', 'hard', each containing a list of negatives.\n",
    "    \"\"\"\n",
    "    anchor_idx = term_to_idx[anchor]\n",
    "    anchor_emb = embeddings[anchor_idx:anchor_idx+1]\n",
    "    \n",
    "    # Compute similarity to all terms\n",
    "    similarities = cosine_similarity(anchor_emb, embeddings)[0]\n",
    "    \n",
    "    # Get candidate negatives (not anchor, not positives)\n",
    "    exclude = positives | {anchor}\n",
    "    \n",
    "    easy_negatives = []\n",
    "    medium_negatives = []\n",
    "    hard_negatives = []\n",
    "    \n",
    "    for idx, sim in enumerate(similarities):\n",
    "        term = terms[idx]\n",
    "        if term in exclude:\n",
    "            continue\n",
    "        \n",
    "        # Categorize by difficulty\n",
    "        if config.easy_range[0] <= sim < config.easy_range[1]:\n",
    "            easy_negatives.append((term, sim))\n",
    "        elif config.medium_range[0] <= sim < config.medium_range[1]:\n",
    "            medium_negatives.append((term, sim))\n",
    "        elif config.hard_range[0] <= sim < config.hard_range[1]:\n",
    "            hard_negatives.append((term, sim))\n",
    "    \n",
    "    return {\n",
    "        \"easy\": easy_negatives,\n",
    "        \"medium\": medium_negatives,\n",
    "        \"hard\": hard_negatives,\n",
    "    }\n",
    "\n",
    "# Test with one anchor\n",
    "test_anchor = list(anchor_to_positives.keys())[0]\n",
    "test_positives = anchor_to_positives[test_anchor]\n",
    "test_negatives = mine_hard_negatives(\n",
    "    test_anchor, test_positives, embeddings, term_to_idx, terms, config\n",
    ")\n",
    "\n",
    "print(f\"Test anchor: {test_anchor}\")\n",
    "print(f\"Positives: {list(test_positives)[:5]}\")\n",
    "print(f\"Easy negatives: {len(test_negatives['easy'])}\")\n",
    "print(f\"Medium negatives: {len(test_negatives['medium'])}\")\n",
    "print(f\"Hard negatives: {len(test_negatives['hard'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled negatives:\n",
      "  lo (sim=0.438, easy)\n",
      "  諡號 (sim=0.611, medium)\n",
      "  기부 (sim=0.328, easy)\n",
      "  이집 (sim=0.353, easy)\n",
      "  데뷔무대 (sim=0.305, easy)\n"
     ]
    }
   ],
   "source": [
    "def sample_balanced_negatives(\n",
    "    negatives_by_difficulty: Dict[str, List[Tuple[str, float]]],\n",
    "    n_total: int,\n",
    "    config: DataConfig,\n",
    ") -> List[Tuple[str, float, str]]:\n",
    "    \"\"\"\n",
    "    Sample negatives with balanced difficulty.\n",
    "    \n",
    "    Returns:\n",
    "        List of (negative_term, similarity, difficulty) tuples.\n",
    "    \"\"\"\n",
    "    n_easy = int(n_total * config.easy_ratio)\n",
    "    n_medium = int(n_total * config.medium_ratio)\n",
    "    n_hard = n_total - n_easy - n_medium\n",
    "    \n",
    "    sampled = []\n",
    "    \n",
    "    # Sample easy\n",
    "    if negatives_by_difficulty[\"easy\"]:\n",
    "        n_sample = min(n_easy, len(negatives_by_difficulty[\"easy\"]))\n",
    "        for term, sim in random.sample(negatives_by_difficulty[\"easy\"], n_sample):\n",
    "            sampled.append((term, sim, \"easy\"))\n",
    "    \n",
    "    # Sample medium\n",
    "    if negatives_by_difficulty[\"medium\"]:\n",
    "        n_sample = min(n_medium, len(negatives_by_difficulty[\"medium\"]))\n",
    "        for term, sim in random.sample(negatives_by_difficulty[\"medium\"], n_sample):\n",
    "            sampled.append((term, sim, \"medium\"))\n",
    "    \n",
    "    # Sample hard\n",
    "    if negatives_by_difficulty[\"hard\"]:\n",
    "        n_sample = min(n_hard, len(negatives_by_difficulty[\"hard\"]))\n",
    "        for term, sim in random.sample(negatives_by_difficulty[\"hard\"], n_sample):\n",
    "            sampled.append((term, sim, \"hard\"))\n",
    "    \n",
    "    # If not enough, fill from available\n",
    "    if len(sampled) < n_total:\n",
    "        all_negatives = (\n",
    "            negatives_by_difficulty[\"easy\"] + \n",
    "            negatives_by_difficulty[\"medium\"] + \n",
    "            negatives_by_difficulty[\"hard\"]\n",
    "        )\n",
    "        already_sampled = {s[0] for s in sampled}\n",
    "        remaining = [n for n in all_negatives if n[0] not in already_sampled]\n",
    "        \n",
    "        n_need = n_total - len(sampled)\n",
    "        for term, sim in random.sample(remaining, min(n_need, len(remaining))):\n",
    "            # Determine difficulty\n",
    "            if config.easy_range[0] <= sim < config.easy_range[1]:\n",
    "                difficulty = \"easy\"\n",
    "            elif config.medium_range[0] <= sim < config.medium_range[1]:\n",
    "                difficulty = \"medium\"\n",
    "            else:\n",
    "                difficulty = \"hard\"\n",
    "            sampled.append((term, sim, difficulty))\n",
    "    \n",
    "    return sampled\n",
    "\n",
    "# Test\n",
    "test_sampled = sample_balanced_negatives(\n",
    "    test_negatives, config.negatives_per_anchor, config\n",
    ")\n",
    "print(f\"Sampled negatives:\")\n",
    "for neg, sim, diff in test_sampled:\n",
    "    print(f\"  {neg} (sim={sim:.3f}, {diff})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Triplet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1febfb9dfb49be84607747e0708b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating triplets:   0%|          | 0/28371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 350,810 triplets\n",
      "Skipped anchors (no negatives): 0\n",
      "\n",
      "Difficulty distribution:\n",
      "  hard: 184,536 (52.6%)\n",
      "  easy: 95,888 (27.3%)\n",
      "  medium: 70,386 (20.1%)\n"
     ]
    }
   ],
   "source": [
    "# Create triplets for all anchors\n",
    "triplets = []\n",
    "skipped = 0\n",
    "difficulty_stats = Counter()\n",
    "\n",
    "for anchor in tqdm(anchor_to_positives.keys(), desc=\"Creating triplets\"):\n",
    "    positives = anchor_to_positives[anchor]\n",
    "    \n",
    "    # Mine negatives\n",
    "    negatives_by_difficulty = mine_hard_negatives(\n",
    "        anchor, positives, embeddings, term_to_idx, terms, config\n",
    "    )\n",
    "    \n",
    "    # Sample balanced negatives\n",
    "    sampled_negatives = sample_balanced_negatives(\n",
    "        negatives_by_difficulty, config.negatives_per_anchor, config\n",
    "    )\n",
    "    \n",
    "    if not sampled_negatives:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    \n",
    "    # Create triplets: one for each positive-negative pair\n",
    "    for positive in positives:\n",
    "        for negative, neg_sim, difficulty in sampled_negatives:\n",
    "            triplet = {\n",
    "                \"anchor\": anchor,\n",
    "                \"positive\": positive,\n",
    "                \"negative\": negative,\n",
    "                \"negative_similarity\": neg_sim,\n",
    "                \"difficulty\": difficulty,\n",
    "            }\n",
    "            triplets.append(triplet)\n",
    "            difficulty_stats[difficulty] += 1\n",
    "\n",
    "print(f\"\\nCreated {len(triplets):,} triplets\")\n",
    "print(f\"Skipped anchors (no negatives): {skipped:,}\")\n",
    "print(f\"\\nDifficulty distribution:\")\n",
    "for diff, count in difficulty_stats.most_common():\n",
    "    print(f\"  {diff}: {count:,} ({100*count/len(triplets):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample triplets:\n",
      "  Anchor: 특수목적\n",
      "  Positive: 특수교육\n",
      "  Negative: 소액 (sim=0.508, medium)\n",
      "\n",
      "  Anchor: 독점계약\n",
      "  Positive: 독점규제\n",
      "  Negative: 대내외 (sim=0.373, easy)\n",
      "\n",
      "  Anchor: 본건매매목적물\n",
      "  Positive: 본건경매목적물\n",
      "  Negative: 설립중 (sim=0.480, easy)\n",
      "\n",
      "  Anchor: 니콜라이\n",
      "  Positive: 성 니콜라우스\n",
      "  Negative: 니콜 (sim=0.760, hard)\n",
      "\n",
      "  Anchor: 야당\n",
      "  Positive: 야당정치인\n",
      "  Negative: 야수 (sim=0.708, hard)\n",
      "\n",
      "  Anchor: 바이어 레버쿠젠\n",
      "  Positive: 레버쿠젠\n",
      "  Negative: 배드민턴 (sim=0.505, medium)\n",
      "\n",
      "  Anchor: 사천면\n",
      "  Positive: 사천\n",
      "  Negative: 조짐 (sim=0.312, easy)\n",
      "\n",
      "  Anchor: 우호지분\n",
      "  Positive: 우호\n",
      "  Negative: 예비학교 (sim=0.358, easy)\n",
      "\n",
      "  Anchor: 공사금채무\n",
      "  Positive: 공사금채권\n",
      "  Negative: 기준임금 (sim=0.506, medium)\n",
      "\n",
      "  Anchor: 남계리\n",
      "  Positive: 금계리\n",
      "  Negative: 남양리 (sim=0.707, hard)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample triplets\n",
    "print(\"\\nSample triplets:\")\n",
    "for triplet in random.sample(triplets, min(10, len(triplets))):\n",
    "    print(f\"  Anchor: {triplet['anchor']}\")\n",
    "    print(f\"  Positive: {triplet['positive']}\")\n",
    "    print(f\"  Negative: {triplet['negative']} (sim={triplet['negative_similarity']:.3f}, {triplet['difficulty']})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Split into Train/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split:\n",
      "  Train: 315,729 (90.0%)\n",
      "  Validation: 35,081 (10.0%)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle triplets\n",
    "random.shuffle(triplets)\n",
    "\n",
    "# Split\n",
    "n_train = int(len(triplets) * config.train_ratio)\n",
    "train_triplets = triplets[:n_train]\n",
    "val_triplets = triplets[n_train:]\n",
    "\n",
    "print(f\"Dataset split:\")\n",
    "print(f\"  Train: {len(train_triplets):,} ({100*len(train_triplets)/len(triplets):.1f}%)\")\n",
    "print(f\"  Validation: {len(val_triplets):,} ({100*len(val_triplets)/len(triplets):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save as HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['anchor', 'positive', 'negative', 'negative_similarity', 'difficulty'],\n",
      "        num_rows: 315729\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['anchor', 'positive', 'negative', 'negative_similarity', 'difficulty'],\n",
      "        num_rows: 35081\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Convert to datasets format\n",
    "def triplets_to_dict(triplets: List[Dict]) -> Dict[str, List]:\n",
    "    \"\"\"Convert list of triplet dicts to dict of lists.\"\"\"\n",
    "    return {\n",
    "        \"anchor\": [t[\"anchor\"] for t in triplets],\n",
    "        \"positive\": [t[\"positive\"] for t in triplets],\n",
    "        \"negative\": [t[\"negative\"] for t in triplets],\n",
    "        \"negative_similarity\": [t[\"negative_similarity\"] for t in triplets],\n",
    "        \"difficulty\": [t[\"difficulty\"] for t in triplets],\n",
    "    }\n",
    "\n",
    "train_dataset = Dataset.from_dict(triplets_to_dict(train_triplets))\n",
    "val_dataset = Dataset.from_dict(triplets_to_dict(val_triplets))\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "})\n",
    "\n",
    "print(f\"Dataset:\")\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d35cf7fcb94341917b629cf8d902c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/315729 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29910feca72d43488776e2e0815f9c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/35081 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.3_filtered_enhanced/triplet_dataset\n"
     ]
    }
   ],
   "source": [
    "# Save dataset\n",
    "dataset_path = OUTPUT_DIR / \"triplet_dataset\"\n",
    "dataset_dict.save_to_disk(str(dataset_path))\n",
    "print(f\"Saved dataset to: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train triplets to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.3_filtered_enhanced/train_triplets.jsonl\n",
      "Saved validation triplets to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.3_filtered_enhanced/val_triplets.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Also save as JSONL for compatibility\n",
    "train_jsonl = OUTPUT_DIR / \"train_triplets.jsonl\"\n",
    "with open(train_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "    for triplet in train_triplets:\n",
    "        f.write(json.dumps(triplet, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved train triplets to: {train_jsonl}\")\n",
    "\n",
    "val_jsonl = OUTPUT_DIR / \"val_triplets.jsonl\"\n",
    "with open(val_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "    for triplet in val_triplets:\n",
    "        f.write(json.dumps(triplet, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved validation triplets to: {val_jsonl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved statistics to: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/dataset/v21.3_filtered_enhanced/data_preparation_stats.json\n"
     ]
    }
   ],
   "source": [
    "# Save data preparation statistics\n",
    "stats = {\n",
    "    \"config\": {\n",
    "        \"easy_range\": list(config.easy_range),\n",
    "        \"medium_range\": list(config.medium_range),\n",
    "        \"hard_range\": list(config.hard_range),\n",
    "        \"easy_ratio\": config.easy_ratio,\n",
    "        \"medium_ratio\": config.medium_ratio,\n",
    "        \"hard_ratio\": config.hard_ratio,\n",
    "        \"negatives_per_anchor\": config.negatives_per_anchor,\n",
    "        \"train_ratio\": config.train_ratio,\n",
    "        \"val_ratio\": config.val_ratio,\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"total_triplets\": len(triplets),\n",
    "        \"train_triplets\": len(train_triplets),\n",
    "        \"val_triplets\": len(val_triplets),\n",
    "        \"unique_anchors\": len(anchor_to_positives),\n",
    "        \"skipped_anchors\": skipped,\n",
    "    },\n",
    "    \"difficulty_distribution\": dict(difficulty_stats),\n",
    "}\n",
    "\n",
    "stats_path = OUTPUT_DIR / \"data_preparation_stats.json\"\n",
    "with open(stats_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(stats, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved statistics to: {stats_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Data Preparation Complete\n",
    "\n",
    "Created triplet dataset with balanced hard negatives:\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Total Triplets | See stats above |\n",
    "| Train Set | 90% |\n",
    "| Validation Set | 10% |\n",
    "\n",
    "### Difficulty Balance\n",
    "\n",
    "| Difficulty | Range | Target Ratio |\n",
    "|------------|-------|-------------|\n",
    "| Easy | 0.3-0.5 | 33% |\n",
    "| Medium | 0.5-0.7 | 33% |\n",
    "| Hard | 0.7-0.9 | 34% |\n",
    "\n",
    "### Output Files\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `triplet_dataset/` | HuggingFace Dataset format |\n",
    "| `train_triplets.jsonl` | Training triplets (JSONL) |\n",
    "| `val_triplets.jsonl` | Validation triplets (JSONL) |\n",
    "| `data_preparation_stats.json` | Statistics |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. `03_training.ipynb`: Train SPLADE model with triplet loss\n",
    "2. `04_evaluation.ipynb`: Evaluate with Recall@K, MRR, nDCG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

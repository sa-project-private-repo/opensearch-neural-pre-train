{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v21.3 Inference Test - Korean Synonym Expansion\n",
    "\n",
    "This notebook tests the trained model's Korean synonym expansion capability.\n",
    "\n",
    "## Tests\n",
    "\n",
    "1. **General terms**: Common Korean words\n",
    "2. **Legal terms**: 법률 용어\n",
    "3. **Medical terms**: 의료 용어\n",
    "4. **Comparison with v21.2**: Performance difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "V21_3_MODEL_PATH = PROJECT_ROOT / \"outputs\" / \"v21.3_korean_enhanced\" / \"best_model.pt\"\n",
    "V21_2_MODEL_PATH = PROJECT_ROOT / \"outputs\" / \"v21.2_korean_legal_medical\" / \"best_model.pt\"\n",
    "\n",
    "print(f\"v21.3 model: {V21_3_MODEL_PATH}\")\n",
    "print(f\"v21.2 model: {V21_2_MODEL_PATH}\")\n",
    "print(f\"v21.3 exists: {V21_3_MODEL_PATH.exists()}\")\n",
    "print(f\"v21.2 exists: {V21_2_MODEL_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define SPLADE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPLADEModel(nn.Module):\n",
    "    \"\"\"SPLADE model for Korean sparse retrieval.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"skt/A.X-Encoder-base\"):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        self.config = self.model.config\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        token_scores = torch.log1p(self.relu(logits))\n",
    "        mask = attention_mask.unsqueeze(-1).float()\n",
    "        token_scores = token_scores * mask\n",
    "        sparse_repr, _ = token_scores.max(dim=1)\n",
    "        token_weights = token_scores.max(dim=-1).values\n",
    "        return sparse_repr, token_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "MODEL_NAME = \"skt/A.X-Encoder-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Tokenizer loaded: {MODEL_NAME}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load v21.3 model\n",
    "model_v21_3 = None\n",
    "if V21_3_MODEL_PATH.exists():\n",
    "    model_v21_3 = SPLADEModel(MODEL_NAME)\n",
    "    checkpoint = torch.load(V21_3_MODEL_PATH, map_location=device)\n",
    "    model_v21_3.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model_v21_3 = model_v21_3.to(device)\n",
    "    model_v21_3.eval()\n",
    "    print(f\"v21.3 model loaded\")\n",
    "    if \"eval_results\" in checkpoint:\n",
    "        print(f\"  Eval results: {checkpoint['eval_results']}\")\n",
    "else:\n",
    "    print(\"v21.3 model not found - please run 03_training.ipynb first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load v21.2 model for comparison\n",
    "model_v21_2 = None\n",
    "if V21_2_MODEL_PATH.exists():\n",
    "    model_v21_2 = SPLADEModel(MODEL_NAME)\n",
    "    checkpoint = torch.load(V21_2_MODEL_PATH, map_location=device)\n",
    "    model_v21_2.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model_v21_2 = model_v21_2.to(device)\n",
    "    model_v21_2.eval()\n",
    "    print(f\"v21.2 model loaded for comparison\")\n",
    "else:\n",
    "    print(\"v21.2 model not found - comparison will be skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Synonym Expansion Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonym_expansion(\n",
    "    model: SPLADEModel,\n",
    "    tokenizer,\n",
    "    text: str,\n",
    "    device: torch.device,\n",
    "    top_k: int = 15,\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Get top-k activated tokens for a given text.\n",
    "    \n",
    "    Returns:\n",
    "        List of (token, weight) tuples sorted by weight descending.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Build special token set\n",
    "    special_ids = {tokenizer.pad_token_id, tokenizer.cls_token_id,\n",
    "                   tokenizer.sep_token_id, tokenizer.unk_token_id}\n",
    "    special_ids = {t for t in special_ids if t is not None}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        weights, _ = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "        weights = weights[0]\n",
    "        \n",
    "        # Mask special tokens\n",
    "        for tid in special_ids:\n",
    "            weights[tid] = -float('inf')\n",
    "        \n",
    "        # Get top tokens\n",
    "        top_weights, top_indices = weights.topk(top_k)\n",
    "        \n",
    "        results = []\n",
    "        for idx, weight in zip(top_indices.tolist(), top_weights.tolist()):\n",
    "            token = tokenizer.decode([idx]).strip()\n",
    "            if token and len(token) >= 1:\n",
    "                results.append((token, weight))\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test General Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Korean terms\n",
    "general_terms = [\n",
    "    \"추천\",\n",
    "    \"검색\",\n",
    "    \"인공지능\",\n",
    "    \"기계학습\",\n",
    "    \"데이터베이스\",\n",
    "    \"컴퓨터\",\n",
    "    \"스마트폰\",\n",
    "    \"프로그래밍\",\n",
    "]\n",
    "\n",
    "if model_v21_3:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"General Terms - v21.3\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for term in general_terms:\n",
    "        expansions = get_synonym_expansion(model_v21_3, tokenizer, term, device)\n",
    "        print(f\"\\n{term}:\")\n",
    "        for token, weight in expansions[:10]:\n",
    "            print(f\"  {token}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Legal Terms (법률 용어)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legal terms\n",
    "legal_terms = [\n",
    "    \"손해배상\",\n",
    "    \"판결\",\n",
    "    \"소송\",\n",
    "    \"계약\",\n",
    "    \"위반\",\n",
    "    \"피고\",\n",
    "    \"원고\",\n",
    "    \"변호사\",\n",
    "]\n",
    "\n",
    "if model_v21_3:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Legal Terms (법률) - v21.3\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for term in legal_terms:\n",
    "        expansions = get_synonym_expansion(model_v21_3, tokenizer, term, device)\n",
    "        print(f\"\\n{term}:\")\n",
    "        for token, weight in expansions[:10]:\n",
    "            print(f\"  {token}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Medical Terms (의료 용어)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medical terms\n",
    "medical_terms = [\n",
    "    \"진단\",\n",
    "    \"치료\",\n",
    "    \"처방\",\n",
    "    \"증상\",\n",
    "    \"질환\",\n",
    "    \"당뇨병\",\n",
    "    \"고혈압\",\n",
    "    \"인슐린\",\n",
    "]\n",
    "\n",
    "if model_v21_3:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Medical Terms (의료) - v21.3\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for term in medical_terms:\n",
    "        expansions = get_synonym_expansion(model_v21_3, tokenizer, term, device)\n",
    "        print(f\"\\n{term}:\")\n",
    "        for token, weight in expansions[:10]:\n",
    "            print(f\"  {token}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare v21.3 vs v21.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases with expected synonyms\n",
    "test_cases = [\n",
    "    # General\n",
    "    (\"추천\", [\"권장\", \"제안\", \"권유\", \"소개\"]),\n",
    "    (\"검색\", [\"탐색\", \"조회\", \"찾기\", \"서치\"]),\n",
    "    (\"인공지능\", [\"AI\", \"에이아이\", \"기계지능\"]),\n",
    "    # Legal\n",
    "    (\"손해배상\", [\"배상\", \"보상\", \"책임\", \"손해\"]),\n",
    "    (\"판결\", [\"판례\", \"선고\", \"결정\", \"심판\"]),\n",
    "    # Medical\n",
    "    (\"진단\", [\"진찰\", \"검진\", \"소견\", \"판단\"]),\n",
    "    (\"치료\", [\"처치\", \"요법\", \"치유\", \"시술\"]),\n",
    "]\n",
    "\n",
    "def evaluate_model(model, model_name):\n",
    "    \"\"\"Evaluate model on test cases.\"\"\"\n",
    "    if model is None:\n",
    "        return None\n",
    "    \n",
    "    results = []\n",
    "    for source, expected in test_cases:\n",
    "        expansions = get_synonym_expansion(model, tokenizer, source, device, top_k=20)\n",
    "        top_tokens = [t for t, _ in expansions]\n",
    "        \n",
    "        # Check how many expected synonyms are in top tokens\n",
    "        hits = 0\n",
    "        for syn in expected:\n",
    "            for tok in top_tokens:\n",
    "                if syn.lower() in tok.lower() or tok.lower() in syn.lower():\n",
    "                    hits += 1\n",
    "                    break\n",
    "        \n",
    "        recall = hits / len(expected) * 100\n",
    "        results.append({\n",
    "            \"source\": source,\n",
    "            \"expected\": expected,\n",
    "            \"top_tokens\": top_tokens[:10],\n",
    "            \"hits\": hits,\n",
    "            \"recall\": recall,\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate both models\n",
    "results_v21_3 = evaluate_model(model_v21_3, \"v21.3\")\n",
    "results_v21_2 = evaluate_model(model_v21_2, \"v21.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "if results_v21_3 and results_v21_2:\n",
    "    print(\"=\" * 100)\n",
    "    print(\"Comparison: v21.3 vs v21.2\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    print(f\"\\n{'Source':<15} {'Expected':<30} {'v21.3 Recall':>12} {'v21.2 Recall':>12} {'Diff':>8}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    total_v21_3 = 0\n",
    "    total_v21_2 = 0\n",
    "    \n",
    "    for r3, r2 in zip(results_v21_3, results_v21_2):\n",
    "        diff = r3[\"recall\"] - r2[\"recall\"]\n",
    "        diff_str = f\"+{diff:.0f}%\" if diff > 0 else f\"{diff:.0f}%\"\n",
    "        \n",
    "        print(f\"{r3['source']:<15} {str(r3['expected'][:3]):<30} {r3['recall']:>10.0f}% {r2['recall']:>10.0f}% {diff_str:>8}\")\n",
    "        \n",
    "        total_v21_3 += r3[\"recall\"]\n",
    "        total_v21_2 += r2[\"recall\"]\n",
    "    \n",
    "    avg_v21_3 = total_v21_3 / len(results_v21_3)\n",
    "    avg_v21_2 = total_v21_2 / len(results_v21_2)\n",
    "    avg_diff = avg_v21_3 - avg_v21_2\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{'AVERAGE':<45} {avg_v21_3:>10.1f}% {avg_v21_2:>10.1f}% {'+' if avg_diff > 0 else ''}{avg_diff:.1f}%\")\n",
    "    \n",
    "elif results_v21_3:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"v21.3 Results (v21.2 not available for comparison)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for result in results_v21_3:\n",
    "        print(f\"\\n{result['source']}:\")\n",
    "        print(f\"  Expected: {result['expected']}\")\n",
    "        print(f\"  Top tokens: {result['top_tokens']}\")\n",
    "        print(f\"  Recall: {result['recall']:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sparsity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sparsity(model, tokenizer, texts, device):\n",
    "    \"\"\"Analyze sparsity of model outputs.\"\"\"\n",
    "    if model is None:\n",
    "        return None\n",
    "    \n",
    "    sparsity_stats = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            weights, _ = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "            weights = weights[0].cpu().numpy()\n",
    "            \n",
    "            # Count non-zero activations\n",
    "            non_zero = np.sum(weights > 0.01)\n",
    "            total = len(weights)\n",
    "            sparsity = (total - non_zero) / total * 100\n",
    "            \n",
    "            sparsity_stats.append({\n",
    "                \"text\": text,\n",
    "                \"non_zero\": non_zero,\n",
    "                \"sparsity\": sparsity,\n",
    "            })\n",
    "    \n",
    "    return sparsity_stats\n",
    "\n",
    "# Analyze sparsity\n",
    "test_texts = general_terms + legal_terms[:4] + medical_terms[:4]\n",
    "\n",
    "sparsity_v21_3 = analyze_sparsity(model_v21_3, tokenizer, test_texts, device)\n",
    "sparsity_v21_2 = analyze_sparsity(model_v21_2, tokenizer, test_texts, device)\n",
    "\n",
    "if sparsity_v21_3:\n",
    "    avg_sparsity = np.mean([s[\"sparsity\"] for s in sparsity_v21_3])\n",
    "    avg_nonzero = np.mean([s[\"non_zero\"] for s in sparsity_v21_3])\n",
    "    print(f\"\\nv21.3 Sparsity Analysis:\")\n",
    "    print(f\"  Average sparsity: {avg_sparsity:.2f}%\")\n",
    "    print(f\"  Average non-zero activations: {avg_nonzero:.1f}\")\n",
    "\n",
    "if sparsity_v21_2:\n",
    "    avg_sparsity = np.mean([s[\"sparsity\"] for s in sparsity_v21_2])\n",
    "    avg_nonzero = np.mean([s[\"non_zero\"] for s in sparsity_v21_2])\n",
    "    print(f\"\\nv21.2 Sparsity Analysis:\")\n",
    "    print(f\"  Average sparsity: {avg_sparsity:.2f}%\")\n",
    "    print(f\"  Average non-zero activations: {avg_nonzero:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "| Aspect | v21.2 | v21.3 |\n",
    "|--------|-------|-------|\n",
    "| Data Quality | ~50% noise | < 10% noise |\n",
    "| Evaluation | Saturated at 100% | Recall@K, MRR |\n",
    "| Medical Domain | Load failed | All 4 configs |\n",
    "| Hard Negatives | Random | Difficulty-balanced |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Upload to HuggingFace Hub\n",
    "2. Deploy to OpenSearch\n",
    "3. Run end-to-end retrieval evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

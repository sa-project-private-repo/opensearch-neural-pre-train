{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v21.4 HuggingFace Korean Dataset Loading\n",
    "\n",
    "Load and process Korean datasets from HuggingFace for training data augmentation.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Sequential Loading**: Memory-safe sequential loading with garbage collection\n",
    "- **Progress Tracking**: Individual progress bars for each dataset\n",
    "- **Error Handling**: Graceful failure handling - if one dataset fails, others continue\n",
    "- **Type Safety**: Full type hints throughout\n",
    "\n",
    "## Datasets\n",
    "\n",
    "| Dataset | Type | Size | Use Case |\n",
    "|---------|------|------|----------|\n",
    "| williamjeong2/msmarco-triplets-ko-v1 | Query-Doc Triplets | 50K | Direct triplet training |\n",
    "| klue (nli, sts) | NLI, STS | 45K | Semantic similarity pairs |\n",
    "| squad_kor_v1 | QA | 30K | Question-context pairs |\n",
    "| nsmc | Sentiment | 50K | Text corpus for negatives |\n",
    "| skt/kobest_v1 (copa) | COPA | 5K | Premise-alternative pairs |\n",
    "| daekeun-ml/naver-news-summarization-ko | News | 10K | Title-summary pairs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_project_root() -> Path:\n",
    "    \"\"\"Find the project root directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, Future\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set random seed\n",
    "random.seed(42)\n",
    "\n",
    "# Thread-safe lock for progress updates\n",
    "PROGRESS_LOCK = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets library available\n"
     ]
    }
   ],
   "source": [
    "# Install datasets if needed\n",
    "try:\n",
    "    from datasets import load_dataset, Dataset\n",
    "    print(\"datasets library available\")\n",
    "except ImportError:\n",
    "    print(\"Installing datasets...\")\n",
    "    %pip install datasets\n",
    "    from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace data directory: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/data/huggingface_korean\n"
     ]
    }
   ],
   "source": [
    "# Output directories\n",
    "HF_DATA_DIR = PROJECT_ROOT / \"data\" / \"huggingface_korean\"\n",
    "HF_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"HuggingFace data directory: {HF_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Classes and Type Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetResult:\n",
    "    \"\"\"Result container for a dataset loading operation.\"\"\"\n",
    "    \n",
    "    name: str\n",
    "    success: bool\n",
    "    data: List[Dict[str, Any]] = field(default_factory=list)\n",
    "    corpus: List[str] = field(default_factory=list)\n",
    "    error_message: Optional[str] = None\n",
    "    sample_count: int = 0\n",
    "    \n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Calculate sample count after initialization.\"\"\"\n",
    "        if self.data:\n",
    "            self.sample_count = len(self.data)\n",
    "        elif self.corpus:\n",
    "            self.sample_count = len(self.corpus)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LoadingProgress:\n",
    "    \"\"\"Track loading progress for a dataset.\"\"\"\n",
    "    \n",
    "    name: str\n",
    "    status: str = \"pending\"\n",
    "    current: int = 0\n",
    "    total: int = 0\n",
    "    \n",
    "    def update(self, current: int, total: int) -> None:\n",
    "        \"\"\"Update progress values.\"\"\"\n",
    "        self.current = current\n",
    "        self.total = total\n",
    "        self.status = \"loading\"\n",
    "    \n",
    "    def complete(self, success: bool = True) -> None:\n",
    "        \"\"\"Mark as completed.\"\"\"\n",
    "        self.status = \"completed\" if success else \"failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading Functions\n",
    "\n",
    "Each loader function is designed to:\n",
    "1. Load data from HuggingFace\n",
    "2. Process into standardized format\n",
    "3. Return a `DatasetResult` with success/failure status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_msmarco_korean(max_samples: int = 50000) -> DatasetResult:\n",
    "    \"\"\"\n",
    "    Load Korean MS MARCO triplets.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load.\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with triplet data.\n",
    "    \"\"\"\n",
    "    name = \"msmarco_ko\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\n",
    "            \"williamjeong2/msmarco-triplets-ko-v1\",\n",
    "            split=\"train\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return DatasetResult(\n",
    "            name=name,\n",
    "            success=False,\n",
    "            error_message=f\"Failed to load dataset: {e}\"\n",
    "        )\n",
    "    \n",
    "    triplets: List[Dict[str, Any]] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        query = item.get(\"query\", \"\")\n",
    "        positives = item.get(\"pos\", [])\n",
    "        negatives = item.get(\"neg\", [])\n",
    "        \n",
    "        if not query or not positives:\n",
    "            continue\n",
    "        \n",
    "        pos = positives[0] if positives else \"\"\n",
    "        neg = negatives[0] if negatives else \"\"\n",
    "        \n",
    "        if pos:\n",
    "            triplets.append({\n",
    "                \"anchor\": query,\n",
    "                \"positive\": pos,\n",
    "                \"negative\": neg if neg else \"\",\n",
    "                \"source\": name,\n",
    "            })\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, data=triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_klue_nli(max_samples: int = 30000) -> DatasetResult:\n",
    "    \"\"\"\n",
    "    Load KLUE NLI dataset.\n",
    "    \n",
    "    Extracts entailment pairs as positives and contradiction as negatives.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load.\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with NLI pairs.\n",
    "    \"\"\"\n",
    "    name = \"klue_nli\"\n",
    "    \n",
    "    try:\n",
    "        # Use \"klue\" not \"klue/klue\"\n",
    "        dataset = load_dataset(\"klue\", \"nli\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(\n",
    "            name=name,\n",
    "            success=False,\n",
    "            error_message=f\"Failed to load dataset: {e}\"\n",
    "        )\n",
    "    \n",
    "    pairs: List[Dict[str, Any]] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        premise = item.get(\"premise\", \"\")\n",
    "        hypothesis = item.get(\"hypothesis\", \"\")\n",
    "        label = item.get(\"label\", -1)\n",
    "        \n",
    "        if not premise or not hypothesis:\n",
    "            continue\n",
    "        \n",
    "        # label: 0=entailment, 1=neutral, 2=contradiction\n",
    "        if label == 0:  # Entailment\n",
    "            pairs.append({\n",
    "                \"source\": premise,\n",
    "                \"target\": hypothesis,\n",
    "                \"similarity\": 0.9,\n",
    "                \"category\": \"nli_entailment\",\n",
    "                \"pair_type\": \"klue_nli\",\n",
    "            })\n",
    "        elif label == 2:  # Contradiction (hard negative)\n",
    "            pairs.append({\n",
    "                \"source\": premise,\n",
    "                \"target\": hypothesis,\n",
    "                \"similarity\": 0.1,\n",
    "                \"category\": \"nli_contradiction\",\n",
    "                \"pair_type\": \"klue_nli_neg\",\n",
    "            })\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, data=pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_klue_sts(max_samples: int = 15000) -> DatasetResult:\n",
    "    \"\"\"\n",
    "    Load KLUE STS dataset.\n",
    "    \n",
    "    Extracts high-similarity sentence pairs.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load.\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with STS pairs.\n",
    "    \"\"\"\n",
    "    name = \"klue_sts\"\n",
    "    \n",
    "    try:\n",
    "        # Use \"klue\" not \"klue/klue\"\n",
    "        dataset = load_dataset(\"klue\", \"sts\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(\n",
    "            name=name,\n",
    "            success=False,\n",
    "            error_message=f\"Failed to load dataset: {e}\"\n",
    "        )\n",
    "    \n",
    "    pairs: List[Dict[str, Any]] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        sentence1 = item.get(\"sentence1\", \"\")\n",
    "        sentence2 = item.get(\"sentence2\", \"\")\n",
    "        \n",
    "        # Handle labels field - it's a dict with 'label' (binary) and 'real-label' (0-5 float)\n",
    "        labels = item.get(\"labels\", {})\n",
    "        score = 0.0\n",
    "        if isinstance(labels, dict):\n",
    "            score = labels.get(\"real-label\", 0)\n",
    "        \n",
    "        if not sentence1 or not sentence2:\n",
    "            continue\n",
    "        \n",
    "        # Normalize score (0-5 scale to 0-1)\n",
    "        normalized_score = score / 5.0 if score > 0 else 0\n",
    "        \n",
    "        # Include pairs with similarity >= 0.5\n",
    "        if normalized_score >= 0.5:\n",
    "            pairs.append({\n",
    "                \"source\": sentence1,\n",
    "                \"target\": sentence2,\n",
    "                \"similarity\": normalized_score,\n",
    "                \"category\": \"sts\",\n",
    "                \"pair_type\": \"klue_sts\",\n",
    "            })\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, data=pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_korquad(max_samples: int = 30000) -> DatasetResult:\n",
    "    \"\"\"\n",
    "    Load KorQuAD dataset.\n",
    "    \n",
    "    Extracts question-answer and question-context pairs.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load.\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with QA pairs.\n",
    "    \"\"\"\n",
    "    name = \"korquad\"\n",
    "    \n",
    "    try:\n",
    "        # Use the correct dataset name\n",
    "        dataset = load_dataset(\"squad_kor_v1\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(\n",
    "            name=name,\n",
    "            success=False,\n",
    "            error_message=f\"Failed to load dataset: {e}\"\n",
    "        )\n",
    "    \n",
    "    pairs: List[Dict[str, Any]] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        question = item.get(\"question\", \"\")\n",
    "        context = item.get(\"context\", \"\")\n",
    "        answers = item.get(\"answers\", {})\n",
    "        \n",
    "        if not question:\n",
    "            continue\n",
    "        \n",
    "        # Get answer text\n",
    "        answer_texts = answers.get(\"text\", []) if isinstance(answers, dict) else []\n",
    "        answer = answer_texts[0] if answer_texts else \"\"\n",
    "        \n",
    "        if answer:\n",
    "            pairs.append({\n",
    "                \"source\": question,\n",
    "                \"target\": answer,\n",
    "                \"similarity\": 0.85,\n",
    "                \"category\": \"qa\",\n",
    "                \"pair_type\": \"korquad\",\n",
    "            })\n",
    "        \n",
    "        if context and len(context) > 20:\n",
    "            truncated_context = context[:200].strip()\n",
    "            pairs.append({\n",
    "                \"source\": question,\n",
    "                \"target\": truncated_context,\n",
    "                \"similarity\": 0.7,\n",
    "                \"category\": \"qa_context\",\n",
    "                \"pair_type\": \"korquad_context\",\n",
    "            })\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, data=pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kobest_copa(max_samples: int = 5000) -> DatasetResult:\n",
    "    \"\"\"\n",
    "    Load KoBEST COPA dataset.\n",
    "    \n",
    "    Extracts premise-alternative pairs for causal reasoning.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load.\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with COPA pairs.\n",
    "    \"\"\"\n",
    "    name = \"kobest_copa\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\"skt/kobest_v1\", \"copa\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(\n",
    "            name=name,\n",
    "            success=False,\n",
    "            error_message=f\"Failed to load dataset: {e}\"\n",
    "        )\n",
    "    \n",
    "    pairs: List[Dict[str, Any]] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        # Correct field names: alternative_1, alternative_2 (with underscores)\n",
    "        premise = item.get(\"premise\", \"\")\n",
    "        alternative1 = item.get(\"alternative_1\", \"\")\n",
    "        alternative2 = item.get(\"alternative_2\", \"\")\n",
    "        label = item.get(\"label\", 0)\n",
    "        \n",
    "        if not premise:\n",
    "            continue\n",
    "        \n",
    "        # label=0 means alternative_1 is correct, label=1 means alternative_2\n",
    "        correct = alternative1 if label == 0 else alternative2\n",
    "        \n",
    "        if correct:\n",
    "            pairs.append({\n",
    "                \"source\": premise,\n",
    "                \"target\": correct,\n",
    "                \"similarity\": 0.8,\n",
    "                \"category\": \"copa\",\n",
    "                \"pair_type\": \"kobest_copa\",\n",
    "            })\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, data=pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_naver_news(max_samples: int = 10000) -> DatasetResult:\n",
    "    \"\"\"\n",
    "    Load Naver News summarization dataset.\n",
    "    \n",
    "    Extracts title-summary and content-summary pairs.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load.\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with news pairs.\n",
    "    \"\"\"\n",
    "    name = \"naver_news\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\n",
    "            \"daekeun-ml/naver-news-summarization-ko\",\n",
    "            split=\"train\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return DatasetResult(\n",
    "            name=name,\n",
    "            success=False,\n",
    "            error_message=f\"Failed to load dataset: {e}\"\n",
    "        )\n",
    "    \n",
    "    pairs: List[Dict[str, Any]] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        title = item.get(\"title\", \"\") or item.get(\"document_title\", \"\")\n",
    "        content = (\n",
    "            item.get(\"document\", \"\") or \n",
    "            item.get(\"content\", \"\") or \n",
    "            item.get(\"text\", \"\")\n",
    "        )\n",
    "        summary = item.get(\"summary\", \"\") or item.get(\"abstractive\", \"\")\n",
    "        \n",
    "        if title and summary:\n",
    "            pairs.append({\n",
    "                \"source\": title,\n",
    "                \"target\": summary[:200],\n",
    "                \"similarity\": 0.75,\n",
    "                \"category\": \"news_summary\",\n",
    "                \"pair_type\": \"naver_news\",\n",
    "            })\n",
    "        \n",
    "        if content and summary:\n",
    "            pairs.append({\n",
    "                \"source\": content[:200],\n",
    "                \"target\": summary[:200],\n",
    "                \"similarity\": 0.8,\n",
    "                \"category\": \"news_summary\",\n",
    "                \"pair_type\": \"naver_news\",\n",
    "            })\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, data=pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KorMedLawQA dataset is currently unavailable (causes crashes)\n",
    "# Placeholder function that returns empty result\n",
    "def load_kormedlaw(max_samples: int = 5000) -> DatasetResult:\n",
    "    \"\"\"\n",
    "    Load Korean Medical Law QA dataset.\n",
    "    \n",
    "    NOTE: This dataset is currently unavailable due to issues.\n",
    "    Returns empty result.\n",
    "    \"\"\"\n",
    "    name = \"kormedlaw\"\n",
    "    print(f\"  [SKIP] {name}: Dataset currently unavailable\")\n",
    "    return DatasetResult(name=name, success=True, data=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nsmc_corpus(max_samples: int = 50000) -> DatasetResult:\n",
    "    \"\"\"\n",
    "    Load NSMC corpus for negative mining.\n",
    "    \n",
    "    Returns movie review texts as a corpus.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load.\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with corpus texts.\n",
    "    \"\"\"\n",
    "    name = \"nsmc\"\n",
    "    \n",
    "    try:\n",
    "        # Use \"nsmc\" directly\n",
    "        dataset = load_dataset(\"nsmc\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(\n",
    "            name=name,\n",
    "            success=False,\n",
    "            error_message=f\"Failed to load dataset: {e}\"\n",
    "        )\n",
    "    \n",
    "    texts: List[str] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        document = item.get(\"document\", \"\")\n",
    "        if document and len(document) > 5:\n",
    "            texts.append(document)\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, corpus=texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-threaded Dataset Loader\n",
    "\n",
    "Core class that manages parallel dataset loading with progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    \"\"\"Configuration for a dataset loading task.\"\"\"\n",
    "    \n",
    "    name: str\n",
    "    loader_fn: Callable[..., DatasetResult]\n",
    "    max_samples: int\n",
    "    description: str = \"\"\n",
    "\n",
    "\n",
    "class MultiThreadedDataLoader:\n",
    "    \"\"\"\n",
    "    Multi-threaded dataset loader with progress tracking.\n",
    "    \n",
    "    Loads multiple HuggingFace datasets in parallel using ThreadPoolExecutor.\n",
    "    Provides graceful error handling so that failure of one dataset\n",
    "    does not affect others.\n",
    "    \n",
    "    Attributes:\n",
    "        max_workers: Maximum number of parallel threads.\n",
    "        results: Dictionary mapping dataset names to their results.\n",
    "        progress: Dictionary mapping dataset names to progress info.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_workers: int = 4) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the multi-threaded loader.\n",
    "        \n",
    "        Args:\n",
    "            max_workers: Maximum number of concurrent threads.\n",
    "        \"\"\"\n",
    "        self.max_workers = max_workers\n",
    "        self.results: Dict[str, DatasetResult] = {}\n",
    "        self.progress: Dict[str, LoadingProgress] = {}\n",
    "        self._lock = threading.Lock()\n",
    "    \n",
    "    def _execute_loader(\n",
    "        self,\n",
    "        config: DatasetConfig\n",
    "    ) -> DatasetResult:\n",
    "        \"\"\"\n",
    "        Execute a single dataset loader.\n",
    "        \n",
    "        Args:\n",
    "            config: Dataset configuration.\n",
    "            \n",
    "        Returns:\n",
    "            DatasetResult from the loader function.\n",
    "        \"\"\"\n",
    "        with self._lock:\n",
    "            self.progress[config.name] = LoadingProgress(\n",
    "                name=config.name,\n",
    "                status=\"loading\"\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            result = config.loader_fn(max_samples=config.max_samples)\n",
    "            \n",
    "            with self._lock:\n",
    "                self.progress[config.name].complete(success=result.success)\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            with self._lock:\n",
    "                self.progress[config.name].complete(success=False)\n",
    "            \n",
    "            return DatasetResult(\n",
    "                name=config.name,\n",
    "                success=False,\n",
    "                error_message=f\"Unexpected error: {e}\"\n",
    "            )\n",
    "    \n",
    "    def load_all(\n",
    "        self,\n",
    "        configs: List[DatasetConfig]\n",
    "    ) -> Dict[str, DatasetResult]:\n",
    "        \"\"\"\n",
    "        Load all datasets in parallel.\n",
    "        \n",
    "        Args:\n",
    "            configs: List of dataset configurations.\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping dataset names to results.\n",
    "        \"\"\"\n",
    "        print(f\"\\nLoading {len(configs)} datasets with {self.max_workers} workers...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Initialize progress tracking\n",
    "        for config in configs:\n",
    "            self.progress[config.name] = LoadingProgress(\n",
    "                name=config.name,\n",
    "                status=\"pending\"\n",
    "            )\n",
    "        \n",
    "        # Submit all tasks\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            future_to_config: Dict[Future, DatasetConfig] = {\n",
    "                executor.submit(self._execute_loader, config): config\n",
    "                for config in configs\n",
    "            }\n",
    "            \n",
    "            # Process completed futures\n",
    "            for future in as_completed(future_to_config):\n",
    "                config = future_to_config[future]\n",
    "                \n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    self.results[config.name] = result\n",
    "                    \n",
    "                    status = \"SUCCESS\" if result.success else \"FAILED\"\n",
    "                    count = result.sample_count\n",
    "                    print(f\"  [{status}] {config.name}: {count} samples\")\n",
    "                    \n",
    "                    if not result.success:\n",
    "                        print(f\"    Error: {result.error_message}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  [ERROR] {config.name}: {e}\")\n",
    "                    self.results[config.name] = DatasetResult(\n",
    "                        name=config.name,\n",
    "                        success=False,\n",
    "                        error_message=str(e)\n",
    "                    )\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        return self.results\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get loading summary statistics.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with summary statistics.\n",
    "        \"\"\"\n",
    "        successful = [r for r in self.results.values() if r.success]\n",
    "        failed = [r for r in self.results.values() if not r.success]\n",
    "        \n",
    "        total_samples = sum(r.sample_count for r in successful)\n",
    "        \n",
    "        return {\n",
    "            \"total_datasets\": len(self.results),\n",
    "            \"successful\": len(successful),\n",
    "            \"failed\": len(failed),\n",
    "            \"total_samples\": total_samples,\n",
    "            \"failed_datasets\": [r.name for r in failed],\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Dataset Loading Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset configurations:\n",
      "  - msmarco_ko: Korean MS MARCO triplets (max: 50000)\n",
      "  - klue_nli: KLUE Natural Language Inference (max: 30000)\n",
      "  - klue_sts: KLUE Semantic Textual Similarity (max: 15000)\n",
      "  - korquad: Korean Question Answering (max: 30000)\n",
      "  - kobest_copa: KoBEST COPA reasoning (max: 5000)\n",
      "  - naver_news: Naver News summarization (max: 10000)\n",
      "  - nsmc: NSMC movie review corpus (max: 50000)\n"
     ]
    }
   ],
   "source": [
    "# Define all dataset configurations\n",
    "# Note: kormedlaw removed due to dataset issues\n",
    "DATASET_CONFIGS: List[DatasetConfig] = [\n",
    "    DatasetConfig(\n",
    "        name=\"msmarco_ko\",\n",
    "        loader_fn=load_msmarco_korean,\n",
    "        max_samples=50000,\n",
    "        description=\"Korean MS MARCO triplets\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"klue_nli\",\n",
    "        loader_fn=load_klue_nli,\n",
    "        max_samples=30000,\n",
    "        description=\"KLUE Natural Language Inference\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"klue_sts\",\n",
    "        loader_fn=load_klue_sts,\n",
    "        max_samples=15000,\n",
    "        description=\"KLUE Semantic Textual Similarity\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"korquad\",\n",
    "        loader_fn=load_korquad,\n",
    "        max_samples=30000,\n",
    "        description=\"Korean Question Answering\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"kobest_copa\",\n",
    "        loader_fn=load_kobest_copa,\n",
    "        max_samples=5000,\n",
    "        description=\"KoBEST COPA reasoning\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"naver_news\",\n",
    "        loader_fn=load_naver_news,\n",
    "        max_samples=10000,\n",
    "        description=\"Naver News summarization\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"nsmc\",\n",
    "        loader_fn=load_nsmc_corpus,\n",
    "        max_samples=50000,\n",
    "        description=\"NSMC movie review corpus\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Dataset configurations:\")\n",
    "for config in DATASET_CONFIGS:\n",
    "    print(f\"  - {config.name}: {config.description} (max: {config.max_samples})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Parallel Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading 7 datasets sequentially...\n",
      "============================================================\n",
      "\n",
      "[msmarco_ko] Loading Korean MS MARCO triplets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77660bbe10664245bab563214d421bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0b72a634cb4e55bffe45d75450ebd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191a0a84d18c419bbafe664141223e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3a831c794b461e923348d965dc27da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing msmarco_ko:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [SUCCESS] 50,000 samples\n",
      "\n",
      "[klue_nli] Loading KLUE Natural Language Inference...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32998343cfd2475f8a43fe33b5fd3341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing klue_nli:   0%|          | 0/24998 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [SUCCESS] 17,050 samples\n",
      "\n",
      "[klue_sts] Loading KLUE Semantic Textual Similarity...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59147e596944780bd81c88f1fe79e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing klue_sts:   0%|          | 0/11668 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [SUCCESS] 6,016 samples\n",
      "\n",
      "[korquad] Loading Korean Question Answering...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511af69c1ffe42a788f5ccaabac9134b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing korquad:   0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [SUCCESS] 60,000 samples\n",
      "\n",
      "[kobest_copa] Loading KoBEST COPA reasoning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21f6720c31c4338a7f0cefde81f0c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing kobest_copa:   0%|          | 0/3076 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [SUCCESS] 3,076 samples\n",
      "\n",
      "[naver_news] Loading Naver News summarization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ec8a79059e46d782039ee98aa16de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing naver_news:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [SUCCESS] 20,000 samples\n",
      "\n",
      "[nsmc] Loading NSMC movie review corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030d91cda8db4b9aa593053c7ec08089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing nsmc:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [SUCCESS] 47,976 samples\n",
      "\n",
      "============================================================\n",
      "\n",
      "Loading Summary:\n",
      "  Successful: 7/7\n",
      "  Total samples: 204,118\n"
     ]
    }
   ],
   "source": [
    "# Load datasets SEQUENTIALLY to avoid memory issues\n",
    "# Multi-threading with HuggingFace can cause kernel crashes due to memory pressure\n",
    "\n",
    "import gc\n",
    "\n",
    "def load_datasets_sequential(configs: List[DatasetConfig]) -> Dict[str, DatasetResult]:\n",
    "    \"\"\"Load datasets one by one to avoid memory issues.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"\\nLoading {len(configs)} datasets sequentially...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\n[{config.name}] Loading {config.description}...\")\n",
    "        try:\n",
    "            result = config.loader_fn(max_samples=config.max_samples)\n",
    "            results[config.name] = result\n",
    "            \n",
    "            status = \"SUCCESS\" if result.success else \"FAILED\"\n",
    "            print(f\"  [{status}] {result.sample_count:,} samples\")\n",
    "            \n",
    "            if not result.success:\n",
    "                print(f\"    Error: {result.error_message}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] {e}\")\n",
    "            results[config.name] = DatasetResult(\n",
    "                name=config.name,\n",
    "                success=False,\n",
    "                error_message=str(e)\n",
    "            )\n",
    "        \n",
    "        # Force garbage collection after each dataset\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    return results\n",
    "\n",
    "\n",
    "# Load all datasets\n",
    "results = load_datasets_sequential(DATASET_CONFIGS)\n",
    "\n",
    "# Summary\n",
    "successful = [r for r in results.values() if r.success]\n",
    "failed = [r for r in results.values() if not r.success]\n",
    "total_samples = sum(r.sample_count for r in successful)\n",
    "\n",
    "print(f\"\\nLoading Summary:\")\n",
    "print(f\"  Successful: {len(successful)}/{len(results)}\")\n",
    "print(f\"  Total samples: {total_samples:,}\")\n",
    "\n",
    "if failed:\n",
    "    print(f\"  Failed datasets: {', '.join(r.name for r in failed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted results:\n",
      "  MS MARCO triplets: 50000\n",
      "  KLUE NLI pairs: 17050\n",
      "  KLUE STS pairs: 6016\n",
      "  KorQuAD pairs: 60000\n",
      "  KoBEST COPA pairs: 3076\n",
      "  Naver News pairs: 20000\n",
      "  NSMC texts: 47976\n"
     ]
    }
   ],
   "source": [
    "# Extract individual results\n",
    "msmarco_triplets = results.get(\"msmarco_ko\", DatasetResult(name=\"msmarco_ko\", success=False)).data\n",
    "klue_nli_pairs = results.get(\"klue_nli\", DatasetResult(name=\"klue_nli\", success=False)).data\n",
    "klue_sts_pairs = results.get(\"klue_sts\", DatasetResult(name=\"klue_sts\", success=False)).data\n",
    "korquad_pairs = results.get(\"korquad\", DatasetResult(name=\"korquad\", success=False)).data\n",
    "kobest_pairs = results.get(\"kobest_copa\", DatasetResult(name=\"kobest_copa\", success=False)).data\n",
    "naver_news_pairs = results.get(\"naver_news\", DatasetResult(name=\"naver_news\", success=False)).data\n",
    "nsmc_texts = results.get(\"nsmc\", DatasetResult(name=\"nsmc\", success=False)).corpus\n",
    "\n",
    "print(\"Extracted results:\")\n",
    "print(f\"  MS MARCO triplets: {len(msmarco_triplets)}\")\n",
    "print(f\"  KLUE NLI pairs: {len(klue_nli_pairs)}\")\n",
    "print(f\"  KLUE STS pairs: {len(klue_sts_pairs)}\")\n",
    "print(f\"  KorQuAD pairs: {len(korquad_pairs)}\")\n",
    "print(f\"  KoBEST COPA pairs: {len(kobest_pairs)}\")\n",
    "print(f\"  Naver News pairs: {len(naver_news_pairs)}\")\n",
    "print(f\"  NSMC texts: {len(nsmc_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample data from each dataset:\n",
      "============================================================\n",
      "\n",
      "MS MARCO triplet:\n",
      "{\n",
      "  \"anchor\": \"인문학이란 무엇인가요?\",\n",
      "  \"positive\": \"자유 예술. 1. 전문 또는 기술 과목과 달리 일반적인 지식을 제공하기 위한 대학의 학술 과정을 의미하며 예술, 인문학, 자연 과학 및 사회 과학을 포함합니다.\",\n",
      "  \"negative\": \"자유 교육: 개인에게 힘을 실어주고 복잡성, 다양성 및 변화를 다룰 수 있도록 준비시키는 대학 학습에 대한 접근 방식입니다. 이 접근 방식은 특정 관심 분야의 심층적인 성취와 함께 더 넓은 세계(예: 과학, 문화, 사회)에 대한 폭넓은 지식을 강조합니다.\",\n",
      "  \"source\": \"msmarco_ko\"\n",
      "}\n",
      "\n",
      "KLUE NLI pair:\n",
      "{\n",
      "  \"source\": \"힛걸 진심 최고다 그 어떤 히어로보다 멋지다\",\n",
      "  \"target\": \"힛걸 진심 최고로 멋지다.\",\n",
      "  \"similarity\": 0.9,\n",
      "  \"category\": \"nli_entailment\",\n",
      "  \"pair_type\": \"klue_nli\"\n",
      "}\n",
      "\n",
      "KorQuAD pair:\n",
      "{\n",
      "  \"source\": \"바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?\",\n",
      "  \"target\": \"교향곡\",\n",
      "  \"similarity\": 0.85,\n",
      "  \"category\": \"qa\",\n",
      "  \"pair_type\": \"korquad\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Show sample from each successful dataset\n",
    "print(\"\\nSample data from each dataset:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if msmarco_triplets:\n",
    "    print(\"\\nMS MARCO triplet:\")\n",
    "    print(json.dumps(msmarco_triplets[0], ensure_ascii=False, indent=2))\n",
    "\n",
    "if klue_nli_pairs:\n",
    "    print(\"\\nKLUE NLI pair:\")\n",
    "    print(json.dumps(klue_nli_pairs[0], ensure_ascii=False, indent=2))\n",
    "\n",
    "if korquad_pairs:\n",
    "    print(\"\\nKorQuAD pair:\")\n",
    "    print(json.dumps(korquad_pairs[0], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge All Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_pairs(\n",
    "    klue_nli: List[Dict],\n",
    "    klue_sts: List[Dict],\n",
    "    korquad: List[Dict],\n",
    "    kobest: List[Dict],\n",
    "    naver_news: List[Dict],\n",
    "    msmarco: List[Dict],\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Merge all pair datasets into a unified format.\n",
    "    \n",
    "    Args:\n",
    "        klue_nli: KLUE NLI pairs.\n",
    "        klue_sts: KLUE STS pairs.\n",
    "        korquad: KorQuAD pairs.\n",
    "        kobest: KoBEST COPA pairs.\n",
    "        naver_news: Naver News pairs.\n",
    "        msmarco: MS MARCO triplets.\n",
    "        \n",
    "    Returns:\n",
    "        List of merged pairs.\n",
    "    \"\"\"\n",
    "    all_pairs: List[Dict[str, Any]] = []\n",
    "    \n",
    "    # Add KLUE pairs (skip contradictions for positive training)\n",
    "    for pair in klue_nli:\n",
    "        if pair.get(\"pair_type\") != \"klue_nli_neg\":\n",
    "            all_pairs.append(pair)\n",
    "    \n",
    "    all_pairs.extend(klue_sts)\n",
    "    all_pairs.extend(korquad)\n",
    "    all_pairs.extend(kobest)\n",
    "    all_pairs.extend(naver_news)\n",
    "    \n",
    "    # Convert MS MARCO triplets to pairs\n",
    "    for triplet in msmarco:\n",
    "        if triplet.get(\"positive\"):\n",
    "            all_pairs.append({\n",
    "                \"source\": triplet[\"anchor\"],\n",
    "                \"target\": triplet[\"positive\"],\n",
    "                \"similarity\": 0.85,\n",
    "                \"category\": \"retrieval\",\n",
    "                \"pair_type\": \"msmarco_ko\",\n",
    "            })\n",
    "    \n",
    "    return all_pairs\n",
    "\n",
    "\n",
    "def deduplicate_pairs(pairs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Remove duplicate pairs based on source-target combination.\n",
    "    \n",
    "    Args:\n",
    "        pairs: List of pairs to deduplicate.\n",
    "        \n",
    "    Returns:\n",
    "        Deduplicated list of pairs.\n",
    "    \"\"\"\n",
    "    seen: set = set()\n",
    "    unique_pairs: List[Dict[str, Any]] = []\n",
    "    \n",
    "    for pair in pairs:\n",
    "        key = (pair.get(\"source\", \"\"), pair.get(\"target\", \"\"))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique_pairs.append(pair)\n",
    "    \n",
    "    return unique_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs collected: 147653\n",
      "Unique pairs after deduplication: 146680\n"
     ]
    }
   ],
   "source": [
    "# Merge all pairs\n",
    "all_pairs = merge_all_pairs(\n",
    "    klue_nli=klue_nli_pairs,\n",
    "    klue_sts=klue_sts_pairs,\n",
    "    korquad=korquad_pairs,\n",
    "    kobest=kobest_pairs,\n",
    "    naver_news=naver_news_pairs,\n",
    "    msmarco=msmarco_triplets,\n",
    ")\n",
    "\n",
    "print(f\"Total pairs collected: {len(all_pairs)}\")\n",
    "\n",
    "# Deduplicate\n",
    "unique_pairs = deduplicate_pairs(all_pairs)\n",
    "print(f\"Unique pairs after deduplication: {len(unique_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pairs by source:\n",
      "  msmarco_ko: 49,998\n",
      "  korquad: 29,926\n",
      "  korquad_context: 29,924\n",
      "  naver_news: 19,195\n",
      "  klue_nli: 8,560\n",
      "  klue_sts: 6,009\n",
      "  kobest_copa: 3,068\n"
     ]
    }
   ],
   "source": [
    "# Statistics by source\n",
    "source_counts: Dict[str, int] = defaultdict(int)\n",
    "for pair in unique_pairs:\n",
    "    source_counts[pair.get(\"pair_type\", \"unknown\")] += 1\n",
    "\n",
    "print(\"\\nPairs by source:\")\n",
    "for source, count in sorted(source_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {source}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(data: List[Dict], output_path: Path) -> int:\n",
    "    \"\"\"\n",
    "    Save data to JSONL format.\n",
    "    \n",
    "    Args:\n",
    "        data: List of dictionaries to save.\n",
    "        output_path: Path to output file.\n",
    "        \n",
    "    Returns:\n",
    "        Number of records saved.\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "    return len(data)\n",
    "\n",
    "\n",
    "def save_text_corpus(texts: List[str], output_path: Path) -> int:\n",
    "    \"\"\"\n",
    "    Save text corpus to file.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings.\n",
    "        output_path: Path to output file.\n",
    "        \n",
    "    Returns:\n",
    "        Number of texts saved.\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for text in texts:\n",
    "            f.write(text + \"\\n\")\n",
    "    return len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 146,680 pairs to /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/data/huggingface_korean/huggingface_synonym_pairs.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Save synonym pairs\n",
    "pairs_output_path = HF_DATA_DIR / \"huggingface_synonym_pairs.jsonl\"\n",
    "saved_pairs = save_jsonl(unique_pairs, pairs_output_path)\n",
    "print(f\"Saved {saved_pairs:,} pairs to {pairs_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50,000 MS MARCO triplets to /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/data/huggingface_korean/msmarco_triplets.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Save MS MARCO triplets separately (for direct triplet training)\n",
    "triplets_with_negatives = [\n",
    "    t for t in msmarco_triplets\n",
    "    if t.get(\"positive\") and t.get(\"negative\")\n",
    "]\n",
    "\n",
    "triplets_output_path = HF_DATA_DIR / \"msmarco_triplets.jsonl\"\n",
    "saved_triplets = save_jsonl(triplets_with_negatives, triplets_output_path)\n",
    "print(f\"Saved {saved_triplets:,} MS MARCO triplets to {triplets_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 47,976 texts to /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/data/huggingface_korean/nsmc_corpus.txt\n"
     ]
    }
   ],
   "source": [
    "# Save corpus for negative mining\n",
    "corpus_output_path = HF_DATA_DIR / \"nsmc_corpus.txt\"\n",
    "saved_texts = save_text_corpus(nsmc_texts, corpus_output_path)\n",
    "print(f\"Saved {saved_texts:,} texts to {corpus_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "HuggingFace Korean Data Loading Summary\n",
      "============================================================\n",
      "\n",
      "Loading Performance:\n",
      "  Mode: Sequential (memory-safe)\n",
      "  Datasets attempted: 7\n",
      "  Successful: 7\n",
      "  Failed: 0\n",
      "\n",
      "Datasets Loaded:\n",
      "  MS MARCO Korean Triplets: 50,000\n",
      "  KLUE NLI Pairs: 17,050\n",
      "  KLUE STS Pairs: 6,016\n",
      "  KorQuAD Pairs: 60,000\n",
      "  KoBEST COPA Pairs: 3,076\n",
      "  Naver News Pairs: 20,000\n",
      "  NSMC Corpus: 47,976 texts\n",
      "\n",
      "Total Unique Pairs: 146,680\n",
      "\n",
      "Output Files:\n",
      "  huggingface_synonym_pairs.jsonl: 68.12 MB\n",
      "  msmarco_triplets.jsonl: 43.01 MB\n",
      "  nsmc_corpus.txt: 4.18 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HuggingFace Korean Data Loading Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nLoading Performance:\")\n",
    "print(f\"  Mode: Sequential (memory-safe)\")\n",
    "print(f\"  Datasets attempted: {len(results)}\")\n",
    "print(f\"  Successful: {len(successful)}\")\n",
    "print(f\"  Failed: {len(failed)}\")\n",
    "\n",
    "print(f\"\\nDatasets Loaded:\")\n",
    "print(f\"  MS MARCO Korean Triplets: {len(msmarco_triplets):,}\")\n",
    "print(f\"  KLUE NLI Pairs: {len(klue_nli_pairs):,}\")\n",
    "print(f\"  KLUE STS Pairs: {len(klue_sts_pairs):,}\")\n",
    "print(f\"  KorQuAD Pairs: {len(korquad_pairs):,}\")\n",
    "print(f\"  KoBEST COPA Pairs: {len(kobest_pairs):,}\")\n",
    "print(f\"  Naver News Pairs: {len(naver_news_pairs):,}\")\n",
    "print(f\"  NSMC Corpus: {len(nsmc_texts):,} texts\")\n",
    "\n",
    "print(f\"\\nTotal Unique Pairs: {len(unique_pairs):,}\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "for f in sorted(HF_DATA_DIR.glob(\"*\")):\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"  {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run `01_data_augmentation.ipynb` to merge with existing synonym pairs\n",
    "2. The HuggingFace data will be automatically incorporated into training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v21.4 Data Augmentation\n",
    "\n",
    "## Problem Analysis from v21.3\n",
    "\n",
    "Problem terms (추천, 데이터베이스, 증상, 질환, 인슐린) have:\n",
    "- **ZERO** exact matches as source/target in training synonym pairs\n",
    "- Only appear embedded in compound words\n",
    "- Some appear as negatives (counterproductive)\n",
    "\n",
    "## Solution\n",
    "\n",
    "1. Add explicit single-term synonym pairs\n",
    "2. Add identity pairs for self-reconstruction\n",
    "3. Apply domain-specific filtering thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load v21.3 Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Paths\nV21_3_DATA_DIR = PROJECT_ROOT / \"dataset\" / \"v21.3_filtered_enhanced\"\nV21_4_DATA_DIR = PROJECT_ROOT / \"data\" / \"v21.4\"\nV21_4_DATA_DIR.mkdir(parents=True, exist_ok=True)\n\n# Load v21.3 filtered pairs\nfiltered_pairs_path = V21_3_DATA_DIR / \"filtered_synonym_pairs.jsonl\"\n\nv21_3_pairs = []\nif filtered_pairs_path.exists():\n    with open(filtered_pairs_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            v21_3_pairs.append(json.loads(line))\n    print(f\"Loaded {len(v21_3_pairs)} filtered pairs from v21.3\")\nelse:\n    print(f\"Warning: {filtered_pairs_path} not found\")\n\n# Show sample\nif v21_3_pairs:\n    print(\"\\nSample pair:\")\n    print(json.dumps(v21_3_pairs[0], ensure_ascii=False, indent=2))"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Analyze Problem Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem terms identified from v21.3 inference test\n",
    "PROBLEM_TERMS = [\n",
    "    \"추천\",\n",
    "    \"데이터베이스\",\n",
    "    \"증상\",\n",
    "    \"질환\",\n",
    "    \"인슐린\",\n",
    "]\n",
    "\n",
    "# Check coverage in v21.3 data\n",
    "def check_term_coverage(pairs: List[Dict], terms: List[str]) -> Dict[str, Dict]:\n",
    "    \"\"\"Check how many times each term appears as source/target.\"\"\"\n",
    "    coverage = {term: {\"as_source\": 0, \"as_target\": 0, \"partial_source\": 0, \"partial_target\": 0} \n",
    "                for term in terms}\n",
    "    \n",
    "    for pair in pairs:\n",
    "        source = pair.get(\"source\", \"\")\n",
    "        target = pair.get(\"target\", \"\")\n",
    "        \n",
    "        for term in terms:\n",
    "            # Exact match\n",
    "            if source == term:\n",
    "                coverage[term][\"as_source\"] += 1\n",
    "            elif term in source:\n",
    "                coverage[term][\"partial_source\"] += 1\n",
    "                \n",
    "            if target == term:\n",
    "                coverage[term][\"as_target\"] += 1\n",
    "            elif term in target:\n",
    "                coverage[term][\"partial_target\"] += 1\n",
    "    \n",
    "    return coverage\n",
    "\n",
    "coverage = check_term_coverage(v21_3_pairs, PROBLEM_TERMS)\n",
    "\n",
    "print(\"Problem Term Coverage in v21.3 Data:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Term':<15} {'As Source':>12} {'As Target':>12} {'Partial Src':>12} {'Partial Tgt':>12}\")\n",
    "print(\"-\" * 70)\n",
    "for term, stats in coverage.items():\n",
    "    print(f\"{term:<15} {stats['as_source']:>12} {stats['as_target']:>12} {stats['partial_source']:>12} {stats['partial_target']:>12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Define Single-term Synonym Pairs\n",
    "\n",
    "Manually curated synonym pairs for problem terms and other common single terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-term synonym pairs - manually curated\n",
    "SINGLE_TERM_SYNONYMS = {\n",
    "    # Problem terms from v21.3\n",
    "    \"추천\": [\"권장\", \"권유\", \"제안\", \"소개\", \"추천서\"],\n",
    "    \"데이터베이스\": [\"DB\", \"디비\", \"저장소\", \"데이터저장소\"],\n",
    "    \"증상\": [\"증세\", \"징후\", \"양상\", \"현상\", \"이상\"],\n",
    "    \"질환\": [\"질병\", \"병\", \"병증\", \"환\", \"이환\"],\n",
    "    \"인슐린\": [\"insulin\", \"인슐린호르몬\", \"혈당조절호르몬\"],\n",
    "    \n",
    "    # General terms\n",
    "    \"검색\": [\"탐색\", \"조회\", \"찾기\", \"서치\", \"search\"],\n",
    "    \"컴퓨터\": [\"PC\", \"computer\", \"전산\", \"컴\", \"피씨\"],\n",
    "    \"인공지능\": [\"AI\", \"에이아이\", \"기계지능\", \"artificial intelligence\"],\n",
    "    \"스마트폰\": [\"휴대폰\", \"핸드폰\", \"모바일폰\", \"smartphone\"],\n",
    "    \"프로그래밍\": [\"코딩\", \"개발\", \"programming\", \"프로그램작성\"],\n",
    "    \n",
    "    # Legal terms\n",
    "    \"손해배상\": [\"배상\", \"보상\", \"손해보전\", \"피해배상\"],\n",
    "    \"판결\": [\"판례\", \"선고\", \"결정\", \"심판\", \"재결\"],\n",
    "    \"소송\": [\"재판\", \"법적분쟁\", \"송사\", \"쟁송\"],\n",
    "    \"계약\": [\"약정\", \"협약\", \"협정\", \"계약서\"],\n",
    "    \"위반\": [\"위법\", \"불법\", \"법위반\", \"규정위반\"],\n",
    "    \"피고\": [\"피고인\", \"피소인\", \"소송상대방\"],\n",
    "    \"원고\": [\"소송인\", \"제소자\", \"소제기자\"],\n",
    "    \"변호사\": [\"법률가\", \"법조인\", \"변호인\", \"lawyer\"],\n",
    "    \n",
    "    # Medical terms\n",
    "    \"진단\": [\"진찰\", \"검진\", \"판단\", \"diagnosis\"],\n",
    "    \"치료\": [\"처치\", \"요법\", \"치유\", \"시술\", \"therapy\"],\n",
    "    \"처방\": [\"투약\", \"처방전\", \"약처방\", \"prescription\"],\n",
    "    \"당뇨병\": [\"당뇨\", \"diabetes\", \"혈당질환\"],\n",
    "    \"고혈압\": [\"혈압높음\", \"hypertension\", \"고혈압증\"],\n",
    "    \"두통\": [\"머리아픔\", \"headache\", \"편두통\"],\n",
    "    \"감기\": [\"cold\", \"감기증상\", \"코감기\"],\n",
    "    \"독감\": [\"인플루엔자\", \"flu\", \"influenza\"],\n",
    "}\n",
    "\n",
    "print(f\"Defined {len(SINGLE_TERM_SYNONYMS)} single-term synonym groups\")\n",
    "print(f\"Total pairs: {sum(len(v) for v in SINGLE_TERM_SYNONYMS.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Generate Augmented Synonym Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SynonymPair:\n",
    "    source: str\n",
    "    target: str\n",
    "    similarity: float\n",
    "    category: str\n",
    "    pair_type: str  # \"original\", \"single_term\", \"identity\"\n",
    "\n",
    "\n",
    "def generate_single_term_pairs(synonym_dict: Dict[str, List[str]]) -> List[SynonymPair]:\n",
    "    \"\"\"Generate bidirectional synonym pairs from dictionary.\"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    for source, targets in synonym_dict.items():\n",
    "        for target in targets:\n",
    "            # Forward pair\n",
    "            pairs.append(SynonymPair(\n",
    "                source=source,\n",
    "                target=target,\n",
    "                similarity=0.9,  # High similarity for curated pairs\n",
    "                category=\"single_term\",\n",
    "                pair_type=\"single_term\",\n",
    "            ))\n",
    "            # Backward pair\n",
    "            pairs.append(SynonymPair(\n",
    "                source=target,\n",
    "                target=source,\n",
    "                similarity=0.9,\n",
    "                category=\"single_term\",\n",
    "                pair_type=\"single_term\",\n",
    "            ))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "def generate_identity_pairs(terms: List[str]) -> List[SynonymPair]:\n",
    "    \"\"\"Generate identity pairs (term -> term) for self-reconstruction.\"\"\"\n",
    "    return [\n",
    "        SynonymPair(\n",
    "            source=term,\n",
    "            target=term,\n",
    "            similarity=1.0,\n",
    "            category=\"identity\",\n",
    "            pair_type=\"identity\",\n",
    "        )\n",
    "        for term in terms\n",
    "    ]\n",
    "\n",
    "\n",
    "# Generate single-term pairs\n",
    "single_term_pairs = generate_single_term_pairs(SINGLE_TERM_SYNONYMS)\n",
    "print(f\"Generated {len(single_term_pairs)} single-term synonym pairs\")\n",
    "\n",
    "# Generate identity pairs for all unique terms\n",
    "all_terms = set(SINGLE_TERM_SYNONYMS.keys())\n",
    "for targets in SINGLE_TERM_SYNONYMS.values():\n",
    "    all_terms.update(targets)\n",
    "\n",
    "identity_pairs = generate_identity_pairs(list(all_terms))\n",
    "print(f\"Generated {len(identity_pairs)} identity pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Merge with v21.3 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_v21_3_pair(pair: Dict) -> SynonymPair:\n",
    "    \"\"\"Convert v21.3 pair format to SynonymPair.\"\"\"\n",
    "    return SynonymPair(\n",
    "        source=pair[\"source\"],\n",
    "        target=pair[\"target\"],\n",
    "        similarity=pair.get(\"similarity\", 0.8),\n",
    "        category=pair.get(\"category\", \"unknown\"),\n",
    "        pair_type=\"original\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Convert v21.3 pairs\n",
    "original_pairs = [convert_v21_3_pair(p) for p in v21_3_pairs]\n",
    "print(f\"Original v21.3 pairs: {len(original_pairs)}\")\n",
    "\n",
    "# Merge all pairs\n",
    "all_pairs = original_pairs + single_term_pairs + identity_pairs\n",
    "\n",
    "# Remove duplicates\n",
    "seen = set()\n",
    "unique_pairs = []\n",
    "for pair in all_pairs:\n",
    "    key = (pair.source, pair.target)\n",
    "    if key not in seen:\n",
    "        seen.add(key)\n",
    "        unique_pairs.append(pair)\n",
    "\n",
    "print(f\"\\nTotal pairs after merge: {len(all_pairs)}\")\n",
    "print(f\"Unique pairs: {len(unique_pairs)}\")\n",
    "\n",
    "# Statistics by type\n",
    "type_counts = defaultdict(int)\n",
    "for pair in unique_pairs:\n",
    "    type_counts[pair.pair_type] += 1\n",
    "\n",
    "print(f\"\\nPairs by type:\")\n",
    "for pair_type, count in sorted(type_counts.items()):\n",
    "    print(f\"  {pair_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Verify Problem Term Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that problem terms are now covered\n",
    "pair_dicts = [asdict(p) for p in unique_pairs]\n",
    "new_coverage = check_term_coverage(pair_dicts, PROBLEM_TERMS)\n",
    "\n",
    "print(\"Problem Term Coverage After Augmentation:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Term':<15} {'As Source':>12} {'As Target':>12} {'Partial Src':>12} {'Partial Tgt':>12}\")\n",
    "print(\"-\" * 70)\n",
    "for term, stats in new_coverage.items():\n",
    "    src_change = stats['as_source'] - coverage[term]['as_source']\n",
    "    tgt_change = stats['as_target'] - coverage[term]['as_target']\n",
    "    src_str = f\"{stats['as_source']} (+{src_change})\" if src_change > 0 else str(stats['as_source'])\n",
    "    tgt_str = f\"{stats['as_target']} (+{tgt_change})\" if tgt_change > 0 else str(stats['as_target'])\n",
    "    print(f\"{term:<15} {src_str:>12} {tgt_str:>12} {stats['partial_source']:>12} {stats['partial_target']:>12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 7. Save Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save augmented pairs\n",
    "output_path = V21_4_DATA_DIR / \"augmented_synonym_pairs.jsonl\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for pair in unique_pairs:\n",
    "        f.write(json.dumps(asdict(pair), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(unique_pairs)} pairs to {output_path}\")\n",
    "\n",
    "# Save single-term pairs separately for reference\n",
    "single_term_path = V21_4_DATA_DIR / \"single_term_pairs.jsonl\"\n",
    "with open(single_term_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for pair in single_term_pairs + identity_pairs:\n",
    "        f.write(json.dumps(asdict(pair), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(single_term_pairs) + len(identity_pairs)} single-term pairs to {single_term_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category distribution\n",
    "category_counts = defaultdict(int)\n",
    "for pair in unique_pairs:\n",
    "    category_counts[pair.category] += 1\n",
    "\n",
    "print(\"Category Distribution:\")\n",
    "print(\"=\" * 40)\n",
    "for category, count in sorted(category_counts.items(), key=lambda x: -x[1]):\n",
    "    pct = count / len(unique_pairs) * 100\n",
    "    print(f\"  {category:<20}: {count:>6} ({pct:.1f}%)\")\n",
    "\n",
    "# Source length distribution\n",
    "length_bins = {\"1 token\": 0, \"2-3 tokens\": 0, \"4-5 tokens\": 0, \"6+ tokens\": 0}\n",
    "for pair in unique_pairs:\n",
    "    src_len = len(pair.source)\n",
    "    if src_len <= 2:\n",
    "        length_bins[\"1 token\"] += 1\n",
    "    elif src_len <= 6:\n",
    "        length_bins[\"2-3 tokens\"] += 1\n",
    "    elif src_len <= 10:\n",
    "        length_bins[\"4-5 tokens\"] += 1\n",
    "    else:\n",
    "        length_bins[\"6+ tokens\"] += 1\n",
    "\n",
    "print(\"\\nSource Length Distribution:\")\n",
    "print(\"=\" * 40)\n",
    "for length, count in length_bins.items():\n",
    "    pct = count / len(unique_pairs) * 100\n",
    "    print(f\"  {length:<15}: {count:>6} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Data Augmentation Results\n",
    "\n",
    "| Metric | v21.3 | v21.4 |\n",
    "|--------|-------|-------|\n",
    "| Original pairs | - | From v21.3 |\n",
    "| Single-term pairs | 0 | Added |\n",
    "| Identity pairs | 0 | Added |\n",
    "| Problem terms covered | 0 | All 5 |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Run `02_data_preparation.ipynb` to generate training triplets\n",
    "2. Apply length-balanced sampling for curriculum learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
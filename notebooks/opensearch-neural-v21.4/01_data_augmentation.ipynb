{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v21.4 Data Augmentation\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "1. **v21.3 Filtered Pairs**: Base synonym pairs from previous version\n",
    "2. **HuggingFace Korean Datasets**: Large-scale Korean NLP data\n",
    "3. **Single-term Synonyms**: Manually curated pairs for problem terms\n",
    "4. **Identity Pairs**: Self-reconstruction pairs\n",
    "\n",
    "## Problem Analysis from v21.3\n",
    "\n",
    "Problem terms (추천, 데이터베이스, 증상, 질환, 인슐린) have:\n",
    "- **ZERO** exact matches as source/target in training synonym pairs\n",
    "- Only appear embedded in compound words\n",
    "- Some appear as negatives (counterproductive)\n",
    "\n",
    "## Solution\n",
    "\n",
    "1. Add HuggingFace Korean datasets (KLUE, KorQuAD, MS MARCO, etc.)\n",
    "2. Add explicit single-term synonym pairs\n",
    "3. Add identity pairs for self-reconstruction\n",
    "4. Apply domain-specific filtering thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load v21.3 Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 66070 filtered pairs from v21.3\n",
      "\n",
      "Sample pair:\n",
      "{\n",
      "  \"source\": \"李滉\",\n",
      "  \"target\": \"李穡\",\n",
      "  \"similarity\": 0.9999999999999999,\n",
      "  \"category\": \"cluster\",\n",
      "  \"ig_score\": 0.0,\n",
      "  \"pmi_score\": 5.079281042856596,\n",
      "  \"ce_score\": 0.9992297068028129,\n",
      "  \"n_filters_passed\": 2\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "V21_3_DATA_DIR = PROJECT_ROOT / \"dataset\" / \"v21.3_filtered_enhanced\"\n",
    "V21_4_DATA_DIR = PROJECT_ROOT / \"data\" / \"v21.4\"\n",
    "HF_DATA_DIR = PROJECT_ROOT / \"data\" / \"huggingface_korean\"\n",
    "V21_4_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load v21.3 filtered pairs\n",
    "filtered_pairs_path = V21_3_DATA_DIR / \"filtered_synonym_pairs.jsonl\"\n",
    "\n",
    "v21_3_pairs = []\n",
    "if filtered_pairs_path.exists():\n",
    "    with open(filtered_pairs_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            v21_3_pairs.append(json.loads(line))\n",
    "    print(f\"Loaded {len(v21_3_pairs)} filtered pairs from v21.3\")\n",
    "else:\n",
    "    print(f\"Warning: {filtered_pairs_path} not found\")\n",
    "\n",
    "# Show sample\n",
    "if v21_3_pairs:\n",
    "    print(\"\\nSample pair:\")\n",
    "    print(json.dumps(v21_3_pairs[0], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Load HuggingFace Korean Data\n",
    "\n",
    "Load preprocessed data from `00_huggingface_data_loading.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 146680 pairs from HuggingFace datasets\n",
      "\n",
      "HuggingFace pairs by source:\n",
      "  msmarco_ko: 49998\n",
      "  korquad: 29926\n",
      "  korquad_context: 29924\n",
      "  naver_news: 19195\n",
      "  klue_nli: 8560\n",
      "  klue_sts: 6009\n",
      "  kobest_copa: 3068\n"
     ]
    }
   ],
   "source": [
    "# Load HuggingFace synonym pairs\n",
    "hf_pairs_path = HF_DATA_DIR / \"huggingface_synonym_pairs.jsonl\"\n",
    "\n",
    "hf_pairs = []\n",
    "if hf_pairs_path.exists():\n",
    "    with open(hf_pairs_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            hf_pairs.append(json.loads(line))\n",
    "    print(f\"Loaded {len(hf_pairs)} pairs from HuggingFace datasets\")\n",
    "    \n",
    "    # Statistics by source\n",
    "    source_counts = defaultdict(int)\n",
    "    for pair in hf_pairs:\n",
    "        source_counts[pair.get(\"pair_type\", \"unknown\")] += 1\n",
    "    \n",
    "    print(\"\\nHuggingFace pairs by source:\")\n",
    "    for source, count in sorted(source_counts.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {source}: {count}\")\n",
    "else:\n",
    "    print(f\"Warning: {hf_pairs_path} not found\")\n",
    "    print(\"Please run 00_huggingface_data_loading.ipynb first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 MS MARCO triplets\n"
     ]
    }
   ],
   "source": [
    "# Load MS MARCO triplets for direct use\n",
    "msmarco_triplets_path = HF_DATA_DIR / \"msmarco_triplets.jsonl\"\n",
    "\n",
    "msmarco_triplets = []\n",
    "if msmarco_triplets_path.exists():\n",
    "    with open(msmarco_triplets_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            msmarco_triplets.append(json.loads(line))\n",
    "    print(f\"Loaded {len(msmarco_triplets)} MS MARCO triplets\")\n",
    "else:\n",
    "    print(f\"Warning: {msmarco_triplets_path} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Analyze Problem Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem Term Coverage in v21.3 Data:\n",
      "======================================================================\n",
      "Term               As Source    As Target  Partial Src  Partial Tgt\n",
      "----------------------------------------------------------------------\n",
      "추천                         0            0            1            1\n",
      "데이터베이스                     0            0            0            0\n",
      "증상                         0            0            2            4\n",
      "질환                         0            0            4            4\n",
      "인슐린                        0            0            0            0\n"
     ]
    }
   ],
   "source": [
    "# Problem terms identified from v21.3 inference test\n",
    "PROBLEM_TERMS = [\n",
    "    \"추천\",\n",
    "    \"데이터베이스\",\n",
    "    \"증상\",\n",
    "    \"질환\",\n",
    "    \"인슐린\",\n",
    "]\n",
    "\n",
    "# Check coverage in v21.3 data\n",
    "def check_term_coverage(pairs: List[Dict], terms: List[str]) -> Dict[str, Dict]:\n",
    "    \"\"\"Check how many times each term appears as source/target.\"\"\"\n",
    "    coverage = {term: {\"as_source\": 0, \"as_target\": 0, \"partial_source\": 0, \"partial_target\": 0} \n",
    "                for term in terms}\n",
    "    \n",
    "    for pair in pairs:\n",
    "        source = pair.get(\"source\", \"\")\n",
    "        target = pair.get(\"target\", \"\")\n",
    "        \n",
    "        for term in terms:\n",
    "            # Exact match\n",
    "            if source == term:\n",
    "                coverage[term][\"as_source\"] += 1\n",
    "            elif term in source:\n",
    "                coverage[term][\"partial_source\"] += 1\n",
    "                \n",
    "            if target == term:\n",
    "                coverage[term][\"as_target\"] += 1\n",
    "            elif term in target:\n",
    "                coverage[term][\"partial_target\"] += 1\n",
    "    \n",
    "    return coverage\n",
    "\n",
    "coverage = check_term_coverage(v21_3_pairs, PROBLEM_TERMS)\n",
    "\n",
    "print(\"Problem Term Coverage in v21.3 Data:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Term':<15} {'As Source':>12} {'As Target':>12} {'Partial Src':>12} {'Partial Tgt':>12}\")\n",
    "print(\"-\" * 70)\n",
    "for term, stats in coverage.items():\n",
    "    print(f\"{term:<15} {stats['as_source']:>12} {stats['as_target']:>12} {stats['partial_source']:>12} {stats['partial_target']:>12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Define Single-term Synonym Pairs\n",
    "\n",
    "Manually curated synonym pairs for problem terms and other common single terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 26 single-term synonym groups\n",
      "Total pairs: 103\n"
     ]
    }
   ],
   "source": [
    "# Single-term synonym pairs - manually curated\n",
    "SINGLE_TERM_SYNONYMS = {\n",
    "    # Problem terms from v21.3\n",
    "    \"추천\": [\"권장\", \"권유\", \"제안\", \"소개\", \"추천서\"],\n",
    "    \"데이터베이스\": [\"DB\", \"디비\", \"저장소\", \"데이터저장소\"],\n",
    "    \"증상\": [\"증세\", \"징후\", \"양상\", \"현상\", \"이상\"],\n",
    "    \"질환\": [\"질병\", \"병\", \"병증\", \"환\", \"이환\"],\n",
    "    \"인슐린\": [\"insulin\", \"인슐린호르몬\", \"혈당조절호르몬\"],\n",
    "    \n",
    "    # General terms\n",
    "    \"검색\": [\"탐색\", \"조회\", \"찾기\", \"서치\", \"search\"],\n",
    "    \"컴퓨터\": [\"PC\", \"computer\", \"전산\", \"컴\", \"피씨\"],\n",
    "    \"인공지능\": [\"AI\", \"에이아이\", \"기계지능\", \"artificial intelligence\"],\n",
    "    \"스마트폰\": [\"휴대폰\", \"핸드폰\", \"모바일폰\", \"smartphone\"],\n",
    "    \"프로그래밍\": [\"코딩\", \"개발\", \"programming\", \"프로그램작성\"],\n",
    "    \n",
    "    # Legal terms\n",
    "    \"손해배상\": [\"배상\", \"보상\", \"손해보전\", \"피해배상\"],\n",
    "    \"판결\": [\"판례\", \"선고\", \"결정\", \"심판\", \"재결\"],\n",
    "    \"소송\": [\"재판\", \"법적분쟁\", \"송사\", \"쟁송\"],\n",
    "    \"계약\": [\"약정\", \"협약\", \"협정\", \"계약서\"],\n",
    "    \"위반\": [\"위법\", \"불법\", \"법위반\", \"규정위반\"],\n",
    "    \"피고\": [\"피고인\", \"피소인\", \"소송상대방\"],\n",
    "    \"원고\": [\"소송인\", \"제소자\", \"소제기자\"],\n",
    "    \"변호사\": [\"법률가\", \"법조인\", \"변호인\", \"lawyer\"],\n",
    "    \n",
    "    # Medical terms\n",
    "    \"진단\": [\"진찰\", \"검진\", \"판단\", \"diagnosis\"],\n",
    "    \"치료\": [\"처치\", \"요법\", \"치유\", \"시술\", \"therapy\"],\n",
    "    \"처방\": [\"투약\", \"처방전\", \"약처방\", \"prescription\"],\n",
    "    \"당뇨병\": [\"당뇨\", \"diabetes\", \"혈당질환\"],\n",
    "    \"고혈압\": [\"혈압높음\", \"hypertension\", \"고혈압증\"],\n",
    "    \"두통\": [\"머리아픔\", \"headache\", \"편두통\"],\n",
    "    \"감기\": [\"cold\", \"감기증상\", \"코감기\"],\n",
    "    \"독감\": [\"인플루엔자\", \"flu\", \"influenza\"],\n",
    "}\n",
    "\n",
    "print(f\"Defined {len(SINGLE_TERM_SYNONYMS)} single-term synonym groups\")\n",
    "print(f\"Total pairs: {sum(len(v) for v in SINGLE_TERM_SYNONYMS.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Generate Augmented Synonym Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 206 single-term synonym pairs\n",
      "Generated 129 identity pairs\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class SynonymPair:\n",
    "    source: str\n",
    "    target: str\n",
    "    similarity: float\n",
    "    category: str\n",
    "    pair_type: str  # \"original\", \"single_term\", \"identity\", \"huggingface\"\n",
    "\n",
    "\n",
    "def generate_single_term_pairs(synonym_dict: Dict[str, List[str]]) -> List[SynonymPair]:\n",
    "    \"\"\"Generate bidirectional synonym pairs from dictionary.\"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    for source, targets in synonym_dict.items():\n",
    "        for target in targets:\n",
    "            # Forward pair\n",
    "            pairs.append(SynonymPair(\n",
    "                source=source,\n",
    "                target=target,\n",
    "                similarity=0.9,  # High similarity for curated pairs\n",
    "                category=\"single_term\",\n",
    "                pair_type=\"single_term\",\n",
    "            ))\n",
    "            # Backward pair\n",
    "            pairs.append(SynonymPair(\n",
    "                source=target,\n",
    "                target=source,\n",
    "                similarity=0.9,\n",
    "                category=\"single_term\",\n",
    "                pair_type=\"single_term\",\n",
    "            ))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "def generate_identity_pairs(terms: List[str]) -> List[SynonymPair]:\n",
    "    \"\"\"Generate identity pairs (term -> term) for self-reconstruction.\"\"\"\n",
    "    return [\n",
    "        SynonymPair(\n",
    "            source=term,\n",
    "            target=term,\n",
    "            similarity=1.0,\n",
    "            category=\"identity\",\n",
    "            pair_type=\"identity\",\n",
    "        )\n",
    "        for term in terms\n",
    "    ]\n",
    "\n",
    "\n",
    "# Generate single-term pairs\n",
    "single_term_pairs = generate_single_term_pairs(SINGLE_TERM_SYNONYMS)\n",
    "print(f\"Generated {len(single_term_pairs)} single-term synonym pairs\")\n",
    "\n",
    "# Generate identity pairs for all unique terms\n",
    "all_terms = set(SINGLE_TERM_SYNONYMS.keys())\n",
    "for targets in SINGLE_TERM_SYNONYMS.values():\n",
    "    all_terms.update(targets)\n",
    "\n",
    "identity_pairs = generate_identity_pairs(list(all_terms))\n",
    "print(f\"Generated {len(identity_pairs)} identity pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Convert HuggingFace Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 146680 HuggingFace pairs\n"
     ]
    }
   ],
   "source": [
    "def convert_hf_pair(pair: Dict) -> SynonymPair:\n",
    "    \"\"\"Convert HuggingFace pair format to SynonymPair.\"\"\"\n",
    "    return SynonymPair(\n",
    "        source=pair[\"source\"],\n",
    "        target=pair[\"target\"],\n",
    "        similarity=pair.get(\"similarity\", 0.8),\n",
    "        category=pair.get(\"category\", \"huggingface\"),\n",
    "        pair_type=pair.get(\"pair_type\", \"huggingface\"),\n",
    "    )\n",
    "\n",
    "\n",
    "# Convert HuggingFace pairs\n",
    "huggingface_pairs = [convert_hf_pair(p) for p in hf_pairs]\n",
    "print(f\"Converted {len(huggingface_pairs)} HuggingFace pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 7. Merge All Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original v21.3 pairs: 66070\n",
      "\n",
      "Total pairs after merge: 213085\n",
      "Unique pairs: 213081\n",
      "\n",
      "Pairs by type:\n",
      "  original: 66070 (31.0%)\n",
      "  msmarco_ko: 49998 (23.5%)\n",
      "  korquad: 29926 (14.0%)\n",
      "  korquad_context: 29924 (14.0%)\n",
      "  naver_news: 19195 (9.0%)\n",
      "  klue_nli: 8560 (4.0%)\n",
      "  klue_sts: 6009 (2.8%)\n",
      "  kobest_copa: 3068 (1.4%)\n",
      "  single_term: 202 (0.1%)\n",
      "  identity: 129 (0.1%)\n",
      "\n",
      "Pairs by type:\n",
      "  original: 66070 (31.0%)\n",
      "  msmarco_ko: 49998 (23.5%)\n",
      "  korquad: 29926 (14.0%)\n",
      "  korquad_context: 29924 (14.0%)\n",
      "  naver_news: 19195 (9.0%)\n",
      "  klue_nli: 8560 (4.0%)\n",
      "  klue_sts: 6009 (2.8%)\n",
      "  kobest_copa: 3068 (1.4%)\n",
      "  single_term: 202 (0.1%)\n",
      "  identity: 129 (0.1%)\n"
     ]
    }
   ],
   "source": [
    "def convert_v21_3_pair(pair: Dict) -> SynonymPair:\n",
    "    \"\"\"Convert v21.3 pair format to SynonymPair.\"\"\"\n",
    "    return SynonymPair(\n",
    "        source=pair[\"source\"],\n",
    "        target=pair[\"target\"],\n",
    "        similarity=pair.get(\"similarity\", 0.8),\n",
    "        category=pair.get(\"category\", \"unknown\"),\n",
    "        pair_type=\"original\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Convert v21.3 pairs\n",
    "original_pairs = [convert_v21_3_pair(p) for p in v21_3_pairs]\n",
    "print(f\"Original v21.3 pairs: {len(original_pairs)}\")\n",
    "\n",
    "# Merge all pairs\n",
    "all_pairs = original_pairs + huggingface_pairs + single_term_pairs + identity_pairs\n",
    "\n",
    "# Remove duplicates\n",
    "seen = set()\n",
    "unique_pairs = []\n",
    "for pair in all_pairs:\n",
    "    key = (pair.source, pair.target)\n",
    "    if key not in seen:\n",
    "        seen.add(key)\n",
    "        unique_pairs.append(pair)\n",
    "\n",
    "print(f\"\\nTotal pairs after merge: {len(all_pairs)}\")\n",
    "print(f\"Unique pairs: {len(unique_pairs)}\")\n",
    "\n",
    "# Statistics by type\n",
    "type_counts = defaultdict(int)\n",
    "for pair in unique_pairs:\n",
    "    type_counts[pair.pair_type] += 1\n",
    "\n",
    "print(f\"\\nPairs by type:\")\n",
    "for pair_type, count in sorted(type_counts.items(), key=lambda x: -x[1]):\n",
    "    pct = count / len(unique_pairs) * 100\n",
    "    print(f\"  {pair_type}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 8. Verify Problem Term Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem Term Coverage After Augmentation:\n",
      "======================================================================\n",
      "Term               As Source    As Target  Partial Src  Partial Tgt\n",
      "----------------------------------------------------------------------\n",
      "추천                    6 (+6)       6 (+6)          287          439\n",
      "데이터베이스                5 (+5)       5 (+5)           19          154\n",
      "증상                    6 (+6)       6 (+6)          432         1269\n",
      "질환                    6 (+6)       6 (+6)          207         1355\n",
      "인슐린                   4 (+4)       4 (+4)           26           97\n"
     ]
    }
   ],
   "source": [
    "# Verify that problem terms are now covered\n",
    "pair_dicts = [asdict(p) for p in unique_pairs]\n",
    "new_coverage = check_term_coverage(pair_dicts, PROBLEM_TERMS)\n",
    "\n",
    "print(\"Problem Term Coverage After Augmentation:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Term':<15} {'As Source':>12} {'As Target':>12} {'Partial Src':>12} {'Partial Tgt':>12}\")\n",
    "print(\"-\" * 70)\n",
    "for term, stats in new_coverage.items():\n",
    "    src_change = stats['as_source'] - coverage[term]['as_source']\n",
    "    tgt_change = stats['as_target'] - coverage[term]['as_target']\n",
    "    src_str = f\"{stats['as_source']} (+{src_change})\" if src_change > 0 else str(stats['as_source'])\n",
    "    tgt_str = f\"{stats['as_target']} (+{tgt_change})\" if tgt_change > 0 else str(stats['as_target'])\n",
    "    print(f\"{term:<15} {src_str:>12} {tgt_str:>12} {stats['partial_source']:>12} {stats['partial_target']:>12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 9. Save Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 213081 pairs to /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/data/v21.4/augmented_synonym_pairs.jsonl\n",
      "Saved 335 single-term pairs to /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/data/v21.4/single_term_pairs.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Save augmented pairs\n",
    "output_path = V21_4_DATA_DIR / \"augmented_synonym_pairs.jsonl\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for pair in unique_pairs:\n",
    "        f.write(json.dumps(asdict(pair), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(unique_pairs)} pairs to {output_path}\")\n",
    "\n",
    "# Save single-term pairs separately for reference\n",
    "single_term_path = V21_4_DATA_DIR / \"single_term_pairs.jsonl\"\n",
    "with open(single_term_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for pair in single_term_pairs + identity_pairs:\n",
    "        f.write(json.dumps(asdict(pair), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(single_term_pairs) + len(identity_pairs)} single-term pairs to {single_term_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved MS MARCO triplets to /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/data/v21.4/msmarco_direct_triplets.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Save MS MARCO triplets for direct training use (Phase 3)\n",
    "if msmarco_triplets:\n",
    "    msmarco_output_path = V21_4_DATA_DIR / \"msmarco_direct_triplets.jsonl\"\n",
    "    \n",
    "    with open(msmarco_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for triplet in msmarco_triplets:\n",
    "            # Convert to standard triplet format\n",
    "            output_triplet = {\n",
    "                \"anchor\": triplet[\"anchor\"],\n",
    "                \"positive\": triplet[\"positive\"],\n",
    "                \"negative\": triplet.get(\"negative\", \"\"),\n",
    "                \"difficulty\": \"medium\",\n",
    "                \"length_class\": \"sentence\",\n",
    "                \"pair_type\": \"msmarco_direct\",\n",
    "            }\n",
    "            if output_triplet[\"negative\"]:  # Only save if has negative\n",
    "                f.write(json.dumps(output_triplet, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    print(f\"Saved MS MARCO triplets to {msmarco_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Distribution:\n",
      "==================================================\n",
      "  cluster                  :    66070 (31.0%)\n",
      "  retrieval                :    49998 (23.5%)\n",
      "  qa                       :    29926 (14.0%)\n",
      "  qa_context               :    29924 (14.0%)\n",
      "  news_summary             :    19195 (9.0%)\n",
      "  nli_entailment           :     8560 (4.0%)\n",
      "  sts                      :     6009 (2.8%)\n",
      "  copa                     :     3068 (1.4%)\n",
      "  single_term              :      202 (0.1%)\n",
      "  identity                 :      129 (0.1%)\n",
      "\n",
      "Source Length Distribution:\n",
      "==================================================\n",
      "  1-2 chars      :     9645 (4.5%)\n",
      "  3-5 chars      :    44547 (20.9%)\n",
      "  6-10 chars     :    20827 (9.8%)\n",
      "  11-20 chars    :    40045 (18.8%)\n",
      "  20+ chars      :    98017 (46.0%)\n"
     ]
    }
   ],
   "source": [
    "# Category distribution\n",
    "category_counts = defaultdict(int)\n",
    "for pair in unique_pairs:\n",
    "    category_counts[pair.category] += 1\n",
    "\n",
    "print(\"Category Distribution:\")\n",
    "print(\"=\" * 50)\n",
    "for category, count in sorted(category_counts.items(), key=lambda x: -x[1])[:20]:\n",
    "    pct = count / len(unique_pairs) * 100\n",
    "    print(f\"  {category:<25}: {count:>8} ({pct:.1f}%)\")\n",
    "\n",
    "# Source length distribution\n",
    "length_bins = {\"1-2 chars\": 0, \"3-5 chars\": 0, \"6-10 chars\": 0, \"11-20 chars\": 0, \"20+ chars\": 0}\n",
    "for pair in unique_pairs:\n",
    "    src_len = len(pair.source)\n",
    "    if src_len <= 2:\n",
    "        length_bins[\"1-2 chars\"] += 1\n",
    "    elif src_len <= 5:\n",
    "        length_bins[\"3-5 chars\"] += 1\n",
    "    elif src_len <= 10:\n",
    "        length_bins[\"6-10 chars\"] += 1\n",
    "    elif src_len <= 20:\n",
    "        length_bins[\"11-20 chars\"] += 1\n",
    "    else:\n",
    "        length_bins[\"20+ chars\"] += 1\n",
    "\n",
    "print(\"\\nSource Length Distribution:\")\n",
    "print(\"=\" * 50)\n",
    "for length, count in length_bins.items():\n",
    "    pct = count / len(unique_pairs) * 100\n",
    "    print(f\"  {length:<15}: {count:>8} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "v21.4 Data Augmentation Summary\n",
      "============================================================\n",
      "\n",
      "Data Sources:\n",
      "  v21.3 Original Pairs: 66070\n",
      "  HuggingFace Pairs: 146680\n",
      "  Single-term Pairs: 206\n",
      "  Identity Pairs: 129\n",
      "  MS MARCO Triplets: 50000\n",
      "\n",
      "Final Output:\n",
      "  Total Unique Pairs: 213081\n",
      "\n",
      "Output Files:\n",
      "  single_term_pairs.jsonl: 0.04 MB\n",
      "  phase2_balanced_triplets.jsonl: 2.61 MB\n",
      "  phase1_single_term_focus_triplets.jsonl: 17.70 MB\n",
      "  validation_triplets.jsonl: 2.63 MB\n",
      "  augmented_synonym_pairs.jsonl: 76.58 MB\n",
      "  phase3_full_triplets.jsonl: 26.59 MB\n",
      "  msmarco_direct_triplets.jsonl: 45.82 MB\n",
      "  training_triplets.jsonl: 23.95 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"v21.4 Data Augmentation Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nData Sources:\")\n",
    "print(f\"  v21.3 Original Pairs: {len(original_pairs)}\")\n",
    "print(f\"  HuggingFace Pairs: {len(huggingface_pairs)}\")\n",
    "print(f\"  Single-term Pairs: {len(single_term_pairs)}\")\n",
    "print(f\"  Identity Pairs: {len(identity_pairs)}\")\n",
    "print(f\"  MS MARCO Triplets: {len(msmarco_triplets)}\")\n",
    "\n",
    "print(f\"\\nFinal Output:\")\n",
    "print(f\"  Total Unique Pairs: {len(unique_pairs)}\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "for f in V21_4_DATA_DIR.glob(\"*.jsonl\"):\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"  {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Data Augmentation Results\n",
    "\n",
    "| Source | Count |\n",
    "|--------|-------|\n",
    "| v21.3 Original | From filtered data |\n",
    "| HuggingFace (KLUE, KorQuAD, etc.) | Large-scale |\n",
    "| Single-term pairs | Manually curated |\n",
    "| Identity pairs | Self-reconstruction |\n",
    "| MS MARCO Korean | Direct triplets |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Run `02_data_preparation.ipynb` to generate training triplets\n",
    "2. Apply length-balanced sampling for curriculum learning\n",
    "3. Include MS MARCO triplets in Phase 3 training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

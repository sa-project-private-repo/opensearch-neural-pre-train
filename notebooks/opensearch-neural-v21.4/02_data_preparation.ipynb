{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v21.4 Data Preparation\n",
    "\n",
    "Generate training triplets with length-balanced sampling for curriculum learning.\n",
    "\n",
    "## Key Improvements\n",
    "\n",
    "1. **Length-balanced batches**: 30% single-term, 30% short phrases, 40% sentences\n",
    "2. **Difficulty-balanced negatives**: Easy/Medium/Hard\n",
    "3. **Curriculum-aware splits**: Phase-specific data subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set, Tuple, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "V21_4_DATA_DIR = PROJECT_ROOT / \"data\" / \"v21.4\"\n",
    "CORPUS_DIR = PROJECT_ROOT / \"dataset\" / \"v21.3_filtered_enhanced\"\n",
    "\n",
    "# Load augmented pairs\n",
    "augmented_pairs_path = V21_4_DATA_DIR / \"augmented_synonym_pairs.jsonl\"\n",
    "\n",
    "pairs = []\n",
    "with open(augmented_pairs_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        pairs.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(pairs)} augmented pairs\")\n",
    "\n",
    "# Build source -> targets mapping\n",
    "source_to_targets: Dict[str, Set[str]] = defaultdict(set)\n",
    "for pair in pairs:\n",
    "    source_to_targets[pair[\"source\"]].add(pair[\"target\"])\n",
    "\n",
    "print(f\"Unique sources: {len(source_to_targets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Load Corpus for Negative Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus vocabulary for negative mining\n",
    "corpus_vocab_path = CORPUS_DIR / \"corpus_vocabulary.json\"\n",
    "\n",
    "if corpus_vocab_path.exists():\n",
    "    with open(corpus_vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        corpus_vocab = json.load(f)\n",
    "    print(f\"Loaded corpus vocabulary: {len(corpus_vocab)} terms\")\n",
    "else:\n",
    "    # Build vocabulary from pairs\n",
    "    corpus_vocab = {}\n",
    "    all_terms = set()\n",
    "    for pair in pairs:\n",
    "        all_terms.add(pair[\"source\"])\n",
    "        all_terms.add(pair[\"target\"])\n",
    "    for term in all_terms:\n",
    "        corpus_vocab[term] = 1  # Dummy count\n",
    "    print(f\"Built vocabulary from pairs: {len(corpus_vocab)} terms\")\n",
    "\n",
    "# Get all unique terms\n",
    "all_terms = list(corpus_vocab.keys())\n",
    "print(f\"Total unique terms: {len(all_terms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Classify Pairs by Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_length(text: str) -> str:\n",
    "    \"\"\"Classify text by character length (proxy for token count).\"\"\"\n",
    "    length = len(text)\n",
    "    if length <= 3:  # 1-2 syllables (single term)\n",
    "        return \"single_term\"\n",
    "    elif length <= 8:  # 3-4 syllables (short phrase)\n",
    "        return \"short_phrase\"\n",
    "    else:  # 5+ syllables (sentence/long phrase)\n",
    "        return \"sentence\"\n",
    "\n",
    "\n",
    "# Classify all pairs\n",
    "pairs_by_length = defaultdict(list)\n",
    "for pair in pairs:\n",
    "    length_class = classify_length(pair[\"source\"])\n",
    "    pairs_by_length[length_class].append(pair)\n",
    "\n",
    "print(\"Pairs by length class:\")\n",
    "for length_class, class_pairs in pairs_by_length.items():\n",
    "    pct = len(class_pairs) / len(pairs) * 100\n",
    "    print(f\"  {length_class:<15}: {len(class_pairs):>6} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Hard Negative Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingTriplet:\n",
    "    anchor: str\n",
    "    positive: str\n",
    "    negative: str\n",
    "    difficulty: str  # \"easy\", \"medium\", \"hard\"\n",
    "    length_class: str  # \"single_term\", \"short_phrase\", \"sentence\"\n",
    "    pair_type: str  # \"original\", \"single_term\", \"identity\"\n",
    "\n",
    "\n",
    "def find_similar_negatives(source: str, all_terms: List[str], \n",
    "                           positives: Set[str], n: int = 10) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Find similar-looking but semantically different negatives.\n",
    "    Returns list of (negative, difficulty) tuples.\n",
    "    \"\"\"\n",
    "    negatives = []\n",
    "    source_len = len(source)\n",
    "    \n",
    "    # Shuffle for randomness\n",
    "    shuffled_terms = random.sample(all_terms, min(len(all_terms), 5000))\n",
    "    \n",
    "    for term in shuffled_terms:\n",
    "        if term == source or term in positives:\n",
    "            continue\n",
    "        \n",
    "        term_len = len(term)\n",
    "        \n",
    "        # Calculate similarity based on character overlap and length\n",
    "        common_chars = len(set(source) & set(term))\n",
    "        len_diff = abs(source_len - term_len)\n",
    "        \n",
    "        # Hard: Similar length, some character overlap\n",
    "        if len_diff <= 2 and common_chars >= 1:\n",
    "            negatives.append((term, \"hard\"))\n",
    "        # Medium: Similar length OR some overlap\n",
    "        elif len_diff <= 3 or common_chars >= 1:\n",
    "            negatives.append((term, \"medium\"))\n",
    "        # Easy: Different length, no overlap\n",
    "        else:\n",
    "            negatives.append((term, \"easy\"))\n",
    "        \n",
    "        if len(negatives) >= n * 3:  # Get extra for balancing\n",
    "            break\n",
    "    \n",
    "    return negatives\n",
    "\n",
    "\n",
    "def generate_triplets_for_pair(pair: Dict, all_terms: List[str],\n",
    "                               source_to_targets: Dict[str, Set[str]],\n",
    "                               n_negatives: int = 5) -> List[TrainingTriplet]:\n",
    "    \"\"\"Generate training triplets for a synonym pair.\"\"\"\n",
    "    source = pair[\"source\"]\n",
    "    target = pair[\"target\"]\n",
    "    positives = source_to_targets.get(source, set())\n",
    "    length_class = classify_length(source)\n",
    "    pair_type = pair.get(\"pair_type\", \"original\")\n",
    "    \n",
    "    # Find negatives with difficulty labels\n",
    "    negatives = find_similar_negatives(source, all_terms, positives, n_negatives * 2)\n",
    "    \n",
    "    # Balance by difficulty\n",
    "    by_difficulty = defaultdict(list)\n",
    "    for neg, diff in negatives:\n",
    "        by_difficulty[diff].append(neg)\n",
    "    \n",
    "    triplets = []\n",
    "    for difficulty in [\"easy\", \"medium\", \"hard\"]:\n",
    "        candidates = by_difficulty[difficulty]\n",
    "        n_select = min(len(candidates), max(1, n_negatives // 3))\n",
    "        selected = random.sample(candidates, n_select) if candidates else []\n",
    "        \n",
    "        for neg in selected:\n",
    "            triplets.append(TrainingTriplet(\n",
    "                anchor=source,\n",
    "                positive=target,\n",
    "                negative=neg,\n",
    "                difficulty=difficulty,\n",
    "                length_class=length_class,\n",
    "                pair_type=pair_type,\n",
    "            ))\n",
    "    \n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate triplets for all pairs\n",
    "all_triplets = []\n",
    "\n",
    "for pair in tqdm(pairs, desc=\"Generating triplets\"):\n",
    "    triplets = generate_triplets_for_pair(\n",
    "        pair, all_terms, source_to_targets, n_negatives=5\n",
    "    )\n",
    "    all_triplets.extend(triplets)\n",
    "\n",
    "print(f\"\\nGenerated {len(all_triplets)} triplets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Triplet Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "print(\"Triplet Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# By difficulty\n",
    "difficulty_counts = defaultdict(int)\n",
    "for t in all_triplets:\n",
    "    difficulty_counts[t.difficulty] += 1\n",
    "\n",
    "print(\"\\nBy Difficulty:\")\n",
    "for diff, count in sorted(difficulty_counts.items()):\n",
    "    pct = count / len(all_triplets) * 100\n",
    "    print(f\"  {diff:<10}: {count:>8} ({pct:.1f}%)\")\n",
    "\n",
    "# By length class\n",
    "length_counts = defaultdict(int)\n",
    "for t in all_triplets:\n",
    "    length_counts[t.length_class] += 1\n",
    "\n",
    "print(\"\\nBy Length Class:\")\n",
    "for lc, count in sorted(length_counts.items()):\n",
    "    pct = count / len(all_triplets) * 100\n",
    "    print(f\"  {lc:<15}: {count:>8} ({pct:.1f}%)\")\n",
    "\n",
    "# By pair type\n",
    "type_counts = defaultdict(int)\n",
    "for t in all_triplets:\n",
    "    type_counts[t.pair_type] += 1\n",
    "\n",
    "print(\"\\nBy Pair Type:\")\n",
    "for pt, count in sorted(type_counts.items()):\n",
    "    pct = count / len(all_triplets) * 100\n",
    "    print(f\"  {pt:<15}: {count:>8} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Create Length-Balanced Subsets for Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_curriculum_splits(triplets: List[TrainingTriplet]) -> Dict[str, List[TrainingTriplet]]:\n",
    "    \"\"\"\n",
    "    Create curriculum learning splits:\n",
    "    - Phase 1: Single-term heavy (50% single, 30% short, 20% sentence)\n",
    "    - Phase 2: Balanced (33% each)\n",
    "    - Phase 3: Full data\n",
    "    \"\"\"\n",
    "    # Group by length class\n",
    "    by_length = defaultdict(list)\n",
    "    for t in triplets:\n",
    "        by_length[t.length_class].append(t)\n",
    "    \n",
    "    single_term = by_length[\"single_term\"]\n",
    "    short_phrase = by_length[\"short_phrase\"]\n",
    "    sentence = by_length[\"sentence\"]\n",
    "    \n",
    "    print(f\"Available: single={len(single_term)}, short={len(short_phrase)}, sentence={len(sentence)}\")\n",
    "    \n",
    "    # Phase 1: Single-term focus\n",
    "    # Use all single-term, sample from others\n",
    "    phase1_size = len(single_term) * 2  # Target total size\n",
    "    phase1 = single_term.copy()\n",
    "    phase1 += random.sample(short_phrase, min(len(short_phrase), int(phase1_size * 0.3)))\n",
    "    phase1 += random.sample(sentence, min(len(sentence), int(phase1_size * 0.2)))\n",
    "    random.shuffle(phase1)\n",
    "    \n",
    "    # Phase 2: Balanced\n",
    "    min_class_size = min(len(single_term), len(short_phrase), len(sentence))\n",
    "    phase2_per_class = min_class_size\n",
    "    phase2 = []\n",
    "    phase2 += random.sample(single_term, min(len(single_term), phase2_per_class))\n",
    "    phase2 += random.sample(short_phrase, min(len(short_phrase), phase2_per_class))\n",
    "    phase2 += random.sample(sentence, min(len(sentence), phase2_per_class))\n",
    "    random.shuffle(phase2)\n",
    "    \n",
    "    # Phase 3: Full data\n",
    "    phase3 = triplets.copy()\n",
    "    random.shuffle(phase3)\n",
    "    \n",
    "    return {\n",
    "        \"phase1_single_term_focus\": phase1,\n",
    "        \"phase2_balanced\": phase2,\n",
    "        \"phase3_full\": phase3,\n",
    "    }\n",
    "\n",
    "\n",
    "curriculum_splits = create_curriculum_splits(all_triplets)\n",
    "\n",
    "print(\"\\nCurriculum Splits:\")\n",
    "for phase, data in curriculum_splits.items():\n",
    "    print(f\"  {phase}: {len(data)} triplets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 7. Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(triplets: List[TrainingTriplet], \n",
    "                    val_ratio: float = 0.1) -> Tuple[List[TrainingTriplet], List[TrainingTriplet]]:\n",
    "    \"\"\"Split triplets into train and validation sets.\"\"\"\n",
    "    # Group by anchor to prevent data leakage\n",
    "    by_anchor = defaultdict(list)\n",
    "    for t in triplets:\n",
    "        by_anchor[t.anchor].append(t)\n",
    "    \n",
    "    anchors = list(by_anchor.keys())\n",
    "    random.shuffle(anchors)\n",
    "    \n",
    "    val_size = int(len(anchors) * val_ratio)\n",
    "    val_anchors = set(anchors[:val_size])\n",
    "    \n",
    "    train_triplets = []\n",
    "    val_triplets = []\n",
    "    \n",
    "    for anchor, anchor_triplets in by_anchor.items():\n",
    "        if anchor in val_anchors:\n",
    "            val_triplets.extend(anchor_triplets)\n",
    "        else:\n",
    "            train_triplets.extend(anchor_triplets)\n",
    "    \n",
    "    return train_triplets, val_triplets\n",
    "\n",
    "\n",
    "# Split full dataset\n",
    "train_triplets, val_triplets = train_val_split(all_triplets, val_ratio=0.1)\n",
    "\n",
    "print(f\"Train triplets: {len(train_triplets)}\")\n",
    "print(f\"Validation triplets: {len(val_triplets)}\")\n",
    "print(f\"Validation ratio: {len(val_triplets) / (len(train_triplets) + len(val_triplets)) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 8. Save Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_triplets(triplets: List[TrainingTriplet], path: Path):\n",
    "    \"\"\"Save triplets to JSONL file.\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for t in triplets:\n",
    "            f.write(json.dumps(asdict(t), ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"Saved {len(triplets)} triplets to {path}\")\n",
    "\n",
    "\n",
    "# Save main splits\n",
    "save_triplets(train_triplets, V21_4_DATA_DIR / \"training_triplets.jsonl\")\n",
    "save_triplets(val_triplets, V21_4_DATA_DIR / \"validation_triplets.jsonl\")\n",
    "\n",
    "# Save curriculum splits\n",
    "for phase, data in curriculum_splits.items():\n",
    "    save_triplets(data, V21_4_DATA_DIR / f\"{phase}_triplets.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 9. Verify Single-term Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify problem terms are in training data\n",
    "PROBLEM_TERMS = [\"추천\", \"데이터베이스\", \"증상\", \"질환\", \"인슐린\"]\n",
    "\n",
    "print(\"Problem Term Coverage in Training Triplets:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for term in PROBLEM_TERMS:\n",
    "    as_anchor = sum(1 for t in train_triplets if t.anchor == term)\n",
    "    as_positive = sum(1 for t in train_triplets if t.positive == term)\n",
    "    total = as_anchor + as_positive\n",
    "    print(f\"{term:<15}: anchor={as_anchor:>4}, positive={as_positive:>4}, total={total:>4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"v21.4 Data Preparation Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nInput:\")\n",
    "print(f\"  Augmented pairs: {len(pairs)}\")\n",
    "\n",
    "print(f\"\\nOutput:\")\n",
    "print(f\"  Total triplets: {len(all_triplets)}\")\n",
    "print(f\"  Training triplets: {len(train_triplets)}\")\n",
    "print(f\"  Validation triplets: {len(val_triplets)}\")\n",
    "\n",
    "print(f\"\\nCurriculum Phases:\")\n",
    "for phase, data in curriculum_splits.items():\n",
    "    length_dist = defaultdict(int)\n",
    "    for t in data:\n",
    "        length_dist[t.length_class] += 1\n",
    "    print(f\"  {phase}:\")\n",
    "    for lc in [\"single_term\", \"short_phrase\", \"sentence\"]:\n",
    "        pct = length_dist[lc] / len(data) * 100 if data else 0\n",
    "        print(f\"    {lc}: {length_dist[lc]} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "for f in V21_4_DATA_DIR.glob(\"*.jsonl\"):\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"  {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run `03_training.ipynb` with curriculum learning\n",
    "2. Use phase-specific data for each training phase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

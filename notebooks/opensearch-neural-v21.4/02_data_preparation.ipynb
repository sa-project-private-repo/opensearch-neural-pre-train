{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v21.4 Data Preparation\n",
    "\n",
    "Generate training triplets with length-balanced sampling for curriculum learning.\n",
    "\n",
    "## Key Improvements\n",
    "\n",
    "1. **Length-balanced batches**: 30% single-term, 30% short phrases, 40% sentences\n",
    "2. **Difficulty-balanced negatives**: Easy/Medium/Hard\n",
    "3. **Curriculum-aware splits**: Phase-specific data subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set, Tuple, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 213081 augmented pairs\n",
      "Unique sources: 143062\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "V21_4_DATA_DIR = PROJECT_ROOT / \"data\" / \"v21.4\"\n",
    "CORPUS_DIR = PROJECT_ROOT / \"dataset\" / \"v21.3_filtered_enhanced\"\n",
    "\n",
    "# Load augmented pairs\n",
    "augmented_pairs_path = V21_4_DATA_DIR / \"augmented_synonym_pairs.jsonl\"\n",
    "\n",
    "pairs = []\n",
    "with open(augmented_pairs_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        pairs.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(pairs)} augmented pairs\")\n",
    "\n",
    "# Build source -> targets mapping\n",
    "source_to_targets: Dict[str, Set[str]] = defaultdict(set)\n",
    "for pair in pairs:\n",
    "    source_to_targets[pair[\"source\"]].add(pair[\"target\"])\n",
    "\n",
    "print(f\"Unique sources: {len(source_to_targets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Load Corpus for Negative Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded term_list.json as fallback: 150000 terms\n",
      "Total unique terms: 150000\n"
     ]
    }
   ],
   "source": [
    "# Load corpus vocabulary for negative mining\n",
    "corpus_vocab_path = CORPUS_DIR / \"corpus_vocabulary.json\"\n",
    "term_list_path = CORPUS_DIR / \"term_list.json\"\n",
    "\n",
    "if corpus_vocab_path.exists():\n",
    "    with open(corpus_vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        corpus_vocab = json.load(f)\n",
    "    print(f\"Loaded corpus vocabulary: {len(corpus_vocab)} terms\")\n",
    "elif term_list_path.exists():\n",
    "    # Fallback: use term_list.json\n",
    "    with open(term_list_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        term_list = json.load(f)\n",
    "    # Convert list to dictionary format\n",
    "    corpus_vocab = {term: 1 for term in term_list}\n",
    "    print(f\"Loaded term_list.json as fallback: {len(corpus_vocab)} terms\")\n",
    "else:\n",
    "    # Build vocabulary from pairs\n",
    "    corpus_vocab = {}\n",
    "    all_terms = set()\n",
    "    for pair in pairs:\n",
    "        all_terms.add(pair[\"source\"])\n",
    "        all_terms.add(pair[\"target\"])\n",
    "    for term in all_terms:\n",
    "        corpus_vocab[term] = 1  # Dummy count\n",
    "    print(f\"Built vocabulary from pairs: {len(corpus_vocab)} terms\")\n",
    "\n",
    "# Get all unique terms\n",
    "all_terms = list(corpus_vocab.keys())\n",
    "print(f\"Total unique terms: {len(all_terms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Classify Pairs by Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs by length class:\n",
      "  single_term    :  27599 (13.0%)\n",
      "  short_phrase   :  41300 (19.4%)\n",
      "  sentence       : 144182 (67.7%)\n"
     ]
    }
   ],
   "source": [
    "def classify_length(text: str) -> str:\n",
    "    \"\"\"Classify text by character length (proxy for token count).\"\"\"\n",
    "    length = len(text)\n",
    "    if length <= 3:  # 1-2 syllables (single term)\n",
    "        return \"single_term\"\n",
    "    elif length <= 8:  # 3-4 syllables (short phrase)\n",
    "        return \"short_phrase\"\n",
    "    else:  # 5+ syllables (sentence/long phrase)\n",
    "        return \"sentence\"\n",
    "\n",
    "\n",
    "# Classify all pairs\n",
    "pairs_by_length = defaultdict(list)\n",
    "for pair in pairs:\n",
    "    length_class = classify_length(pair[\"source\"])\n",
    "    pairs_by_length[length_class].append(pair)\n",
    "\n",
    "print(\"Pairs by length class:\")\n",
    "for length_class, class_pairs in pairs_by_length.items():\n",
    "    pct = len(class_pairs) / len(pairs) * 100\n",
    "    print(f\"  {length_class:<15}: {len(class_pairs):>6} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Hard Negative Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingTriplet:\n",
    "    anchor: str\n",
    "    positive: str\n",
    "    negative: str\n",
    "    difficulty: str  # \"easy\", \"medium\", \"hard\"\n",
    "    length_class: str  # \"single_term\", \"short_phrase\", \"sentence\"\n",
    "    pair_type: str  # \"original\", \"single_term\", \"identity\"\n",
    "\n",
    "\n",
    "def find_similar_negatives(source: str, all_terms: List[str], \n",
    "                           positives: Set[str], n: int = 10) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Find similar-looking but semantically different negatives.\n",
    "    Returns list of (negative, difficulty) tuples.\n",
    "    \"\"\"\n",
    "    negatives = []\n",
    "    source_len = len(source)\n",
    "    \n",
    "    # Shuffle for randomness\n",
    "    shuffled_terms = random.sample(all_terms, min(len(all_terms), 5000))\n",
    "    \n",
    "    for term in shuffled_terms:\n",
    "        if term == source or term in positives:\n",
    "            continue\n",
    "        \n",
    "        term_len = len(term)\n",
    "        \n",
    "        # Calculate similarity based on character overlap and length\n",
    "        common_chars = len(set(source) & set(term))\n",
    "        len_diff = abs(source_len - term_len)\n",
    "        \n",
    "        # Hard: Similar length, some character overlap\n",
    "        if len_diff <= 2 and common_chars >= 1:\n",
    "            negatives.append((term, \"hard\"))\n",
    "        # Medium: Similar length OR some overlap\n",
    "        elif len_diff <= 3 or common_chars >= 1:\n",
    "            negatives.append((term, \"medium\"))\n",
    "        # Easy: Different length, no overlap\n",
    "        else:\n",
    "            negatives.append((term, \"easy\"))\n",
    "        \n",
    "        if len(negatives) >= n * 3:  # Get extra for balancing\n",
    "            break\n",
    "    \n",
    "    return negatives\n",
    "\n",
    "\n",
    "def generate_triplets_for_pair(pair: Dict, all_terms: List[str],\n",
    "                               source_to_targets: Dict[str, Set[str]],\n",
    "                               n_negatives: int = 5) -> List[TrainingTriplet]:\n",
    "    \"\"\"Generate training triplets for a synonym pair.\"\"\"\n",
    "    source = pair[\"source\"]\n",
    "    target = pair[\"target\"]\n",
    "    positives = source_to_targets.get(source, set())\n",
    "    length_class = classify_length(source)\n",
    "    pair_type = pair.get(\"pair_type\", \"original\")\n",
    "    \n",
    "    # Find negatives with difficulty labels\n",
    "    negatives = find_similar_negatives(source, all_terms, positives, n_negatives * 2)\n",
    "    \n",
    "    # Balance by difficulty\n",
    "    by_difficulty = defaultdict(list)\n",
    "    for neg, diff in negatives:\n",
    "        by_difficulty[diff].append(neg)\n",
    "    \n",
    "    triplets = []\n",
    "    for difficulty in [\"easy\", \"medium\", \"hard\"]:\n",
    "        candidates = by_difficulty[difficulty]\n",
    "        n_select = min(len(candidates), max(1, n_negatives // 3))\n",
    "        selected = random.sample(candidates, n_select) if candidates else []\n",
    "        \n",
    "        for neg in selected:\n",
    "            triplets.append(TrainingTriplet(\n",
    "                anchor=source,\n",
    "                positive=target,\n",
    "                negative=neg,\n",
    "                difficulty=difficulty,\n",
    "                length_class=length_class,\n",
    "                pair_type=pair_type,\n",
    "            ))\n",
    "    \n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating triplets: 100%|██████████| 213081/213081 [03:38<00:00, 976.97it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 470675 triplets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate triplets for all pairs\n",
    "all_triplets = []\n",
    "\n",
    "for pair in tqdm(pairs, desc=\"Generating triplets\"):\n",
    "    triplets = generate_triplets_for_pair(\n",
    "        pair, all_terms, source_to_targets, n_negatives=5\n",
    "    )\n",
    "    all_triplets.extend(triplets)\n",
    "\n",
    "print(f\"\\nGenerated {len(all_triplets)} triplets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Triplet Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Statistics:\n",
      "==================================================\n",
      "\n",
      "By Difficulty:\n",
      "  easy      :   205489 (43.7%)\n",
      "  hard      :    52380 (11.1%)\n",
      "  medium    :   212806 (45.2%)\n",
      "\n",
      "By Length Class:\n",
      "  sentence       :   294475 (62.6%)\n",
      "  short_phrase   :   108141 (23.0%)\n",
      "  single_term    :    68059 (14.5%)\n",
      "\n",
      "By Pair Type:\n",
      "  identity       :      319 (0.1%)\n",
      "  klue_nli       :    17115 (3.6%)\n",
      "  klue_sts       :    12043 (2.6%)\n",
      "  kobest_copa    :     6733 (1.4%)\n",
      "  korquad        :    60096 (12.8%)\n",
      "  korquad_context:    60079 (12.8%)\n",
      "  msmarco_ko     :   107580 (22.9%)\n",
      "  naver_news     :    38460 (8.2%)\n",
      "  original       :   167753 (35.6%)\n",
      "  single_term    :      497 (0.1%)\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "print(\"Triplet Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# By difficulty\n",
    "difficulty_counts = defaultdict(int)\n",
    "for t in all_triplets:\n",
    "    difficulty_counts[t.difficulty] += 1\n",
    "\n",
    "print(\"\\nBy Difficulty:\")\n",
    "for diff, count in sorted(difficulty_counts.items()):\n",
    "    pct = count / len(all_triplets) * 100\n",
    "    print(f\"  {diff:<10}: {count:>8} ({pct:.1f}%)\")\n",
    "\n",
    "# By length class\n",
    "length_counts = defaultdict(int)\n",
    "for t in all_triplets:\n",
    "    length_counts[t.length_class] += 1\n",
    "\n",
    "print(\"\\nBy Length Class:\")\n",
    "for lc, count in sorted(length_counts.items()):\n",
    "    pct = count / len(all_triplets) * 100\n",
    "    print(f\"  {lc:<15}: {count:>8} ({pct:.1f}%)\")\n",
    "\n",
    "# By pair type\n",
    "type_counts = defaultdict(int)\n",
    "for t in all_triplets:\n",
    "    type_counts[t.pair_type] += 1\n",
    "\n",
    "print(\"\\nBy Pair Type:\")\n",
    "for pt, count in sorted(type_counts.items()):\n",
    "    pct = count / len(all_triplets) * 100\n",
    "    print(f\"  {pt:<15}: {count:>8} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Create Length-Balanced Subsets for Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available: single=68059, short=108141, sentence=294475\n",
      "\n",
      "Curriculum Splits:\n",
      "  phase1_single_term_focus: 136117 triplets\n",
      "  phase2_balanced: 204177 triplets\n",
      "  phase3_full: 470675 triplets\n"
     ]
    }
   ],
   "source": [
    "def create_curriculum_splits(triplets: List[TrainingTriplet]) -> Dict[str, List[TrainingTriplet]]:\n",
    "    \"\"\"\n",
    "    Create curriculum learning splits:\n",
    "    - Phase 1: Single-term heavy (50% single, 30% short, 20% sentence)\n",
    "    - Phase 2: Balanced (33% each)\n",
    "    - Phase 3: Full data\n",
    "    \"\"\"\n",
    "    # Group by length class\n",
    "    by_length = defaultdict(list)\n",
    "    for t in triplets:\n",
    "        by_length[t.length_class].append(t)\n",
    "    \n",
    "    single_term = by_length[\"single_term\"]\n",
    "    short_phrase = by_length[\"short_phrase\"]\n",
    "    sentence = by_length[\"sentence\"]\n",
    "    \n",
    "    print(f\"Available: single={len(single_term)}, short={len(short_phrase)}, sentence={len(sentence)}\")\n",
    "    \n",
    "    # Phase 1: Single-term focus\n",
    "    # Use all single-term, sample from others\n",
    "    phase1_size = len(single_term) * 2  # Target total size\n",
    "    phase1 = single_term.copy()\n",
    "    phase1 += random.sample(short_phrase, min(len(short_phrase), int(phase1_size * 0.3)))\n",
    "    phase1 += random.sample(sentence, min(len(sentence), int(phase1_size * 0.2)))\n",
    "    random.shuffle(phase1)\n",
    "    \n",
    "    # Phase 2: Balanced\n",
    "    min_class_size = min(len(single_term), len(short_phrase), len(sentence))\n",
    "    phase2_per_class = min_class_size\n",
    "    phase2 = []\n",
    "    phase2 += random.sample(single_term, min(len(single_term), phase2_per_class))\n",
    "    phase2 += random.sample(short_phrase, min(len(short_phrase), phase2_per_class))\n",
    "    phase2 += random.sample(sentence, min(len(sentence), phase2_per_class))\n",
    "    random.shuffle(phase2)\n",
    "    \n",
    "    # Phase 3: Full data\n",
    "    phase3 = triplets.copy()\n",
    "    random.shuffle(phase3)\n",
    "    \n",
    "    return {\n",
    "        \"phase1_single_term_focus\": phase1,\n",
    "        \"phase2_balanced\": phase2,\n",
    "        \"phase3_full\": phase3,\n",
    "    }\n",
    "\n",
    "\n",
    "curriculum_splits = create_curriculum_splits(all_triplets)\n",
    "\n",
    "print(\"\\nCurriculum Splits:\")\n",
    "for phase, data in curriculum_splits.items():\n",
    "    print(f\"  {phase}: {len(data)} triplets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313uj2zb52n",
   "metadata": {},
   "source": [
    "## 6.1 Load and Merge MS MARCO Direct Triplets\n",
    "\n",
    "MS MARCO triplets already have query-positive-negative triplets, so we add them directly to Phase 3 training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gms3ro1pxwg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 MS MARCO direct triplets\n",
      "Phase 3 size: 470675 -> 520675\n",
      "Added 50000 MS MARCO triplets to Phase 3\n"
     ]
    }
   ],
   "source": [
    "# Load MS MARCO direct triplets\n",
    "msmarco_triplets_path = V21_4_DATA_DIR / \"msmarco_direct_triplets.jsonl\"\n",
    "\n",
    "msmarco_triplets: List[TrainingTriplet] = []\n",
    "\n",
    "if msmarco_triplets_path.exists():\n",
    "    with open(msmarco_triplets_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            msmarco_triplets.append(TrainingTriplet(\n",
    "                anchor=item[\"anchor\"],\n",
    "                positive=item[\"positive\"],\n",
    "                negative=item.get(\"negative\", \"\"),\n",
    "                difficulty=item.get(\"difficulty\", \"medium\"),\n",
    "                length_class=item.get(\"length_class\", \"sentence\"),\n",
    "                pair_type=item.get(\"pair_type\", \"msmarco_direct\"),\n",
    "            ))\n",
    "    print(f\"Loaded {len(msmarco_triplets)} MS MARCO direct triplets\")\n",
    "    \n",
    "    # Merge into Phase 3 (full data)\n",
    "    original_phase3_size = len(curriculum_splits[\"phase3_full\"])\n",
    "    curriculum_splits[\"phase3_full\"].extend(msmarco_triplets)\n",
    "    random.shuffle(curriculum_splits[\"phase3_full\"])\n",
    "    \n",
    "    print(f\"Phase 3 size: {original_phase3_size} -> {len(curriculum_splits['phase3_full'])}\")\n",
    "    print(f\"Added {len(msmarco_triplets)} MS MARCO triplets to Phase 3\")\n",
    "else:\n",
    "    print(f\"Warning: {msmarco_triplets_path} not found\")\n",
    "    print(\"Run 01_data_augmentation.ipynb first to generate MS MARCO triplets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 7. Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train triplets: 423582\n",
      "Validation triplets: 47093\n",
      "Validation ratio: 10.0%\n"
     ]
    }
   ],
   "source": [
    "def train_val_split(triplets: List[TrainingTriplet], \n",
    "                    val_ratio: float = 0.1) -> Tuple[List[TrainingTriplet], List[TrainingTriplet]]:\n",
    "    \"\"\"Split triplets into train and validation sets.\"\"\"\n",
    "    # Group by anchor to prevent data leakage\n",
    "    by_anchor = defaultdict(list)\n",
    "    for t in triplets:\n",
    "        by_anchor[t.anchor].append(t)\n",
    "    \n",
    "    anchors = list(by_anchor.keys())\n",
    "    random.shuffle(anchors)\n",
    "    \n",
    "    val_size = int(len(anchors) * val_ratio)\n",
    "    val_anchors = set(anchors[:val_size])\n",
    "    \n",
    "    train_triplets = []\n",
    "    val_triplets = []\n",
    "    \n",
    "    for anchor, anchor_triplets in by_anchor.items():\n",
    "        if anchor in val_anchors:\n",
    "            val_triplets.extend(anchor_triplets)\n",
    "        else:\n",
    "            train_triplets.extend(anchor_triplets)\n",
    "    \n",
    "    return train_triplets, val_triplets\n",
    "\n",
    "\n",
    "# Split full dataset\n",
    "train_triplets, val_triplets = train_val_split(all_triplets, val_ratio=0.1)\n",
    "\n",
    "print(f\"Train triplets: {len(train_triplets)}\")\n",
    "print(f\"Validation triplets: {len(val_triplets)}\")\n",
    "print(f\"Validation ratio: {len(val_triplets) / (len(train_triplets) + len(val_triplets)) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 8. Save Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 423582 triplets to /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/data/v21.4/training_triplets.jsonl\n",
      "Saved 47093 triplets to /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/data/v21.4/validation_triplets.jsonl\n",
      "Saved 136117 triplets to /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/data/v21.4/phase1_single_term_focus_triplets.jsonl\n",
      "Saved 204177 triplets to /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/data/v21.4/phase2_balanced_triplets.jsonl\n",
      "Saved 520675 triplets to /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/data/v21.4/phase3_full_triplets.jsonl\n"
     ]
    }
   ],
   "source": [
    "def save_triplets(triplets: List[TrainingTriplet], path: Path):\n",
    "    \"\"\"Save triplets to JSONL file.\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for t in triplets:\n",
    "            f.write(json.dumps(asdict(t), ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"Saved {len(triplets)} triplets to {path}\")\n",
    "\n",
    "\n",
    "# Save main splits\n",
    "save_triplets(train_triplets, V21_4_DATA_DIR / \"training_triplets.jsonl\")\n",
    "save_triplets(val_triplets, V21_4_DATA_DIR / \"validation_triplets.jsonl\")\n",
    "\n",
    "# Save curriculum splits\n",
    "for phase, data in curriculum_splits.items():\n",
    "    save_triplets(data, V21_4_DATA_DIR / f\"{phase}_triplets.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 9. Verify Single-term Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem Term Coverage in Training Triplets:\n",
      "============================================================\n",
      "추천             : anchor=  15, positive=  12, total=  27\n",
      "데이터베이스         : anchor=   0, positive=   9, total=   9\n",
      "증상             : anchor=  16, positive=  14, total=  30\n",
      "질환             : anchor=  12, positive=  13, total=  25\n",
      "인슐린            : anchor=  10, positive=  10, total=  20\n"
     ]
    }
   ],
   "source": [
    "# Verify problem terms are in training data\n",
    "PROBLEM_TERMS = [\"추천\", \"데이터베이스\", \"증상\", \"질환\", \"인슐린\"]\n",
    "\n",
    "print(\"Problem Term Coverage in Training Triplets:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for term in PROBLEM_TERMS:\n",
    "    as_anchor = sum(1 for t in train_triplets if t.anchor == term)\n",
    "    as_positive = sum(1 for t in train_triplets if t.positive == term)\n",
    "    total = as_anchor + as_positive\n",
    "    print(f\"{term:<15}: anchor={as_anchor:>4}, positive={as_positive:>4}, total={total:>4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "v21.4 Data Preparation Summary\n",
      "============================================================\n",
      "\n",
      "Input:\n",
      "  Augmented pairs: 213081\n",
      "\n",
      "Output:\n",
      "  Total triplets: 470675\n",
      "  Training triplets: 423582\n",
      "  Validation triplets: 47093\n",
      "\n",
      "Curriculum Phases:\n",
      "  phase1_single_term_focus:\n",
      "    single_term: 68059 (50.0%)\n",
      "    short_phrase: 40835 (30.0%)\n",
      "    sentence: 27223 (20.0%)\n",
      "  phase2_balanced:\n",
      "    single_term: 68059 (33.3%)\n",
      "    short_phrase: 68059 (33.3%)\n",
      "    sentence: 68059 (33.3%)\n",
      "  phase3_full:\n",
      "    single_term: 68059 (13.1%)\n",
      "    short_phrase: 108141 (20.8%)\n",
      "    sentence: 344475 (66.2%)\n",
      "\n",
      "Output Files:\n",
      "  single_term_pairs.jsonl: 0.04 MB\n",
      "  phase2_balanced_triplets.jsonl: 57.20 MB\n",
      "  phase1_single_term_focus_triplets.jsonl: 31.75 MB\n",
      "  validation_triplets.jsonl: 17.73 MB\n",
      "  augmented_synonym_pairs.jsonl: 76.58 MB\n",
      "  phase3_full_triplets.jsonl: 222.92 MB\n",
      "  msmarco_direct_triplets.jsonl: 45.82 MB\n",
      "  training_triplets.jsonl: 159.37 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"v21.4 Data Preparation Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nInput:\")\n",
    "print(f\"  Augmented pairs: {len(pairs)}\")\n",
    "\n",
    "print(f\"\\nOutput:\")\n",
    "print(f\"  Total triplets: {len(all_triplets)}\")\n",
    "print(f\"  Training triplets: {len(train_triplets)}\")\n",
    "print(f\"  Validation triplets: {len(val_triplets)}\")\n",
    "\n",
    "print(f\"\\nCurriculum Phases:\")\n",
    "for phase, data in curriculum_splits.items():\n",
    "    length_dist = defaultdict(int)\n",
    "    for t in data:\n",
    "        length_dist[t.length_class] += 1\n",
    "    print(f\"  {phase}:\")\n",
    "    for lc in [\"single_term\", \"short_phrase\", \"sentence\"]:\n",
    "        pct = length_dist[lc] / len(data) * 100 if data else 0\n",
    "        print(f\"    {lc}: {length_dist[lc]} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "for f in V21_4_DATA_DIR.glob(\"*.jsonl\"):\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"  {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run `03_training.ipynb` with curriculum learning\n",
    "2. Use phase-specific data for each training phase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v21.4 Training with Curriculum Learning\n",
    "\n",
    "## Key Improvements over v21.3\n",
    "\n",
    "1. **Dynamic Lambda Self**: 8.0 for single terms → 4.0 for sentences\n",
    "2. **Minimum Activation Loss**: Prevents garbage outputs\n",
    "3. **Curriculum Learning**: 3 phases (single-term focus → balanced → full)\n",
    "4. **Adjusted FLOPS**: 3e-3 → 5e-3 (phased increase)\n",
    "\n",
    "## Target Metrics\n",
    "\n",
    "| Metric | v21.3 | v21.4 Target |\n",
    "|--------|-------|--------------|\n",
    "| Single-term Recall | 63.1% | 80%+ |\n",
    "| Garbage Ratio | ~15% | < 5% |\n",
    "| Sparsity | 95.55% | 95%+ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train\n",
      "PyTorch version: 2.10.0.dev20251109+cu130\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GB10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/west/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  queued_call()\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, get_linear_schedule_with_warmup\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Model: skt/A.X-Encoder-base\n",
      "  Epochs: 30\n",
      "  Batch size: 64\n",
      "  Learning rate: 3e-06\n",
      "  Lambda self range: 4.0 - 8.0\n",
      "\n",
      "Curriculum Phases:\n",
      "  phase1_single_term: epochs 1-10, lambda_flops=0.003, lr_mult=1.0\n",
      "  phase2_balanced: epochs 11-20, lambda_flops=0.004, lr_mult=0.5\n",
      "  phase3_full: epochs 21-30, lambda_flops=0.005, lr_mult=0.25\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class CurriculumPhase:\n",
    "    \"\"\"Configuration for a curriculum learning phase.\"\"\"\n",
    "    name: str\n",
    "    start_epoch: int\n",
    "    end_epoch: int\n",
    "    lambda_flops: float\n",
    "    lambda_min_activation: float\n",
    "    lr_multiplier: float\n",
    "    data_file: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration.\"\"\"\n",
    "    # Model\n",
    "    model_name: str = \"skt/A.X-Encoder-base\"\n",
    "    max_length: int = 64\n",
    "    \n",
    "    # Training\n",
    "    total_epochs: int = 30\n",
    "    batch_size: int = 64\n",
    "    learning_rate: float = 3e-6\n",
    "    warmup_ratio: float = 0.1\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Loss weights (static)\n",
    "    lambda_synonym: float = 10.0\n",
    "    lambda_margin: float = 2.5\n",
    "    target_margin: float = 1.5\n",
    "    \n",
    "    # Dynamic lambda_self\n",
    "    lambda_self_min: float = 4.0\n",
    "    lambda_self_max: float = 8.0\n",
    "    lambda_self_decay_tokens: int = 10\n",
    "    \n",
    "    # Minimum activation\n",
    "    min_activation_k: int = 5\n",
    "    \n",
    "    # Curriculum phases\n",
    "    phases: List[CurriculumPhase] = field(default_factory=list)\n",
    "    \n",
    "    # Paths\n",
    "    data_dir: Path = None\n",
    "    output_dir: Path = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.phases:\n",
    "            self.phases = [\n",
    "                CurriculumPhase(\n",
    "                    name=\"phase1_single_term\",\n",
    "                    start_epoch=1,\n",
    "                    end_epoch=10,\n",
    "                    lambda_flops=3e-3,\n",
    "                    lambda_min_activation=2.0,\n",
    "                    lr_multiplier=1.0,\n",
    "                    data_file=\"phase1_single_term_focus_triplets.jsonl\",\n",
    "                ),\n",
    "                CurriculumPhase(\n",
    "                    name=\"phase2_balanced\",\n",
    "                    start_epoch=11,\n",
    "                    end_epoch=20,\n",
    "                    lambda_flops=4e-3,\n",
    "                    lambda_min_activation=1.5,\n",
    "                    lr_multiplier=0.5,\n",
    "                    data_file=\"phase2_balanced_triplets.jsonl\",\n",
    "                ),\n",
    "                CurriculumPhase(\n",
    "                    name=\"phase3_full\",\n",
    "                    start_epoch=21,\n",
    "                    end_epoch=30,\n",
    "                    lambda_flops=5e-3,\n",
    "                    lambda_min_activation=1.0,\n",
    "                    lr_multiplier=0.25,\n",
    "                    data_file=\"phase3_full_triplets.jsonl\",\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "\n",
    "# Create config\n",
    "config = TrainingConfig(\n",
    "    data_dir=PROJECT_ROOT / \"data\" / \"v21.4\",\n",
    "    output_dir=PROJECT_ROOT / \"outputs\" / \"v21.4_korean_enhanced\",\n",
    ")\n",
    "config.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Epochs: {config.total_epochs}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Lambda self range: {config.lambda_self_min} - {config.lambda_self_max}\")\n",
    "print(f\"\\nCurriculum Phases:\")\n",
    "for phase in config.phases:\n",
    "    print(f\"  {phase.name}: epochs {phase.start_epoch}-{phase.end_epoch}, \"\n",
    "          f\"lambda_flops={phase.lambda_flops}, lr_mult={phase.lr_multiplier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: skt/A.X-Encoder-base\n",
      "Device: cuda\n",
      "Parameters: 149,372,240\n"
     ]
    }
   ],
   "source": [
    "class SPLADEModel(nn.Module):\n",
    "    \"\"\"SPLADE model for Korean sparse retrieval with v21.4 improvements.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"skt/A.X-Encoder-base\"):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        self.config = self.model.config\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Returns:\n",
    "            sparse_repr: [batch_size, vocab_size] - max-pooled sparse representation\n",
    "            token_weights: [batch_size, seq_len] - per-token max weights\n",
    "        \"\"\"\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # SPLADE transformation: log(1 + ReLU(x))\n",
    "        token_scores = torch.log1p(self.relu(logits))\n",
    "        \n",
    "        # Apply attention mask\n",
    "        mask = attention_mask.unsqueeze(-1).float()\n",
    "        token_scores = token_scores * mask\n",
    "        \n",
    "        # Max pooling over sequence length\n",
    "        sparse_repr, _ = token_scores.max(dim=1)\n",
    "        \n",
    "        # Token weights for analysis\n",
    "        token_weights = token_scores.max(dim=-1).values\n",
    "        \n",
    "        return sparse_repr, token_weights\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SPLADEModel(config.model_name)\n",
    "model = model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "print(f\"Model loaded: {config.model_name}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 47093 triplets from validation_triplets.jsonl\n"
     ]
    }
   ],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    \"\"\"Dataset for triplet training.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: Path, tokenizer, max_length: int = 64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "        \n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} triplets from {data_path.name}\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        anchor = self.tokenizer(\n",
    "            item[\"anchor\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        positive = self.tokenizer(\n",
    "            item[\"positive\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        negative = self.tokenizer(\n",
    "            item[\"negative\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"anchor_input_ids\": anchor[\"input_ids\"].squeeze(0),\n",
    "            \"anchor_attention_mask\": anchor[\"attention_mask\"].squeeze(0),\n",
    "            \"positive_input_ids\": positive[\"input_ids\"].squeeze(0),\n",
    "            \"positive_attention_mask\": positive[\"attention_mask\"].squeeze(0),\n",
    "            \"negative_input_ids\": negative[\"input_ids\"].squeeze(0),\n",
    "            \"negative_attention_mask\": negative[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "\n",
    "# Load validation data\n",
    "val_dataset = TripletDataset(\n",
    "    config.data_dir / \"validation_triplets.jsonl\",\n",
    "    tokenizer,\n",
    "    config.max_length,\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special token IDs: {0, 1, 2, 3, 4, 5, 49999}\n"
     ]
    }
   ],
   "source": [
    "def compute_dynamic_lambda_self(\n",
    "    attention_mask: torch.Tensor,\n",
    "    lambda_min: float = 4.0,\n",
    "    lambda_max: float = 8.0,\n",
    "    decay_tokens: int = 10,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute per-sample lambda_self based on input length.\n",
    "    \n",
    "    Short inputs (1-3 tokens) get higher lambda (8.0)\n",
    "    Long inputs (10+ tokens) get lower lambda (4.0)\n",
    "    \"\"\"\n",
    "    # Count actual tokens (excluding padding, CLS, SEP)\n",
    "    token_counts = attention_mask.sum(dim=1) - 2  # subtract CLS and SEP\n",
    "    token_counts = token_counts.clamp(min=1).float()\n",
    "    \n",
    "    # Linear interpolation\n",
    "    # lambda(n) = max(lambda_min, lambda_max - slope * (n - 1))\n",
    "    slope = (lambda_max - lambda_min) / (decay_tokens - 1)\n",
    "    lambda_weights = lambda_max - slope * (token_counts - 1)\n",
    "    lambda_weights = lambda_weights.clamp(min=lambda_min, max=lambda_max)\n",
    "    \n",
    "    return lambda_weights\n",
    "\n",
    "\n",
    "def compute_self_reconstruction_loss(\n",
    "    sparse_repr: torch.Tensor,\n",
    "    input_ids: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    lambda_weights: torch.Tensor,\n",
    "    pad_token_id: int,\n",
    "    special_token_ids: set,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute weighted self-reconstruction loss.\n",
    "    \n",
    "    Encourages the model to activate tokens that appear in the input.\n",
    "    \"\"\"\n",
    "    batch_size = sparse_repr.size(0)\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Get valid token IDs (exclude padding and special tokens)\n",
    "        valid_mask = (attention_mask[i] == 1)\n",
    "        token_ids = input_ids[i][valid_mask]\n",
    "        \n",
    "        # Filter special tokens\n",
    "        valid_ids = [tid.item() for tid in token_ids \n",
    "                     if tid.item() not in special_token_ids and tid.item() != pad_token_id]\n",
    "        \n",
    "        if not valid_ids:\n",
    "            losses.append(torch.tensor(0.0, device=sparse_repr.device))\n",
    "            continue\n",
    "        \n",
    "        # Get activations for input tokens\n",
    "        activations = sparse_repr[i, valid_ids]\n",
    "        \n",
    "        # Negative mean activation (we want to maximize)\n",
    "        loss = -activations.mean()\n",
    "        losses.append(loss * lambda_weights[i])\n",
    "    \n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "def compute_positive_activation_loss(\n",
    "    anchor_repr: torch.Tensor,\n",
    "    positive_input_ids: torch.Tensor,\n",
    "    positive_attention_mask: torch.Tensor,\n",
    "    pad_token_id: int,\n",
    "    special_token_ids: set,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Encourage anchor to activate positive synonym tokens.\n",
    "    \"\"\"\n",
    "    batch_size = anchor_repr.size(0)\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        valid_mask = (positive_attention_mask[i] == 1)\n",
    "        token_ids = positive_input_ids[i][valid_mask]\n",
    "        \n",
    "        valid_ids = [tid.item() for tid in token_ids \n",
    "                     if tid.item() not in special_token_ids and tid.item() != pad_token_id]\n",
    "        \n",
    "        if not valid_ids:\n",
    "            losses.append(torch.tensor(0.0, device=anchor_repr.device))\n",
    "            continue\n",
    "        \n",
    "        activations = anchor_repr[i, valid_ids]\n",
    "        loss = -activations.mean()\n",
    "        losses.append(loss)\n",
    "    \n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "def compute_triplet_margin_loss(\n",
    "    anchor_repr: torch.Tensor,\n",
    "    positive_repr: torch.Tensor,\n",
    "    negative_repr: torch.Tensor,\n",
    "    margin: float = 1.5,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Triplet margin loss with cosine similarity.\n",
    "    \"\"\"\n",
    "    pos_sim = F.cosine_similarity(anchor_repr, positive_repr, dim=-1)\n",
    "    neg_sim = F.cosine_similarity(anchor_repr, negative_repr, dim=-1)\n",
    "    \n",
    "    loss = F.relu(margin - pos_sim + neg_sim)\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def compute_flops_loss(sparse_repr: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    FLOPS regularization loss for sparsity.\n",
    "    \"\"\"\n",
    "    # Mean activation per vocabulary term across batch\n",
    "    mean_activations = sparse_repr.mean(dim=0)  # [vocab_size]\n",
    "    \n",
    "    # Sum of squared means\n",
    "    flops = (mean_activations ** 2).sum()\n",
    "    \n",
    "    return flops\n",
    "\n",
    "\n",
    "def compute_minimum_activation_loss(\n",
    "    sparse_repr: torch.Tensor,\n",
    "    k: int = 5,\n",
    "    epsilon: float = 1e-8,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Minimum activation loss to prevent garbage outputs.\n",
    "    \n",
    "    Ensures top-k activations maintain sufficient strength.\n",
    "    \"\"\"\n",
    "    # Get top-k activations for each sample\n",
    "    topk_values, _ = torch.topk(sparse_repr, k=k, dim=-1)\n",
    "    \n",
    "    # Mean of top-k per sample\n",
    "    mean_topk = topk_values.mean(dim=-1)\n",
    "    \n",
    "    # Negative log (encourages higher activations)\n",
    "    loss = -torch.log(mean_topk + epsilon).mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "# Get special token IDs\n",
    "special_token_ids = set()\n",
    "for attr in ['pad_token_id', 'cls_token_id', 'sep_token_id', \n",
    "             'unk_token_id', 'mask_token_id', 'bos_token_id', 'eos_token_id']:\n",
    "    token_id = getattr(tokenizer, attr, None)\n",
    "    if token_id is not None:\n",
    "        special_token_ids.add(token_id)\n",
    "\n",
    "print(f\"Special token IDs: {special_token_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler,\n",
    "    config: TrainingConfig,\n",
    "    phase: CurriculumPhase,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    loss_components = defaultdict(float)\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch} ({phase.name})\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        # Move to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        anchor_repr, _ = model(batch[\"anchor_input_ids\"], batch[\"anchor_attention_mask\"])\n",
    "        positive_repr, _ = model(batch[\"positive_input_ids\"], batch[\"positive_attention_mask\"])\n",
    "        negative_repr, _ = model(batch[\"negative_input_ids\"], batch[\"negative_attention_mask\"])\n",
    "        \n",
    "        # Compute dynamic lambda_self\n",
    "        lambda_weights = compute_dynamic_lambda_self(\n",
    "            batch[\"anchor_attention_mask\"],\n",
    "            config.lambda_self_min,\n",
    "            config.lambda_self_max,\n",
    "            config.lambda_self_decay_tokens,\n",
    "        )\n",
    "        \n",
    "        # Compute losses\n",
    "        loss_self = compute_self_reconstruction_loss(\n",
    "            anchor_repr, batch[\"anchor_input_ids\"], batch[\"anchor_attention_mask\"],\n",
    "            lambda_weights, tokenizer.pad_token_id, special_token_ids,\n",
    "        )\n",
    "        \n",
    "        loss_positive = compute_positive_activation_loss(\n",
    "            anchor_repr, batch[\"positive_input_ids\"], batch[\"positive_attention_mask\"],\n",
    "            tokenizer.pad_token_id, special_token_ids,\n",
    "        )\n",
    "        \n",
    "        loss_triplet = compute_triplet_margin_loss(\n",
    "            anchor_repr, positive_repr, negative_repr, config.target_margin,\n",
    "        )\n",
    "        \n",
    "        loss_flops = compute_flops_loss(anchor_repr)\n",
    "        \n",
    "        loss_min_act = compute_minimum_activation_loss(\n",
    "            anchor_repr, k=config.min_activation_k,\n",
    "        )\n",
    "        \n",
    "        # Total loss with phase-specific weights\n",
    "        loss = (\n",
    "            loss_self +\n",
    "            config.lambda_synonym * loss_positive +\n",
    "            config.lambda_margin * loss_triplet +\n",
    "            phase.lambda_flops * loss_flops +\n",
    "            phase.lambda_min_activation * loss_min_act\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track losses\n",
    "        total_loss += loss.item()\n",
    "        loss_components[\"self\"] += loss_self.item()\n",
    "        loss_components[\"positive\"] += loss_positive.item()\n",
    "        loss_components[\"triplet\"] += loss_triplet.item()\n",
    "        loss_components[\"flops\"] += loss_flops.item()\n",
    "        loss_components[\"min_act\"] += loss_min_act.item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{loss.item():.4f}\",\n",
    "            \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\",\n",
    "        })\n",
    "    \n",
    "    n_batches = len(dataloader)\n",
    "    return {\n",
    "        \"total\": total_loss / n_batches,\n",
    "        **{k: v / n_batches for k, v in loss_components.items()},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    tokenizer,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_recalls = []\n",
    "    all_mrrs = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        anchor_repr, _ = model(batch[\"anchor_input_ids\"], batch[\"anchor_attention_mask\"])\n",
    "        positive_repr, _ = model(batch[\"positive_input_ids\"], batch[\"positive_attention_mask\"])\n",
    "        negative_repr, _ = model(batch[\"negative_input_ids\"], batch[\"negative_attention_mask\"])\n",
    "        \n",
    "        # Compute similarities\n",
    "        pos_sim = F.cosine_similarity(anchor_repr, positive_repr, dim=-1)\n",
    "        neg_sim = F.cosine_similarity(anchor_repr, negative_repr, dim=-1)\n",
    "        \n",
    "        # Recall: positive should rank higher than negative\n",
    "        recalls = (pos_sim > neg_sim).float()\n",
    "        all_recalls.extend(recalls.cpu().tolist())\n",
    "        \n",
    "        # MRR\n",
    "        for i in range(len(pos_sim)):\n",
    "            if pos_sim[i] > neg_sim[i]:\n",
    "                all_mrrs.append(1.0)\n",
    "            else:\n",
    "                all_mrrs.append(0.5)\n",
    "    \n",
    "    return {\n",
    "        \"recall\": np.mean(all_recalls) * 100,\n",
    "        \"mrr\": np.mean(all_mrrs),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_curriculum_training(model, config, device):\n",
    "    \"\"\"Run full curriculum training.\"\"\"\n",
    "    training_history = []\n",
    "    best_recall = 0.0\n",
    "    \n",
    "    for phase in config.phases:\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"Starting {phase.name}: Epochs {phase.start_epoch}-{phase.end_epoch}\")\n",
    "        print(f\"lambda_flops={phase.lambda_flops}, lambda_min_act={phase.lambda_min_activation}\")\n",
    "        print(f\"lr_multiplier={phase.lr_multiplier}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        \n",
    "        # Load phase-specific data\n",
    "        train_dataset = TripletDataset(\n",
    "            config.data_dir / phase.data_file,\n",
    "            tokenizer,\n",
    "            config.max_length,\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer and scheduler for this phase\n",
    "        phase_epochs = phase.end_epoch - phase.start_epoch + 1\n",
    "        total_steps = len(train_loader) * phase_epochs\n",
    "        warmup_steps = int(total_steps * config.warmup_ratio)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config.learning_rate * phase.lr_multiplier,\n",
    "            weight_decay=0.01,\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps,\n",
    "        )\n",
    "        \n",
    "        # Train for phase epochs\n",
    "        for epoch in range(phase.start_epoch, phase.end_epoch + 1):\n",
    "            train_metrics = train_epoch(\n",
    "                model, train_loader, optimizer, scheduler,\n",
    "                config, phase, device, epoch,\n",
    "            )\n",
    "            \n",
    "            # Evaluate\n",
    "            eval_metrics = evaluate(model, val_loader, device, tokenizer)\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch}: Loss={train_metrics['total']:.4f}, \"\n",
    "                  f\"Recall={eval_metrics['recall']:.1f}%, MRR={eval_metrics['mrr']:.4f}\")\n",
    "            \n",
    "            # Save history\n",
    "            history_entry = {\n",
    "                \"epoch\": epoch,\n",
    "                \"phase\": phase.name,\n",
    "                **train_metrics,\n",
    "                **eval_metrics,\n",
    "            }\n",
    "            training_history.append(history_entry)\n",
    "            \n",
    "            # Save best model\n",
    "            if eval_metrics[\"recall\"] > best_recall:\n",
    "                best_recall = eval_metrics[\"recall\"]\n",
    "                torch.save({\n",
    "                    \"epoch\": epoch,\n",
    "                    \"phase\": phase.name,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"eval_results\": eval_metrics,\n",
    "                    \"config\": {\n",
    "                        \"model_name\": config.model_name,\n",
    "                        \"max_length\": config.max_length,\n",
    "                    },\n",
    "                }, config.output_dir / \"best_model.pt\")\n",
    "                print(f\"  -> New best model saved! Recall: {best_recall:.1f}%\")\n",
    "        \n",
    "        # Save phase checkpoint\n",
    "        torch.save({\n",
    "            \"epoch\": phase.end_epoch,\n",
    "            \"phase\": phase.name,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "        }, config.output_dir / f\"{phase.name}_checkpoint.pt\")\n",
    "        print(f\"\\n{phase.name} checkpoint saved.\")\n",
    "    \n",
    "    # Save training history\n",
    "    with open(config.output_dir / \"training_history.json\", \"w\") as f:\n",
    "        json.dump(training_history, f, indent=2)\n",
    "    \n",
    "    return training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting v21.4 Curriculum Training...\n",
      "Output directory: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/outputs/v21.4_korean_enhanced\n",
      "\n",
      "============================================================\n",
      "Starting phase1_single_term: Epochs 1-10\n",
      "lambda_flops=0.003, lambda_min_act=2.0\n",
      "lr_multiplier=1.0\n",
      "============================================================\n",
      "Loaded 136117 triplets from phase1_single_term_focus_triplets.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088718ba0cd44d168417090fc09509f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 (phase1_single_term):   0%|          | 0/2126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e515d913a6be4ed9a8762f3acac5585e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Loss=-32.5524, Recall=97.8%, MRR=0.9888\n",
      "  -> New best model saved! Recall: 97.8%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2bed11464442f896befbf8da96cf0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 (phase1_single_term):   0%|          | 0/2126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deda0e80a3ee4299a8c94eef30eb4a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: Loss=-45.0634, Recall=98.3%, MRR=0.9917\n",
      "  -> New best model saved! Recall: 98.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dab38a3d1a548658f1ece72138e6c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 (phase1_single_term):   0%|          | 0/2126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55aa6d463c0c46e1b033ca6c32e76ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: Loss=-46.6957, Recall=98.6%, MRR=0.9929\n",
      "  -> New best model saved! Recall: 98.6%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44fd2e4574e49458fa5f39afc1ca34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 (phase1_single_term):   0%|          | 0/2126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8cb0cb622b494589134122149f762d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: Loss=-47.7580, Recall=98.7%, MRR=0.9935\n",
      "  -> New best model saved! Recall: 98.7%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b59d48f6a914cd8bb0729dfbe9833ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 (phase1_single_term):   0%|          | 0/2126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdb05cbc7914ba8995381715ecc3305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: Loss=-48.5300, Recall=98.7%, MRR=0.9936\n",
      "  -> New best model saved! Recall: 98.7%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a14b533579040ee9d3347296efba44d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 (phase1_single_term):   0%|          | 0/2126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d873ef28725d472d92f585a385a6e30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run training\n",
    "print(\"Starting v21.4 Curriculum Training...\")\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "\n",
    "history = run_curriculum_training(model, config, device)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "epochs = [h[\"epoch\"] for h in history]\n",
    "\n",
    "# Total Loss\n",
    "axes[0, 0].plot(epochs, [h[\"total\"] for h in history], 'b-')\n",
    "axes[0, 0].set_xlabel(\"Epoch\")\n",
    "axes[0, 0].set_ylabel(\"Total Loss\")\n",
    "axes[0, 0].set_title(\"Total Loss\")\n",
    "axes[0, 0].axvline(x=10.5, color='r', linestyle='--', alpha=0.5, label='Phase change')\n",
    "axes[0, 0].axvline(x=20.5, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Recall\n",
    "axes[0, 1].plot(epochs, [h[\"recall\"] for h in history], 'g-')\n",
    "axes[0, 1].set_xlabel(\"Epoch\")\n",
    "axes[0, 1].set_ylabel(\"Recall (%)\")\n",
    "axes[0, 1].set_title(\"Validation Recall\")\n",
    "axes[0, 1].axvline(x=10.5, color='r', linestyle='--', alpha=0.5)\n",
    "axes[0, 1].axvline(x=20.5, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Loss Components\n",
    "for component in [\"self\", \"positive\", \"triplet\"]:\n",
    "    axes[1, 0].plot(epochs, [h[component] for h in history], label=component)\n",
    "axes[1, 0].set_xlabel(\"Epoch\")\n",
    "axes[1, 0].set_ylabel(\"Loss\")\n",
    "axes[1, 0].set_title(\"Loss Components\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# FLOPS and Min Activation\n",
    "ax1 = axes[1, 1]\n",
    "ax1.plot(epochs, [h[\"flops\"] for h in history], 'b-', label='FLOPS')\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"FLOPS Loss\", color='b')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs, [h[\"min_act\"] for h in history], 'r-', label='Min Act')\n",
    "ax2.set_ylabel(\"Min Activation Loss\", color='r')\n",
    "axes[1, 1].set_title(\"Regularization Losses\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.output_dir / \"training_curves.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\nFinal Results:\")\n",
    "print(f\"  Best Recall: {max(h['recall'] for h in history):.1f}%\")\n",
    "print(f\"  Best MRR: {max(h['mrr'] for h in history):.4f}\")\n",
    "print(f\"  Final Loss: {history[-1]['total']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run `04_evaluation.ipynb` for comprehensive evaluation\n",
    "2. Compare with v21.2 and v21.3 baselines\n",
    "3. Test on problem terms (추천, 데이터베이스, 증상, 질환, 인슐린)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

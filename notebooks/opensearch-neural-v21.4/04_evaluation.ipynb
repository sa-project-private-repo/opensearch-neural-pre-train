{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v21.4 Comprehensive Evaluation\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "1. **Single-term Recall**: Focus on previously problematic terms\n",
    "2. **Garbage Detection**: Identify invalid token outputs\n",
    "3. **Sentence-level Performance**: MRR, Recall@K\n",
    "4. **Version Comparison**: v21.2 vs v21.3 vs v21.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from typing import Dict, List, Tuple, Set\n",
    "from collections import defaultdict\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v21.4: True\n",
      "v21.3: True\n",
      "v21.2: True\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "V21_4_MODEL_PATH = PROJECT_ROOT / \"outputs\" / \"v21.4_korean_enhanced\" / \"best_model.pt\"\n",
    "V21_3_MODEL_PATH = PROJECT_ROOT / \"outputs\" / \"v21.3_korean_enhanced\" / \"best_model.pt\"\n",
    "V21_2_MODEL_PATH = PROJECT_ROOT / \"outputs\" / \"v21.2_korean_legal_medical\" / \"best_model.pt\"\n",
    "\n",
    "MODEL_NAME = \"skt/A.X-Encoder-base\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"v21.4: {V21_4_MODEL_PATH.exists()}\")\n",
    "print(f\"v21.3: {V21_3_MODEL_PATH.exists()}\")\n",
    "print(f\"v21.2: {V21_2_MODEL_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/west/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  queued_call()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v21.4 loaded: True\n",
      "v21.3 loaded: True\n",
      "v21.2 loaded: True\n"
     ]
    }
   ],
   "source": [
    "class SPLADEModel(nn.Module):\n",
    "    \"\"\"SPLADE model for Korean sparse retrieval.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"skt/A.X-Encoder-base\"):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        self.config = self.model.config\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        token_scores = torch.log1p(self.relu(logits))\n",
    "        mask = attention_mask.unsqueeze(-1).float()\n",
    "        token_scores = token_scores * mask\n",
    "        sparse_repr, _ = token_scores.max(dim=1)\n",
    "        token_weights = token_scores.max(dim=-1).values\n",
    "        return sparse_repr, token_weights\n",
    "\n",
    "\n",
    "def load_model(path: Path, model_name: str, device):\n",
    "    \"\"\"Load a trained model.\"\"\"\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    \n",
    "    model = SPLADEModel(model_name)\n",
    "    checkpoint = torch.load(path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load models\n",
    "model_v21_4 = load_model(V21_4_MODEL_PATH, MODEL_NAME, device)\n",
    "model_v21_3 = load_model(V21_3_MODEL_PATH, MODEL_NAME, device)\n",
    "model_v21_2 = load_model(V21_2_MODEL_PATH, MODEL_NAME, device)\n",
    "\n",
    "print(f\"v21.4 loaded: {model_v21_4 is not None}\")\n",
    "print(f\"v21.3 loaded: {model_v21_3 is not None}\")\n",
    "print(f\"v21.2 loaded: {model_v21_2 is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Token Validation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_korean_token(token: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a token is valid Korean/English.\n",
    "    \n",
    "    Valid tokens contain only:\n",
    "    - Korean characters (Hangul syllables, Jamo)\n",
    "    - ASCII letters and digits\n",
    "    - Basic CJK characters\n",
    "    \"\"\"\n",
    "    if not token:\n",
    "        return False\n",
    "    \n",
    "    # Remove ## prefix\n",
    "    clean = token.replace('##', '').strip()\n",
    "    if not clean:\n",
    "        return False\n",
    "    \n",
    "    # Skip special tokens\n",
    "    if '<' in token or '>' in token or '[' in token or ']' in token:\n",
    "        return False\n",
    "    \n",
    "    for char in clean:\n",
    "        code = ord(char)\n",
    "        \n",
    "        # Hangul syllables (가-힣)\n",
    "        if 0xAC00 <= code <= 0xD7A3:\n",
    "            continue\n",
    "        # Hangul Jamo\n",
    "        if 0x1100 <= code <= 0x11FF:\n",
    "            continue\n",
    "        # Hangul Compatibility Jamo\n",
    "        if 0x3130 <= code <= 0x318F:\n",
    "            continue\n",
    "        # ASCII letters and digits\n",
    "        if (0x0041 <= code <= 0x005A) or (0x0061 <= code <= 0x007A) or (0x0030 <= code <= 0x0039):\n",
    "            continue\n",
    "        # Basic CJK\n",
    "        if 0x4E00 <= code <= 0x9FFF:\n",
    "            continue\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "# Build special token set\n",
    "special_token_ids = set()\n",
    "for attr in ['pad_token_id', 'cls_token_id', 'sep_token_id', \n",
    "             'unk_token_id', 'mask_token_id', 'bos_token_id', 'eos_token_id']:\n",
    "    token_id = getattr(tokenizer, attr, None)\n",
    "    if token_id is not None:\n",
    "        special_token_ids.add(token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Expansion Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_synonym_expansion(\n",
    "    model: nn.Module,\n",
    "    tokenizer,\n",
    "    text: str,\n",
    "    device: torch.device,\n",
    "    top_k: int = 20,\n",
    "    min_weight: float = 0.5,\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Get top-k activated tokens for a given text.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    weights, _ = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "    weights = weights[0].cpu()\n",
    "    \n",
    "    # Mask special tokens\n",
    "    for tid in special_token_ids:\n",
    "        if tid < len(weights):\n",
    "            weights[tid] = -float('inf')\n",
    "    \n",
    "    # Get top tokens\n",
    "    top_weights, top_indices = weights.topk(min(top_k * 10, len(weights)))\n",
    "    \n",
    "    results = []\n",
    "    for idx, weight in zip(top_indices.tolist(), top_weights.tolist()):\n",
    "        if weight < min_weight:\n",
    "            continue\n",
    "        \n",
    "        token = tokenizer.decode([idx]).strip()\n",
    "        \n",
    "        if not is_valid_korean_token(token):\n",
    "            continue\n",
    "        \n",
    "        results.append((token, weight))\n",
    "        \n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Problem Terms Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem terms from v21.3\n",
    "PROBLEM_TERMS = {\n",
    "    \"추천\": [\"권장\", \"권유\", \"제안\", \"소개\"],\n",
    "    \"데이터베이스\": [\"DB\", \"디비\", \"저장소\", \"데이터\"],\n",
    "    \"증상\": [\"증세\", \"징후\", \"현상\", \"이상\"],\n",
    "    \"질환\": [\"질병\", \"병\", \"병증\", \"이환\"],\n",
    "    \"인슐린\": [\"insulin\", \"호르몬\", \"혈당\", \"당뇨\"],\n",
    "}\n",
    "\n",
    "\n",
    "def evaluate_problem_terms(model, name: str) -> Dict:\n",
    "    \"\"\"Evaluate model on problem terms.\"\"\"\n",
    "    if model is None:\n",
    "        return None\n",
    "    \n",
    "    results = {}\n",
    "    total_recall = 0\n",
    "    total_garbage = 0\n",
    "    \n",
    "    print(f\"\\n{name} - Problem Terms:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for term, expected in PROBLEM_TERMS.items():\n",
    "        expansions = get_synonym_expansion(model, tokenizer, term, device, top_k=15)\n",
    "        \n",
    "        # Check recall\n",
    "        top_tokens = [t for t, _ in expansions]\n",
    "        hits = 0\n",
    "        for exp in expected:\n",
    "            for tok in top_tokens:\n",
    "                if exp.lower() in tok.lower() or tok.lower() in exp.lower():\n",
    "                    hits += 1\n",
    "                    break\n",
    "        recall = hits / len(expected) * 100\n",
    "        total_recall += recall\n",
    "        \n",
    "        # Check garbage ratio\n",
    "        if len(expansions) == 0:\n",
    "            garbage_ratio = 100\n",
    "            total_garbage += 1\n",
    "        else:\n",
    "            garbage_ratio = 0\n",
    "        \n",
    "        print(f\"\\n{term}:\")\n",
    "        print(f\"  Expected: {expected}\")\n",
    "        print(f\"  Recall: {recall:.0f}%\")\n",
    "        if expansions:\n",
    "            print(f\"  Top expansions: {[(t, f'{w:.2f}') for t, w in expansions[:5]]}\")\n",
    "        else:\n",
    "            print(f\"  Top expansions: (GARBAGE OUTPUT)\")\n",
    "        \n",
    "        results[term] = {\n",
    "            \"recall\": recall,\n",
    "            \"top_tokens\": top_tokens[:5],\n",
    "            \"is_garbage\": len(expansions) == 0,\n",
    "        }\n",
    "    \n",
    "    avg_recall = total_recall / len(PROBLEM_TERMS)\n",
    "    garbage_count = total_garbage\n",
    "    \n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  Average Recall: {avg_recall:.1f}%\")\n",
    "    print(f\"  Garbage outputs: {garbage_count}/{len(PROBLEM_TERMS)}\")\n",
    "    \n",
    "    return {\n",
    "        \"terms\": results,\n",
    "        \"avg_recall\": avg_recall,\n",
    "        \"garbage_count\": garbage_count,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "v21.4 - Problem Terms:\n",
      "============================================================\n",
      "\n",
      "추천:\n",
      "  Expected: ['권장', '권유', '제안', '소개']\n",
      "  Recall: 100%\n",
      "  Top expansions: [('추천', '3.53'), ('권장', '2.99'), ('권유', '2.92'), ('##천', '2.87'), ('소개', '2.87')]\n",
      "\n",
      "데이터베이스:\n",
      "  Expected: ['DB', '디비', '저장소', '데이터']\n",
      "  Recall: 50%\n",
      "  Top expansions: [('데이터베이스', '3.45'), ('DB', '3.09'), ('데이터', '2.98'), ('##베이스', '2.75'), ('컴퓨터', '2.67')]\n",
      "\n",
      "증상:\n",
      "  Expected: ['증세', '징후', '현상', '이상']\n",
      "  Recall: 75%\n",
      "  Top expansions: [('증상', '3.54'), ('증세', '3.19'), ('증후', '3.03'), ('질환', '2.99'), ('##증', '2.97')]\n",
      "\n",
      "질환:\n",
      "  Expected: ['질병', '병', '병증', '이환']\n",
      "  Recall: 75%\n",
      "  Top expansions: [('질환', '3.49'), ('질병', '3.28'), ('증상', '3.07'), ('병', '3.07'), ('##병', '2.98')]\n",
      "\n",
      "인슐린:\n",
      "  Expected: ['insulin', '호르몬', '혈당', '당뇨']\n",
      "  Recall: 75%\n",
      "  Top expansions: [('인슐린', '3.50'), ('##슐린', '3.07'), ('호르몬', '3.01'), ('혈당', '2.98'), ('당뇨병', '2.83')]\n",
      "\n",
      "Summary:\n",
      "  Average Recall: 75.0%\n",
      "  Garbage outputs: 0/5\n",
      "\n",
      "v21.3 - Problem Terms:\n",
      "============================================================\n",
      "\n",
      "추천:\n",
      "  Expected: ['권장', '권유', '제안', '소개']\n",
      "  Recall: 0%\n",
      "  Top expansions: [('##헟', '1.58'), ('乨', '1.52'), ('빇', '1.51'), ('엺', '1.48'), ('툔', '1.47')]\n",
      "\n",
      "데이터베이스:\n",
      "  Expected: ['DB', '디비', '저장소', '데이터']\n",
      "  Recall: 0%\n",
      "  Top expansions: [('秞', '1.16'), ('晢', '1.16'), ('趡', '1.15'), ('꼾', '1.11'), ('憓', '1.08')]\n",
      "\n",
      "증상:\n",
      "  Expected: ['증세', '징후', '현상', '이상']\n",
      "  Recall: 0%\n",
      "  Top expansions: [('톡톡히', '1.47'), ('괜한', '1.36'), ('왖', '1.35'), ('으뜸', '1.30'), ('제법', '1.27')]\n",
      "\n",
      "질환:\n",
      "  Expected: ['질병', '병', '병증', '이환']\n",
      "  Recall: 0%\n",
      "  Top expansions: [('왖', '1.36'), ('炟', '1.34'), ('볔', '1.34'), ('제격', '1.32'), ('鄏', '1.31')]\n",
      "\n",
      "인슐린:\n",
      "  Expected: ['insulin', '호르몬', '혈당', '당뇨']\n",
      "  Recall: 0%\n",
      "  Top expansions: [('鄏', '1.45'), ('##됋', '1.45'), ('볔', '1.44'), ('秞', '1.43'), ('##븎', '1.42')]\n",
      "\n",
      "Summary:\n",
      "  Average Recall: 0.0%\n",
      "  Garbage outputs: 0/5\n",
      "\n",
      "v21.2 - Problem Terms:\n",
      "============================================================\n",
      "\n",
      "추천:\n",
      "  Expected: ['권장', '권유', '제안', '소개']\n",
      "  Recall: 100%\n",
      "  Top expansions: [('추천', '3.09'), ('##천', '2.68'), ('##추', '2.43'), ('강추', '2.35'), ('권유', '2.27')]\n",
      "\n",
      "데이터베이스:\n",
      "  Expected: ['DB', '디비', '저장소', '데이터']\n",
      "  Recall: 50%\n",
      "  Top expansions: [('데이터베이스', '2.99'), ('DB', '2.58'), ('데이터', '2.56'), ('##베이스', '2.31'), ('컴퓨터', '2.29')]\n",
      "\n",
      "증상:\n",
      "  Expected: ['증세', '징후', '현상', '이상']\n",
      "  Recall: 75%\n",
      "  Top expansions: [('증상', '3.13'), ('##증', '2.71'), ('증세', '2.61'), ('##상', '2.61'), ('현상', '2.51')]\n",
      "\n",
      "질환:\n",
      "  Expected: ['질병', '병', '병증', '이환']\n",
      "  Recall: 75%\n",
      "  Top expansions: [('질환', '2.94'), ('##환', '2.63'), ('질병', '2.53'), ('##병', '2.36'), ('disease', '2.33')]\n",
      "\n",
      "인슐린:\n",
      "  Expected: ['insulin', '호르몬', '혈당', '당뇨']\n",
      "  Recall: 75%\n",
      "  Top expansions: [('인슐린', '3.14'), ('##슐린', '2.86'), ('##인', '2.54'), ('호르몬', '2.44'), ('당뇨병', '2.44')]\n",
      "\n",
      "Summary:\n",
      "  Average Recall: 75.0%\n",
      "  Garbage outputs: 0/5\n"
     ]
    }
   ],
   "source": [
    "# Evaluate all models\n",
    "results_v21_4 = evaluate_problem_terms(model_v21_4, \"v21.4\")\n",
    "results_v21_3 = evaluate_problem_terms(model_v21_3, \"v21.3\")\n",
    "results_v21_2 = evaluate_problem_terms(model_v21_2, \"v21.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. General Terms Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General evaluation test cases\n",
    "GENERAL_TEST_CASES = [\n",
    "    # General\n",
    "    (\"검색\", [\"탐색\", \"조회\", \"찾기\", \"서치\"]),\n",
    "    (\"인공지능\", [\"AI\", \"에이아이\", \"기계지능\"]),\n",
    "    (\"컴퓨터\", [\"PC\", \"전산\", \"컴퓨팅\"]),\n",
    "    # Legal\n",
    "    (\"손해배상\", [\"배상\", \"보상\", \"손해\"]),\n",
    "    (\"판결\", [\"판례\", \"선고\", \"결정\"]),\n",
    "    (\"소송\", [\"재판\", \"법정\", \"송사\"]),\n",
    "    # Medical\n",
    "    (\"진단\", [\"진찰\", \"검진\", \"판단\"]),\n",
    "    (\"치료\", [\"처치\", \"요법\", \"치유\"]),\n",
    "    (\"당뇨병\", [\"당뇨\", \"혈당\", \"diabetes\"]),\n",
    "]\n",
    "\n",
    "\n",
    "def evaluate_general_terms(model, name: str) -> Dict:\n",
    "    \"\"\"Evaluate model on general test cases.\"\"\"\n",
    "    if model is None:\n",
    "        return None\n",
    "    \n",
    "    total_recall = 0\n",
    "    results = []\n",
    "    \n",
    "    for source, expected in GENERAL_TEST_CASES:\n",
    "        expansions = get_synonym_expansion(model, tokenizer, source, device, top_k=20)\n",
    "        top_tokens = [t for t, _ in expansions]\n",
    "        \n",
    "        hits = 0\n",
    "        for exp in expected:\n",
    "            for tok in top_tokens:\n",
    "                if exp.lower() in tok.lower() or tok.lower() in exp.lower():\n",
    "                    hits += 1\n",
    "                    break\n",
    "        \n",
    "        recall = hits / len(expected) * 100\n",
    "        total_recall += recall\n",
    "        results.append({\"source\": source, \"recall\": recall})\n",
    "    \n",
    "    avg_recall = total_recall / len(GENERAL_TEST_CASES)\n",
    "    return {\"results\": results, \"avg_recall\": avg_recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "General Terms Recall Comparison:\n",
      "================================================================================\n",
      "Source          Expected                       v21.4      v21.3      v21.2\n",
      "--------------------------------------------------------------------------------\n",
      "검색              ['탐색', '조회']                     75%        75%        75%\n",
      "인공지능            ['AI', '에이아이']                   67%        67%        67%\n",
      "컴퓨터             ['PC', '전산']                    100%       100%       100%\n",
      "손해배상            ['배상', '보상']                    100%       100%       100%\n",
      "판결              ['판례', '선고']                    100%       100%       100%\n",
      "소송              ['재판', '법정']                     33%        67%        33%\n",
      "진단              ['진찰', '검진']                     67%        67%       100%\n",
      "치료              ['처치', '요법']                    100%        67%        67%\n",
      "당뇨병             ['당뇨', '혈당']                     67%        67%        67%\n",
      "--------------------------------------------------------------------------------\n",
      "AVERAGE                                       78.7%      78.7%      78.7%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate all models\n",
    "general_v21_4 = evaluate_general_terms(model_v21_4, \"v21.4\")\n",
    "general_v21_3 = evaluate_general_terms(model_v21_3, \"v21.3\")\n",
    "general_v21_2 = evaluate_general_terms(model_v21_2, \"v21.2\")\n",
    "\n",
    "print(\"\\nGeneral Terms Recall Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Source':<15} {'Expected':<25} {'v21.4':>10} {'v21.3':>10} {'v21.2':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, (source, expected) in enumerate(GENERAL_TEST_CASES):\n",
    "    r4 = general_v21_4[\"results\"][i][\"recall\"] if general_v21_4 else 0\n",
    "    r3 = general_v21_3[\"results\"][i][\"recall\"] if general_v21_3 else 0\n",
    "    r2 = general_v21_2[\"results\"][i][\"recall\"] if general_v21_2 else 0\n",
    "    \n",
    "    print(f\"{source:<15} {str(expected[:2]):<25} {r4:>9.0f}% {r3:>9.0f}% {r2:>9.0f}%\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "avg_4 = general_v21_4[\"avg_recall\"] if general_v21_4 else 0\n",
    "avg_3 = general_v21_3[\"avg_recall\"] if general_v21_3 else 0\n",
    "avg_2 = general_v21_2[\"avg_recall\"] if general_v21_2 else 0\n",
    "print(f\"{'AVERAGE':<40} {avg_4:>9.1f}% {avg_3:>9.1f}% {avg_2:>9.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. Natural Language Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "v21.4 - Natural Language Queries:\n",
      "======================================================================\n",
      "\n",
      "Query: 손해배상 청구 소송을 제기하려면 어떻게 해야 하나요?\n",
      "Top expansions: [('##을', '3.64'), ('##상', '3.48'), ('##하', '3.44'), ('##를', '3.42'), ('##는', '3.41'), ('##면', '3.34')]\n",
      "\n",
      "Query: 고혈압 환자의 식이요법에 대해 알려주세요.\n",
      "Top expansions: [('##에', '3.66'), ('##의', '3.62'), ('##는', '3.39'), ('##을', '3.36'), ('고', '3.35'), ('##법', '3.33')]\n",
      "\n",
      "Query: 인공지능 기술의 최신 발전 동향\n",
      "Top expansions: [('##의', '3.63'), ('##는', '3.41'), ('기술', '3.35'), ('인공지능', '3.34'), ('##을', '3.26'), ('##에', '3.26')]\n",
      "\n",
      "Query: 서울에서 부산까지 KTX 소요 시간\n",
      "Top expansions: [('##에', '3.64'), ('##서', '3.54'), ('##지', '3.54'), ('시간', '3.38'), ('##까', '3.35'), ('서울', '3.30')]\n",
      "\n",
      "v21.3 - Natural Language Queries:\n",
      "======================================================================\n",
      "\n",
      "Query: 손해배상 청구 소송을 제기하려면 어떻게 해야 하나요?\n",
      "Top expansions: [('##하', '3.27'), ('##상', '3.24'), ('##면', '3.23'), ('##요', '3.18'), ('소송', '3.14'), ('하나', '3.10')]\n",
      "\n",
      "Query: 고혈압 환자의 식이요법에 대해 알려주세요.\n",
      "Top expansions: [('##법', '3.41'), ('##주', '3.31'), ('고', '3.24'), ('##요', '3.14'), ('##의', '3.02'), ('식이', '2.99')]\n",
      "\n",
      "Query: 인공지능 기술의 최신 발전 동향\n",
      "Top expansions: [('최신', '3.13'), ('인공지능', '3.06'), ('발전', '2.94'), ('##의', '2.81'), ('동향', '2.79'), ('최근', '2.70')]\n",
      "\n",
      "Query: 서울에서 부산까지 KTX 소요 시간\n",
      "Top expansions: [('시간', '3.19'), ('소요', '3.09'), ('##시간', '3.07'), ('부산', '3.04'), ('서울', '3.00'), ('##까', '2.97')]\n"
     ]
    }
   ],
   "source": [
    "# Natural language queries\n",
    "NL_QUERIES = [\n",
    "    \"손해배상 청구 소송을 제기하려면 어떻게 해야 하나요?\",\n",
    "    \"고혈압 환자의 식이요법에 대해 알려주세요.\",\n",
    "    \"인공지능 기술의 최신 발전 동향\",\n",
    "    \"서울에서 부산까지 KTX 소요 시간\",\n",
    "]\n",
    "\n",
    "\n",
    "def evaluate_nl_queries(model, name: str):\n",
    "    \"\"\"Evaluate model on natural language queries.\"\"\"\n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{name} - Natural Language Queries:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for query in NL_QUERIES:\n",
    "        expansions = get_synonym_expansion(model, tokenizer, query, device, top_k=10)\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        if expansions:\n",
    "            print(f\"Top expansions: {[(t, f'{w:.2f}') for t, w in expansions[:6]]}\")\n",
    "        else:\n",
    "            print(\"Top expansions: (GARBAGE OUTPUT)\")\n",
    "\n",
    "\n",
    "evaluate_nl_queries(model_v21_4, \"v21.4\")\n",
    "evaluate_nl_queries(model_v21_3, \"v21.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 7. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "v21.4 Evaluation Summary\n",
      "======================================================================\n",
      "\n",
      "| Metric | v21.2 | v21.3 | v21.4 | Target |\n",
      "|--------|-------|-------|-------|--------|\n",
      "| Problem Terms Recall | 75.0% | 0.0% | 75.0% | 80%+ |\n",
      "| General Terms Recall | 78.7% | 78.7% | 78.7% | 80%+ |\n",
      "| Garbage Outputs | 0/5 | 0/5 | 0/5 | 0/5 |\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Improvement over v21.3: +75.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"v21.4 Evaluation Summary\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n| Metric | v21.2 | v21.3 | v21.4 | Target |\")\n",
    "print(\"|--------|-------|-------|-------|--------|\")\n",
    "\n",
    "# Problem terms recall\n",
    "prob_4 = results_v21_4[\"avg_recall\"] if results_v21_4 else 0\n",
    "prob_3 = results_v21_3[\"avg_recall\"] if results_v21_3 else 0\n",
    "prob_2 = results_v21_2[\"avg_recall\"] if results_v21_2 else 0\n",
    "print(f\"| Problem Terms Recall | {prob_2:.1f}% | {prob_3:.1f}% | {prob_4:.1f}% | 80%+ |\")\n",
    "\n",
    "# General terms recall\n",
    "gen_4 = general_v21_4[\"avg_recall\"] if general_v21_4 else 0\n",
    "gen_3 = general_v21_3[\"avg_recall\"] if general_v21_3 else 0\n",
    "gen_2 = general_v21_2[\"avg_recall\"] if general_v21_2 else 0\n",
    "print(f\"| General Terms Recall | {gen_2:.1f}% | {gen_3:.1f}% | {gen_4:.1f}% | 80%+ |\")\n",
    "\n",
    "# Garbage outputs\n",
    "garb_4 = results_v21_4[\"garbage_count\"] if results_v21_4 else 0\n",
    "garb_3 = results_v21_3[\"garbage_count\"] if results_v21_3 else 0\n",
    "garb_2 = results_v21_2[\"garbage_count\"] if results_v21_2 else 0\n",
    "print(f\"| Garbage Outputs | {garb_2}/5 | {garb_3}/5 | {garb_4}/5 | 0/5 |\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Improvement analysis\n",
    "if results_v21_4 and results_v21_3:\n",
    "    improvement = prob_4 - prob_3\n",
    "    if improvement > 0:\n",
    "        print(f\"\\nImprovement over v21.3: +{improvement:.1f}%\")\n",
    "    else:\n",
    "        print(f\"\\nRegression from v21.3: {improvement:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v22.0 HuggingFace Korean Dataset Loading\n",
    "\n",
    "Load and process Korean datasets from HuggingFace for training data augmentation.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Sequential Loading**: Memory-safe sequential loading with garbage collection\n",
    "- **Progress Tracking**: Individual progress bars for each dataset\n",
    "- **Error Handling**: Graceful failure handling - if one dataset fails, others continue\n",
    "- **Type Safety**: Full type hints throughout\n",
    "\n",
    "## Datasets\n",
    "\n",
    "| Dataset | Type | Size | Use Case |\n",
    "|---------|------|------|----------|\n",
    "| williamjeong2/msmarco-triplets-ko-v1 | Query-Doc Triplets | 50K | Direct triplet training |\n",
    "| klue (nli, sts) | NLI, STS | 45K | Semantic similarity pairs |\n",
    "| squad_kor_v1 | QA | 30K | Question-context pairs |\n",
    "| nsmc | Sentiment | 50K | Text corpus for negatives |\n",
    "| skt/kobest_v1 (copa) | COPA | 5K | Premise-alternative pairs |\n",
    "| daekeun-ml/naver-news-summarization-ko | News | 10K | Title-summary pairs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_project_root() -> Path:\n",
    "    \"\"\"Find the project root directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Callable, Dict, List, Optional\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set random seed\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install datasets if needed\n",
    "try:\n",
    "    from datasets import load_dataset, Dataset\n",
    "    print(\"datasets library available\")\n",
    "except ImportError:\n",
    "    print(\"Installing datasets...\")\n",
    "    %pip install datasets\n",
    "    from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directories - v22.0 specific\n",
    "HF_DATA_DIR = PROJECT_ROOT / \"data\" / \"huggingface_korean\"\n",
    "V22_DATA_DIR = PROJECT_ROOT / \"data\" / \"v22.0\"\n",
    "HF_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "V22_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"HuggingFace data directory: {HF_DATA_DIR}\")\n",
    "print(f\"v22.0 data directory: {V22_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Classes and Type Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetResult:\n",
    "    \"\"\"Result container for a dataset loading operation.\"\"\"\n",
    "    \n",
    "    name: str\n",
    "    success: bool\n",
    "    data: List[Dict[str, Any]] = field(default_factory=list)\n",
    "    corpus: List[str] = field(default_factory=list)\n",
    "    error_message: Optional[str] = None\n",
    "    sample_count: int = 0\n",
    "    \n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Calculate sample count after initialization.\"\"\"\n",
    "        if self.data:\n",
    "            self.sample_count = len(self.data)\n",
    "        elif self.corpus:\n",
    "            self.sample_count = len(self.corpus)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    \"\"\"Configuration for a dataset loading task.\"\"\"\n",
    "    \n",
    "    name: str\n",
    "    loader_fn: Callable[..., DatasetResult]\n",
    "    max_samples: int\n",
    "    description: str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_msmarco_korean(max_samples: int = 50000) -> DatasetResult:\n",
    "    \"\"\"Load Korean MS MARCO triplets.\"\"\"\n",
    "    name = \"msmarco_ko\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\n",
    "            \"williamjeong2/msmarco-triplets-ko-v1\",\n",
    "            split=\"train\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return DatasetResult(name=name, success=False, error_message=f\"Failed to load: {e}\")\n",
    "    \n",
    "    triplets: List[Dict[str, Any]] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        query = item.get(\"query\", \"\")\n",
    "        positives = item.get(\"pos\", [])\n",
    "        negatives = item.get(\"neg\", [])\n",
    "        \n",
    "        if not query or not positives:\n",
    "            continue\n",
    "        \n",
    "        pos = positives[0] if positives else \"\"\n",
    "        neg = negatives[0] if negatives else \"\"\n",
    "        \n",
    "        if pos:\n",
    "            triplets.append({\n",
    "                \"anchor\": query,\n",
    "                \"positive\": pos,\n",
    "                \"negative\": neg if neg else \"\",\n",
    "                \"source\": name,\n",
    "            })\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, data=triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_klue_nli(max_samples: int = 30000) -> DatasetResult:\n",
    "    \"\"\"Load KLUE NLI dataset.\"\"\"\n",
    "    name = \"klue_nli\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\"klue\", \"nli\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(name=name, success=False, error_message=f\"Failed to load: {e}\")\n",
    "    \n",
    "    pairs: List[Dict[str, Any]] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        premise = item.get(\"premise\", \"\")\n",
    "        hypothesis = item.get(\"hypothesis\", \"\")\n",
    "        label = item.get(\"label\", -1)\n",
    "        \n",
    "        if not premise or not hypothesis:\n",
    "            continue\n",
    "        \n",
    "        if label == 0:  # Entailment\n",
    "            pairs.append({\n",
    "                \"source\": premise,\n",
    "                \"target\": hypothesis,\n",
    "                \"similarity\": 0.9,\n",
    "                \"category\": \"nli_entailment\",\n",
    "                \"pair_type\": \"klue_nli\",\n",
    "            })\n",
    "        elif label == 2:  # Contradiction (hard negative)\n",
    "            pairs.append({\n",
    "                \"source\": premise,\n",
    "                \"target\": hypothesis,\n",
    "                \"similarity\": 0.1,\n",
    "                \"category\": \"nli_contradiction\",\n",
    "                \"pair_type\": \"klue_nli_neg\",\n",
    "            })\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, data=pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_klue_sts(max_samples: int = 15000) -> DatasetResult:\n",
    "    \"\"\"Load KLUE STS dataset.\"\"\"\n",
    "    name = \"klue_sts\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\"klue\", \"sts\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(name=name, success=False, error_message=f\"Failed to load: {e}\")\n",
    "    \n",
    "    pairs: List[Dict[str, Any]] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        sentence1 = item.get(\"sentence1\", \"\")\n",
    "        sentence2 = item.get(\"sentence2\", \"\")\n",
    "        \n",
    "        labels = item.get(\"labels\", {})\n",
    "        score = labels.get(\"real-label\", 0) if isinstance(labels, dict) else 0\n",
    "        \n",
    "        if not sentence1 or not sentence2:\n",
    "            continue\n",
    "        \n",
    "        normalized_score = score / 5.0 if score > 0 else 0\n",
    "        \n",
    "        if normalized_score >= 0.5:\n",
    "            pairs.append({\n",
    "                \"source\": sentence1,\n",
    "                \"target\": sentence2,\n",
    "                \"similarity\": normalized_score,\n",
    "                \"category\": \"sts\",\n",
    "                \"pair_type\": \"klue_sts\",\n",
    "            })\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, data=pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_korquad(max_samples: int = 30000) -> DatasetResult:\n",
    "    \"\"\"Load KorQuAD dataset.\"\"\"\n",
    "    name = \"korquad\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\"squad_kor_v1\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(name=name, success=False, error_message=f\"Failed to load: {e}\")\n",
    "    \n",
    "    pairs: List[Dict[str, Any]] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        question = item.get(\"question\", \"\")\n",
    "        context = item.get(\"context\", \"\")\n",
    "        answers = item.get(\"answers\", {})\n",
    "        \n",
    "        if not question:\n",
    "            continue\n",
    "        \n",
    "        answer_texts = answers.get(\"text\", []) if isinstance(answers, dict) else []\n",
    "        answer = answer_texts[0] if answer_texts else \"\"\n",
    "        \n",
    "        if answer:\n",
    "            pairs.append({\n",
    "                \"source\": question,\n",
    "                \"target\": answer,\n",
    "                \"similarity\": 0.85,\n",
    "                \"category\": \"qa\",\n",
    "                \"pair_type\": \"korquad\",\n",
    "            })\n",
    "        \n",
    "        if context and len(context) > 20:\n",
    "            truncated_context = context[:200].strip()\n",
    "            pairs.append({\n",
    "                \"source\": question,\n",
    "                \"target\": truncated_context,\n",
    "                \"similarity\": 0.7,\n",
    "                \"category\": \"qa_context\",\n",
    "                \"pair_type\": \"korquad_context\",\n",
    "            })\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, data=pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kobest_copa(max_samples: int = 5000) -> DatasetResult:\n",
    "    \"\"\"Load KoBEST COPA dataset.\"\"\"\n",
    "    name = \"kobest_copa\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\"skt/kobest_v1\", \"copa\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(name=name, success=False, error_message=f\"Failed to load: {e}\")\n",
    "    \n",
    "    pairs: List[Dict[str, Any]] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        premise = item.get(\"premise\", \"\")\n",
    "        alternative1 = item.get(\"alternative_1\", \"\")\n",
    "        alternative2 = item.get(\"alternative_2\", \"\")\n",
    "        label = item.get(\"label\", 0)\n",
    "        \n",
    "        if not premise:\n",
    "            continue\n",
    "        \n",
    "        correct = alternative1 if label == 0 else alternative2\n",
    "        \n",
    "        if correct:\n",
    "            pairs.append({\n",
    "                \"source\": premise,\n",
    "                \"target\": correct,\n",
    "                \"similarity\": 0.8,\n",
    "                \"category\": \"copa\",\n",
    "                \"pair_type\": \"kobest_copa\",\n",
    "            })\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, data=pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_naver_news(max_samples: int = 10000) -> DatasetResult:\n",
    "    \"\"\"Load Naver News summarization dataset.\"\"\"\n",
    "    name = \"naver_news\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\n",
    "            \"daekeun-ml/naver-news-summarization-ko\",\n",
    "            split=\"train\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return DatasetResult(name=name, success=False, error_message=f\"Failed to load: {e}\")\n",
    "    \n",
    "    pairs: List[Dict[str, Any]] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        title = item.get(\"title\", \"\") or item.get(\"document_title\", \"\")\n",
    "        content = (\n",
    "            item.get(\"document\", \"\") or \n",
    "            item.get(\"content\", \"\") or \n",
    "            item.get(\"text\", \"\")\n",
    "        )\n",
    "        summary = item.get(\"summary\", \"\") or item.get(\"abstractive\", \"\")\n",
    "        \n",
    "        if title and summary:\n",
    "            pairs.append({\n",
    "                \"source\": title,\n",
    "                \"target\": summary[:200],\n",
    "                \"similarity\": 0.75,\n",
    "                \"category\": \"news_summary\",\n",
    "                \"pair_type\": \"naver_news\",\n",
    "            })\n",
    "        \n",
    "        if content and summary:\n",
    "            pairs.append({\n",
    "                \"source\": content[:200],\n",
    "                \"target\": summary[:200],\n",
    "                \"similarity\": 0.8,\n",
    "                \"category\": \"news_summary\",\n",
    "                \"pair_type\": \"naver_news\",\n",
    "            })\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, data=pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nsmc_corpus(max_samples: int = 50000) -> DatasetResult:\n",
    "    \"\"\"Load NSMC corpus for negative mining.\"\"\"\n",
    "    name = \"nsmc\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\"nsmc\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(name=name, success=False, error_message=f\"Failed to load: {e}\")\n",
    "    \n",
    "    texts: List[str] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        document = item.get(\"document\", \"\")\n",
    "        if document and len(document) > 5:\n",
    "            texts.append(document)\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, corpus=texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and Execute Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CONFIGS: List[DatasetConfig] = [\n",
    "    DatasetConfig(\n",
    "        name=\"msmarco_ko\",\n",
    "        loader_fn=load_msmarco_korean,\n",
    "        max_samples=50000,\n",
    "        description=\"Korean MS MARCO triplets\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"klue_nli\",\n",
    "        loader_fn=load_klue_nli,\n",
    "        max_samples=30000,\n",
    "        description=\"KLUE Natural Language Inference\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"klue_sts\",\n",
    "        loader_fn=load_klue_sts,\n",
    "        max_samples=15000,\n",
    "        description=\"KLUE Semantic Textual Similarity\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"korquad\",\n",
    "        loader_fn=load_korquad,\n",
    "        max_samples=30000,\n",
    "        description=\"Korean Question Answering\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"kobest_copa\",\n",
    "        loader_fn=load_kobest_copa,\n",
    "        max_samples=5000,\n",
    "        description=\"KoBEST COPA reasoning\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"naver_news\",\n",
    "        loader_fn=load_naver_news,\n",
    "        max_samples=10000,\n",
    "        description=\"Naver News summarization\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"nsmc\",\n",
    "        loader_fn=load_nsmc_corpus,\n",
    "        max_samples=50000,\n",
    "        description=\"NSMC movie review corpus\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Dataset configurations:\")\n",
    "for config in DATASET_CONFIGS:\n",
    "    print(f\"  - {config.name}: {config.description} (max: {config.max_samples})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_sequential(configs: List[DatasetConfig]) -> Dict[str, DatasetResult]:\n",
    "    \"\"\"Load datasets one by one to avoid memory issues.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"\\nLoading {len(configs)} datasets sequentially...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\n[{config.name}] Loading {config.description}...\")\n",
    "        try:\n",
    "            result = config.loader_fn(max_samples=config.max_samples)\n",
    "            results[config.name] = result\n",
    "            \n",
    "            status = \"SUCCESS\" if result.success else \"FAILED\"\n",
    "            print(f\"  [{status}] {result.sample_count:,} samples\")\n",
    "            \n",
    "            if not result.success:\n",
    "                print(f\"    Error: {result.error_message}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] {e}\")\n",
    "            results[config.name] = DatasetResult(\n",
    "                name=config.name,\n",
    "                success=False,\n",
    "                error_message=str(e)\n",
    "            )\n",
    "        \n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    return results\n",
    "\n",
    "\n",
    "# Load all datasets\n",
    "results = load_datasets_sequential(DATASET_CONFIGS)\n",
    "\n",
    "# Summary\n",
    "successful = [r for r in results.values() if r.success]\n",
    "failed = [r for r in results.values() if not r.success]\n",
    "total_samples = sum(r.sample_count for r in successful)\n",
    "\n",
    "print(f\"\\nLoading Summary:\")\n",
    "print(f\"  Successful: {len(successful)}/{len(results)}\")\n",
    "print(f\"  Total samples: {total_samples:,}\")\n",
    "\n",
    "if failed:\n",
    "    print(f\"  Failed datasets: {', '.join(r.name for r in failed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Merge Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract individual results\n",
    "msmarco_triplets = results.get(\"msmarco_ko\", DatasetResult(name=\"msmarco_ko\", success=False)).data\n",
    "klue_nli_pairs = results.get(\"klue_nli\", DatasetResult(name=\"klue_nli\", success=False)).data\n",
    "klue_sts_pairs = results.get(\"klue_sts\", DatasetResult(name=\"klue_sts\", success=False)).data\n",
    "korquad_pairs = results.get(\"korquad\", DatasetResult(name=\"korquad\", success=False)).data\n",
    "kobest_pairs = results.get(\"kobest_copa\", DatasetResult(name=\"kobest_copa\", success=False)).data\n",
    "naver_news_pairs = results.get(\"naver_news\", DatasetResult(name=\"naver_news\", success=False)).data\n",
    "nsmc_texts = results.get(\"nsmc\", DatasetResult(name=\"nsmc\", success=False)).corpus\n",
    "\n",
    "print(\"Extracted results:\")\n",
    "print(f\"  MS MARCO triplets: {len(msmarco_triplets)}\")\n",
    "print(f\"  KLUE NLI pairs: {len(klue_nli_pairs)}\")\n",
    "print(f\"  KLUE STS pairs: {len(klue_sts_pairs)}\")\n",
    "print(f\"  KorQuAD pairs: {len(korquad_pairs)}\")\n",
    "print(f\"  KoBEST COPA pairs: {len(kobest_pairs)}\")\n",
    "print(f\"  Naver News pairs: {len(naver_news_pairs)}\")\n",
    "print(f\"  NSMC texts: {len(nsmc_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_pairs(\n",
    "    klue_nli: List[Dict],\n",
    "    klue_sts: List[Dict],\n",
    "    korquad: List[Dict],\n",
    "    kobest: List[Dict],\n",
    "    naver_news: List[Dict],\n",
    "    msmarco: List[Dict],\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Merge all pair datasets into a unified format.\"\"\"\n",
    "    all_pairs: List[Dict[str, Any]] = []\n",
    "    \n",
    "    # Add KLUE pairs (skip contradictions for positive training)\n",
    "    for pair in klue_nli:\n",
    "        if pair.get(\"pair_type\") != \"klue_nli_neg\":\n",
    "            all_pairs.append(pair)\n",
    "    \n",
    "    all_pairs.extend(klue_sts)\n",
    "    all_pairs.extend(korquad)\n",
    "    all_pairs.extend(kobest)\n",
    "    all_pairs.extend(naver_news)\n",
    "    \n",
    "    # Convert MS MARCO triplets to pairs\n",
    "    for triplet in msmarco:\n",
    "        if triplet.get(\"positive\"):\n",
    "            all_pairs.append({\n",
    "                \"source\": triplet[\"anchor\"],\n",
    "                \"target\": triplet[\"positive\"],\n",
    "                \"similarity\": 0.85,\n",
    "                \"category\": \"retrieval\",\n",
    "                \"pair_type\": \"msmarco_ko\",\n",
    "            })\n",
    "    \n",
    "    return all_pairs\n",
    "\n",
    "\n",
    "def deduplicate_pairs(pairs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Remove duplicate pairs.\"\"\"\n",
    "    seen: set = set()\n",
    "    unique_pairs: List[Dict[str, Any]] = []\n",
    "    \n",
    "    for pair in pairs:\n",
    "        key = (pair.get(\"source\", \"\"), pair.get(\"target\", \"\"))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique_pairs.append(pair)\n",
    "    \n",
    "    return unique_pairs\n",
    "\n",
    "\n",
    "# Merge all pairs\n",
    "all_pairs = merge_all_pairs(\n",
    "    klue_nli=klue_nli_pairs,\n",
    "    klue_sts=klue_sts_pairs,\n",
    "    korquad=korquad_pairs,\n",
    "    kobest=kobest_pairs,\n",
    "    naver_news=naver_news_pairs,\n",
    "    msmarco=msmarco_triplets,\n",
    ")\n",
    "\n",
    "print(f\"Total pairs collected: {len(all_pairs)}\")\n",
    "\n",
    "unique_pairs = deduplicate_pairs(all_pairs)\n",
    "print(f\"Unique pairs after deduplication: {len(unique_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics by source\n",
    "source_counts: Dict[str, int] = defaultdict(int)\n",
    "for pair in unique_pairs:\n",
    "    source_counts[pair.get(\"pair_type\", \"unknown\")] += 1\n",
    "\n",
    "print(\"\\nPairs by source:\")\n",
    "for source, count in sorted(source_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {source}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(data: List[Dict], output_path: Path) -> int:\n",
    "    \"\"\"Save data to JSONL format.\"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "    return len(data)\n",
    "\n",
    "\n",
    "def save_text_corpus(texts: List[str], output_path: Path) -> int:\n",
    "    \"\"\"Save text corpus to file.\"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for text in texts:\n",
    "            f.write(text + \"\\n\")\n",
    "    return len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save synonym pairs\n",
    "pairs_output_path = HF_DATA_DIR / \"huggingface_synonym_pairs.jsonl\"\n",
    "saved_pairs = save_jsonl(unique_pairs, pairs_output_path)\n",
    "print(f\"Saved {saved_pairs:,} pairs to {pairs_output_path}\")\n",
    "\n",
    "# Save MS MARCO triplets separately\n",
    "triplets_with_negatives = [\n",
    "    t for t in msmarco_triplets\n",
    "    if t.get(\"positive\") and t.get(\"negative\")\n",
    "]\n",
    "\n",
    "triplets_output_path = HF_DATA_DIR / \"msmarco_triplets.jsonl\"\n",
    "saved_triplets = save_jsonl(triplets_with_negatives, triplets_output_path)\n",
    "print(f\"Saved {saved_triplets:,} MS MARCO triplets to {triplets_output_path}\")\n",
    "\n",
    "# Save corpus for negative mining\n",
    "corpus_output_path = HF_DATA_DIR / \"nsmc_corpus.txt\"\n",
    "saved_texts = save_text_corpus(nsmc_texts, corpus_output_path)\n",
    "print(f\"Saved {saved_texts:,} texts to {corpus_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"v22.0 HuggingFace Korean Data Loading Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nLoading Performance:\")\n",
    "print(f\"  Mode: Sequential (memory-safe)\")\n",
    "print(f\"  Datasets attempted: {len(results)}\")\n",
    "print(f\"  Successful: {len(successful)}\")\n",
    "print(f\"  Failed: {len(failed)}\")\n",
    "\n",
    "print(f\"\\nDatasets Loaded:\")\n",
    "print(f\"  MS MARCO Korean Triplets: {len(msmarco_triplets):,}\")\n",
    "print(f\"  KLUE NLI Pairs: {len(klue_nli_pairs):,}\")\n",
    "print(f\"  KLUE STS Pairs: {len(klue_sts_pairs):,}\")\n",
    "print(f\"  KorQuAD Pairs: {len(korquad_pairs):,}\")\n",
    "print(f\"  KoBEST COPA Pairs: {len(kobest_pairs):,}\")\n",
    "print(f\"  Naver News Pairs: {len(naver_news_pairs):,}\")\n",
    "print(f\"  NSMC Corpus: {len(nsmc_texts):,} texts\")\n",
    "\n",
    "print(f\"\\nTotal Unique Pairs: {len(unique_pairs):,}\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "for f in sorted(HF_DATA_DIR.glob(\"*\")):\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"  {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run `01_data_augmentation.ipynb` to merge with single-term expanded data\n",
    "2. The HuggingFace data will be automatically incorporated into training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

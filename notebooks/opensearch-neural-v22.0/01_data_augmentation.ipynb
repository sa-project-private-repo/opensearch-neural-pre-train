{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v22.0 Data Augmentation\n",
    "\n",
    "## Key Improvements over v21.4\n",
    "\n",
    "1. **Single-term Expansion**: 448 → 29,322 triplets (65x increase)\n",
    "2. **Automatic Synonym Extraction**: From curated lists, Wikipedia, and training data\n",
    "3. **Hard Negative Mining**: Character overlap-based difficulty scoring\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "1. **v21.3 Filtered Pairs**: Base synonym pairs from previous version\n",
    "2. **HuggingFace Korean Datasets**: Large-scale Korean NLP data\n",
    "3. **v22.0 Single-term Expanded**: Auto-extracted single-term synonym pairs\n",
    "4. **Identity Pairs**: Self-reconstruction pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load v21.3 Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "V21_3_DATA_DIR = PROJECT_ROOT / \"dataset\" / \"v21.3_filtered_enhanced\"\n",
    "V22_DATA_DIR = PROJECT_ROOT / \"data\" / \"v22.0\"\n",
    "HF_DATA_DIR = PROJECT_ROOT / \"data\" / \"huggingface_korean\"\n",
    "V22_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load v21.3 filtered pairs\n",
    "filtered_pairs_path = V21_3_DATA_DIR / \"filtered_synonym_pairs.jsonl\"\n",
    "\n",
    "v21_3_pairs = []\n",
    "if filtered_pairs_path.exists():\n",
    "    with open(filtered_pairs_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            v21_3_pairs.append(json.loads(line))\n",
    "    print(f\"Loaded {len(v21_3_pairs):,} filtered pairs from v21.3\")\n",
    "else:\n",
    "    print(f\"Warning: {filtered_pairs_path} not found\")\n",
    "\n",
    "if v21_3_pairs:\n",
    "    print(\"\\nSample pair:\")\n",
    "    print(json.dumps(v21_3_pairs[0], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load HuggingFace Korean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HuggingFace synonym pairs\n",
    "hf_pairs_path = HF_DATA_DIR / \"huggingface_synonym_pairs.jsonl\"\n",
    "\n",
    "hf_pairs = []\n",
    "if hf_pairs_path.exists():\n",
    "    with open(hf_pairs_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            hf_pairs.append(json.loads(line))\n",
    "    print(f\"Loaded {len(hf_pairs):,} pairs from HuggingFace datasets\")\n",
    "    \n",
    "    # Statistics by source\n",
    "    source_counts = defaultdict(int)\n",
    "    for pair in hf_pairs:\n",
    "        source_counts[pair.get(\"pair_type\", \"unknown\")] += 1\n",
    "    \n",
    "    print(\"\\nHuggingFace pairs by source:\")\n",
    "    for source, count in sorted(source_counts.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {source}: {count:,}\")\n",
    "else:\n",
    "    print(f\"Warning: {hf_pairs_path} not found\")\n",
    "    print(\"Please run 00_data_loading.ipynb first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load v22.0 Single-term Expanded Data\n",
    "\n",
    "This is the key improvement in v22.0 - automatically extracted single-term synonym pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load v22.0 single-term expanded data\n",
    "single_term_path = V22_DATA_DIR / \"single_term_expanded.jsonl\"\n",
    "\n",
    "single_term_expanded = []\n",
    "if single_term_path.exists():\n",
    "    with open(single_term_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            single_term_expanded.append(json.loads(line))\n",
    "    print(f\"Loaded {len(single_term_expanded):,} single-term expanded triplets\")\n",
    "    \n",
    "    # Statistics by difficulty\n",
    "    difficulty_counts = defaultdict(int)\n",
    "    for item in single_term_expanded:\n",
    "        difficulty_counts[item.get(\"difficulty\", \"unknown\")] += 1\n",
    "    \n",
    "    print(\"\\nBy difficulty:\")\n",
    "    for diff, count in sorted(difficulty_counts.items()):\n",
    "        pct = count / len(single_term_expanded) * 100\n",
    "        print(f\"  {diff}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\nSample triplet:\")\n",
    "    print(json.dumps(single_term_expanded[0], ensure_ascii=False, indent=2))\n",
    "else:\n",
    "    print(f\"Warning: {single_term_path} not found\")\n",
    "    print(\"Please run: python scripts/extract_synonyms.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MS MARCO triplets\n",
    "msmarco_triplets_path = HF_DATA_DIR / \"msmarco_triplets.jsonl\"\n",
    "\n",
    "msmarco_triplets = []\n",
    "if msmarco_triplets_path.exists():\n",
    "    with open(msmarco_triplets_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            msmarco_triplets.append(json.loads(line))\n",
    "    print(f\"Loaded {len(msmarco_triplets):,} MS MARCO triplets\")\n",
    "else:\n",
    "    print(f\"Warning: {msmarco_triplets_path} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Problem Terms Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBLEM_TERMS = [\n",
    "    \"추천\",\n",
    "    \"데이터베이스\",\n",
    "    \"증상\",\n",
    "    \"질환\",\n",
    "    \"인슐린\",\n",
    "]\n",
    "\n",
    "def check_term_coverage(pairs: List[Dict], terms: List[str]) -> Dict[str, Dict]:\n",
    "    \"\"\"Check how many times each term appears.\"\"\"\n",
    "    coverage = {term: {\"as_anchor\": 0, \"as_positive\": 0, \"partial\": 0} for term in terms}\n",
    "    \n",
    "    for pair in pairs:\n",
    "        anchor = pair.get(\"anchor\", pair.get(\"source\", \"\"))\n",
    "        positive = pair.get(\"positive\", pair.get(\"target\", \"\"))\n",
    "        \n",
    "        for term in terms:\n",
    "            if anchor == term:\n",
    "                coverage[term][\"as_anchor\"] += 1\n",
    "            elif term in anchor:\n",
    "                coverage[term][\"partial\"] += 1\n",
    "            \n",
    "            if positive == term:\n",
    "                coverage[term][\"as_positive\"] += 1\n",
    "    \n",
    "    return coverage\n",
    "\n",
    "# Check v21.3 coverage\n",
    "v21_3_coverage = check_term_coverage(v21_3_pairs, PROBLEM_TERMS)\n",
    "\n",
    "print(\"Problem Term Coverage in v21.3 Data:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Term':<15} {'As Anchor':>12} {'As Positive':>12} {'Partial':>12}\")\n",
    "print(\"-\" * 60)\n",
    "for term, stats in v21_3_coverage.items():\n",
    "    print(f\"{term:<15} {stats['as_anchor']:>12} {stats['as_positive']:>12} {stats['partial']:>12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check v22.0 single-term expanded coverage\n",
    "if single_term_expanded:\n",
    "    v22_coverage = check_term_coverage(single_term_expanded, PROBLEM_TERMS)\n",
    "    \n",
    "    print(\"\\nProblem Term Coverage in v22.0 Single-term Expanded Data:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Term':<15} {'As Anchor':>12} {'As Positive':>12} {'Partial':>12}\")\n",
    "    print(\"-\" * 60)\n",
    "    for term in PROBLEM_TERMS:\n",
    "        old = v21_3_coverage[term]\n",
    "        new = v22_coverage[term]\n",
    "        anchor_change = new['as_anchor'] - old['as_anchor']\n",
    "        pos_change = new['as_positive'] - old['as_positive']\n",
    "        \n",
    "        anchor_str = f\"{new['as_anchor']} (+{anchor_change})\" if anchor_change > 0 else str(new['as_anchor'])\n",
    "        pos_str = f\"{new['as_positive']} (+{pos_change})\" if pos_change > 0 else str(new['as_positive'])\n",
    "        \n",
    "        print(f\"{term:<15} {anchor_str:>12} {pos_str:>12} {new['partial']:>12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Augmented Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SynonymPair:\n",
    "    source: str\n",
    "    target: str\n",
    "    similarity: float\n",
    "    category: str\n",
    "    pair_type: str\n",
    "\n",
    "\n",
    "def convert_v21_3_pair(pair: Dict) -> SynonymPair:\n",
    "    \"\"\"Convert v21.3 pair format to SynonymPair.\"\"\"\n",
    "    return SynonymPair(\n",
    "        source=pair[\"source\"],\n",
    "        target=pair[\"target\"],\n",
    "        similarity=pair.get(\"similarity\", 0.8),\n",
    "        category=pair.get(\"category\", \"unknown\"),\n",
    "        pair_type=\"original\",\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_hf_pair(pair: Dict) -> SynonymPair:\n",
    "    \"\"\"Convert HuggingFace pair format to SynonymPair.\"\"\"\n",
    "    return SynonymPair(\n",
    "        source=pair[\"source\"],\n",
    "        target=pair[\"target\"],\n",
    "        similarity=pair.get(\"similarity\", 0.8),\n",
    "        category=pair.get(\"category\", \"huggingface\"),\n",
    "        pair_type=pair.get(\"pair_type\", \"huggingface\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_single_term_triplet(triplet: Dict) -> SynonymPair:\n",
    "    \"\"\"Convert single-term triplet to SynonymPair.\"\"\"\n",
    "    return SynonymPair(\n",
    "        source=triplet[\"anchor\"],\n",
    "        target=triplet[\"positive\"],\n",
    "        similarity=0.9,\n",
    "        category=\"single_term_expanded\",\n",
    "        pair_type=\"single_term_expanded\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all pairs\n",
    "original_pairs = [convert_v21_3_pair(p) for p in v21_3_pairs]\n",
    "print(f\"Original v21.3 pairs: {len(original_pairs):,}\")\n",
    "\n",
    "huggingface_pairs = [convert_hf_pair(p) for p in hf_pairs]\n",
    "print(f\"HuggingFace pairs: {len(huggingface_pairs):,}\")\n",
    "\n",
    "# Convert single-term expanded (extract unique pairs)\n",
    "single_term_pairs = []\n",
    "seen_pairs = set()\n",
    "for triplet in single_term_expanded:\n",
    "    key = (triplet[\"anchor\"], triplet[\"positive\"])\n",
    "    if key not in seen_pairs:\n",
    "        seen_pairs.add(key)\n",
    "        single_term_pairs.append(convert_single_term_triplet(triplet))\n",
    "print(f\"Single-term expanded pairs: {len(single_term_pairs):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all pairs\n",
    "all_pairs = original_pairs + huggingface_pairs + single_term_pairs\n",
    "\n",
    "# Remove duplicates\n",
    "seen = set()\n",
    "unique_pairs = []\n",
    "for pair in all_pairs:\n",
    "    key = (pair.source, pair.target)\n",
    "    if key not in seen:\n",
    "        seen.add(key)\n",
    "        unique_pairs.append(pair)\n",
    "\n",
    "print(f\"\\nTotal pairs after merge: {len(all_pairs):,}\")\n",
    "print(f\"Unique pairs: {len(unique_pairs):,}\")\n",
    "\n",
    "# Statistics by type\n",
    "type_counts = defaultdict(int)\n",
    "for pair in unique_pairs:\n",
    "    type_counts[pair.pair_type] += 1\n",
    "\n",
    "print(f\"\\nPairs by type:\")\n",
    "for pair_type, count in sorted(type_counts.items(), key=lambda x: -x[1]):\n",
    "    pct = count / len(unique_pairs) * 100\n",
    "    print(f\"  {pair_type}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify Problem Term Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that problem terms are now covered\n",
    "pair_dicts = [{\"source\": p.source, \"target\": p.target} for p in unique_pairs]\n",
    "new_coverage = check_term_coverage(pair_dicts, PROBLEM_TERMS)\n",
    "\n",
    "print(\"Problem Term Coverage After Augmentation:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Term':<15} {'As Source':>15} {'As Target':>15} {'Improvement':>15}\")\n",
    "print(\"-\" * 70)\n",
    "for term in PROBLEM_TERMS:\n",
    "    old_total = v21_3_coverage[term]['as_anchor'] + v21_3_coverage[term]['as_positive']\n",
    "    new_total = new_coverage[term]['as_anchor'] + new_coverage[term]['as_positive']\n",
    "    improvement = new_total - old_total\n",
    "    \n",
    "    print(f\"{term:<15} {new_coverage[term]['as_anchor']:>15} {new_coverage[term]['as_positive']:>15} {'+' + str(improvement):>15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save augmented pairs\n",
    "output_path = V22_DATA_DIR / \"augmented_synonym_pairs.jsonl\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for pair in unique_pairs:\n",
    "        f.write(json.dumps(asdict(pair), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(unique_pairs):,} pairs to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save MS MARCO triplets for direct training use\n",
    "if msmarco_triplets:\n",
    "    msmarco_output_path = V22_DATA_DIR / \"msmarco_direct_triplets.jsonl\"\n",
    "    \n",
    "    with open(msmarco_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for triplet in msmarco_triplets:\n",
    "            output_triplet = {\n",
    "                \"anchor\": triplet[\"anchor\"],\n",
    "                \"positive\": triplet[\"positive\"],\n",
    "                \"negative\": triplet.get(\"negative\", \"\"),\n",
    "                \"difficulty\": \"medium\",\n",
    "                \"length_class\": \"sentence\",\n",
    "                \"pair_type\": \"msmarco_direct\",\n",
    "            }\n",
    "            if output_triplet[\"negative\"]:\n",
    "                f.write(json.dumps(output_triplet, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    print(f\"Saved MS MARCO triplets to {msmarco_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category distribution\n",
    "category_counts = defaultdict(int)\n",
    "for pair in unique_pairs:\n",
    "    category_counts[pair.category] += 1\n",
    "\n",
    "print(\"Category Distribution:\")\n",
    "print(\"=\" * 50)\n",
    "for category, count in sorted(category_counts.items(), key=lambda x: -x[1])[:15]:\n",
    "    pct = count / len(unique_pairs) * 100\n",
    "    print(f\"  {category:<25}: {count:>8} ({pct:.1f}%)\")\n",
    "\n",
    "# Source length distribution\n",
    "length_bins = {\"1-3 chars\": 0, \"4-8 chars\": 0, \"9-20 chars\": 0, \"20+ chars\": 0}\n",
    "for pair in unique_pairs:\n",
    "    src_len = len(pair.source)\n",
    "    if src_len <= 3:\n",
    "        length_bins[\"1-3 chars\"] += 1\n",
    "    elif src_len <= 8:\n",
    "        length_bins[\"4-8 chars\"] += 1\n",
    "    elif src_len <= 20:\n",
    "        length_bins[\"9-20 chars\"] += 1\n",
    "    else:\n",
    "        length_bins[\"20+ chars\"] += 1\n",
    "\n",
    "print(\"\\nSource Length Distribution:\")\n",
    "print(\"=\" * 50)\n",
    "for length, count in length_bins.items():\n",
    "    pct = count / len(unique_pairs) * 100\n",
    "    print(f\"  {length:<15}: {count:>8} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"v22.0 Data Augmentation Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nData Sources:\")\n",
    "print(f\"  v21.3 Original Pairs: {len(original_pairs):,}\")\n",
    "print(f\"  HuggingFace Pairs: {len(huggingface_pairs):,}\")\n",
    "print(f\"  Single-term Expanded: {len(single_term_pairs):,} (NEW in v22.0)\")\n",
    "print(f\"  MS MARCO Triplets: {len(msmarco_triplets):,}\")\n",
    "\n",
    "print(f\"\\nKey Improvement:\")\n",
    "print(f\"  Single-term pairs: 448 (v21.4) -> {len(single_term_pairs):,} (v22.0)\")\n",
    "print(f\"  Increase: {len(single_term_pairs) / 448:.0f}x\")\n",
    "\n",
    "print(f\"\\nFinal Output:\")\n",
    "print(f\"  Total Unique Pairs: {len(unique_pairs):,}\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "for f in V22_DATA_DIR.glob(\"*.jsonl\"):\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"  {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run `02_data_preparation.ipynb` to generate curriculum learning splits\n",
    "2. The expanded single-term data will be used for Phase 1 training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

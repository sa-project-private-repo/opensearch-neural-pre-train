{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v22.0 Data Preparation\n",
    "\n",
    "Generate training triplets with curriculum learning phases optimized for InfoNCE loss.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. **InfoNCE-optimized Curriculum**: Temperature annealing across phases\n",
    "2. **Single-term Focus**: Phase 1 emphasizes single-term pairs (50%)\n",
    "3. **Hard Negative Mining**: Character overlap-based difficulty scoring\n",
    "\n",
    "## Curriculum Phases\n",
    "\n",
    "| Phase | Epochs | λ_infonce | Temperature | Data Focus |\n",
    "|-------|--------|-----------|-------------|------------|\n",
    "| 1 | 1-10 | 1.0 | 0.07 | 50% single-term |\n",
    "| 2 | 11-20 | 1.5 | 0.05 | 33% balanced |\n",
    "| 3 | 21-30 | 2.0 | 0.03 | Full + hard negatives |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "V22_DATA_DIR = PROJECT_ROOT / \"data\" / \"v22.0\"\n",
    "CORPUS_DIR = PROJECT_ROOT / \"dataset\" / \"v21.3_filtered_enhanced\"\n",
    "\n",
    "# Load augmented pairs\n",
    "augmented_pairs_path = V22_DATA_DIR / \"augmented_synonym_pairs.jsonl\"\n",
    "\n",
    "pairs = []\n",
    "with open(augmented_pairs_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        pairs.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(pairs):,} augmented pairs\")\n",
    "\n",
    "# Build source -> targets mapping\n",
    "source_to_targets: Dict[str, Set[str]] = defaultdict(set)\n",
    "for pair in pairs:\n",
    "    source_to_targets[pair[\"source\"]].add(pair[\"target\"])\n",
    "\n",
    "print(f\"Unique sources: {len(source_to_targets):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load single-term expanded triplets (already have hard negatives)\n",
    "single_term_path = V22_DATA_DIR / \"single_term_expanded.jsonl\"\n",
    "\n",
    "single_term_triplets = []\n",
    "if single_term_path.exists():\n",
    "    with open(single_term_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            single_term_triplets.append(json.loads(line))\n",
    "    print(f\"Loaded {len(single_term_triplets):,} single-term expanded triplets\")\n",
    "else:\n",
    "    print(f\"Warning: {single_term_path} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Corpus for Negative Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus vocabulary for negative mining\n",
    "term_list_path = CORPUS_DIR / \"term_list.json\"\n",
    "\n",
    "if term_list_path.exists():\n",
    "    with open(term_list_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        term_list = json.load(f)\n",
    "    corpus_vocab = {term: 1 for term in term_list}\n",
    "    print(f\"Loaded term_list.json: {len(corpus_vocab):,} terms\")\n",
    "else:\n",
    "    # Build vocabulary from pairs\n",
    "    corpus_vocab = {}\n",
    "    all_terms = set()\n",
    "    for pair in pairs:\n",
    "        all_terms.add(pair[\"source\"])\n",
    "        all_terms.add(pair[\"target\"])\n",
    "    for term in all_terms:\n",
    "        corpus_vocab[term] = 1\n",
    "    print(f\"Built vocabulary from pairs: {len(corpus_vocab):,} terms\")\n",
    "\n",
    "all_terms = list(corpus_vocab.keys())\n",
    "print(f\"Total unique terms: {len(all_terms):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classify Pairs by Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_length(text: str) -> str:\n",
    "    \"\"\"Classify text by character length.\"\"\"\n",
    "    length = len(text)\n",
    "    if length <= 3:  # 1-2 syllables (single term)\n",
    "        return \"single_term\"\n",
    "    elif length <= 8:  # 3-4 syllables (short phrase)\n",
    "        return \"short_phrase\"\n",
    "    else:  # 5+ syllables (sentence/long phrase)\n",
    "        return \"sentence\"\n",
    "\n",
    "\n",
    "# Classify all pairs\n",
    "pairs_by_length = defaultdict(list)\n",
    "for pair in pairs:\n",
    "    length_class = classify_length(pair[\"source\"])\n",
    "    pairs_by_length[length_class].append(pair)\n",
    "\n",
    "print(\"Pairs by length class:\")\n",
    "for length_class, class_pairs in pairs_by_length.items():\n",
    "    pct = len(class_pairs) / len(pairs) * 100\n",
    "    print(f\"  {length_class:<15}: {len(class_pairs):>8} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hard Negative Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingTriplet:\n",
    "    anchor: str\n",
    "    positive: str\n",
    "    negative: str\n",
    "    difficulty: str  # \"easy\", \"medium\", \"hard\"\n",
    "    length_class: str  # \"single_term\", \"short_phrase\", \"sentence\"\n",
    "    pair_type: str\n",
    "\n",
    "\n",
    "def find_similar_negatives(source: str, all_terms: List[str], \n",
    "                           positives: Set[str], n: int = 10) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Find similar-looking but semantically different negatives.\n",
    "    Returns list of (negative, difficulty) tuples.\n",
    "    \"\"\"\n",
    "    negatives = []\n",
    "    source_len = len(source)\n",
    "    \n",
    "    # Shuffle for randomness\n",
    "    shuffled_terms = random.sample(all_terms, min(len(all_terms), 5000))\n",
    "    \n",
    "    for term in shuffled_terms:\n",
    "        if term == source or term in positives:\n",
    "            continue\n",
    "        \n",
    "        term_len = len(term)\n",
    "        \n",
    "        # Calculate similarity based on character overlap and length\n",
    "        common_chars = len(set(source) & set(term))\n",
    "        len_diff = abs(source_len - term_len)\n",
    "        \n",
    "        # Hard: Similar length, some character overlap\n",
    "        if len_diff <= 2 and common_chars >= 1:\n",
    "            negatives.append((term, \"hard\"))\n",
    "        # Medium: Similar length OR some overlap\n",
    "        elif len_diff <= 3 or common_chars >= 1:\n",
    "            negatives.append((term, \"medium\"))\n",
    "        # Easy: Different length, no overlap\n",
    "        else:\n",
    "            negatives.append((term, \"easy\"))\n",
    "        \n",
    "        if len(negatives) >= n * 3:\n",
    "            break\n",
    "    \n",
    "    return negatives\n",
    "\n",
    "\n",
    "def generate_triplets_for_pair(pair: Dict, all_terms: List[str],\n",
    "                               source_to_targets: Dict[str, Set[str]],\n",
    "                               n_negatives: int = 3) -> List[TrainingTriplet]:\n",
    "    \"\"\"Generate training triplets for a synonym pair.\"\"\"\n",
    "    source = pair[\"source\"]\n",
    "    target = pair[\"target\"]\n",
    "    positives = source_to_targets.get(source, set())\n",
    "    length_class = classify_length(source)\n",
    "    pair_type = pair.get(\"pair_type\", \"original\")\n",
    "    \n",
    "    # Find negatives with difficulty labels\n",
    "    negatives = find_similar_negatives(source, all_terms, positives, n_negatives * 2)\n",
    "    \n",
    "    # Balance by difficulty\n",
    "    by_difficulty = defaultdict(list)\n",
    "    for neg, diff in negatives:\n",
    "        by_difficulty[diff].append(neg)\n",
    "    \n",
    "    triplets = []\n",
    "    for difficulty in [\"easy\", \"medium\", \"hard\"]:\n",
    "        candidates = by_difficulty[difficulty]\n",
    "        n_select = min(len(candidates), max(1, n_negatives // 3))\n",
    "        selected = random.sample(candidates, n_select) if candidates else []\n",
    "        \n",
    "        for neg in selected:\n",
    "            triplets.append(TrainingTriplet(\n",
    "                anchor=source,\n",
    "                positive=target,\n",
    "                negative=neg,\n",
    "                difficulty=difficulty,\n",
    "                length_class=length_class,\n",
    "                pair_type=pair_type,\n",
    "            ))\n",
    "    \n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate triplets for augmented pairs\n",
    "augmented_triplets = []\n",
    "\n",
    "for pair in tqdm(pairs, desc=\"Generating triplets from pairs\"):\n",
    "    triplets = generate_triplets_for_pair(\n",
    "        pair, all_terms, source_to_targets, n_negatives=3\n",
    "    )\n",
    "    augmented_triplets.extend(triplets)\n",
    "\n",
    "print(f\"\\nGenerated {len(augmented_triplets):,} triplets from augmented pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert single-term expanded triplets\n",
    "single_term_training = []\n",
    "for triplet in single_term_triplets:\n",
    "    single_term_training.append(TrainingTriplet(\n",
    "        anchor=triplet[\"anchor\"],\n",
    "        positive=triplet[\"positive\"],\n",
    "        negative=triplet[\"negative\"],\n",
    "        difficulty=triplet.get(\"difficulty\", \"medium\"),\n",
    "        length_class=\"single_term\",\n",
    "        pair_type=\"single_term_expanded\",\n",
    "    ))\n",
    "\n",
    "print(f\"Converted {len(single_term_training):,} single-term expanded triplets\")\n",
    "\n",
    "# Merge all triplets\n",
    "all_triplets = augmented_triplets + single_term_training\n",
    "print(f\"\\nTotal triplets: {len(all_triplets):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Triplet Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Triplet Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# By difficulty\n",
    "difficulty_counts = defaultdict(int)\n",
    "for t in all_triplets:\n",
    "    difficulty_counts[t.difficulty] += 1\n",
    "\n",
    "print(\"\\nBy Difficulty:\")\n",
    "for diff, count in sorted(difficulty_counts.items()):\n",
    "    pct = count / len(all_triplets) * 100\n",
    "    print(f\"  {diff:<10}: {count:>8,} ({pct:.1f}%)\")\n",
    "\n",
    "# By length class\n",
    "length_counts = defaultdict(int)\n",
    "for t in all_triplets:\n",
    "    length_counts[t.length_class] += 1\n",
    "\n",
    "print(\"\\nBy Length Class:\")\n",
    "for lc, count in sorted(length_counts.items()):\n",
    "    pct = count / len(all_triplets) * 100\n",
    "    print(f\"  {lc:<15}: {count:>8,} ({pct:.1f}%)\")\n",
    "\n",
    "# By pair type\n",
    "type_counts = defaultdict(int)\n",
    "for t in all_triplets:\n",
    "    type_counts[t.pair_type] += 1\n",
    "\n",
    "print(\"\\nBy Pair Type:\")\n",
    "for pt, count in sorted(type_counts.items(), key=lambda x: -x[1])[:10]:\n",
    "    pct = count / len(all_triplets) * 100\n",
    "    print(f\"  {pt:<20}: {count:>8,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Curriculum Learning Splits\n",
    "\n",
    "Optimized for InfoNCE loss with temperature annealing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_curriculum_splits(triplets: List[TrainingTriplet]) -> Dict[str, List[TrainingTriplet]]:\n",
    "    \"\"\"\n",
    "    Create curriculum learning splits optimized for InfoNCE:\n",
    "    - Phase 1: Single-term heavy (50% single, 30% short, 20% sentence)\n",
    "    - Phase 2: Balanced (33% each)\n",
    "    - Phase 3: Full data with hard negatives\n",
    "    \"\"\"\n",
    "    # Group by length class\n",
    "    by_length = defaultdict(list)\n",
    "    for t in triplets:\n",
    "        by_length[t.length_class].append(t)\n",
    "    \n",
    "    single_term = by_length[\"single_term\"]\n",
    "    short_phrase = by_length[\"short_phrase\"]\n",
    "    sentence = by_length[\"sentence\"]\n",
    "    \n",
    "    print(f\"Available: single={len(single_term):,}, short={len(short_phrase):,}, sentence={len(sentence):,}\")\n",
    "    \n",
    "    # Phase 1: Single-term focus (50% single-term)\n",
    "    # This is critical for InfoNCE as it needs strong single-term representations\n",
    "    phase1_size = len(single_term) * 2\n",
    "    phase1 = single_term.copy()\n",
    "    phase1 += random.sample(short_phrase, min(len(short_phrase), int(phase1_size * 0.3)))\n",
    "    phase1 += random.sample(sentence, min(len(sentence), int(phase1_size * 0.2)))\n",
    "    random.shuffle(phase1)\n",
    "    \n",
    "    # Phase 2: Balanced learning\n",
    "    min_class_size = min(len(single_term), len(short_phrase), len(sentence))\n",
    "    phase2 = []\n",
    "    phase2 += random.sample(single_term, min(len(single_term), min_class_size))\n",
    "    phase2 += random.sample(short_phrase, min(len(short_phrase), min_class_size))\n",
    "    phase2 += random.sample(sentence, min(len(sentence), min_class_size))\n",
    "    random.shuffle(phase2)\n",
    "    \n",
    "    # Phase 3: Full data (all triplets)\n",
    "    phase3 = triplets.copy()\n",
    "    random.shuffle(phase3)\n",
    "    \n",
    "    return {\n",
    "        \"phase1_single_term_focus\": phase1,\n",
    "        \"phase2_balanced\": phase2,\n",
    "        \"phase3_full\": phase3,\n",
    "    }\n",
    "\n",
    "\n",
    "curriculum_splits = create_curriculum_splits(all_triplets)\n",
    "\n",
    "print(\"\\nCurriculum Splits:\")\n",
    "for phase, data in curriculum_splits.items():\n",
    "    print(f\"  {phase}: {len(data):,} triplets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify single-term percentage in Phase 1\n",
    "for phase_name, phase_data in curriculum_splits.items():\n",
    "    length_dist = defaultdict(int)\n",
    "    for t in phase_data:\n",
    "        length_dist[t.length_class] += 1\n",
    "    \n",
    "    print(f\"\\n{phase_name}:\")\n",
    "    for lc in [\"single_term\", \"short_phrase\", \"sentence\"]:\n",
    "        pct = length_dist[lc] / len(phase_data) * 100 if phase_data else 0\n",
    "        print(f\"  {lc}: {length_dist[lc]:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Add MS MARCO Triplets to Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MS MARCO direct triplets\n",
    "msmarco_triplets_path = V22_DATA_DIR / \"msmarco_direct_triplets.jsonl\"\n",
    "\n",
    "msmarco_triplets: List[TrainingTriplet] = []\n",
    "\n",
    "if msmarco_triplets_path.exists():\n",
    "    with open(msmarco_triplets_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            msmarco_triplets.append(TrainingTriplet(\n",
    "                anchor=item[\"anchor\"],\n",
    "                positive=item[\"positive\"],\n",
    "                negative=item.get(\"negative\", \"\"),\n",
    "                difficulty=item.get(\"difficulty\", \"medium\"),\n",
    "                length_class=item.get(\"length_class\", \"sentence\"),\n",
    "                pair_type=item.get(\"pair_type\", \"msmarco_direct\"),\n",
    "            ))\n",
    "    print(f\"Loaded {len(msmarco_triplets):,} MS MARCO direct triplets\")\n",
    "    \n",
    "    # Merge into Phase 3\n",
    "    original_phase3_size = len(curriculum_splits[\"phase3_full\"])\n",
    "    curriculum_splits[\"phase3_full\"].extend(msmarco_triplets)\n",
    "    random.shuffle(curriculum_splits[\"phase3_full\"])\n",
    "    \n",
    "    print(f\"Phase 3 size: {original_phase3_size:,} -> {len(curriculum_splits['phase3_full']):,}\")\n",
    "else:\n",
    "    print(f\"Warning: {msmarco_triplets_path} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(triplets: List[TrainingTriplet], \n",
    "                    val_ratio: float = 0.1) -> Tuple[List[TrainingTriplet], List[TrainingTriplet]]:\n",
    "    \"\"\"Split triplets into train and validation sets by anchor.\"\"\"\n",
    "    # Group by anchor to prevent data leakage\n",
    "    by_anchor = defaultdict(list)\n",
    "    for t in triplets:\n",
    "        by_anchor[t.anchor].append(t)\n",
    "    \n",
    "    anchors = list(by_anchor.keys())\n",
    "    random.shuffle(anchors)\n",
    "    \n",
    "    val_size = int(len(anchors) * val_ratio)\n",
    "    val_anchors = set(anchors[:val_size])\n",
    "    \n",
    "    train_triplets = []\n",
    "    val_triplets = []\n",
    "    \n",
    "    for anchor, anchor_triplets in by_anchor.items():\n",
    "        if anchor in val_anchors:\n",
    "            val_triplets.extend(anchor_triplets)\n",
    "        else:\n",
    "            train_triplets.extend(anchor_triplets)\n",
    "    \n",
    "    return train_triplets, val_triplets\n",
    "\n",
    "\n",
    "# Split full dataset\n",
    "train_triplets, val_triplets = train_val_split(all_triplets, val_ratio=0.1)\n",
    "\n",
    "print(f\"Train triplets: {len(train_triplets):,}\")\n",
    "print(f\"Validation triplets: {len(val_triplets):,}\")\n",
    "print(f\"Validation ratio: {len(val_triplets) / (len(train_triplets) + len(val_triplets)) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_triplets(triplets: List[TrainingTriplet], path: Path):\n",
    "    \"\"\"Save triplets to JSONL file.\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for t in triplets:\n",
    "            f.write(json.dumps(asdict(t), ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"Saved {len(triplets):,} triplets to {path}\")\n",
    "\n",
    "\n",
    "# Save main splits\n",
    "save_triplets(train_triplets, V22_DATA_DIR / \"training_triplets.jsonl\")\n",
    "save_triplets(val_triplets, V22_DATA_DIR / \"validation_triplets.jsonl\")\n",
    "\n",
    "# Save curriculum splits\n",
    "for phase, data in curriculum_splits.items():\n",
    "    save_triplets(data, V22_DATA_DIR / f\"{phase}_triplets.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Verify Problem Term Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBLEM_TERMS = [\"추천\", \"데이터베이스\", \"증상\", \"질환\", \"인슐린\"]\n",
    "\n",
    "print(\"Problem Term Coverage in Training Triplets:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for term in PROBLEM_TERMS:\n",
    "    as_anchor = sum(1 for t in train_triplets if t.anchor == term)\n",
    "    as_positive = sum(1 for t in train_triplets if t.positive == term)\n",
    "    total = as_anchor + as_positive\n",
    "    print(f\"{term:<15}: anchor={as_anchor:>4}, positive={as_positive:>4}, total={total:>4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"v22.0 Data Preparation Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nInput:\")\n",
    "print(f\"  Augmented pairs: {len(pairs):,}\")\n",
    "print(f\"  Single-term expanded: {len(single_term_triplets):,}\")\n",
    "\n",
    "print(f\"\\nOutput:\")\n",
    "print(f\"  Total triplets: {len(all_triplets):,}\")\n",
    "print(f\"  Training triplets: {len(train_triplets):,}\")\n",
    "print(f\"  Validation triplets: {len(val_triplets):,}\")\n",
    "\n",
    "print(f\"\\nCurriculum Phases (optimized for InfoNCE):\")\n",
    "for phase, data in curriculum_splits.items():\n",
    "    length_dist = defaultdict(int)\n",
    "    for t in data:\n",
    "        length_dist[t.length_class] += 1\n",
    "    print(f\"  {phase}:\")\n",
    "    for lc in [\"single_term\", \"short_phrase\", \"sentence\"]:\n",
    "        pct = length_dist[lc] / len(data) * 100 if data else 0\n",
    "        print(f\"    {lc}: {length_dist[lc]:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "for f in V22_DATA_DIR.glob(\"*.jsonl\"):\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"  {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run `03_training.ipynb` with InfoNCE loss and temperature annealing\n",
    "2. Use phase-specific data for curriculum learning:\n",
    "   - Phase 1 (epochs 1-10): `phase1_single_term_focus_triplets.jsonl` with τ=0.07\n",
    "   - Phase 2 (epochs 11-20): `phase2_balanced_triplets.jsonl` with τ=0.05\n",
    "   - Phase 3 (epochs 21-30): `phase3_full_triplets.jsonl` with τ=0.03"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

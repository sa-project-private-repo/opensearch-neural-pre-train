{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v22.0 Training with InfoNCE Contrastive Loss\n",
    "\n",
    "## Key Improvements over v21.4\n",
    "\n",
    "1. **InfoNCE Contrastive Loss**: In-batch negatives for better representation learning\n",
    "2. **Temperature Annealing**: 0.07 → 0.05 → 0.03 (sharper discrimination)\n",
    "3. **Expanded Single-term Data**: 448 → 29,322 triplets (65x increase)\n",
    "4. **Curriculum Learning**: 3 phases with dynamic InfoNCE weight\n",
    "\n",
    "## Target Metrics\n",
    "\n",
    "| Metric | v21.4 | v22.0 Target |\n",
    "|--------|-------|--------------|\n",
    "| Recall@1 | 38.1% | 70%+ |\n",
    "| MRR | 0.4412 | 0.75+ |\n",
    "| Single-term Recall | ~30% | 80%+ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, get_linear_schedule_with_warmup\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Import v22.0 loss functions\n",
    "from src.model.losses import (\n",
    "    InfoNCELoss,\n",
    "    SelfReconstructionLoss,\n",
    "    PositiveActivationLoss,\n",
    "    TripletMarginLoss,\n",
    "    FLOPSLoss,\n",
    "    MinimumActivationLoss,\n",
    "    SPLADELossV22,\n",
    ")\n",
    "\n",
    "# Set seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Configuration with InfoNCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CurriculumPhaseV22:\n",
    "    \"\"\"Configuration for v22.0 curriculum learning phase.\"\"\"\n",
    "    name: str\n",
    "    start_epoch: int\n",
    "    end_epoch: int\n",
    "    # InfoNCE parameters\n",
    "    lambda_infonce: float\n",
    "    temperature: float\n",
    "    # Other loss weights\n",
    "    lambda_flops: float\n",
    "    lambda_min_activation: float\n",
    "    lr_multiplier: float\n",
    "    data_file: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfigV22:\n",
    "    \"\"\"Training configuration for v22.0.\"\"\"\n",
    "    # Model\n",
    "    model_name: str = \"skt/A.X-Encoder-base\"\n",
    "    max_length: int = 64\n",
    "    \n",
    "    # Training\n",
    "    total_epochs: int = 30\n",
    "    batch_size: int = 64\n",
    "    learning_rate: float = 3e-6\n",
    "    warmup_ratio: float = 0.1\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Loss weights (static across phases)\n",
    "    lambda_self: float = 4.0\n",
    "    lambda_positive: float = 10.0\n",
    "    lambda_margin: float = 2.5\n",
    "    target_margin: float = 1.5\n",
    "    \n",
    "    # Minimum activation\n",
    "    min_activation_k: int = 5\n",
    "    min_activation_threshold: float = 0.5\n",
    "    \n",
    "    # Curriculum phases\n",
    "    phases: List[CurriculumPhaseV22] = field(default_factory=list)\n",
    "    \n",
    "    # Paths\n",
    "    data_dir: Path = None\n",
    "    output_dir: Path = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.phases:\n",
    "            # v22.0 Curriculum: Temperature annealing + InfoNCE weight increase\n",
    "            self.phases = [\n",
    "                CurriculumPhaseV22(\n",
    "                    name=\"phase1_single_term\",\n",
    "                    start_epoch=1,\n",
    "                    end_epoch=10,\n",
    "                    lambda_infonce=1.0,\n",
    "                    temperature=0.07,  # Warm start\n",
    "                    lambda_flops=3e-3,\n",
    "                    lambda_min_activation=2.0,\n",
    "                    lr_multiplier=1.0,\n",
    "                    data_file=\"phase1_single_term_focus_triplets.jsonl\",\n",
    "                ),\n",
    "                CurriculumPhaseV22(\n",
    "                    name=\"phase2_balanced\",\n",
    "                    start_epoch=11,\n",
    "                    end_epoch=20,\n",
    "                    lambda_infonce=1.5,\n",
    "                    temperature=0.05,  # Sharper\n",
    "                    lambda_flops=4e-3,\n",
    "                    lambda_min_activation=1.5,\n",
    "                    lr_multiplier=0.5,\n",
    "                    data_file=\"phase2_balanced_triplets.jsonl\",\n",
    "                ),\n",
    "                CurriculumPhaseV22(\n",
    "                    name=\"phase3_full\",\n",
    "                    start_epoch=21,\n",
    "                    end_epoch=30,\n",
    "                    lambda_infonce=2.0,\n",
    "                    temperature=0.03,  # Sharp discrimination\n",
    "                    lambda_flops=5e-3,\n",
    "                    lambda_min_activation=1.0,\n",
    "                    lr_multiplier=0.25,\n",
    "                    data_file=\"phase3_full_triplets.jsonl\",\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "\n",
    "# Create config\n",
    "config = TrainingConfigV22(\n",
    "    data_dir=PROJECT_ROOT / \"data\" / \"v22.0\",\n",
    "    output_dir=PROJECT_ROOT / \"outputs\" / \"v22.0_infonce\",\n",
    ")\n",
    "config.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"v22.0 Configuration:\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Epochs: {config.total_epochs}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"\\nCurriculum Phases (with InfoNCE):\")\n",
    "for phase in config.phases:\n",
    "    print(f\"  {phase.name}: epochs {phase.start_epoch}-{phase.end_epoch}\")\n",
    "    print(f\"    lambda_infonce={phase.lambda_infonce}, temp={phase.temperature}\")\n",
    "    print(f\"    lambda_flops={phase.lambda_flops}, lr_mult={phase.lr_multiplier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPLADEModelV22(nn.Module):\n",
    "    \"\"\"SPLADE model for v22.0 with InfoNCE support.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"skt/A.X-Encoder-base\"):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        self.config = self.model.config\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Returns:\n",
    "            sparse_repr: [batch_size, vocab_size] - max-pooled sparse representation\n",
    "            token_weights: [batch_size, seq_len] - per-token max weights\n",
    "        \"\"\"\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # SPLADE transformation: log(1 + ReLU(x))\n",
    "        token_scores = torch.log1p(self.relu(logits))\n",
    "        \n",
    "        # Apply attention mask\n",
    "        mask = attention_mask.unsqueeze(-1).float()\n",
    "        token_scores = token_scores * mask\n",
    "        \n",
    "        # Max pooling over sequence length\n",
    "        sparse_repr, _ = token_scores.max(dim=1)\n",
    "        \n",
    "        # Token weights for analysis\n",
    "        token_weights = token_scores.max(dim=-1).values\n",
    "        \n",
    "        return sparse_repr, token_weights\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SPLADEModelV22(config.model_name)\n",
    "model = model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "print(f\"Model loaded: {config.model_name}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    \"\"\"Dataset for triplet training.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: Path, tokenizer, max_length: int = 64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "        \n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "        \n",
    "        print(f\"Loaded {len(self.data):,} triplets from {data_path.name}\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        anchor = self.tokenizer(\n",
    "            item[\"anchor\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        positive = self.tokenizer(\n",
    "            item[\"positive\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        negative = self.tokenizer(\n",
    "            item[\"negative\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"anchor_input_ids\": anchor[\"input_ids\"].squeeze(0),\n",
    "            \"anchor_attention_mask\": anchor[\"attention_mask\"].squeeze(0),\n",
    "            \"positive_input_ids\": positive[\"input_ids\"].squeeze(0),\n",
    "            \"positive_attention_mask\": positive[\"attention_mask\"].squeeze(0),\n",
    "            \"negative_input_ids\": negative[\"input_ids\"].squeeze(0),\n",
    "            \"negative_attention_mask\": negative[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "\n",
    "# Load validation data\n",
    "val_dataset = TripletDataset(\n",
    "    config.data_dir / \"validation_triplets.jsonl\",\n",
    "    tokenizer,\n",
    "    config.max_length,\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. v22.0 Loss Functions (InfoNCE + Combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get special token IDs for self-reconstruction loss\n",
    "special_token_ids = set()\n",
    "for attr in ['pad_token_id', 'cls_token_id', 'sep_token_id', \n",
    "             'unk_token_id', 'mask_token_id', 'bos_token_id', 'eos_token_id']:\n",
    "    token_id = getattr(tokenizer, attr, None)\n",
    "    if token_id is not None:\n",
    "        special_token_ids.add(token_id)\n",
    "\n",
    "print(f\"Special token IDs: {special_token_ids}\")\n",
    "\n",
    "\n",
    "def compute_self_reconstruction_loss(\n",
    "    sparse_repr: torch.Tensor,\n",
    "    input_ids: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    pad_token_id: int,\n",
    "    special_token_ids: set,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute self-reconstruction loss (encourage input token activation).\"\"\"\n",
    "    batch_size = sparse_repr.size(0)\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        valid_mask = (attention_mask[i] == 1)\n",
    "        token_ids = input_ids[i][valid_mask]\n",
    "        \n",
    "        valid_ids = [tid.item() for tid in token_ids \n",
    "                     if tid.item() not in special_token_ids and tid.item() != pad_token_id]\n",
    "        \n",
    "        if not valid_ids:\n",
    "            losses.append(torch.tensor(0.0, device=sparse_repr.device))\n",
    "            continue\n",
    "        \n",
    "        activations = sparse_repr[i, valid_ids]\n",
    "        loss = -activations.mean()\n",
    "        losses.append(loss)\n",
    "    \n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "def compute_positive_activation_loss(\n",
    "    anchor_repr: torch.Tensor,\n",
    "    positive_input_ids: torch.Tensor,\n",
    "    positive_attention_mask: torch.Tensor,\n",
    "    pad_token_id: int,\n",
    "    special_token_ids: set,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Encourage anchor to activate positive synonym tokens.\"\"\"\n",
    "    batch_size = anchor_repr.size(0)\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        valid_mask = (positive_attention_mask[i] == 1)\n",
    "        token_ids = positive_input_ids[i][valid_mask]\n",
    "        \n",
    "        valid_ids = [tid.item() for tid in token_ids \n",
    "                     if tid.item() not in special_token_ids and tid.item() != pad_token_id]\n",
    "        \n",
    "        if not valid_ids:\n",
    "            losses.append(torch.tensor(0.0, device=anchor_repr.device))\n",
    "            continue\n",
    "        \n",
    "        activations = anchor_repr[i, valid_ids]\n",
    "        loss = -activations.mean()\n",
    "        losses.append(loss)\n",
    "    \n",
    "    return torch.stack(losses).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Training Loop with InfoNCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_v22(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler,\n",
    "    config: TrainingConfigV22,\n",
    "    phase: CurriculumPhaseV22,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    infonce_loss_fn: InfoNCELoss,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Train for one epoch with InfoNCE.\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    loss_components = defaultdict(float)\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch} ({phase.name})\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        # Move to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        anchor_repr, _ = model(batch[\"anchor_input_ids\"], batch[\"anchor_attention_mask\"])\n",
    "        positive_repr, _ = model(batch[\"positive_input_ids\"], batch[\"positive_attention_mask\"])\n",
    "        negative_repr, _ = model(batch[\"negative_input_ids\"], batch[\"negative_attention_mask\"])\n",
    "        \n",
    "        # 1. InfoNCE Loss (NEW in v22.0)\n",
    "        loss_infonce = infonce_loss_fn(anchor_repr, positive_repr, negative_repr)\n",
    "        \n",
    "        # 2. Self-reconstruction loss\n",
    "        loss_self = compute_self_reconstruction_loss(\n",
    "            anchor_repr, batch[\"anchor_input_ids\"], batch[\"anchor_attention_mask\"],\n",
    "            tokenizer.pad_token_id, special_token_ids,\n",
    "        )\n",
    "        \n",
    "        # 3. Positive activation loss\n",
    "        loss_positive = compute_positive_activation_loss(\n",
    "            anchor_repr, batch[\"positive_input_ids\"], batch[\"positive_attention_mask\"],\n",
    "            tokenizer.pad_token_id, special_token_ids,\n",
    "        )\n",
    "        \n",
    "        # 4. Triplet margin loss\n",
    "        pos_sim = F.cosine_similarity(anchor_repr, positive_repr, dim=-1)\n",
    "        neg_sim = F.cosine_similarity(anchor_repr, negative_repr, dim=-1)\n",
    "        loss_triplet = F.relu(config.target_margin - pos_sim + neg_sim).mean()\n",
    "        \n",
    "        # 5. FLOPS regularization\n",
    "        mean_activations = anchor_repr.mean(dim=0)\n",
    "        loss_flops = (mean_activations ** 2).sum()\n",
    "        \n",
    "        # 6. Minimum activation loss\n",
    "        topk_values, _ = torch.topk(anchor_repr, k=config.min_activation_k, dim=-1)\n",
    "        mean_topk = topk_values.mean(dim=-1)\n",
    "        loss_min_act = F.relu(config.min_activation_threshold - mean_topk).mean()\n",
    "        \n",
    "        # Total loss with phase-specific weights\n",
    "        loss = (\n",
    "            phase.lambda_infonce * loss_infonce +\n",
    "            config.lambda_self * loss_self +\n",
    "            config.lambda_positive * loss_positive +\n",
    "            config.lambda_margin * loss_triplet +\n",
    "            phase.lambda_flops * loss_flops +\n",
    "            phase.lambda_min_activation * loss_min_act\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track losses\n",
    "        total_loss += loss.item()\n",
    "        loss_components[\"infonce\"] += loss_infonce.item()\n",
    "        loss_components[\"self\"] += loss_self.item()\n",
    "        loss_components[\"positive\"] += loss_positive.item()\n",
    "        loss_components[\"triplet\"] += loss_triplet.item()\n",
    "        loss_components[\"flops\"] += loss_flops.item()\n",
    "        loss_components[\"min_act\"] += loss_min_act.item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{loss.item():.4f}\",\n",
    "            \"infonce\": f\"{loss_infonce.item():.4f}\",\n",
    "            \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\",\n",
    "        })\n",
    "    \n",
    "    n_batches = len(dataloader)\n",
    "    return {\n",
    "        \"total\": total_loss / n_batches,\n",
    "        **{k: v / n_batches for k, v in loss_components.items()},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_v22(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_recalls = []\n",
    "    all_mrrs = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        anchor_repr, _ = model(batch[\"anchor_input_ids\"], batch[\"anchor_attention_mask\"])\n",
    "        positive_repr, _ = model(batch[\"positive_input_ids\"], batch[\"positive_attention_mask\"])\n",
    "        negative_repr, _ = model(batch[\"negative_input_ids\"], batch[\"negative_attention_mask\"])\n",
    "        \n",
    "        # Compute similarities\n",
    "        pos_sim = F.cosine_similarity(anchor_repr, positive_repr, dim=-1)\n",
    "        neg_sim = F.cosine_similarity(anchor_repr, negative_repr, dim=-1)\n",
    "        \n",
    "        # Recall: positive should rank higher than negative\n",
    "        recalls = (pos_sim > neg_sim).float()\n",
    "        all_recalls.extend(recalls.cpu().tolist())\n",
    "        \n",
    "        # MRR\n",
    "        for i in range(len(pos_sim)):\n",
    "            if pos_sim[i] > neg_sim[i]:\n",
    "                all_mrrs.append(1.0)\n",
    "            else:\n",
    "                all_mrrs.append(0.5)\n",
    "    \n",
    "    return {\n",
    "        \"recall\": np.mean(all_recalls) * 100,\n",
    "        \"mrr\": np.mean(all_mrrs),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Run v22.0 Curriculum Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_curriculum_training_v22(model, config, device):\n",
    "    \"\"\"Run full v22.0 curriculum training with InfoNCE.\"\"\"\n",
    "    training_history = []\n",
    "    best_recall = 0.0\n",
    "    \n",
    "    for phase in config.phases:\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"Starting {phase.name}: Epochs {phase.start_epoch}-{phase.end_epoch}\")\n",
    "        print(f\"InfoNCE: lambda={phase.lambda_infonce}, temp={phase.temperature}\")\n",
    "        print(f\"lambda_flops={phase.lambda_flops}, lambda_min_act={phase.lambda_min_activation}\")\n",
    "        print(f\"lr_multiplier={phase.lr_multiplier}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        \n",
    "        # Create InfoNCE loss with phase-specific temperature\n",
    "        infonce_loss_fn = InfoNCELoss(\n",
    "            temperature=phase.temperature,\n",
    "            similarity=\"cosine\",\n",
    "        )\n",
    "        \n",
    "        # Load phase-specific data\n",
    "        train_dataset = TripletDataset(\n",
    "            config.data_dir / phase.data_file,\n",
    "            tokenizer,\n",
    "            config.max_length,\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer and scheduler for this phase\n",
    "        phase_epochs = phase.end_epoch - phase.start_epoch + 1\n",
    "        total_steps = len(train_loader) * phase_epochs\n",
    "        warmup_steps = int(total_steps * config.warmup_ratio)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config.learning_rate * phase.lr_multiplier,\n",
    "            weight_decay=0.01,\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps,\n",
    "        )\n",
    "        \n",
    "        # Train for phase epochs\n",
    "        for epoch in range(phase.start_epoch, phase.end_epoch + 1):\n",
    "            train_metrics = train_epoch_v22(\n",
    "                model, train_loader, optimizer, scheduler,\n",
    "                config, phase, device, epoch, infonce_loss_fn,\n",
    "            )\n",
    "            \n",
    "            # Evaluate\n",
    "            eval_metrics = evaluate_v22(model, val_loader, device)\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch}: Loss={train_metrics['total']:.4f}, \"\n",
    "                  f\"InfoNCE={train_metrics['infonce']:.4f}, \"\n",
    "                  f\"Recall={eval_metrics['recall']:.1f}%, MRR={eval_metrics['mrr']:.4f}\")\n",
    "            \n",
    "            # Save history\n",
    "            history_entry = {\n",
    "                \"epoch\": epoch,\n",
    "                \"phase\": phase.name,\n",
    "                \"temperature\": phase.temperature,\n",
    "                **train_metrics,\n",
    "                **eval_metrics,\n",
    "            }\n",
    "            training_history.append(history_entry)\n",
    "            \n",
    "            # Save best model\n",
    "            if eval_metrics[\"recall\"] > best_recall:\n",
    "                best_recall = eval_metrics[\"recall\"]\n",
    "                torch.save({\n",
    "                    \"epoch\": epoch,\n",
    "                    \"phase\": phase.name,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"eval_results\": eval_metrics,\n",
    "                    \"config\": {\n",
    "                        \"model_name\": config.model_name,\n",
    "                        \"max_length\": config.max_length,\n",
    "                        \"version\": \"v22.0\",\n",
    "                    },\n",
    "                }, config.output_dir / \"best_model.pt\")\n",
    "                print(f\"  -> New best model saved! Recall: {best_recall:.1f}%\")\n",
    "        \n",
    "        # Save phase checkpoint\n",
    "        torch.save({\n",
    "            \"epoch\": phase.end_epoch,\n",
    "            \"phase\": phase.name,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "        }, config.output_dir / f\"{phase.name}_checkpoint.pt\")\n",
    "        print(f\"\\n{phase.name} checkpoint saved.\")\n",
    "    \n",
    "    # Save training history\n",
    "    with open(config.output_dir / \"training_history.json\", \"w\") as f:\n",
    "        json.dump(training_history, f, indent=2)\n",
    "    \n",
    "    return training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "print(\"Starting v22.0 Curriculum Training with InfoNCE...\")\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"\\nData distribution:\")\n",
    "for phase in config.phases:\n",
    "    data_path = config.data_dir / phase.data_file\n",
    "    if data_path.exists():\n",
    "        with open(data_path) as f:\n",
    "            count = sum(1 for _ in f)\n",
    "        print(f\"  {phase.name}: {count:,} triplets\")\n",
    "\n",
    "history = run_curriculum_training_v22(model, config, device)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "epochs = [h[\"epoch\"] for h in history]\n",
    "\n",
    "# Total Loss\n",
    "axes[0, 0].plot(epochs, [h[\"total\"] for h in history], 'b-')\n",
    "axes[0, 0].set_xlabel(\"Epoch\")\n",
    "axes[0, 0].set_ylabel(\"Total Loss\")\n",
    "axes[0, 0].set_title(\"Total Loss\")\n",
    "axes[0, 0].axvline(x=10.5, color='r', linestyle='--', alpha=0.5, label='Phase change')\n",
    "axes[0, 0].axvline(x=20.5, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "# InfoNCE Loss (NEW)\n",
    "axes[0, 1].plot(epochs, [h[\"infonce\"] for h in history], 'purple')\n",
    "axes[0, 1].set_xlabel(\"Epoch\")\n",
    "axes[0, 1].set_ylabel(\"InfoNCE Loss\")\n",
    "axes[0, 1].set_title(\"InfoNCE Contrastive Loss\")\n",
    "axes[0, 1].axvline(x=10.5, color='r', linestyle='--', alpha=0.5)\n",
    "axes[0, 1].axvline(x=20.5, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Recall\n",
    "axes[0, 2].plot(epochs, [h[\"recall\"] for h in history], 'g-')\n",
    "axes[0, 2].set_xlabel(\"Epoch\")\n",
    "axes[0, 2].set_ylabel(\"Recall (%)\")\n",
    "axes[0, 2].set_title(\"Validation Recall\")\n",
    "axes[0, 2].axvline(x=10.5, color='r', linestyle='--', alpha=0.5)\n",
    "axes[0, 2].axvline(x=20.5, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Loss Components\n",
    "for component in [\"self\", \"positive\", \"triplet\"]:\n",
    "    axes[1, 0].plot(epochs, [h[component] for h in history], label=component)\n",
    "axes[1, 0].set_xlabel(\"Epoch\")\n",
    "axes[1, 0].set_ylabel(\"Loss\")\n",
    "axes[1, 0].set_title(\"Loss Components\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Temperature Schedule\n",
    "temps = [h[\"temperature\"] for h in history]\n",
    "axes[1, 1].plot(epochs, temps, 'orange', marker='o', markersize=2)\n",
    "axes[1, 1].set_xlabel(\"Epoch\")\n",
    "axes[1, 1].set_ylabel(\"Temperature\")\n",
    "axes[1, 1].set_title(\"Temperature Annealing\")\n",
    "\n",
    "# FLOPS and Min Activation\n",
    "ax1 = axes[1, 2]\n",
    "ax1.plot(epochs, [h[\"flops\"] for h in history], 'b-', label='FLOPS')\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"FLOPS Loss\", color='b')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs, [h[\"min_act\"] for h in history], 'r-', label='Min Act')\n",
    "ax2.set_ylabel(\"Min Activation Loss\", color='r')\n",
    "axes[1, 2].set_title(\"Regularization Losses\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.output_dir / \"training_curves_v22.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\nv22.0 Final Results:\")\n",
    "print(f\"  Best Recall: {max(h['recall'] for h in history):.1f}%\")\n",
    "print(f\"  Best MRR: {max(h['mrr'] for h in history):.4f}\")\n",
    "print(f\"  Final InfoNCE: {history[-1]['infonce']:.4f}\")\n",
    "print(f\"  Final Loss: {history[-1]['total']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 8. Save Final Model for Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model in a format suitable for benchmark\n",
    "final_checkpoint = {\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"config\": {\n",
    "        \"model_name\": config.model_name,\n",
    "        \"max_length\": config.max_length,\n",
    "        \"vocab_size\": model.config.vocab_size,\n",
    "        \"hidden_size\": model.config.hidden_size,\n",
    "        \"version\": \"v22.0\",\n",
    "    },\n",
    "    \"training_info\": {\n",
    "        \"total_epochs\": config.total_epochs,\n",
    "        \"final_recall\": history[-1][\"recall\"],\n",
    "        \"final_mrr\": history[-1][\"mrr\"],\n",
    "        \"best_recall\": max(h[\"recall\"] for h in history),\n",
    "    },\n",
    "}\n",
    "\n",
    "torch.save(final_checkpoint, config.output_dir / \"checkpoint.pt\")\n",
    "print(f\"Final model saved to {config.output_dir / 'checkpoint.pt'}\")\n",
    "\n",
    "# Also save tokenizer config for later use\n",
    "tokenizer.save_pretrained(config.output_dir / \"tokenizer\")\n",
    "print(f\"Tokenizer saved to {config.output_dir / 'tokenizer'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run benchmark with the new v22.0 model:\n",
    "   ```bash\n",
    "   python benchmark/run_benchmark.py --model-path outputs/v22.0_infonce/checkpoint.pt\n",
    "   ```\n",
    "\n",
    "2. Compare with v21.4 baseline and other methods\n",
    "\n",
    "3. Test on problem terms (추천, 데이터베이스, 증상, 질환, 인슐린)\n",
    "\n",
    "4. If needed, adjust hyperparameters and retrain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

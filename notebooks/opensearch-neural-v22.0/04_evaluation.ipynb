{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v22.0 Comprehensive Evaluation\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "1. **Single-term Recall**: Focus on previously problematic terms\n",
    "2. **Garbage Detection**: Identify invalid token outputs\n",
    "3. **Sentence-level Performance**: MRR, Recall@K\n",
    "4. **Version Comparison**: v21.2 vs v21.3 vs v21.4 vs v22.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from typing import Dict, List, Tuple, Set\n",
    "from collections import defaultdict\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "V22_MODEL_PATH = PROJECT_ROOT / \"outputs\" / \"v22.0_infonce\" / \"best_model.pt\"\n",
    "V21_4_MODEL_PATH = PROJECT_ROOT / \"outputs\" / \"v21.4_korean_enhanced\" / \"best_model.pt\"\n",
    "V21_3_MODEL_PATH = PROJECT_ROOT / \"outputs\" / \"v21.3_korean_enhanced\" / \"best_model.pt\"\n",
    "V21_2_MODEL_PATH = PROJECT_ROOT / \"outputs\" / \"v21.2_korean_legal_medical\" / \"best_model.pt\"\n",
    "\n",
    "MODEL_NAME = \"skt/A.X-Encoder-base\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Model paths:\")\n",
    "print(f\"  v22.0: {V22_MODEL_PATH.exists()}\")\n",
    "print(f\"  v21.4: {V21_4_MODEL_PATH.exists()}\")\n",
    "print(f\"  v21.3: {V21_3_MODEL_PATH.exists()}\")\n",
    "print(f\"  v21.2: {V21_2_MODEL_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPLADEModel(nn.Module):\n",
    "    \"\"\"SPLADE model for Korean sparse retrieval.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"skt/A.X-Encoder-base\"):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        self.config = self.model.config\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        token_scores = torch.log1p(self.relu(logits))\n",
    "        mask = attention_mask.unsqueeze(-1).float()\n",
    "        token_scores = token_scores * mask\n",
    "        sparse_repr, _ = token_scores.max(dim=1)\n",
    "        token_weights = token_scores.max(dim=-1).values\n",
    "        return sparse_repr, token_weights\n",
    "\n",
    "\n",
    "def load_model(path: Path, model_name: str, device) -> SPLADEModel:\n",
    "    \"\"\"Load a trained model.\"\"\"\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    \n",
    "    model = SPLADEModel(model_name)\n",
    "    checkpoint = torch.load(path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load models\n",
    "model_v22 = load_model(V22_MODEL_PATH, MODEL_NAME, device)\n",
    "model_v21_4 = load_model(V21_4_MODEL_PATH, MODEL_NAME, device)\n",
    "model_v21_3 = load_model(V21_3_MODEL_PATH, MODEL_NAME, device)\n",
    "model_v21_2 = load_model(V21_2_MODEL_PATH, MODEL_NAME, device)\n",
    "\n",
    "print(f\"\\nModels loaded:\")\n",
    "print(f\"  v22.0: {model_v22 is not None}\")\n",
    "print(f\"  v21.4: {model_v21_4 is not None}\")\n",
    "print(f\"  v21.3: {model_v21_3 is not None}\")\n",
    "print(f\"  v21.2: {model_v21_2 is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Token Validation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_korean_token(token: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a token is valid Korean/English.\n",
    "    \n",
    "    Valid tokens contain only:\n",
    "    - Korean characters (Hangul syllables, Jamo)\n",
    "    - ASCII letters and digits\n",
    "    - Basic CJK characters\n",
    "    \"\"\"\n",
    "    if not token:\n",
    "        return False\n",
    "    \n",
    "    # Remove ## prefix\n",
    "    clean = token.replace('##', '').strip()\n",
    "    if not clean:\n",
    "        return False\n",
    "    \n",
    "    # Skip special tokens\n",
    "    if '<' in token or '>' in token or '[' in token or ']' in token:\n",
    "        return False\n",
    "    \n",
    "    for char in clean:\n",
    "        code = ord(char)\n",
    "        \n",
    "        # Hangul syllables (가-힣)\n",
    "        if 0xAC00 <= code <= 0xD7A3:\n",
    "            continue\n",
    "        # Hangul Jamo\n",
    "        if 0x1100 <= code <= 0x11FF:\n",
    "            continue\n",
    "        # Hangul Compatibility Jamo\n",
    "        if 0x3130 <= code <= 0x318F:\n",
    "            continue\n",
    "        # ASCII letters and digits\n",
    "        if (0x0041 <= code <= 0x005A) or (0x0061 <= code <= 0x007A) or (0x0030 <= code <= 0x0039):\n",
    "            continue\n",
    "        # Basic CJK\n",
    "        if 0x4E00 <= code <= 0x9FFF:\n",
    "            continue\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "# Build special token set\n",
    "special_token_ids = set()\n",
    "for attr in ['pad_token_id', 'cls_token_id', 'sep_token_id', \n",
    "             'unk_token_id', 'mask_token_id', 'bos_token_id', 'eos_token_id']:\n",
    "    token_id = getattr(tokenizer, attr, None)\n",
    "    if token_id is not None:\n",
    "        special_token_ids.add(token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Expansion Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_synonym_expansion(\n",
    "    model: nn.Module,\n",
    "    tokenizer,\n",
    "    text: str,\n",
    "    device: torch.device,\n",
    "    top_k: int = 20,\n",
    "    min_weight: float = 0.5,\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Get top-k activated tokens for a given text.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    weights, _ = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "    weights = weights[0].cpu()\n",
    "    \n",
    "    # Mask special tokens\n",
    "    for tid in special_token_ids:\n",
    "        if tid < len(weights):\n",
    "            weights[tid] = -float('inf')\n",
    "    \n",
    "    # Get top tokens\n",
    "    top_weights, top_indices = weights.topk(min(top_k * 10, len(weights)))\n",
    "    \n",
    "    results = []\n",
    "    for idx, weight in zip(top_indices.tolist(), top_weights.tolist()):\n",
    "        if weight < min_weight:\n",
    "            continue\n",
    "        \n",
    "        token = tokenizer.decode([idx]).strip()\n",
    "        \n",
    "        if not is_valid_korean_token(token):\n",
    "            continue\n",
    "        \n",
    "        results.append((token, weight))\n",
    "        \n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Problem Terms Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem terms from v21.3\n",
    "PROBLEM_TERMS = {\n",
    "    \"추천\": [\"권장\", \"권유\", \"제안\", \"소개\"],\n",
    "    \"데이터베이스\": [\"DB\", \"디비\", \"저장소\", \"데이터\"],\n",
    "    \"증상\": [\"증세\", \"징후\", \"현상\", \"이상\"],\n",
    "    \"질환\": [\"질병\", \"병\", \"병증\", \"이환\"],\n",
    "    \"인슐린\": [\"insulin\", \"호르몬\", \"혈당\", \"당뇨\"],\n",
    "}\n",
    "\n",
    "\n",
    "def evaluate_problem_terms(model, name: str) -> Dict:\n",
    "    \"\"\"Evaluate model on problem terms.\"\"\"\n",
    "    if model is None:\n",
    "        return None\n",
    "    \n",
    "    results = {}\n",
    "    total_recall = 0\n",
    "    total_garbage = 0\n",
    "    \n",
    "    print(f\"\\n{name} - Problem Terms:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for term, expected in PROBLEM_TERMS.items():\n",
    "        expansions = get_synonym_expansion(model, tokenizer, term, device, top_k=15)\n",
    "        \n",
    "        # Check recall\n",
    "        top_tokens = [t for t, _ in expansions]\n",
    "        hits = 0\n",
    "        for exp in expected:\n",
    "            for tok in top_tokens:\n",
    "                if exp.lower() in tok.lower() or tok.lower() in exp.lower():\n",
    "                    hits += 1\n",
    "                    break\n",
    "        recall = hits / len(expected) * 100\n",
    "        total_recall += recall\n",
    "        \n",
    "        # Check garbage ratio\n",
    "        if len(expansions) == 0:\n",
    "            garbage_ratio = 100\n",
    "            total_garbage += 1\n",
    "        else:\n",
    "            garbage_ratio = 0\n",
    "        \n",
    "        print(f\"\\n{term}:\")\n",
    "        print(f\"  Expected: {expected}\")\n",
    "        print(f\"  Recall: {recall:.0f}%\")\n",
    "        if expansions:\n",
    "            print(f\"  Top expansions: {[(t, f'{w:.2f}') for t, w in expansions[:5]]}\")\n",
    "        else:\n",
    "            print(f\"  Top expansions: (GARBAGE OUTPUT)\")\n",
    "        \n",
    "        results[term] = {\n",
    "            \"recall\": recall,\n",
    "            \"top_tokens\": top_tokens[:5],\n",
    "            \"is_garbage\": len(expansions) == 0,\n",
    "        }\n",
    "    \n",
    "    avg_recall = total_recall / len(PROBLEM_TERMS)\n",
    "    garbage_count = total_garbage\n",
    "    \n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  Average Recall: {avg_recall:.1f}%\")\n",
    "    print(f\"  Garbage outputs: {garbage_count}/{len(PROBLEM_TERMS)}\")\n",
    "    \n",
    "    return {\n",
    "        \"terms\": results,\n",
    "        \"avg_recall\": avg_recall,\n",
    "        \"garbage_count\": garbage_count,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "results_v22 = evaluate_problem_terms(model_v22, \"v22.0\")\n",
    "results_v21_4 = evaluate_problem_terms(model_v21_4, \"v21.4\")\n",
    "results_v21_3 = evaluate_problem_terms(model_v21_3, \"v21.3\")\n",
    "results_v21_2 = evaluate_problem_terms(model_v21_2, \"v21.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. General Terms Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General evaluation test cases\n",
    "GENERAL_TEST_CASES = [\n",
    "    # General\n",
    "    (\"검색\", [\"탐색\", \"조회\", \"찾기\", \"서치\"]),\n",
    "    (\"인공지능\", [\"AI\", \"에이아이\", \"기계지능\"]),\n",
    "    (\"컴퓨터\", [\"PC\", \"전산\", \"컴퓨팅\"]),\n",
    "    # Legal\n",
    "    (\"손해배상\", [\"배상\", \"보상\", \"손해\"]),\n",
    "    (\"판결\", [\"판례\", \"선고\", \"결정\"]),\n",
    "    (\"소송\", [\"재판\", \"법정\", \"송사\"]),\n",
    "    # Medical\n",
    "    (\"진단\", [\"진찰\", \"검진\", \"판단\"]),\n",
    "    (\"치료\", [\"처치\", \"요법\", \"치유\"]),\n",
    "    (\"당뇨병\", [\"당뇨\", \"혈당\", \"diabetes\"]),\n",
    "]\n",
    "\n",
    "\n",
    "def evaluate_general_terms(model, name: str) -> Dict:\n",
    "    \"\"\"Evaluate model on general test cases.\"\"\"\n",
    "    if model is None:\n",
    "        return None\n",
    "    \n",
    "    total_recall = 0\n",
    "    results = []\n",
    "    \n",
    "    for source, expected in GENERAL_TEST_CASES:\n",
    "        expansions = get_synonym_expansion(model, tokenizer, source, device, top_k=20)\n",
    "        top_tokens = [t for t, _ in expansions]\n",
    "        \n",
    "        hits = 0\n",
    "        for exp in expected:\n",
    "            for tok in top_tokens:\n",
    "                if exp.lower() in tok.lower() or tok.lower() in exp.lower():\n",
    "                    hits += 1\n",
    "                    break\n",
    "        \n",
    "        recall = hits / len(expected) * 100\n",
    "        total_recall += recall\n",
    "        results.append({\"source\": source, \"recall\": recall})\n",
    "    \n",
    "    avg_recall = total_recall / len(GENERAL_TEST_CASES)\n",
    "    return {\"results\": results, \"avg_recall\": avg_recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "general_v22 = evaluate_general_terms(model_v22, \"v22.0\")\n",
    "general_v21_4 = evaluate_general_terms(model_v21_4, \"v21.4\")\n",
    "general_v21_3 = evaluate_general_terms(model_v21_3, \"v21.3\")\n",
    "general_v21_2 = evaluate_general_terms(model_v21_2, \"v21.2\")\n",
    "\n",
    "print(\"\\nGeneral Terms Recall Comparison:\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Source':<15} {'Expected':<25} {'v22.0':>10} {'v21.4':>10} {'v21.3':>10} {'v21.2':>10}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for i, (source, expected) in enumerate(GENERAL_TEST_CASES):\n",
    "    r22 = general_v22[\"results\"][i][\"recall\"] if general_v22 else 0\n",
    "    r4 = general_v21_4[\"results\"][i][\"recall\"] if general_v21_4 else 0\n",
    "    r3 = general_v21_3[\"results\"][i][\"recall\"] if general_v21_3 else 0\n",
    "    r2 = general_v21_2[\"results\"][i][\"recall\"] if general_v21_2 else 0\n",
    "    \n",
    "    print(f\"{source:<15} {str(expected[:2]):<25} {r22:>9.0f}% {r4:>9.0f}% {r3:>9.0f}% {r2:>9.0f}%\")\n",
    "\n",
    "print(\"-\" * 90)\n",
    "avg_22 = general_v22[\"avg_recall\"] if general_v22 else 0\n",
    "avg_4 = general_v21_4[\"avg_recall\"] if general_v21_4 else 0\n",
    "avg_3 = general_v21_3[\"avg_recall\"] if general_v21_3 else 0\n",
    "avg_2 = general_v21_2[\"avg_recall\"] if general_v21_2 else 0\n",
    "print(f\"{'AVERAGE':<40} {avg_22:>9.1f}% {avg_4:>9.1f}% {avg_3:>9.1f}% {avg_2:>9.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Natural Language Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural language queries\n",
    "NL_QUERIES = [\n",
    "    \"손해배상 청구 소송을 제기하려면 어떻게 해야 하나요?\",\n",
    "    \"고혈압 환자의 식이요법에 대해 알려주세요.\",\n",
    "    \"인공지능 기술의 최신 발전 동향\",\n",
    "    \"서울에서 부산까지 KTX 소요 시간\",\n",
    "]\n",
    "\n",
    "\n",
    "def evaluate_nl_queries(model, name: str):\n",
    "    \"\"\"Evaluate model on natural language queries.\"\"\"\n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{name} - Natural Language Queries:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for query in NL_QUERIES:\n",
    "        expansions = get_synonym_expansion(model, tokenizer, query, device, top_k=10)\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        if expansions:\n",
    "            print(f\"Top expansions: {[(t, f'{w:.2f}') for t, w in expansions[:6]]}\")\n",
    "        else:\n",
    "            print(\"Top expansions: (GARBAGE OUTPUT)\")\n",
    "\n",
    "\n",
    "evaluate_nl_queries(model_v22, \"v22.0\")\n",
    "evaluate_nl_queries(model_v21_4, \"v21.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Version Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"v22.0 Evaluation Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n| Metric | v21.2 | v21.3 | v21.4 | v22.0 | Target |\")\n",
    "print(\"|--------|-------|-------|-------|-------|--------|\")\n",
    "\n",
    "# Problem terms recall\n",
    "prob_22 = results_v22[\"avg_recall\"] if results_v22 else 0\n",
    "prob_4 = results_v21_4[\"avg_recall\"] if results_v21_4 else 0\n",
    "prob_3 = results_v21_3[\"avg_recall\"] if results_v21_3 else 0\n",
    "prob_2 = results_v21_2[\"avg_recall\"] if results_v21_2 else 0\n",
    "print(f\"| Problem Terms Recall | {prob_2:.1f}% | {prob_3:.1f}% | {prob_4:.1f}% | {prob_22:.1f}% | 80%+ |\")\n",
    "\n",
    "# General terms recall\n",
    "gen_22 = general_v22[\"avg_recall\"] if general_v22 else 0\n",
    "gen_4 = general_v21_4[\"avg_recall\"] if general_v21_4 else 0\n",
    "gen_3 = general_v21_3[\"avg_recall\"] if general_v21_3 else 0\n",
    "gen_2 = general_v21_2[\"avg_recall\"] if general_v21_2 else 0\n",
    "print(f\"| General Terms Recall | {gen_2:.1f}% | {gen_3:.1f}% | {gen_4:.1f}% | {gen_22:.1f}% | 80%+ |\")\n",
    "\n",
    "# Garbage outputs\n",
    "garb_22 = results_v22[\"garbage_count\"] if results_v22 else 0\n",
    "garb_4 = results_v21_4[\"garbage_count\"] if results_v21_4 else 0\n",
    "garb_3 = results_v21_3[\"garbage_count\"] if results_v21_3 else 0\n",
    "garb_2 = results_v21_2[\"garbage_count\"] if results_v21_2 else 0\n",
    "print(f\"| Garbage Outputs | {garb_2}/5 | {garb_3}/5 | {garb_4}/5 | {garb_22}/5 | 0/5 |\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improvement analysis\n",
    "if results_v22 and results_v21_4:\n",
    "    improvement = prob_22 - prob_4\n",
    "    if improvement > 0:\n",
    "        print(f\"\\nImprovement over v21.4: +{improvement:.1f}%\")\n",
    "    else:\n",
    "        print(f\"\\nRegression from v21.4: {improvement:.1f}%\")\n",
    "\n",
    "if results_v22 and results_v21_3:\n",
    "    improvement = prob_22 - prob_3\n",
    "    if improvement > 0:\n",
    "        print(f\"Improvement over v21.3: +{improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Per-Term Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPer-Term Analysis (Problem Terms):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for term, expected in PROBLEM_TERMS.items():\n",
    "    print(f\"\\n{term}:\")\n",
    "    print(f\"  Expected synonyms: {expected}\")\n",
    "    \n",
    "    for name, results in [(\"v22.0\", results_v22), (\"v21.4\", results_v21_4), (\"v21.3\", results_v21_3)]:\n",
    "        if results and term in results[\"terms\"]:\n",
    "            term_result = results[\"terms\"][term]\n",
    "            print(f\"  {name}: recall={term_result['recall']:.0f}%, top={term_result['top_tokens'][:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "output_dir = PROJECT_ROOT / \"outputs\" / \"v22.0_infonce\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "evaluation_results = {\n",
    "    \"v22.0\": {\n",
    "        \"problem_terms_recall\": prob_22,\n",
    "        \"general_terms_recall\": gen_22,\n",
    "        \"garbage_count\": garb_22,\n",
    "    },\n",
    "    \"v21.4\": {\n",
    "        \"problem_terms_recall\": prob_4,\n",
    "        \"general_terms_recall\": gen_4,\n",
    "        \"garbage_count\": garb_4,\n",
    "    },\n",
    "    \"v21.3\": {\n",
    "        \"problem_terms_recall\": prob_3,\n",
    "        \"general_terms_recall\": gen_3,\n",
    "        \"garbage_count\": garb_3,\n",
    "    },\n",
    "    \"v21.2\": {\n",
    "        \"problem_terms_recall\": prob_2,\n",
    "        \"general_terms_recall\": gen_2,\n",
    "        \"garbage_count\": garb_2,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(output_dir / \"evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(evaluation_results, f, indent=2)\n",
    "\n",
    "print(f\"Evaluation results saved to {output_dir / 'evaluation_results.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### v22.0 Key Improvements\n",
    "\n",
    "1. **InfoNCE Contrastive Loss**: Better discriminative representations\n",
    "2. **Temperature Annealing**: Sharper discrimination in later phases\n",
    "3. **Single-term Expansion**: 448 → 29,322 triplets (65x increase)\n",
    "4. **Curriculum Learning**: Progressive difficulty with InfoNCE weight increase\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Run full OpenSearch benchmark to measure Recall@1, MRR, and latency\n",
    "2. Compare with semantic and hybrid search methods\n",
    "3. Deploy best model to production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

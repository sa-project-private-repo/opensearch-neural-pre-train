{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v22.1 Multi-Source Korean Data Collection\n",
    "\n",
    "Comprehensive data collection from multiple sources for Neural Sparse model training.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Multi-Source Collection**: HuggingFace, AI Hub, Modumalgunji, Public Data Portal\n",
    "- **Sequential Loading**: Memory-safe sequential loading with garbage collection\n",
    "- **Unified Schema**: Consistent data format across all sources\n",
    "- **S3 Integration**: Optional upload to S3 for distributed processing\n",
    "- **Progress Tracking**: Individual progress bars for each dataset\n",
    "- **Error Handling**: Graceful failure handling - if one dataset fails, others continue\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "### HuggingFace Datasets (Automatic)\n",
    "\n",
    "| Dataset | Type | Size | Use Case |\n",
    "|---------|------|------|----------|\n",
    "| williamjeong2/msmarco-triplets-ko-v1 | Query-Doc Triplets | 50K | Direct triplet training |\n",
    "| klue (nli, sts) | NLI, STS | 45K | Semantic similarity pairs |\n",
    "| squad_kor_v1 | QA | 30K | Question-context pairs |\n",
    "| skt/kobest_v1 (copa) | COPA | 5K | Premise-alternative pairs |\n",
    "| nsmc | Sentiment | 50K | Text corpus for negatives |\n",
    "| daekeun-ml/naver-news-summarization-ko | News | 10K | Title-summary pairs |\n",
    "| Bingsu/ko_alpaca_data | Instruction | 52K | Instruction-response pairs |\n",
    "| nlpai-lab/kullm-v2 | Instruction | 150K | Instruction-response pairs |\n",
    "| heegyu/korquad-chat-v1 | QA Chat | 50K | Conversational QA |\n",
    "| maywell/korean_textbooks | Educational | 100K | Structured educational text |\n",
    "| beomi/KoAlpaca-v1.1a | Instruction | 52K | Instruction-response pairs |\n",
    "| nlpai-lab/ko-sarcasm | Sentiment | 9K | Sarcasm detection corpus |\n",
    "\n",
    "### Manual Download Sources (Placeholders)\n",
    "\n",
    "| Source | Description | Manual Download Required |\n",
    "|--------|-------------|-------------------------|\n",
    "| AI Hub | Government AI datasets | https://aihub.or.kr |\n",
    "| Modumalgunji | National Institute of Korean Language corpus | https://corpus.korean.go.kr |\n",
    "| Public Data Portal | Korean government open data | https://data.go.kr |\n",
    "\n",
    "## Unified Schema\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"text1\": str,       # Primary text (query, premise, instruction)\n",
    "    \"text2\": str,       # Secondary text (document, hypothesis, response)\n",
    "    \"label\": float,     # Similarity/relevance score (0.0-1.0)\n",
    "    \"source\": str,      # Dataset source identifier\n",
    "    \"pair_type\": str,   # Type of pair (qa, nli, sts, instruction, etc.)\n",
    "    \"metadata\": dict    # Additional metadata\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_project_root() -> Path:\n",
    "    \"\"\"Find the project root directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Callable, Dict, List, Optional\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    print(\"python-dotenv library available\")\n",
    "except ImportError:\n",
    "    print(\"Installing python-dotenv...\")\n",
    "    %pip install python-dotenv\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file\n",
    "env_path = PROJECT_ROOT / \".env\"\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"Loaded environment from: {env_path}\")\n",
    "else:\n",
    "    print(f\"Warning: .env file not found at {env_path}\")\n",
    "    print(\"Copy .env_sample to .env and configure your settings\")\n",
    "\n",
    "# Environment configuration\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "S3_BUCKET_NAME = os.getenv(\"S3_BUCKET_NAME\", \"\")\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"\")\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  AWS_REGION: {AWS_REGION}\")\n",
    "print(f\"  S3_BUCKET_NAME: {S3_BUCKET_NAME or '(not configured)'}\")\n",
    "print(f\"  HF_TOKEN: {'(configured)' if HF_TOKEN else '(not configured)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "try:\n",
    "    from datasets import load_dataset, Dataset\n",
    "    print(\"datasets library available\")\n",
    "except ImportError:\n",
    "    print(\"Installing datasets...\")\n",
    "    %pip install datasets\n",
    "    from datasets import load_dataset, Dataset\n",
    "\n",
    "try:\n",
    "    import boto3\n",
    "    print(\"boto3 library available\")\n",
    "except ImportError:\n",
    "    print(\"Installing boto3...\")\n",
    "    %pip install boto3\n",
    "    import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directories - v22.1 specific\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"v22.1\"\n",
    "RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
    "HF_DATA_DIR = RAW_DATA_DIR / \"huggingface\"\n",
    "MANUAL_DATA_DIR = RAW_DATA_DIR / \"manual\"\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [DATA_DIR, RAW_DATA_DIR, HF_DATA_DIR, MANUAL_DATA_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# S3 path configuration\n",
    "S3_RAW_PATH = f\"s3://{S3_BUCKET_NAME}/spark-meta/neural/raw/\" if S3_BUCKET_NAME else \"\"\n",
    "\n",
    "print(f\"Local data directory: {DATA_DIR}\")\n",
    "print(f\"HuggingFace data directory: {HF_DATA_DIR}\")\n",
    "print(f\"Manual data directory: {MANUAL_DATA_DIR}\")\n",
    "print(f\"S3 raw path: {S3_RAW_PATH or '(not configured)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Classes and Type Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class UnifiedRecord:\n",
    "    \"\"\"Unified record schema for all data sources.\"\"\"\n",
    "    \n",
    "    text1: str\n",
    "    text2: str\n",
    "    label: float\n",
    "    source: str\n",
    "    pair_type: str\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n",
    "        return {\n",
    "            \"text1\": self.text1,\n",
    "            \"text2\": self.text2,\n",
    "            \"label\": self.label,\n",
    "            \"source\": self.source,\n",
    "            \"pair_type\": self.pair_type,\n",
    "            \"metadata\": self.metadata,\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetResult:\n",
    "    \"\"\"Result container for a dataset loading operation.\"\"\"\n",
    "    \n",
    "    name: str\n",
    "    success: bool\n",
    "    records: List[UnifiedRecord] = field(default_factory=list)\n",
    "    corpus: List[str] = field(default_factory=list)\n",
    "    error_message: Optional[str] = None\n",
    "    sample_count: int = 0\n",
    "    \n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Calculate sample count after initialization.\"\"\"\n",
    "        if self.records:\n",
    "            self.sample_count = len(self.records)\n",
    "        elif self.corpus:\n",
    "            self.sample_count = len(self.corpus)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    \"\"\"Configuration for a dataset loading task.\"\"\"\n",
    "    \n",
    "    name: str\n",
    "    loader_fn: Callable[..., DatasetResult]\n",
    "    max_samples: int\n",
    "    description: str = \"\"\n",
    "    category: str = \"huggingface\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(records: List[UnifiedRecord], output_path: Path) -> int:\n",
    "    \"\"\"Save records to JSONL format.\n",
    "    \n",
    "    Args:\n",
    "        records: List of UnifiedRecord objects\n",
    "        output_path: Path to output file\n",
    "        \n",
    "    Returns:\n",
    "        Number of records saved\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for record in records:\n",
    "            f.write(json.dumps(record.to_dict(), ensure_ascii=False) + \"\\n\")\n",
    "    return len(records)\n",
    "\n",
    "\n",
    "def save_text_corpus(texts: List[str], output_path: Path) -> int:\n",
    "    \"\"\"Save text corpus to file.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        output_path: Path to output file\n",
    "        \n",
    "    Returns:\n",
    "        Number of texts saved\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for text in texts:\n",
    "            f.write(text + \"\\n\")\n",
    "    return len(texts)\n",
    "\n",
    "\n",
    "def upload_to_s3(local_path: Path, s3_bucket: str, s3_key: str) -> bool:\n",
    "    \"\"\"Upload file to S3.\n",
    "    \n",
    "    Args:\n",
    "        local_path: Local file path\n",
    "        s3_bucket: S3 bucket name\n",
    "        s3_key: S3 object key\n",
    "        \n",
    "    Returns:\n",
    "        True if upload successful, False otherwise\n",
    "    \"\"\"\n",
    "    if not s3_bucket:\n",
    "        print(f\"  S3 bucket not configured, skipping upload\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        s3_client = boto3.client(\"s3\", region_name=AWS_REGION)\n",
    "        s3_client.upload_file(str(local_path), s3_bucket, s3_key)\n",
    "        print(f\"  Uploaded to s3://{s3_bucket}/{s3_key}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  S3 upload failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def truncate_text(text: str, max_length: int = 512) -> str:\n",
    "    \"\"\"Truncate text to maximum length.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        max_length: Maximum character length\n",
    "        \n",
    "    Returns:\n",
    "        Truncated text\n",
    "    \"\"\"\n",
    "    if len(text) <= max_length:\n",
    "        return text\n",
    "    return text[:max_length].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HuggingFace Dataset Loaders - Existing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_msmarco_korean(max_samples: int = 50000) -> DatasetResult:\n",
    "    \"\"\"Load Korean MS MARCO triplets.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with loaded records\n",
    "    \"\"\"\n",
    "    name = \"msmarco_ko\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\n",
    "            \"williamjeong2/msmarco-triplets-ko-v1\",\n",
    "            split=\"train\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return DatasetResult(name=name, success=False, error_message=f\"Failed to load: {e}\")\n",
    "    \n",
    "    records: List[UnifiedRecord] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        query = item.get(\"query\", \"\")\n",
    "        positives = item.get(\"pos\", [])\n",
    "        negatives = item.get(\"neg\", [])\n",
    "        \n",
    "        if not query or not positives:\n",
    "            continue\n",
    "        \n",
    "        pos = positives[0] if positives else \"\"\n",
    "        neg = negatives[0] if negatives else \"\"\n",
    "        \n",
    "        if pos:\n",
    "            records.append(UnifiedRecord(\n",
    "                text1=query,\n",
    "                text2=pos,\n",
    "                label=1.0,\n",
    "                source=name,\n",
    "                pair_type=\"retrieval\",\n",
    "                metadata={\"negative\": neg} if neg else {},\n",
    "            ))\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, records=records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_klue_nli(max_samples: int = 30000) -> DatasetResult:\n",
    "    \"\"\"Load KLUE NLI dataset.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with loaded records\n",
    "    \"\"\"\n",
    "    name = \"klue_nli\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\"klue\", \"nli\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(name=name, success=False, error_message=f\"Failed to load: {e}\")\n",
    "    \n",
    "    records: List[UnifiedRecord] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        premise = item.get(\"premise\", \"\")\n",
    "        hypothesis = item.get(\"hypothesis\", \"\")\n",
    "        label = item.get(\"label\", -1)\n",
    "        \n",
    "        if not premise or not hypothesis:\n",
    "            continue\n",
    "        \n",
    "        # Map labels: 0=entailment (high similarity), 1=neutral, 2=contradiction (low)\n",
    "        label_map = {0: 0.9, 1: 0.5, 2: 0.1}\n",
    "        similarity = label_map.get(label, 0.5)\n",
    "        pair_type = {0: \"nli_entailment\", 1: \"nli_neutral\", 2: \"nli_contradiction\"}.get(label, \"nli\")\n",
    "        \n",
    "        records.append(UnifiedRecord(\n",
    "            text1=premise,\n",
    "            text2=hypothesis,\n",
    "            label=similarity,\n",
    "            source=name,\n",
    "            pair_type=pair_type,\n",
    "            metadata={\"original_label\": label},\n",
    "        ))\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, records=records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_klue_sts(max_samples: int = 15000) -> DatasetResult:\n",
    "    \"\"\"Load KLUE STS dataset.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with loaded records\n",
    "    \"\"\"\n",
    "    name = \"klue_sts\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\"klue\", \"sts\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(name=name, success=False, error_message=f\"Failed to load: {e}\")\n",
    "    \n",
    "    records: List[UnifiedRecord] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        sentence1 = item.get(\"sentence1\", \"\")\n",
    "        sentence2 = item.get(\"sentence2\", \"\")\n",
    "        \n",
    "        labels = item.get(\"labels\", {})\n",
    "        score = labels.get(\"real-label\", 0) if isinstance(labels, dict) else 0\n",
    "        \n",
    "        if not sentence1 or not sentence2:\n",
    "            continue\n",
    "        \n",
    "        # Normalize score from 0-5 to 0-1\n",
    "        normalized_score = score / 5.0 if score > 0 else 0\n",
    "        \n",
    "        records.append(UnifiedRecord(\n",
    "            text1=sentence1,\n",
    "            text2=sentence2,\n",
    "            label=normalized_score,\n",
    "            source=name,\n",
    "            pair_type=\"sts\",\n",
    "            metadata={\"original_score\": score},\n",
    "        ))\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, records=records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_korquad(max_samples: int = 30000) -> DatasetResult:\n",
    "    \"\"\"Load KorQuAD dataset.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with loaded records\n",
    "    \"\"\"\n",
    "    name = \"korquad\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\"squad_kor_v1\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(name=name, success=False, error_message=f\"Failed to load: {e}\")\n",
    "    \n",
    "    records: List[UnifiedRecord] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        question = item.get(\"question\", \"\")\n",
    "        context = item.get(\"context\", \"\")\n",
    "        answers = item.get(\"answers\", {})\n",
    "        \n",
    "        if not question:\n",
    "            continue\n",
    "        \n",
    "        answer_texts = answers.get(\"text\", []) if isinstance(answers, dict) else []\n",
    "        answer = answer_texts[0] if answer_texts else \"\"\n",
    "        \n",
    "        # Question-Answer pair\n",
    "        if answer:\n",
    "            records.append(UnifiedRecord(\n",
    "                text1=question,\n",
    "                text2=answer,\n",
    "                label=0.9,\n",
    "                source=name,\n",
    "                pair_type=\"qa\",\n",
    "                metadata={},\n",
    "            ))\n",
    "        \n",
    "        # Question-Context pair (truncated)\n",
    "        if context and len(context) > 20:\n",
    "            truncated_context = truncate_text(context, 300)\n",
    "            records.append(UnifiedRecord(\n",
    "                text1=question,\n",
    "                text2=truncated_context,\n",
    "                label=0.75,\n",
    "                source=name,\n",
    "                pair_type=\"qa_context\",\n",
    "                metadata={},\n",
    "            ))\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, records=records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kobest_copa(max_samples: int = 5000) -> DatasetResult:\n",
    "    \"\"\"Load KoBEST COPA dataset.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with loaded records\n",
    "    \"\"\"\n",
    "    name = \"kobest_copa\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\"skt/kobest_v1\", \"copa\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(name=name, success=False, error_message=f\"Failed to load: {e}\")\n",
    "    \n",
    "    records: List[UnifiedRecord] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        premise = item.get(\"premise\", \"\")\n",
    "        alternative1 = item.get(\"alternative_1\", \"\")\n",
    "        alternative2 = item.get(\"alternative_2\", \"\")\n",
    "        label = item.get(\"label\", 0)\n",
    "        \n",
    "        if not premise:\n",
    "            continue\n",
    "        \n",
    "        correct = alternative1 if label == 0 else alternative2\n",
    "        incorrect = alternative2 if label == 0 else alternative1\n",
    "        \n",
    "        if correct:\n",
    "            records.append(UnifiedRecord(\n",
    "                text1=premise,\n",
    "                text2=correct,\n",
    "                label=0.85,\n",
    "                source=name,\n",
    "                pair_type=\"copa\",\n",
    "                metadata={\"incorrect_alternative\": incorrect},\n",
    "            ))\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, records=records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_naver_news(max_samples: int = 10000) -> DatasetResult:\n",
    "    \"\"\"Load Naver News summarization dataset.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with loaded records\n",
    "    \"\"\"\n",
    "    name = \"naver_news\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\n",
    "            \"daekeun-ml/naver-news-summarization-ko\",\n",
    "            split=\"train\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return DatasetResult(name=name, success=False, error_message=f\"Failed to load: {e}\")\n",
    "    \n",
    "    records: List[UnifiedRecord] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        title = item.get(\"title\", \"\") or item.get(\"document_title\", \"\")\n",
    "        content = (\n",
    "            item.get(\"document\", \"\") or \n",
    "            item.get(\"content\", \"\") or \n",
    "            item.get(\"text\", \"\")\n",
    "        )\n",
    "        summary = item.get(\"summary\", \"\") or item.get(\"abstractive\", \"\")\n",
    "        \n",
    "        # Title-Summary pair\n",
    "        if title and summary:\n",
    "            records.append(UnifiedRecord(\n",
    "                text1=title,\n",
    "                text2=truncate_text(summary, 300),\n",
    "                label=0.8,\n",
    "                source=name,\n",
    "                pair_type=\"news_title_summary\",\n",
    "                metadata={},\n",
    "            ))\n",
    "        \n",
    "        # Content-Summary pair\n",
    "        if content and summary:\n",
    "            records.append(UnifiedRecord(\n",
    "                text1=truncate_text(content, 300),\n",
    "                text2=truncate_text(summary, 300),\n",
    "                label=0.85,\n",
    "                source=name,\n",
    "                pair_type=\"news_content_summary\",\n",
    "                metadata={},\n",
    "            ))\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, records=records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nsmc_corpus(max_samples: int = 50000) -> DatasetResult:\n",
    "    \"\"\"Load NSMC corpus for negative mining.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with loaded corpus texts\n",
    "    \"\"\"\n",
    "    name = \"nsmc\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\"nsmc\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(name=name, success=False, error_message=f\"Failed to load: {e}\")\n",
    "    \n",
    "    texts: List[str] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        document = item.get(\"document\", \"\")\n",
    "        if document and len(document) > 5:\n",
    "            texts.append(document)\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, corpus=texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. HuggingFace Dataset Loaders - New Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ko_alpaca(max_samples: int = 52000) -> DatasetResult:\n",
    "    \"\"\"Load Korean Alpaca data (Bingsu/ko_alpaca_data).\n",
    "    \n",
    "    Instruction-response pairs for instruction tuning.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with loaded records\n",
    "    \"\"\"\n",
    "    name = \"ko_alpaca\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\"Bingsu/ko_alpaca_data\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(name=name, success=False, error_message=f\"Failed to load: {e}\")\n",
    "    \n",
    "    records: List[UnifiedRecord] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        instruction = item.get(\"instruction\", \"\")\n",
    "        input_text = item.get(\"input\", \"\")\n",
    "        output = item.get(\"output\", \"\")\n",
    "        \n",
    "        if not instruction or not output:\n",
    "            continue\n",
    "        \n",
    "        # Combine instruction and input if input exists\n",
    "        text1 = f\"{instruction}\\n{input_text}\" if input_text else instruction\n",
    "        \n",
    "        records.append(UnifiedRecord(\n",
    "            text1=truncate_text(text1, 512),\n",
    "            text2=truncate_text(output, 512),\n",
    "            label=0.9,\n",
    "            source=name,\n",
    "            pair_type=\"instruction\",\n",
    "            metadata={\"has_input\": bool(input_text)},\n",
    "        ))\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, records=records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kullm_v2(max_samples: int = 150000) -> DatasetResult:\n",
    "    \"\"\"Load KULLM v2 dataset (nlpai-lab/kullm-v2).\n",
    "    \n",
    "    Large-scale Korean instruction tuning dataset.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with loaded records\n",
    "    \"\"\"\n",
    "    name = \"kullm_v2\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\"nlpai-lab/kullm-v2\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(name=name, success=False, error_message=f\"Failed to load: {e}\")\n",
    "    \n",
    "    records: List[UnifiedRecord] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        instruction = item.get(\"instruction\", \"\")\n",
    "        input_text = item.get(\"input\", \"\")\n",
    "        output = item.get(\"output\", \"\")\n",
    "        \n",
    "        if not instruction or not output:\n",
    "            continue\n",
    "        \n",
    "        text1 = f\"{instruction}\\n{input_text}\" if input_text else instruction\n",
    "        \n",
    "        records.append(UnifiedRecord(\n",
    "            text1=truncate_text(text1, 512),\n",
    "            text2=truncate_text(output, 512),\n",
    "            label=0.9,\n",
    "            source=name,\n",
    "            pair_type=\"instruction\",\n",
    "            metadata={\"has_input\": bool(input_text)},\n",
    "        ))\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, records=records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_korquad_chat(max_samples: int = 50000) -> DatasetResult:\n",
    "    \"\"\"Load KorQuAD Chat v1 dataset (heegyu/korquad-chat-v1).\n",
    "    \n",
    "    Conversational QA based on KorQuAD.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with loaded records\n",
    "    \"\"\"\n",
    "    name = \"korquad_chat\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\"heegyu/korquad-chat-v1\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(name=name, success=False, error_message=f\"Failed to load: {e}\")\n",
    "    \n",
    "    records: List[UnifiedRecord] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        # Try different possible field names\n",
    "        question = (\n",
    "            item.get(\"question\", \"\") or\n",
    "            item.get(\"instruction\", \"\") or\n",
    "            item.get(\"input\", \"\")\n",
    "        )\n",
    "        answer = (\n",
    "            item.get(\"answer\", \"\") or\n",
    "            item.get(\"output\", \"\") or\n",
    "            item.get(\"response\", \"\")\n",
    "        )\n",
    "        context = item.get(\"context\", \"\")\n",
    "        \n",
    "        if not question or not answer:\n",
    "            continue\n",
    "        \n",
    "        records.append(UnifiedRecord(\n",
    "            text1=truncate_text(question, 512),\n",
    "            text2=truncate_text(answer, 512),\n",
    "            label=0.9,\n",
    "            source=name,\n",
    "            pair_type=\"qa_chat\",\n",
    "            metadata={\"has_context\": bool(context)},\n",
    "        ))\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, records=records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_korean_textbooks(max_samples: int = 100000) -> DatasetResult:\n",
    "    \"\"\"Load Korean Textbooks dataset (maywell/korean_textbooks).\n",
    "    \n",
    "    Structured educational text for training.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with loaded records\n",
    "    \"\"\"\n",
    "    name = \"korean_textbooks\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\"maywell/korean_textbooks\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(name=name, success=False, error_message=f\"Failed to load: {e}\")\n",
    "    \n",
    "    records: List[UnifiedRecord] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        # Try different possible field structures\n",
    "        text = item.get(\"text\", \"\") or item.get(\"content\", \"\")\n",
    "        title = item.get(\"title\", \"\") or item.get(\"subject\", \"\")\n",
    "        \n",
    "        if not text:\n",
    "            continue\n",
    "        \n",
    "        # Split long text into chunks and create pairs\n",
    "        if title and text:\n",
    "            records.append(UnifiedRecord(\n",
    "                text1=title,\n",
    "                text2=truncate_text(text, 512),\n",
    "                label=0.8,\n",
    "                source=name,\n",
    "                pair_type=\"textbook_title_content\",\n",
    "                metadata={},\n",
    "            ))\n",
    "        elif len(text) > 100:\n",
    "            # Create sentence pair from text if no title\n",
    "            sentences = text.split(\". \")\n",
    "            if len(sentences) >= 2:\n",
    "                first_part = \". \".join(sentences[:len(sentences)//2])\n",
    "                second_part = \". \".join(sentences[len(sentences)//2:])\n",
    "                records.append(UnifiedRecord(\n",
    "                    text1=truncate_text(first_part, 300),\n",
    "                    text2=truncate_text(second_part, 300),\n",
    "                    label=0.7,\n",
    "                    source=name,\n",
    "                    pair_type=\"textbook_continuation\",\n",
    "                    metadata={},\n",
    "                ))\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, records=records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_koalpaca_v1_1a(max_samples: int = 52000) -> DatasetResult:\n",
    "    \"\"\"Load KoAlpaca v1.1a dataset (beomi/KoAlpaca-v1.1a).\n",
    "    \n",
    "    Korean Alpaca instruction tuning dataset.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with loaded records\n",
    "    \"\"\"\n",
    "    name = \"koalpaca_v1_1a\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\"beomi/KoAlpaca-v1.1a\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(name=name, success=False, error_message=f\"Failed to load: {e}\")\n",
    "    \n",
    "    records: List[UnifiedRecord] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        instruction = item.get(\"instruction\", \"\")\n",
    "        output = item.get(\"output\", \"\")\n",
    "        \n",
    "        if not instruction or not output:\n",
    "            continue\n",
    "        \n",
    "        records.append(UnifiedRecord(\n",
    "            text1=truncate_text(instruction, 512),\n",
    "            text2=truncate_text(output, 512),\n",
    "            label=0.9,\n",
    "            source=name,\n",
    "            pair_type=\"instruction\",\n",
    "            metadata={},\n",
    "        ))\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, records=records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ko_sarcasm(max_samples: int = 9000) -> DatasetResult:\n",
    "    \"\"\"Load Korean Sarcasm dataset (nlpai-lab/ko-sarcasm).\n",
    "    \n",
    "    Sarcasm detection corpus for sentiment understanding.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with loaded corpus texts\n",
    "    \"\"\"\n",
    "    name = \"ko_sarcasm\"\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\"nlpai-lab/ko-sarcasm\", split=\"train\")\n",
    "    except Exception as e:\n",
    "        return DatasetResult(name=name, success=False, error_message=f\"Failed to load: {e}\")\n",
    "    \n",
    "    texts: List[str] = []\n",
    "    total = min(len(dataset), max_samples)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(dataset, total=total, desc=f\"Processing {name}\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        text = item.get(\"text\", \"\") or item.get(\"sentence\", \"\")\n",
    "        if text and len(text) > 5:\n",
    "            texts.append(text)\n",
    "    \n",
    "    return DatasetResult(name=name, success=True, corpus=texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Manual Download Placeholders\n",
    "\n",
    "The following sections provide placeholders for manually downloaded datasets.\n",
    "These require registration and manual download from their respective portals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 AI Hub Datasets\n",
    "\n",
    "**Portal:** https://aihub.or.kr\n",
    "\n",
    "**Recommended Datasets:**\n",
    "- Korean-English Parallel Corpus\n",
    "- Korean Conversation Dataset\n",
    "- Korean Question Answering Dataset\n",
    "- Korean Sentiment Analysis Dataset\n",
    "\n",
    "**Instructions:**\n",
    "1. Register at https://aihub.or.kr\n",
    "2. Search for desired datasets\n",
    "3. Request access and download\n",
    "4. Place files in `data/v22.1/raw/manual/aihub/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aihub_datasets(data_dir: Path) -> DatasetResult:\n",
    "    \"\"\"Load AI Hub datasets from manually downloaded files.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing AI Hub data files\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with loaded records\n",
    "    \"\"\"\n",
    "    name = \"aihub\"\n",
    "    aihub_dir = data_dir / \"aihub\"\n",
    "    \n",
    "    if not aihub_dir.exists():\n",
    "        aihub_dir.mkdir(parents=True, exist_ok=True)\n",
    "        return DatasetResult(\n",
    "            name=name,\n",
    "            success=False,\n",
    "            error_message=(\n",
    "                f\"AI Hub data directory not found: {aihub_dir}\\n\"\n",
    "                f\"Please download datasets from https://aihub.or.kr and place them here.\"\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    records: List[UnifiedRecord] = []\n",
    "    \n",
    "    # Look for JSON/JSONL files\n",
    "    json_files = list(aihub_dir.glob(\"*.json\")) + list(aihub_dir.glob(\"*.jsonl\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        return DatasetResult(\n",
    "            name=name,\n",
    "            success=False,\n",
    "            error_message=(\n",
    "                f\"No JSON/JSONL files found in {aihub_dir}\\n\"\n",
    "                f\"Please download datasets from https://aihub.or.kr\"\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    for json_file in tqdm(json_files, desc=\"Processing AI Hub files\"):\n",
    "        try:\n",
    "            with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                if json_file.suffix == \".jsonl\":\n",
    "                    data = [json.loads(line) for line in f]\n",
    "                else:\n",
    "                    data = json.load(f)\n",
    "                    if isinstance(data, dict):\n",
    "                        data = data.get(\"data\", [data])\n",
    "            \n",
    "            for item in data:\n",
    "                # Adapt based on AI Hub data format\n",
    "                text1 = item.get(\"question\", \"\") or item.get(\"source\", \"\") or item.get(\"text1\", \"\")\n",
    "                text2 = item.get(\"answer\", \"\") or item.get(\"target\", \"\") or item.get(\"text2\", \"\")\n",
    "                \n",
    "                if text1 and text2:\n",
    "                    records.append(UnifiedRecord(\n",
    "                        text1=truncate_text(text1, 512),\n",
    "                        text2=truncate_text(text2, 512),\n",
    "                        label=0.85,\n",
    "                        source=f\"aihub_{json_file.stem}\",\n",
    "                        pair_type=\"aihub\",\n",
    "                        metadata={\"file\": json_file.name},\n",
    "                    ))\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Failed to process {json_file}: {e}\")\n",
    "    \n",
    "    if records:\n",
    "        return DatasetResult(name=name, success=True, records=records)\n",
    "    else:\n",
    "        return DatasetResult(\n",
    "            name=name,\n",
    "            success=False,\n",
    "            error_message=\"No valid records extracted from AI Hub files\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Modumalgunji (National Institute of Korean Language Corpus)\n",
    "\n",
    "**Portal:** https://corpus.korean.go.kr (Modu Corpus)\n",
    "\n",
    "**Recommended Datasets:**\n",
    "- Korean Conversation Corpus\n",
    "- Korean Written Language Corpus\n",
    "- Korean News Corpus\n",
    "- Korean Academic Text Corpus\n",
    "\n",
    "**Instructions:**\n",
    "1. Register at https://corpus.korean.go.kr\n",
    "2. Search and request datasets\n",
    "3. Download approved datasets\n",
    "4. Place files in `data/v22.1/raw/manual/modumalgunji/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_modumalgunji_datasets(data_dir: Path) -> DatasetResult:\n",
    "    \"\"\"Load Modumalgunji datasets from manually downloaded files.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing Modumalgunji data files\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with loaded corpus texts\n",
    "    \"\"\"\n",
    "    name = \"modumalgunji\"\n",
    "    modu_dir = data_dir / \"modumalgunji\"\n",
    "    \n",
    "    if not modu_dir.exists():\n",
    "        modu_dir.mkdir(parents=True, exist_ok=True)\n",
    "        return DatasetResult(\n",
    "            name=name,\n",
    "            success=False,\n",
    "            error_message=(\n",
    "                f\"Modumalgunji data directory not found: {modu_dir}\\n\"\n",
    "                f\"Please download datasets from https://corpus.korean.go.kr and place them here.\"\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    texts: List[str] = []\n",
    "    \n",
    "    # Look for JSON/JSONL/TXT files\n",
    "    data_files = (\n",
    "        list(modu_dir.glob(\"*.json\")) + \n",
    "        list(modu_dir.glob(\"*.jsonl\")) +\n",
    "        list(modu_dir.glob(\"*.txt\"))\n",
    "    )\n",
    "    \n",
    "    if not data_files:\n",
    "        return DatasetResult(\n",
    "            name=name,\n",
    "            success=False,\n",
    "            error_message=(\n",
    "                f\"No data files found in {modu_dir}\\n\"\n",
    "                f\"Please download datasets from https://corpus.korean.go.kr\"\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    for data_file in tqdm(data_files, desc=\"Processing Modumalgunji files\"):\n",
    "        try:\n",
    "            if data_file.suffix == \".txt\":\n",
    "                with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip()\n",
    "                        if line and len(line) > 10:\n",
    "                            texts.append(line)\n",
    "            else:\n",
    "                with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    if data_file.suffix == \".jsonl\":\n",
    "                        data = [json.loads(line) for line in f]\n",
    "                    else:\n",
    "                        data = json.load(f)\n",
    "                        if isinstance(data, dict):\n",
    "                            data = data.get(\"document\", []) or data.get(\"data\", [data])\n",
    "                \n",
    "                for item in data:\n",
    "                    text = (\n",
    "                        item.get(\"text\", \"\") or \n",
    "                        item.get(\"sentence\", \"\") or\n",
    "                        item.get(\"form\", \"\")\n",
    "                    )\n",
    "                    if text and len(text) > 10:\n",
    "                        texts.append(text)\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Failed to process {data_file}: {e}\")\n",
    "    \n",
    "    if texts:\n",
    "        return DatasetResult(name=name, success=True, corpus=texts)\n",
    "    else:\n",
    "        return DatasetResult(\n",
    "            name=name,\n",
    "            success=False,\n",
    "            error_message=\"No valid texts extracted from Modumalgunji files\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Public Data Portal (data.go.kr)\n",
    "\n",
    "**Portal:** https://www.data.go.kr\n",
    "\n",
    "**Recommended Datasets:**\n",
    "- Government document datasets\n",
    "- Public service FAQ datasets\n",
    "- Administrative document datasets\n",
    "\n",
    "**Instructions:**\n",
    "1. Register at https://www.data.go.kr\n",
    "2. Search for text/NLP related datasets\n",
    "3. Request API access or download files\n",
    "4. Place files in `data/v22.1/raw/manual/data_go_kr/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_go_kr_datasets(data_dir: Path) -> DatasetResult:\n",
    "    \"\"\"Load Public Data Portal datasets from manually downloaded files.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing data.go.kr data files\n",
    "        \n",
    "    Returns:\n",
    "        DatasetResult with loaded records or corpus\n",
    "    \"\"\"\n",
    "    name = \"data_go_kr\"\n",
    "    portal_dir = data_dir / \"data_go_kr\"\n",
    "    \n",
    "    if not portal_dir.exists():\n",
    "        portal_dir.mkdir(parents=True, exist_ok=True)\n",
    "        return DatasetResult(\n",
    "            name=name,\n",
    "            success=False,\n",
    "            error_message=(\n",
    "                f\"Public Data Portal directory not found: {portal_dir}\\n\"\n",
    "                f\"Please download datasets from https://www.data.go.kr and place them here.\"\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    records: List[UnifiedRecord] = []\n",
    "    texts: List[str] = []\n",
    "    \n",
    "    # Look for data files (CSV, JSON, JSONL)\n",
    "    data_files = (\n",
    "        list(portal_dir.glob(\"*.json\")) + \n",
    "        list(portal_dir.glob(\"*.jsonl\")) +\n",
    "        list(portal_dir.glob(\"*.csv\"))\n",
    "    )\n",
    "    \n",
    "    if not data_files:\n",
    "        return DatasetResult(\n",
    "            name=name,\n",
    "            success=False,\n",
    "            error_message=(\n",
    "                f\"No data files found in {portal_dir}\\n\"\n",
    "                f\"Please download datasets from https://www.data.go.kr\"\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    for data_file in tqdm(data_files, desc=\"Processing data.go.kr files\"):\n",
    "        try:\n",
    "            if data_file.suffix == \".csv\":\n",
    "                import csv\n",
    "                with open(data_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "                    reader = csv.DictReader(f)\n",
    "                    for row in reader:\n",
    "                        # Look for question-answer or title-content pairs\n",
    "                        q = row.get(\"question\", \"\") or row.get(\"title\", \"\") or row.get(\"subject\", \"\")\n",
    "                        a = row.get(\"answer\", \"\") or row.get(\"content\", \"\") or row.get(\"body\", \"\")\n",
    "                        \n",
    "                        if q and a:\n",
    "                            records.append(UnifiedRecord(\n",
    "                                text1=truncate_text(q, 512),\n",
    "                                text2=truncate_text(a, 512),\n",
    "                                label=0.8,\n",
    "                                source=f\"data_go_kr_{data_file.stem}\",\n",
    "                                pair_type=\"public_data\",\n",
    "                                metadata={\"file\": data_file.name},\n",
    "                            ))\n",
    "                        elif q:\n",
    "                            texts.append(q)\n",
    "                        elif a:\n",
    "                            texts.append(a)\n",
    "            else:\n",
    "                with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    if data_file.suffix == \".jsonl\":\n",
    "                        data = [json.loads(line) for line in f]\n",
    "                    else:\n",
    "                        data = json.load(f)\n",
    "                        if isinstance(data, dict):\n",
    "                            data = data.get(\"data\", [data])\n",
    "                \n",
    "                for item in data:\n",
    "                    q = item.get(\"question\", \"\") or item.get(\"title\", \"\")\n",
    "                    a = item.get(\"answer\", \"\") or item.get(\"content\", \"\")\n",
    "                    \n",
    "                    if q and a:\n",
    "                        records.append(UnifiedRecord(\n",
    "                            text1=truncate_text(q, 512),\n",
    "                            text2=truncate_text(a, 512),\n",
    "                            label=0.8,\n",
    "                            source=f\"data_go_kr_{data_file.stem}\",\n",
    "                            pair_type=\"public_data\",\n",
    "                            metadata={\"file\": data_file.name},\n",
    "                        ))\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Failed to process {data_file}: {e}\")\n",
    "    \n",
    "    if records:\n",
    "        return DatasetResult(name=name, success=True, records=records)\n",
    "    elif texts:\n",
    "        return DatasetResult(name=name, success=True, corpus=texts)\n",
    "    else:\n",
    "        return DatasetResult(\n",
    "            name=name,\n",
    "            success=False,\n",
    "            error_message=\"No valid data extracted from data.go.kr files\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Dataset Configurations - Existing\n",
    "HF_EXISTING_CONFIGS: List[DatasetConfig] = [\n",
    "    DatasetConfig(\n",
    "        name=\"msmarco_ko\",\n",
    "        loader_fn=load_msmarco_korean,\n",
    "        max_samples=50000,\n",
    "        description=\"Korean MS MARCO triplets\",\n",
    "        category=\"huggingface\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"klue_nli\",\n",
    "        loader_fn=load_klue_nli,\n",
    "        max_samples=30000,\n",
    "        description=\"KLUE Natural Language Inference\",\n",
    "        category=\"huggingface\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"klue_sts\",\n",
    "        loader_fn=load_klue_sts,\n",
    "        max_samples=15000,\n",
    "        description=\"KLUE Semantic Textual Similarity\",\n",
    "        category=\"huggingface\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"korquad\",\n",
    "        loader_fn=load_korquad,\n",
    "        max_samples=30000,\n",
    "        description=\"Korean Question Answering\",\n",
    "        category=\"huggingface\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"kobest_copa\",\n",
    "        loader_fn=load_kobest_copa,\n",
    "        max_samples=5000,\n",
    "        description=\"KoBEST COPA reasoning\",\n",
    "        category=\"huggingface\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"naver_news\",\n",
    "        loader_fn=load_naver_news,\n",
    "        max_samples=10000,\n",
    "        description=\"Naver News summarization\",\n",
    "        category=\"huggingface\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"nsmc\",\n",
    "        loader_fn=load_nsmc_corpus,\n",
    "        max_samples=50000,\n",
    "        description=\"NSMC movie review corpus\",\n",
    "        category=\"huggingface\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# HuggingFace Dataset Configurations - New\n",
    "HF_NEW_CONFIGS: List[DatasetConfig] = [\n",
    "    DatasetConfig(\n",
    "        name=\"ko_alpaca\",\n",
    "        loader_fn=load_ko_alpaca,\n",
    "        max_samples=52000,\n",
    "        description=\"Korean Alpaca instruction data\",\n",
    "        category=\"huggingface\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"kullm_v2\",\n",
    "        loader_fn=load_kullm_v2,\n",
    "        max_samples=150000,\n",
    "        description=\"KULLM v2 instruction tuning\",\n",
    "        category=\"huggingface\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"korquad_chat\",\n",
    "        loader_fn=load_korquad_chat,\n",
    "        max_samples=50000,\n",
    "        description=\"KorQuAD conversational QA\",\n",
    "        category=\"huggingface\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"korean_textbooks\",\n",
    "        loader_fn=load_korean_textbooks,\n",
    "        max_samples=100000,\n",
    "        description=\"Korean educational textbooks\",\n",
    "        category=\"huggingface\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"koalpaca_v1_1a\",\n",
    "        loader_fn=load_koalpaca_v1_1a,\n",
    "        max_samples=52000,\n",
    "        description=\"KoAlpaca v1.1a instructions\",\n",
    "        category=\"huggingface\",\n",
    "    ),\n",
    "    DatasetConfig(\n",
    "        name=\"ko_sarcasm\",\n",
    "        loader_fn=load_ko_sarcasm,\n",
    "        max_samples=9000,\n",
    "        description=\"Korean sarcasm detection corpus\",\n",
    "        category=\"huggingface\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# All HuggingFace configs\n",
    "HF_CONFIGS = HF_EXISTING_CONFIGS + HF_NEW_CONFIGS\n",
    "\n",
    "print(\"HuggingFace Dataset Configurations:\")\n",
    "print(f\"\\nExisting datasets ({len(HF_EXISTING_CONFIGS)}):\")\n",
    "for config in HF_EXISTING_CONFIGS:\n",
    "    print(f\"  - {config.name}: {config.description} (max: {config.max_samples:,})\")\n",
    "\n",
    "print(f\"\\nNew datasets ({len(HF_NEW_CONFIGS)}):\")\n",
    "for config in HF_NEW_CONFIGS:\n",
    "    print(f\"  - {config.name}: {config.description} (max: {config.max_samples:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Execute Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_sequential(\n",
    "    configs: List[DatasetConfig],\n",
    "    category_name: str = \"datasets\",\n",
    ") -> Dict[str, DatasetResult]:\n",
    "    \"\"\"Load datasets one by one to avoid memory issues.\n",
    "    \n",
    "    Args:\n",
    "        configs: List of dataset configurations\n",
    "        category_name: Name for logging purposes\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping dataset names to results\n",
    "    \"\"\"\n",
    "    results: Dict[str, DatasetResult] = {}\n",
    "    \n",
    "    print(f\"\\nLoading {len(configs)} {category_name} sequentially...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\n[{config.name}] Loading {config.description}...\")\n",
    "        try:\n",
    "            result = config.loader_fn(max_samples=config.max_samples)\n",
    "            results[config.name] = result\n",
    "            \n",
    "            status = \"SUCCESS\" if result.success else \"FAILED\"\n",
    "            print(f\"  [{status}] {result.sample_count:,} samples\")\n",
    "            \n",
    "            if not result.success:\n",
    "                print(f\"    Error: {result.error_message}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] {e}\")\n",
    "            results[config.name] = DatasetResult(\n",
    "                name=config.name,\n",
    "                success=False,\n",
    "                error_message=str(e)\n",
    "            )\n",
    "        \n",
    "        # Force garbage collection after each dataset\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HuggingFace datasets\n",
    "hf_results = load_datasets_sequential(HF_CONFIGS, \"HuggingFace datasets\")\n",
    "\n",
    "# Summary\n",
    "successful_hf = [r for r in hf_results.values() if r.success]\n",
    "failed_hf = [r for r in hf_results.values() if not r.success]\n",
    "total_hf_samples = sum(r.sample_count for r in successful_hf)\n",
    "\n",
    "print(f\"\\nHuggingFace Loading Summary:\")\n",
    "print(f\"  Successful: {len(successful_hf)}/{len(hf_results)}\")\n",
    "print(f\"  Total samples: {total_hf_samples:,}\")\n",
    "\n",
    "if failed_hf:\n",
    "    print(f\"  Failed datasets: {', '.join(r.name for r in failed_hf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load manual download datasets (if available)\n",
    "print(\"\\nLoading manually downloaded datasets...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "manual_results: Dict[str, DatasetResult] = {}\n",
    "\n",
    "# AI Hub\n",
    "print(\"\\n[aihub] Checking AI Hub data...\")\n",
    "aihub_result = load_aihub_datasets(MANUAL_DATA_DIR)\n",
    "manual_results[\"aihub\"] = aihub_result\n",
    "if aihub_result.success:\n",
    "    print(f\"  [SUCCESS] {aihub_result.sample_count:,} samples\")\n",
    "else:\n",
    "    print(f\"  [SKIPPED] {aihub_result.error_message}\")\n",
    "\n",
    "# Modumalgunji\n",
    "print(\"\\n[modumalgunji] Checking Modumalgunji data...\")\n",
    "modu_result = load_modumalgunji_datasets(MANUAL_DATA_DIR)\n",
    "manual_results[\"modumalgunji\"] = modu_result\n",
    "if modu_result.success:\n",
    "    print(f\"  [SUCCESS] {modu_result.sample_count:,} samples\")\n",
    "else:\n",
    "    print(f\"  [SKIPPED] {modu_result.error_message}\")\n",
    "\n",
    "# Public Data Portal\n",
    "print(\"\\n[data_go_kr] Checking Public Data Portal data...\")\n",
    "portal_result = load_data_go_kr_datasets(MANUAL_DATA_DIR)\n",
    "manual_results[\"data_go_kr\"] = portal_result\n",
    "if portal_result.success:\n",
    "    print(f\"  [SUCCESS] {portal_result.sample_count:,} samples\")\n",
    "else:\n",
    "    print(f\"  [SKIPPED] {portal_result.error_message}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Merge and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = {**hf_results, **manual_results}\n",
    "\n",
    "# Collect all records and corpus texts\n",
    "all_records: List[UnifiedRecord] = []\n",
    "all_corpus: List[str] = []\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    if result.success:\n",
    "        all_records.extend(result.records)\n",
    "        all_corpus.extend(result.corpus)\n",
    "\n",
    "print(f\"Total unified records: {len(all_records):,}\")\n",
    "print(f\"Total corpus texts: {len(all_corpus):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_records(records: List[UnifiedRecord]) -> List[UnifiedRecord]:\n",
    "    \"\"\"Remove duplicate records based on text1 and text2.\n",
    "    \n",
    "    Args:\n",
    "        records: List of UnifiedRecord objects\n",
    "        \n",
    "    Returns:\n",
    "        Deduplicated list of records\n",
    "    \"\"\"\n",
    "    seen: set = set()\n",
    "    unique_records: List[UnifiedRecord] = []\n",
    "    \n",
    "    for record in records:\n",
    "        key = (record.text1, record.text2)\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique_records.append(record)\n",
    "    \n",
    "    return unique_records\n",
    "\n",
    "\n",
    "# Deduplicate records\n",
    "unique_records = deduplicate_records(all_records)\n",
    "print(f\"Unique records after deduplication: {len(unique_records):,}\")\n",
    "print(f\"Removed {len(all_records) - len(unique_records):,} duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics by source\n",
    "source_counts: Dict[str, int] = defaultdict(int)\n",
    "pair_type_counts: Dict[str, int] = defaultdict(int)\n",
    "\n",
    "for record in unique_records:\n",
    "    source_counts[record.source] += 1\n",
    "    pair_type_counts[record.pair_type] += 1\n",
    "\n",
    "print(\"\\nRecords by source:\")\n",
    "for source, count in sorted(source_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {source}: {count:,}\")\n",
    "\n",
    "print(\"\\nRecords by pair type:\")\n",
    "for pair_type, count in sorted(pair_type_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {pair_type}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save unified records\n",
    "records_output_path = RAW_DATA_DIR / \"unified_records.jsonl\"\n",
    "saved_count = save_jsonl(unique_records, records_output_path)\n",
    "print(f\"Saved {saved_count:,} unified records to {records_output_path}\")\n",
    "\n",
    "# Save corpus texts\n",
    "if all_corpus:\n",
    "    corpus_output_path = RAW_DATA_DIR / \"corpus.txt\"\n",
    "    corpus_count = save_text_corpus(all_corpus, corpus_output_path)\n",
    "    print(f\"Saved {corpus_count:,} corpus texts to {corpus_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save per-source files for traceability\n",
    "print(\"\\nSaving per-source files...\")\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    if not result.success:\n",
    "        continue\n",
    "    \n",
    "    if result.records:\n",
    "        output_path = HF_DATA_DIR / f\"{name}_records.jsonl\"\n",
    "        count = save_jsonl(result.records, output_path)\n",
    "        print(f\"  {name}: {count:,} records -> {output_path.name}\")\n",
    "    \n",
    "    if result.corpus:\n",
    "        output_path = HF_DATA_DIR / f\"{name}_corpus.txt\"\n",
    "        count = save_text_corpus(result.corpus, output_path)\n",
    "        print(f\"  {name}: {count:,} texts -> {output_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Upload to S3 (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if S3_BUCKET_NAME:\n",
    "    print(f\"\\nUploading to S3 bucket: {S3_BUCKET_NAME}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Upload unified records\n",
    "    upload_to_s3(\n",
    "        records_output_path,\n",
    "        S3_BUCKET_NAME,\n",
    "        \"spark-meta/neural/raw/unified_records.jsonl\"\n",
    "    )\n",
    "    \n",
    "    # Upload corpus if exists\n",
    "    corpus_path = RAW_DATA_DIR / \"corpus.txt\"\n",
    "    if corpus_path.exists():\n",
    "        upload_to_s3(\n",
    "            corpus_path,\n",
    "            S3_BUCKET_NAME,\n",
    "            \"spark-meta/neural/raw/corpus.txt\"\n",
    "        )\n",
    "    \n",
    "    # Upload per-source files\n",
    "    for f in HF_DATA_DIR.glob(\"*.jsonl\"):\n",
    "        upload_to_s3(\n",
    "            f,\n",
    "            S3_BUCKET_NAME,\n",
    "            f\"spark-meta/neural/raw/huggingface/{f.name}\"\n",
    "        )\n",
    "    \n",
    "    for f in HF_DATA_DIR.glob(\"*.txt\"):\n",
    "        upload_to_s3(\n",
    "            f,\n",
    "            S3_BUCKET_NAME,\n",
    "            f\"spark-meta/neural/raw/huggingface/{f.name}\"\n",
    "        )\n",
    "else:\n",
    "    print(\"\\nS3 bucket not configured. Skipping upload.\")\n",
    "    print(\"To enable S3 upload, set S3_BUCKET_NAME in your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"v22.1 Multi-Source Korean Data Collection Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# HuggingFace summary\n",
    "print(f\"\\nHuggingFace Datasets:\")\n",
    "print(f\"  Attempted: {len(hf_results)}\")\n",
    "print(f\"  Successful: {len(successful_hf)}\")\n",
    "print(f\"  Failed: {len(failed_hf)}\")\n",
    "print(f\"  Total samples: {total_hf_samples:,}\")\n",
    "\n",
    "# Manual download summary\n",
    "successful_manual = [r for r in manual_results.values() if r.success]\n",
    "total_manual = sum(r.sample_count for r in successful_manual)\n",
    "print(f\"\\nManual Download Datasets:\")\n",
    "print(f\"  Attempted: {len(manual_results)}\")\n",
    "print(f\"  Successful: {len(successful_manual)}\")\n",
    "print(f\"  Total samples: {total_manual:,}\")\n",
    "\n",
    "# Overall summary\n",
    "print(f\"\\nOverall:\")\n",
    "print(f\"  Total unified records: {len(unique_records):,}\")\n",
    "print(f\"  Total corpus texts: {len(all_corpus):,}\")\n",
    "\n",
    "# File sizes\n",
    "print(f\"\\nOutput Files:\")\n",
    "for f in sorted(RAW_DATA_DIR.glob(\"*\")):\n",
    "    if f.is_file():\n",
    "        size_mb = f.stat().st_size / 1024 / 1024\n",
    "        print(f\"  {f.name}: {size_mb:.2f} MB\")\n",
    "\n",
    "print(f\"\\nHuggingFace Files:\")\n",
    "for f in sorted(HF_DATA_DIR.glob(\"*\")):\n",
    "    if f.is_file():\n",
    "        size_mb = f.stat().st_size / 1024 / 1024\n",
    "        print(f\"  {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Next Steps\n",
    "\n",
    "1. **Manual Data Downloads** (if not done):\n",
    "   - AI Hub: https://aihub.or.kr\n",
    "   - Modumalgunji: https://corpus.korean.go.kr\n",
    "   - Public Data Portal: https://www.data.go.kr\n",
    "\n",
    "2. **Data Processing**:\n",
    "   - Run `01_data_preprocessing.ipynb` for data cleaning and filtering\n",
    "   - Run `02_data_augmentation.ipynb` for data augmentation\n",
    "\n",
    "3. **Training Data Generation**:\n",
    "   - Generate triplets for Neural Sparse model training\n",
    "   - Create train/validation/test splits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

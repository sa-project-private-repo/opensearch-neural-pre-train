{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v22.1 Adaptive Data Preprocessing\n",
    "\n",
    "Preprocesses collected Korean data for Neural Sparse training with adaptive processing based on data size.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Adaptive Processing**: Automatically selects processing method based on data size\n",
    "  - < 1M rows: Use pandas (local processing)\n",
    "  - >= 1M rows: Use Spark via EMR (distributed processing)\n",
    "- **Text Cleaning**: HTML removal, whitespace normalization, special character filtering\n",
    "- **Quality Filtering**: Duplicate removal, length constraints, Korean content ratio check\n",
    "\n",
    "## Processing Pipeline\n",
    "\n",
    "| Step | Description | Output |\n",
    "|------|-------------|--------|\n",
    "| 1 | Count rows to determine method | Processing strategy |\n",
    "| 2 | Text cleaning | Clean text fields |\n",
    "| 3 | Quality filtering | Filtered dataset |\n",
    "| 4 | Save preprocessed data | JSONL output |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_project_root() -> Path:\n",
    "    \"\"\"Find the project root directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"src\").exists() or (parent / \".git\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Iterator, Optional, Protocol\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(PROJECT_ROOT / \".env\")\n",
    "    print(\"Loaded .env file\")\n",
    "except ImportError:\n",
    "    print(\"python-dotenv not installed, using system environment variables\")\n",
    "\n",
    "# Environment configuration\n",
    "S3_BUCKET_NAME = os.getenv(\"S3_BUCKET_NAME\", \"\")\n",
    "EMR_INSTANCE_ID = os.getenv(\"EMR_INSTANCE_ID\", \"\")\n",
    "EMR_SPARK_PORT = int(os.getenv(\"EMR_SPARK_PORT\", \"15002\"))\n",
    "\n",
    "print(f\"S3_BUCKET_NAME: {S3_BUCKET_NAME or '(not set)'}\")\n",
    "print(f\"EMR_INSTANCE_ID: {EMR_INSTANCE_ID or '(not set)'}\")\n",
    "print(f\"EMR_SPARK_PORT: {EMR_SPARK_PORT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class PreprocessingConfig:\n",
    "    \"\"\"Configuration for data preprocessing.\"\"\"\n",
    "\n",
    "    # Thresholds\n",
    "    large_data_threshold: int = 1_000_000  # 1M rows\n",
    "    min_text_length: int = 2\n",
    "    max_text_length: int = 512\n",
    "    min_korean_ratio: float = 0.30  # 30% Korean characters\n",
    "\n",
    "    # Paths\n",
    "    input_dir: Path = PROJECT_ROOT / \"data\" / \"huggingface_korean\"\n",
    "    output_dir: Path = PROJECT_ROOT / \"data\" / \"v22.1\" / \"preprocessed\"\n",
    "    s3_output_prefix: str = \"spark-meta/neural/preprocessed/\"\n",
    "\n",
    "    # Spark settings\n",
    "    spark_host: str = \"localhost\"\n",
    "    spark_port: int = EMR_SPARK_PORT\n",
    "\n",
    "\n",
    "config = PreprocessingConfig()\n",
    "\n",
    "# Ensure output directory exists\n",
    "config.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Input directory: {config.input_dir}\")\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"Large data threshold: {config.large_data_threshold:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Cleaning Functions\n",
    "\n",
    "Shared cleaning logic used by both pandas and Spark implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompiled regex patterns for performance\n",
    "HTML_TAG_PATTERN = re.compile(r\"<[^>]+>\")\n",
    "WHITESPACE_PATTERN = re.compile(r\"\\s+\")\n",
    "SPECIAL_CHAR_PATTERN = re.compile(\n",
    "    r\"[^\\w\\s가-힣a-zA-Z0-9.,!?;:'\\\"()\\[\\]{}\\-]+\"\n",
    ")\n",
    "KOREAN_CHAR_PATTERN = re.compile(r\"[가-힣]\")\n",
    "\n",
    "\n",
    "def remove_html_tags(text: str) -> str:\n",
    "    \"\"\"Remove HTML tags from text.\n",
    "\n",
    "    Args:\n",
    "        text: Input text possibly containing HTML tags.\n",
    "\n",
    "    Returns:\n",
    "        Text with HTML tags removed.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return HTML_TAG_PATTERN.sub(\"\", text)\n",
    "\n",
    "\n",
    "def normalize_whitespace(text: str) -> str:\n",
    "    \"\"\"Normalize whitespace by replacing multiple spaces with single space.\n",
    "\n",
    "    Args:\n",
    "        text: Input text with possible whitespace issues.\n",
    "\n",
    "    Returns:\n",
    "        Text with normalized whitespace.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return WHITESPACE_PATTERN.sub(\" \", text).strip()\n",
    "\n",
    "\n",
    "def remove_special_characters(text: str) -> str:\n",
    "    \"\"\"Remove special characters, keeping Korean, English, numbers, and basic punctuation.\n",
    "\n",
    "    Args:\n",
    "        text: Input text possibly containing special characters.\n",
    "\n",
    "    Returns:\n",
    "        Text with special characters removed.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return SPECIAL_CHAR_PATTERN.sub(\"\", text)\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Apply full text cleaning pipeline.\n",
    "\n",
    "    Pipeline:\n",
    "    1. Remove HTML tags\n",
    "    2. Normalize whitespace\n",
    "    3. Remove special characters\n",
    "\n",
    "    Args:\n",
    "        text: Raw input text.\n",
    "\n",
    "    Returns:\n",
    "        Cleaned text.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = remove_html_tags(text)\n",
    "    text = normalize_whitespace(text)\n",
    "    text = remove_special_characters(text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Test cleaning functions\n",
    "test_cases = [\n",
    "    \"<p>안녕하세요</p>  world!\",\n",
    "    \"Hello\\n\\n\\tWorld\",\n",
    "    \"한글@#$%테스트!!!\",\n",
    "    \"<b>Bold</b> text with &nbsp; entities\",\n",
    "]\n",
    "\n",
    "print(\"Text cleaning tests:\")\n",
    "for test in test_cases:\n",
    "    cleaned = clean_text(test)\n",
    "    print(f\"  '{test}' -> '{cleaned}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quality Filtering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_korean_ratio(text: str) -> float:\n",
    "    \"\"\"Calculate the ratio of Korean characters in text.\n",
    "\n",
    "    Args:\n",
    "        text: Input text.\n",
    "\n",
    "    Returns:\n",
    "        Ratio of Korean characters (0.0 to 1.0).\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    korean_chars = len(KOREAN_CHAR_PATTERN.findall(text))\n",
    "    total_chars = len(text.replace(\" \", \"\"))\n",
    "    if total_chars == 0:\n",
    "        return 0.0\n",
    "    return korean_chars / total_chars\n",
    "\n",
    "\n",
    "def is_valid_text(\n",
    "    text: str,\n",
    "    min_length: int = 2,\n",
    "    max_length: int = 512,\n",
    "    min_korean_ratio: float = 0.30,\n",
    ") -> bool:\n",
    "    \"\"\"Check if text meets quality criteria.\n",
    "\n",
    "    Args:\n",
    "        text: Input text to validate.\n",
    "        min_length: Minimum character length.\n",
    "        max_length: Maximum character length.\n",
    "        min_korean_ratio: Minimum ratio of Korean characters.\n",
    "\n",
    "    Returns:\n",
    "        True if text meets all criteria, False otherwise.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return False\n",
    "\n",
    "    text_length = len(text)\n",
    "    if text_length < min_length or text_length > max_length:\n",
    "        return False\n",
    "\n",
    "    korean_ratio = calculate_korean_ratio(text)\n",
    "    return korean_ratio >= min_korean_ratio\n",
    "\n",
    "\n",
    "def is_valid_pair(\n",
    "    text1: str,\n",
    "    text2: str,\n",
    "    min_length: int = 2,\n",
    "    max_length: int = 512,\n",
    "    min_korean_ratio: float = 0.30,\n",
    ") -> bool:\n",
    "    \"\"\"Check if a text pair meets quality criteria.\n",
    "\n",
    "    Args:\n",
    "        text1: First text in pair.\n",
    "        text2: Second text in pair.\n",
    "        min_length: Minimum character length.\n",
    "        max_length: Maximum character length.\n",
    "        min_korean_ratio: Minimum ratio of Korean characters.\n",
    "\n",
    "    Returns:\n",
    "        True if pair meets all criteria, False otherwise.\n",
    "    \"\"\"\n",
    "    # Both texts must be valid\n",
    "    if not is_valid_text(text1, min_length, max_length, min_korean_ratio):\n",
    "        return False\n",
    "    if not is_valid_text(text2, min_length, max_length, min_korean_ratio):\n",
    "        return False\n",
    "\n",
    "    # text1 and text2 should not be identical\n",
    "    if text1 == text2:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# Test quality filters\n",
    "test_pairs = [\n",
    "    (\"안녕하세요\", \"반갑습니다\"),  # Valid\n",
    "    (\"hi\", \"there\"),  # Invalid: Low Korean ratio\n",
    "    (\"a\", \"b\"),  # Invalid: Too short\n",
    "    (\"동일한텍스트\", \"동일한텍스트\"),  # Invalid: Identical\n",
    "    (\"한글\" * 300, \"테스트\"),  # Invalid: Too long\n",
    "]\n",
    "\n",
    "print(\"\\nQuality filter tests:\")\n",
    "for t1, t2 in test_pairs:\n",
    "    valid = is_valid_pair(\n",
    "        t1, t2,\n",
    "        min_length=config.min_text_length,\n",
    "        max_length=config.max_text_length,\n",
    "        min_korean_ratio=config.min_korean_ratio\n",
    "    )\n",
    "    print(f\"  ('{t1[:20]}...', '{t2[:20]}...') -> {valid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Processing Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProcessingStats:\n",
    "    \"\"\"Statistics for data processing pipeline.\"\"\"\n",
    "\n",
    "    initial_count: int = 0\n",
    "    after_cleaning: int = 0\n",
    "    after_dedup: int = 0\n",
    "    after_identical_filter: int = 0\n",
    "    after_length_filter: int = 0\n",
    "    after_korean_filter: int = 0\n",
    "    final_count: int = 0\n",
    "\n",
    "    def print_summary(self) -> None:\n",
    "        \"\"\"Print processing statistics summary.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Processing Statistics\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"  Initial count:            {self.initial_count:>12,}\")\n",
    "        print(f\"  After cleaning:           {self.after_cleaning:>12,}\")\n",
    "        print(f\"  After deduplication:      {self.after_dedup:>12,} \"\n",
    "              f\"(-{self.after_cleaning - self.after_dedup:,})\")\n",
    "        print(f\"  After identical filter:   {self.after_identical_filter:>12,} \"\n",
    "              f\"(-{self.after_dedup - self.after_identical_filter:,})\")\n",
    "        print(f\"  After length filter:      {self.after_length_filter:>12,} \"\n",
    "              f\"(-{self.after_identical_filter - self.after_length_filter:,})\")\n",
    "        print(f\"  After Korean filter:      {self.after_korean_filter:>12,} \"\n",
    "              f\"(-{self.after_length_filter - self.after_korean_filter:,})\")\n",
    "        print(f\"  Final count:              {self.final_count:>12,}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        if self.initial_count > 0:\n",
    "            retention = self.final_count / self.initial_count * 100\n",
    "            print(f\"  Retention rate: {retention:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Processor Protocol and Implementations\n",
    "\n",
    "Abstract interface with pandas and Spark implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(ABC):\n",
    "    \"\"\"Abstract base class for data processors.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def count_rows(self, input_path: Path) -> int:\n",
    "        \"\"\"Count total rows in input file.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def process(self, input_path: Path, output_path: Path) -> ProcessingStats:\n",
    "        \"\"\"Process data and return statistics.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def cleanup(self) -> None:\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasProcessor(DataProcessor):\n",
    "    \"\"\"Pandas-based data processor for smaller datasets (< 1M rows).\"\"\"\n",
    "\n",
    "    def __init__(self, config: PreprocessingConfig):\n",
    "        \"\"\"Initialize pandas processor.\n",
    "\n",
    "        Args:\n",
    "            config: Preprocessing configuration.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        logger.info(\"Initialized PandasProcessor\")\n",
    "\n",
    "    def count_rows(self, input_path: Path) -> int:\n",
    "        \"\"\"Count rows by reading file line by line.\n",
    "\n",
    "        Args:\n",
    "            input_path: Path to input JSONL file.\n",
    "\n",
    "        Returns:\n",
    "            Number of rows in file.\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for _ in f:\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    def process(self, input_path: Path, output_path: Path) -> ProcessingStats:\n",
    "        \"\"\"Process data using pandas.\n",
    "\n",
    "        Args:\n",
    "            input_path: Path to input JSONL file.\n",
    "            output_path: Path to output JSONL file.\n",
    "\n",
    "        Returns:\n",
    "            Processing statistics.\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "\n",
    "        stats = ProcessingStats()\n",
    "\n",
    "        # Load data\n",
    "        logger.info(f\"Loading data from {input_path}\")\n",
    "        df = pd.read_json(input_path, lines=True)\n",
    "        stats.initial_count = len(df)\n",
    "        logger.info(f\"Loaded {stats.initial_count:,} rows\")\n",
    "\n",
    "        # Identify text columns\n",
    "        text_columns = self._identify_text_columns(df)\n",
    "        logger.info(f\"Text columns: {text_columns}\")\n",
    "\n",
    "        # Step 1: Text cleaning\n",
    "        logger.info(\"Step 1: Cleaning text...\")\n",
    "        for col in text_columns:\n",
    "            df[col] = df[col].fillna(\"\").astype(str).apply(clean_text)\n",
    "        stats.after_cleaning = len(df)\n",
    "        logger.info(f\"After cleaning: {stats.after_cleaning:,} rows\")\n",
    "\n",
    "        # Step 2: Remove exact duplicates\n",
    "        logger.info(\"Step 2: Removing duplicates...\")\n",
    "        df = df.drop_duplicates(subset=text_columns)\n",
    "        stats.after_dedup = len(df)\n",
    "        logger.info(f\"After dedup: {stats.after_dedup:,} rows\")\n",
    "\n",
    "        # Step 3: Remove pairs where text1 == text2\n",
    "        logger.info(\"Step 3: Removing identical pairs...\")\n",
    "        if len(text_columns) >= 2:\n",
    "            col1, col2 = text_columns[:2]\n",
    "            df = df[df[col1] != df[col2]]\n",
    "        stats.after_identical_filter = len(df)\n",
    "        logger.info(f\"After identical filter: {stats.after_identical_filter:,} rows\")\n",
    "\n",
    "        # Step 4: Length filtering\n",
    "        logger.info(\"Step 4: Applying length filter...\")\n",
    "        for col in text_columns:\n",
    "            df = df[\n",
    "                (df[col].str.len() >= self.config.min_text_length) &\n",
    "                (df[col].str.len() <= self.config.max_text_length)\n",
    "            ]\n",
    "        stats.after_length_filter = len(df)\n",
    "        logger.info(f\"After length filter: {stats.after_length_filter:,} rows\")\n",
    "\n",
    "        # Step 5: Korean content ratio check\n",
    "        logger.info(\"Step 5: Checking Korean content ratio...\")\n",
    "        for col in text_columns:\n",
    "            df[f\"{col}_korean_ratio\"] = df[col].apply(calculate_korean_ratio)\n",
    "            df = df[df[f\"{col}_korean_ratio\"] >= self.config.min_korean_ratio]\n",
    "            df = df.drop(columns=[f\"{col}_korean_ratio\"])\n",
    "        stats.after_korean_filter = len(df)\n",
    "        stats.final_count = stats.after_korean_filter\n",
    "        logger.info(f\"After Korean filter: {stats.after_korean_filter:,} rows\")\n",
    "\n",
    "        # Save output\n",
    "        logger.info(f\"Saving to {output_path}\")\n",
    "        df.to_json(output_path, orient=\"records\", lines=True, force_ascii=False)\n",
    "        logger.info(f\"Saved {stats.final_count:,} rows\")\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def _identify_text_columns(self, df) -> list[str]:\n",
    "        \"\"\"Identify text columns in dataframe.\n",
    "\n",
    "        Args:\n",
    "            df: Input dataframe.\n",
    "\n",
    "        Returns:\n",
    "            List of text column names.\n",
    "        \"\"\"\n",
    "        # Priority order of column names\n",
    "        priority_pairs = [\n",
    "            (\"source\", \"target\"),\n",
    "            (\"text1\", \"text2\"),\n",
    "            (\"anchor\", \"positive\"),\n",
    "            (\"query\", \"document\"),\n",
    "        ]\n",
    "\n",
    "        for col1, col2 in priority_pairs:\n",
    "            if col1 in df.columns and col2 in df.columns:\n",
    "                return [col1, col2]\n",
    "\n",
    "        # Fallback: return string columns\n",
    "        return [col for col in df.columns if df[col].dtype == \"object\"][:2]\n",
    "\n",
    "    def cleanup(self) -> None:\n",
    "        \"\"\"Clean up resources (no-op for pandas).\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkProcessor(DataProcessor):\n",
    "    \"\"\"Spark-based data processor for larger datasets (>= 1M rows).\"\"\"\n",
    "\n",
    "    def __init__(self, config: PreprocessingConfig):\n",
    "        \"\"\"Initialize Spark processor.\n",
    "\n",
    "        Args:\n",
    "            config: Preprocessing configuration.\n",
    "\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to Spark.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.spark = None\n",
    "        self._connect()\n",
    "\n",
    "    def _connect(self) -> None:\n",
    "        \"\"\"Connect to EMR Spark via Spark Connect.\n",
    "\n",
    "        Prerequisites:\n",
    "        Run in terminal first:\n",
    "        aws ssm start-session --target $EMR_INSTANCE_ID \\\n",
    "          --document-name AWS-StartPortForwardingSession \\\n",
    "          --parameters '{\"portNumber\":[\"15002\"],\"localPortNumber\":[\"15002\"]}'\n",
    "\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to Spark.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from pyspark.sql import SparkSession\n",
    "\n",
    "            spark_url = f\"sc://{self.config.spark_host}:{self.config.spark_port}\"\n",
    "            logger.info(f\"Connecting to Spark at {spark_url}\")\n",
    "\n",
    "            self.spark = (\n",
    "                SparkSession.builder\n",
    "                .remote(spark_url)\n",
    "                .appName(\"korean_neural_sparse_v22.1\")\n",
    "                .getOrCreate()\n",
    "            )\n",
    "\n",
    "            # Test connection\n",
    "            version = self.spark.version\n",
    "            logger.info(f\"Connected to Spark {version}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to connect to Spark: {e}\")\n",
    "            raise ConnectionError(\n",
    "                f\"Unable to connect to Spark at {self.config.spark_host}:\"\n",
    "                f\"{self.config.spark_port}. \"\n",
    "                f\"Ensure SSM port forwarding is active. Error: {e}\"\n",
    "            )\n",
    "\n",
    "    def count_rows(self, input_path: Path) -> int:\n",
    "        \"\"\"Count rows using Spark.\n",
    "\n",
    "        Args:\n",
    "            input_path: Path to input JSONL file.\n",
    "\n",
    "        Returns:\n",
    "            Number of rows in file.\n",
    "        \"\"\"\n",
    "        df = self.spark.read.json(str(input_path))\n",
    "        return df.count()\n",
    "\n",
    "    def process(self, input_path: Path, output_path: Path) -> ProcessingStats:\n",
    "        \"\"\"Process data using Spark.\n",
    "\n",
    "        Args:\n",
    "            input_path: Path to input JSONL file.\n",
    "            output_path: Path to output JSONL file.\n",
    "\n",
    "        Returns:\n",
    "            Processing statistics.\n",
    "        \"\"\"\n",
    "        from pyspark.sql import functions as F\n",
    "        from pyspark.sql.types import StringType, FloatType, BooleanType\n",
    "\n",
    "        stats = ProcessingStats()\n",
    "\n",
    "        # Register UDFs\n",
    "        clean_text_udf = F.udf(clean_text, StringType())\n",
    "        korean_ratio_udf = F.udf(calculate_korean_ratio, FloatType())\n",
    "\n",
    "        # Load data\n",
    "        logger.info(f\"Loading data from {input_path}\")\n",
    "        df = self.spark.read.json(str(input_path))\n",
    "        stats.initial_count = df.count()\n",
    "        logger.info(f\"Loaded {stats.initial_count:,} rows\")\n",
    "\n",
    "        # Identify text columns\n",
    "        text_columns = self._identify_text_columns(df)\n",
    "        logger.info(f\"Text columns: {text_columns}\")\n",
    "\n",
    "        # Step 1: Text cleaning\n",
    "        logger.info(\"Step 1: Cleaning text...\")\n",
    "        for col in text_columns:\n",
    "            df = df.withColumn(\n",
    "                col,\n",
    "                clean_text_udf(F.coalesce(F.col(col), F.lit(\"\")))\n",
    "            )\n",
    "        stats.after_cleaning = df.count()\n",
    "        logger.info(f\"After cleaning: {stats.after_cleaning:,} rows\")\n",
    "\n",
    "        # Step 2: Remove exact duplicates\n",
    "        logger.info(\"Step 2: Removing duplicates...\")\n",
    "        df = df.dropDuplicates(text_columns)\n",
    "        stats.after_dedup = df.count()\n",
    "        logger.info(f\"After dedup: {stats.after_dedup:,} rows\")\n",
    "\n",
    "        # Step 3: Remove pairs where text1 == text2\n",
    "        logger.info(\"Step 3: Removing identical pairs...\")\n",
    "        if len(text_columns) >= 2:\n",
    "            col1, col2 = text_columns[:2]\n",
    "            df = df.filter(F.col(col1) != F.col(col2))\n",
    "        stats.after_identical_filter = df.count()\n",
    "        logger.info(f\"After identical filter: {stats.after_identical_filter:,} rows\")\n",
    "\n",
    "        # Step 4: Length filtering\n",
    "        logger.info(\"Step 4: Applying length filter...\")\n",
    "        for col in text_columns:\n",
    "            df = df.filter(\n",
    "                (F.length(F.col(col)) >= self.config.min_text_length) &\n",
    "                (F.length(F.col(col)) <= self.config.max_text_length)\n",
    "            )\n",
    "        stats.after_length_filter = df.count()\n",
    "        logger.info(f\"After length filter: {stats.after_length_filter:,} rows\")\n",
    "\n",
    "        # Step 5: Korean content ratio check\n",
    "        logger.info(\"Step 5: Checking Korean content ratio...\")\n",
    "        for col in text_columns:\n",
    "            df = df.withColumn(f\"{col}_korean_ratio\", korean_ratio_udf(F.col(col)))\n",
    "            df = df.filter(\n",
    "                F.col(f\"{col}_korean_ratio\") >= self.config.min_korean_ratio\n",
    "            )\n",
    "            df = df.drop(f\"{col}_korean_ratio\")\n",
    "        stats.after_korean_filter = df.count()\n",
    "        stats.final_count = stats.after_korean_filter\n",
    "        logger.info(f\"After Korean filter: {stats.after_korean_filter:,} rows\")\n",
    "\n",
    "        # Save output\n",
    "        logger.info(f\"Saving to {output_path}\")\n",
    "\n",
    "        # Write to local as single file\n",
    "        temp_output = output_path.parent / \"temp_spark_output\"\n",
    "        df.coalesce(1).write.mode(\"overwrite\").json(str(temp_output))\n",
    "\n",
    "        # Move single file to final location\n",
    "        import glob\n",
    "        import shutil\n",
    "\n",
    "        json_files = glob.glob(str(temp_output / \"*.json\"))\n",
    "        if json_files:\n",
    "            shutil.move(json_files[0], str(output_path))\n",
    "        shutil.rmtree(str(temp_output), ignore_errors=True)\n",
    "\n",
    "        logger.info(f\"Saved {stats.final_count:,} rows\")\n",
    "\n",
    "        # Also save to S3 if bucket is configured\n",
    "        if S3_BUCKET_NAME:\n",
    "            s3_path = f\"s3://{S3_BUCKET_NAME}/{self.config.s3_output_prefix}\"\n",
    "            logger.info(f\"Saving to S3: {s3_path}\")\n",
    "            df.write.mode(\"overwrite\").json(s3_path)\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def _identify_text_columns(self, df) -> list[str]:\n",
    "        \"\"\"Identify text columns in Spark dataframe.\n",
    "\n",
    "        Args:\n",
    "            df: Input Spark dataframe.\n",
    "\n",
    "        Returns:\n",
    "            List of text column names.\n",
    "        \"\"\"\n",
    "        columns = df.columns\n",
    "\n",
    "        # Priority order of column names\n",
    "        priority_pairs = [\n",
    "            (\"source\", \"target\"),\n",
    "            (\"text1\", \"text2\"),\n",
    "            (\"anchor\", \"positive\"),\n",
    "            (\"query\", \"document\"),\n",
    "        ]\n",
    "\n",
    "        for col1, col2 in priority_pairs:\n",
    "            if col1 in columns and col2 in columns:\n",
    "                return [col1, col2]\n",
    "\n",
    "        # Fallback: return first two string columns\n",
    "        string_cols = [\n",
    "            f.name for f in df.schema.fields\n",
    "            if str(f.dataType) == \"StringType()\"\n",
    "        ]\n",
    "        return string_cols[:2]\n",
    "\n",
    "    def cleanup(self) -> None:\n",
    "        \"\"\"Stop Spark session.\"\"\"\n",
    "        if self.spark:\n",
    "            logger.info(\"Stopping Spark session\")\n",
    "            self.spark.stop()\n",
    "            self.spark = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Adaptive Processor Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_processor(\n",
    "    input_path: Path,\n",
    "    config: PreprocessingConfig,\n",
    "    force_pandas: bool = False,\n",
    "    force_spark: bool = False,\n",
    ") -> tuple[DataProcessor, int]:\n",
    "    \"\"\"Create appropriate processor based on data size.\n",
    "\n",
    "    Args:\n",
    "        input_path: Path to input file.\n",
    "        config: Preprocessing configuration.\n",
    "        force_pandas: Force use of pandas processor.\n",
    "        force_spark: Force use of Spark processor.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (processor instance, row count).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If both force_pandas and force_spark are True.\n",
    "    \"\"\"\n",
    "    if force_pandas and force_spark:\n",
    "        raise ValueError(\"Cannot force both pandas and Spark\")\n",
    "\n",
    "    # Quick row count using file iteration\n",
    "    logger.info(f\"Counting rows in {input_path}...\")\n",
    "    row_count = 0\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for _ in tqdm(f, desc=\"Counting rows\", unit=\"rows\"):\n",
    "            row_count += 1\n",
    "\n",
    "    logger.info(f\"Total rows: {row_count:,}\")\n",
    "\n",
    "    # Determine processor type\n",
    "    use_spark = (\n",
    "        force_spark or\n",
    "        (not force_pandas and row_count >= config.large_data_threshold)\n",
    "    )\n",
    "\n",
    "    if use_spark:\n",
    "        logger.info(\n",
    "            f\"Using Spark processor (rows={row_count:,} >= \"\n",
    "            f\"threshold={config.large_data_threshold:,})\"\n",
    "        )\n",
    "        try:\n",
    "            processor = SparkProcessor(config)\n",
    "        except ConnectionError as e:\n",
    "            logger.warning(f\"Spark connection failed: {e}\")\n",
    "            logger.warning(\"Falling back to Pandas processor\")\n",
    "            processor = PandasProcessor(config)\n",
    "    else:\n",
    "        logger.info(\n",
    "            f\"Using Pandas processor (rows={row_count:,} < \"\n",
    "            f\"threshold={config.large_data_threshold:,})\"\n",
    "        )\n",
    "        processor = PandasProcessor(config)\n",
    "\n",
    "    return processor, row_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Main Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(\n",
    "    input_path: Path,\n",
    "    output_path: Path,\n",
    "    config: PreprocessingConfig,\n",
    "    force_pandas: bool = False,\n",
    "    force_spark: bool = False,\n",
    ") -> ProcessingStats:\n",
    "    \"\"\"Run the full preprocessing pipeline.\n",
    "\n",
    "    Args:\n",
    "        input_path: Path to input JSONL file.\n",
    "        output_path: Path to output JSONL file.\n",
    "        config: Preprocessing configuration.\n",
    "        force_pandas: Force use of pandas processor.\n",
    "        force_spark: Force use of Spark processor.\n",
    "\n",
    "    Returns:\n",
    "        Processing statistics.\n",
    "    \"\"\"\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"Starting Data Preprocessing Pipeline\")\n",
    "    logger.info(\"=\" * 60)\n",
    "\n",
    "    # Create processor\n",
    "    processor, row_count = create_processor(\n",
    "        input_path, config, force_pandas, force_spark\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Process data\n",
    "        stats = processor.process(input_path, output_path)\n",
    "\n",
    "        # Print summary\n",
    "        stats.print_summary()\n",
    "\n",
    "        return stats\n",
    "\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        processor.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Execute Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output paths\n",
    "INPUT_FILE = config.input_dir / \"huggingface_synonym_pairs.jsonl\"\n",
    "OUTPUT_FILE = config.output_dir / \"combined_clean.jsonl\"\n",
    "\n",
    "print(f\"Input file: {INPUT_FILE}\")\n",
    "print(f\"Output file: {OUTPUT_FILE}\")\n",
    "print(f\"Input exists: {INPUT_FILE.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if input file exists\n",
    "if not INPUT_FILE.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Input file not found: {INPUT_FILE}\\n\"\n",
    "        f\"Please run 00_data_loading.ipynb first to generate the input data.\"\n",
    "    )\n",
    "\n",
    "# Run preprocessing\n",
    "# Set force_pandas=True to always use pandas (for testing)\n",
    "# Set force_spark=True to always use Spark (requires EMR connection)\n",
    "stats = preprocess_data(\n",
    "    input_path=INPUT_FILE,\n",
    "    output_path=OUTPUT_FILE,\n",
    "    config=config,\n",
    "    force_pandas=False,  # Change to True for local testing\n",
    "    force_spark=False,   # Change to True to force Spark\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Verify Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and inspect output\n",
    "if OUTPUT_FILE.exists():\n",
    "    df_output = pd.read_json(OUTPUT_FILE, lines=True)\n",
    "\n",
    "    print(f\"Output file: {OUTPUT_FILE}\")\n",
    "    print(f\"Total rows: {len(df_output):,}\")\n",
    "    print(f\"Columns: {list(df_output.columns)}\")\n",
    "    print(f\"\\nFile size: {OUTPUT_FILE.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "    print(\"\\nSample rows:\")\n",
    "    display(df_output.head(5))\n",
    "else:\n",
    "    print(f\"Output file not found: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics by pair type\n",
    "if OUTPUT_FILE.exists() and \"pair_type\" in df_output.columns:\n",
    "    print(\"\\nDistribution by pair_type:\")\n",
    "    pair_type_counts = df_output[\"pair_type\"].value_counts()\n",
    "    for pair_type, count in pair_type_counts.items():\n",
    "        pct = count / len(df_output) * 100\n",
    "        print(f\"  {pair_type:<25}: {count:>8,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Korean content quality\n",
    "if OUTPUT_FILE.exists():\n",
    "    text_cols = [\"source\", \"target\"] if \"source\" in df_output.columns else df_output.columns[:2].tolist()\n",
    "\n",
    "    print(\"\\nKorean content ratio verification:\")\n",
    "    for col in text_cols:\n",
    "        if col in df_output.columns:\n",
    "            ratios = df_output[col].apply(calculate_korean_ratio)\n",
    "            print(f\"  {col}:\")\n",
    "            print(f\"    Min: {ratios.min():.2f}\")\n",
    "            print(f\"    Max: {ratios.max():.2f}\")\n",
    "            print(f\"    Mean: {ratios.mean():.2f}\")\n",
    "            print(f\"    Median: {ratios.median():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distribution\n",
    "if OUTPUT_FILE.exists():\n",
    "    print(\"\\nText length distribution:\")\n",
    "    for col in text_cols:\n",
    "        if col in df_output.columns:\n",
    "            lengths = df_output[col].str.len()\n",
    "            print(f\"  {col}:\")\n",
    "            print(f\"    Min: {lengths.min()}\")\n",
    "            print(f\"    Max: {lengths.max()}\")\n",
    "            print(f\"    Mean: {lengths.mean():.1f}\")\n",
    "            print(f\"    Median: {lengths.median():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"v22.1 Data Preprocessing Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Large data threshold: {config.large_data_threshold:,} rows\")\n",
    "print(f\"  Min text length: {config.min_text_length} chars\")\n",
    "print(f\"  Max text length: {config.max_text_length} chars\")\n",
    "print(f\"  Min Korean ratio: {config.min_korean_ratio * 100:.0f}%\")\n",
    "\n",
    "if OUTPUT_FILE.exists():\n",
    "    print(f\"\\nOutput:\")\n",
    "    print(f\"  File: {OUTPUT_FILE}\")\n",
    "    print(f\"  Rows: {len(df_output):,}\")\n",
    "    print(f\"  Size: {OUTPUT_FILE.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "    if S3_BUCKET_NAME:\n",
    "        s3_path = f\"s3://{S3_BUCKET_NAME}/{config.s3_output_prefix}\"\n",
    "        print(f\"  S3 path: {s3_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run `02_data_augmentation.ipynb` to augment the preprocessed data\n",
    "2. The preprocessed data will be used as input for training triplet generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

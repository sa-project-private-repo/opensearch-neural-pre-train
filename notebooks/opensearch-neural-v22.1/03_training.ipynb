{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# v22.1 Training with SPLADELossV23\n",
    "\n",
    "## Key Improvements over v22.0\n",
    "\n",
    "1. **IDF-Aware FLOPS Penalty**: Preserves informative high-IDF tokens while penalizing stopwords\n",
    "2. **Knowledge Distillation**: Dense teacher model guides sparse student learning\n",
    "3. **Curriculum Learning**: 3 phases with decreasing KD weight for student independence\n",
    "4. **Fixed Hyperparameters**: Balanced loss weights based on expert recommendations\n",
    "\n",
    "## Curriculum Phases\n",
    "\n",
    "| Phase | Epochs | Temperature | lambda_kd | Focus |\n",
    "|-------|--------|-------------|-----------|-------|\n",
    "| 1 | 1-7 | 0.08 | 1.5 | Foundation with teacher guidance |\n",
    "| 2 | 8-14 | 0.05 | 1.0 | Balanced multi-type training |\n",
    "| 3 | 15-20 | 0.04 | 0.5 | Hard negative refinement |\n",
    "\n",
    "## Target Metrics\n",
    "\n",
    "| Metric | v22.0 | v22.1 Target |\n",
    "|--------|-------|-------------|\n",
    "| Recall@1 | ~70% | 80%+ |\n",
    "| MRR | ~0.75 | 0.85+ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_project_root() -> Path:\n",
    "    \"\"\"Find the project root directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Import v23 loss functions\n",
    "from src.model import (\n",
    "    SPLADEDoc,\n",
    "    SPLADEDocExpansion,\n",
    "    SPLADELossV23,\n",
    "    IDFAwareFLOPSLoss,\n",
    "    DenseTeacherScorer,\n",
    ")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CurriculumPhaseV23:\n",
    "    \"\"\"Configuration for v22.1 curriculum learning phase.\"\"\"\n",
    "    name: str\n",
    "    start_epoch: int\n",
    "    end_epoch: int\n",
    "    # Temperature for InfoNCE\n",
    "    temperature: float\n",
    "    # Knowledge distillation weight\n",
    "    lambda_kd: float\n",
    "    # Learning rate multiplier\n",
    "    lr_multiplier: float\n",
    "    # Data file for this phase\n",
    "    data_file: str\n",
    "    # Description\n",
    "    description: str = \"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfigV23:\n",
    "    \"\"\"Training configuration for v22.1 with SPLADELossV23.\"\"\"\n",
    "    # Model\n",
    "    model_name: str = \"skt/kobert-base-v1\"\n",
    "    max_length: int = 128\n",
    "    use_expansion: bool = True\n",
    "    expansion_mode: str = \"mlm\"\n",
    "    \n",
    "    # Training\n",
    "    num_epochs: int = 20\n",
    "    batch_size: int = 64\n",
    "    learning_rate: float = 2e-5\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    \n",
    "    # Mixed precision\n",
    "    mixed_precision: str = \"bf16\"  # \"bf16\", \"fp16\", or \"no\"\n",
    "    \n",
    "    # Loss weights (expert recommended)\n",
    "    lambda_infonce: float = 2.5\n",
    "    lambda_self: float = 1.0\n",
    "    lambda_positive: float = 3.0\n",
    "    lambda_margin: float = 0.0  # Disabled (redundant with InfoNCE)\n",
    "    lambda_flops: float = 0.003\n",
    "    lambda_min_act: float = 1.0\n",
    "    lambda_kd: float = 1.0  # Base KD weight (adjusted per phase)\n",
    "    \n",
    "    # Loss hyperparameters\n",
    "    temperature: float = 0.08\n",
    "    margin: float = 0.3\n",
    "    top_k: int = 5\n",
    "    min_activation: float = 0.5\n",
    "    kd_temperature: float = 2.0\n",
    "    \n",
    "    # IDF configuration\n",
    "    use_idf_weighting: bool = True\n",
    "    idf_alpha: float = 2.0\n",
    "    \n",
    "    # Knowledge distillation\n",
    "    teacher_model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    kd_warmup_epochs: int = 0  # Start KD from epoch 1\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_every_n_epochs: int = 5\n",
    "    keep_last_n_checkpoints: int = 3\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping_patience: int = 5\n",
    "    early_stopping_min_delta: float = 0.001\n",
    "    \n",
    "    # Logging\n",
    "    log_every_n_steps: int = 50\n",
    "    \n",
    "    # Curriculum phases\n",
    "    phases: List[CurriculumPhaseV23] = field(default_factory=list)\n",
    "    \n",
    "    # Paths\n",
    "    data_dir: Path = None\n",
    "    output_dir: Path = None\n",
    "    idf_weights_path: Path = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.phases:\n",
    "            # v22.1 Curriculum: Temperature annealing + KD weight decay\n",
    "            self.phases = [\n",
    "                CurriculumPhaseV23(\n",
    "                    name=\"phase1_foundation\",\n",
    "                    start_epoch=1,\n",
    "                    end_epoch=7,\n",
    "                    temperature=0.08,\n",
    "                    lambda_kd=1.5,  # Strong teacher guidance\n",
    "                    lr_multiplier=1.0,\n",
    "                    data_file=\"phase1_single_term_focus_triplets.jsonl\",\n",
    "                    description=\"Foundation with strong teacher guidance\",\n",
    "                ),\n",
    "                CurriculumPhaseV23(\n",
    "                    name=\"phase2_balanced\",\n",
    "                    start_epoch=8,\n",
    "                    end_epoch=14,\n",
    "                    temperature=0.05,\n",
    "                    lambda_kd=1.0,  # Balanced teacher guidance\n",
    "                    lr_multiplier=0.5,\n",
    "                    data_file=\"phase2_balanced_triplets.jsonl\",\n",
    "                    description=\"Balanced multi-type training\",\n",
    "                ),\n",
    "                CurriculumPhaseV23(\n",
    "                    name=\"phase3_refinement\",\n",
    "                    start_epoch=15,\n",
    "                    end_epoch=20,\n",
    "                    temperature=0.04,\n",
    "                    lambda_kd=0.5,  # Reduced teacher guidance\n",
    "                    lr_multiplier=0.25,\n",
    "                    data_file=\"phase3_full_triplets.jsonl\",\n",
    "                    description=\"Hard negative refinement with student independence\",\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "\n",
    "# Create configuration\n",
    "config = TrainingConfigV23(\n",
    "    data_dir=PROJECT_ROOT / \"data\" / \"v22.1\",\n",
    "    output_dir=PROJECT_ROOT / \"outputs\" / \"v22.1\",\n",
    "    idf_weights_path=PROJECT_ROOT / \"data\" / \"v22.1\" / \"idf_weights.pt\",\n",
    ")\n",
    "config.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"v22.1 Configuration (SPLADELossV23):\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Max length: {config.max_length}\")\n",
    "print(f\"  Epochs: {config.num_epochs}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Mixed precision: {config.mixed_precision}\")\n",
    "print(f\"\\nLoss Weights:\")\n",
    "print(f\"  lambda_infonce: {config.lambda_infonce}\")\n",
    "print(f\"  lambda_self: {config.lambda_self}\")\n",
    "print(f\"  lambda_positive: {config.lambda_positive}\")\n",
    "print(f\"  lambda_flops: {config.lambda_flops}\")\n",
    "print(f\"  lambda_min_act: {config.lambda_min_act}\")\n",
    "print(f\"  lambda_kd (base): {config.lambda_kd}\")\n",
    "print(f\"\\nIDF Configuration:\")\n",
    "print(f\"  use_idf_weighting: {config.use_idf_weighting}\")\n",
    "print(f\"  idf_alpha: {config.idf_alpha}\")\n",
    "print(f\"\\nTeacher Model: {config.teacher_model_name}\")\n",
    "print(f\"\\nCurriculum Phases:\")\n",
    "for phase in config.phases:\n",
    "    print(f\"  {phase.name}: epochs {phase.start_epoch}-{phase.end_epoch}\")\n",
    "    print(f\"    temp={phase.temperature}, lambda_kd={phase.lambda_kd}, lr_mult={phase.lr_multiplier}\")\n",
    "    print(f\"    {phase.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 2. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPLADEModelV23(nn.Module):\n",
    "    \"\"\"\n",
    "    SPLADE model for v22.1 with vocabulary expansion support.\n",
    "    \n",
    "    Uses MLM head to enable activation of any vocabulary token,\n",
    "    not just input tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"skt/kobert-base-v1\",\n",
    "        use_expansion: bool = True,\n",
    "        expansion_mode: str = \"mlm\",\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_expansion = use_expansion\n",
    "        self.expansion_mode = expansion_mode\n",
    "        \n",
    "        if use_expansion and expansion_mode == \"mlm\":\n",
    "            self.model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "            self.config = self.model.config\n",
    "        else:\n",
    "            from transformers import AutoModel\n",
    "            self.model = AutoModel.from_pretrained(model_name)\n",
    "            self.config = self.model.config\n",
    "            # Token importance predictor for non-expansion mode\n",
    "            self.token_importance = nn.Sequential(\n",
    "                nn.Linear(self.config.hidden_size, self.config.hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(self.config.hidden_size, 1),\n",
    "            )\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Enable gradient checkpointing for memory efficiency\n",
    "        if hasattr(self.model, \"gradient_checkpointing_enable\"):\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len]\n",
    "            attention_mask: [batch_size, seq_len]\n",
    "        \n",
    "        Returns:\n",
    "            sparse_repr: [batch_size, vocab_size]\n",
    "            token_weights: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        if self.use_expansion and self.expansion_mode == \"mlm\":\n",
    "            # MLM-based expansion: logits over full vocabulary\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            logits = outputs.logits  # [batch, seq_len, vocab_size]\n",
    "            \n",
    "            # SPLADE transformation: log(1 + ReLU(x))\n",
    "            token_scores = torch.log1p(self.relu(logits))\n",
    "            \n",
    "            # Apply attention mask\n",
    "            mask = attention_mask.unsqueeze(-1).float()\n",
    "            token_scores = token_scores * mask\n",
    "            \n",
    "            # Max pooling over sequence length\n",
    "            sparse_repr, _ = token_scores.max(dim=1)\n",
    "            \n",
    "            # Token weights for analysis\n",
    "            token_weights = token_scores.max(dim=-1).values\n",
    "        else:\n",
    "            # Standard SPLADE: only input tokens activated\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            \n",
    "            # Predict token importance\n",
    "            importance_scores = self.token_importance(hidden_states).squeeze(-1)\n",
    "            token_weights = torch.log1p(self.relu(importance_scores))\n",
    "            token_weights = token_weights * attention_mask.float()\n",
    "            \n",
    "            # Create sparse representation\n",
    "            batch_size, seq_len = input_ids.shape\n",
    "            vocab_size = self.config.vocab_size\n",
    "            \n",
    "            one_hot = torch.zeros(\n",
    "                batch_size, seq_len, vocab_size,\n",
    "                device=input_ids.device,\n",
    "                dtype=token_weights.dtype\n",
    "            ).scatter(2, input_ids.unsqueeze(-1), 1)\n",
    "            \n",
    "            mask = attention_mask.unsqueeze(-1).float()\n",
    "            masked_weights = token_weights.unsqueeze(-1) * one_hot * mask\n",
    "            sparse_repr, _ = masked_weights.max(dim=1)\n",
    "        \n",
    "        return sparse_repr, token_weights\n",
    "    \n",
    "    def encode_documents(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Encode documents to sparse representations.\"\"\"\n",
    "        sparse_repr, _ = self.forward(input_ids, attention_mask)\n",
    "        return sparse_repr\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SPLADEModelV23(\n",
    "    model_name=config.model_name,\n",
    "    use_expansion=config.use_expansion,\n",
    "    expansion_mode=config.expansion_mode,\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "print(f\"Model loaded: {config.model_name}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Vocab size: {model.config.vocab_size:,}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"Use expansion: {config.use_expansion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "idf-header",
   "metadata": {},
   "source": [
    "## 3. Load IDF Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "idf-weights",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_idf_weights(\n",
    "    idf_path: Path,\n",
    "    vocab_size: int,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    corpus_path: Optional[Path] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load pre-computed IDF weights or create default ones.\n",
    "    \n",
    "    Args:\n",
    "        idf_path: Path to saved IDF weights\n",
    "        vocab_size: Vocabulary size\n",
    "        tokenizer: Tokenizer for corpus processing\n",
    "        corpus_path: Optional path to corpus for IDF computation\n",
    "    \n",
    "    Returns:\n",
    "        IDF weights tensor [vocab_size]\n",
    "    \"\"\"\n",
    "    if idf_path.exists():\n",
    "        print(f\"Loading IDF weights from {idf_path}\")\n",
    "        idf_weights = torch.load(idf_path, map_location=\"cpu\")\n",
    "        if idf_weights.shape[0] != vocab_size:\n",
    "            print(f\"Warning: IDF weights size mismatch. Expected {vocab_size}, got {idf_weights.shape[0]}\")\n",
    "            print(\"Creating default uniform weights...\")\n",
    "            idf_weights = torch.ones(vocab_size)\n",
    "    elif corpus_path is not None and corpus_path.exists():\n",
    "        print(f\"Computing IDF weights from corpus: {corpus_path}\")\n",
    "        # Load corpus\n",
    "        corpus = []\n",
    "        with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                if \"text\" in data:\n",
    "                    corpus.append(data[\"text\"])\n",
    "                elif \"anchor\" in data:\n",
    "                    corpus.append(data[\"anchor\"])\n",
    "                    if \"positive\" in data:\n",
    "                        corpus.append(data[\"positive\"])\n",
    "        \n",
    "        print(f\"Corpus size: {len(corpus):,} documents\")\n",
    "        \n",
    "        # Compute IDF\n",
    "        idf_weights = IDFAwareFLOPSLoss.compute_idf_from_corpus(\n",
    "            corpus=corpus,\n",
    "            tokenizer=tokenizer,\n",
    "            smoothing=\"bm25\",\n",
    "        )\n",
    "        \n",
    "        # Save for future use\n",
    "        torch.save(idf_weights, idf_path)\n",
    "        print(f\"Saved IDF weights to {idf_path}\")\n",
    "    else:\n",
    "        print(\"No IDF weights found. Creating default uniform weights...\")\n",
    "        print(\"Note: For better results, pre-compute IDF weights from your corpus.\")\n",
    "        idf_weights = torch.ones(vocab_size)\n",
    "    \n",
    "    return idf_weights\n",
    "\n",
    "\n",
    "# Load IDF weights\n",
    "if config.use_idf_weighting:\n",
    "    idf_weights = load_or_create_idf_weights(\n",
    "        idf_path=config.idf_weights_path,\n",
    "        vocab_size=model.config.vocab_size,\n",
    "        tokenizer=tokenizer,\n",
    "        corpus_path=config.data_dir / \"corpus.jsonl\" if config.data_dir.exists() else None,\n",
    "    )\n",
    "    idf_weights = idf_weights.to(device)\n",
    "    \n",
    "    # Analyze IDF distribution\n",
    "    print(f\"\\nIDF Statistics:\")\n",
    "    print(f\"  Min: {idf_weights.min().item():.4f}\")\n",
    "    print(f\"  Max: {idf_weights.max().item():.4f}\")\n",
    "    print(f\"  Mean: {idf_weights.mean().item():.4f}\")\n",
    "    print(f\"  Std: {idf_weights.std().item():.4f}\")\n",
    "else:\n",
    "    idf_weights = None\n",
    "    print(\"IDF weighting disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "teacher-header",
   "metadata": {},
   "source": [
    "## 4. Initialize Dense Teacher for Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "teacher",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dense teacher model for knowledge distillation\n",
    "print(f\"Initializing dense teacher model: {config.teacher_model_name}\")\n",
    "teacher_model = DenseTeacherScorer(\n",
    "    model_name=config.teacher_model_name,\n",
    "    device=str(device),\n",
    ")\n",
    "\n",
    "# Test teacher model\n",
    "test_texts = [\"machine learning\", \"deep learning\", \"natural language processing\"]\n",
    "with torch.no_grad():\n",
    "    test_scores = teacher_model.compute_scores(test_texts[:2], test_texts)\n",
    "    print(f\"\\nTeacher model test (similarity matrix):\")\n",
    "    print(test_scores)\n",
    "\n",
    "print(f\"\\nDense teacher model ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loss-header",
   "metadata": {},
   "source": [
    "## 5. Initialize SPLADELossV23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SPLADELossV23 with all components\n",
    "criterion = SPLADELossV23(\n",
    "    # Loss weights\n",
    "    lambda_infonce=config.lambda_infonce,\n",
    "    lambda_self=config.lambda_self,\n",
    "    lambda_positive=config.lambda_positive,\n",
    "    lambda_margin=config.lambda_margin,\n",
    "    lambda_flops=config.lambda_flops,\n",
    "    lambda_min_act=config.lambda_min_act,\n",
    "    lambda_kd=config.lambda_kd,\n",
    "    # Hyperparameters\n",
    "    temperature=config.temperature,\n",
    "    margin=config.margin,\n",
    "    top_k=config.top_k,\n",
    "    min_activation=config.min_activation,\n",
    "    kd_temperature=config.kd_temperature,\n",
    "    # IDF configuration\n",
    "    vocab_size=model.config.vocab_size,\n",
    "    idf_weights=idf_weights,\n",
    "    idf_alpha=config.idf_alpha,\n",
    "    # Teacher model\n",
    "    teacher_model=teacher_model,\n",
    ")\n",
    "\n",
    "print(\"SPLADELossV23 initialized with:\")\n",
    "print(f\"  InfoNCE (lambda={config.lambda_infonce}, temp={config.temperature})\")\n",
    "print(f\"  Self-reconstruction (lambda={config.lambda_self})\")\n",
    "print(f\"  Positive activation (lambda={config.lambda_positive})\")\n",
    "print(f\"  Triplet margin (lambda={config.lambda_margin}, margin={config.margin})\")\n",
    "print(f\"  IDF-aware FLOPS (lambda={config.lambda_flops}, alpha={config.idf_alpha})\")\n",
    "print(f\"  Minimum activation (lambda={config.lambda_min_act})\")\n",
    "print(f\"  Knowledge distillation (lambda={config.lambda_kd}, temp={config.kd_temperature})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "## 6. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDatasetV23(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for triplet training with text preservation for KD.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: Path,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        max_length: int = 128,\n",
    "        return_raw_text: bool = True,\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.return_raw_text = return_raw_text\n",
    "        self.data = []\n",
    "        \n",
    "        if not data_path.exists():\n",
    "            print(f\"Warning: Data file not found: {data_path}\")\n",
    "            return\n",
    "        \n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "        \n",
    "        print(f\"Loaded {len(self.data):,} triplets from {data_path.name}\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        anchor_text = item[\"anchor\"]\n",
    "        positive_text = item[\"positive\"]\n",
    "        negative_text = item[\"negative\"]\n",
    "        \n",
    "        # Tokenize\n",
    "        anchor = self.tokenizer(\n",
    "            anchor_text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        positive = self.tokenizer(\n",
    "            positive_text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        negative = self.tokenizer(\n",
    "            negative_text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            \"anchor_input_ids\": anchor[\"input_ids\"].squeeze(0),\n",
    "            \"anchor_attention_mask\": anchor[\"attention_mask\"].squeeze(0),\n",
    "            \"positive_input_ids\": positive[\"input_ids\"].squeeze(0),\n",
    "            \"positive_attention_mask\": positive[\"attention_mask\"].squeeze(0),\n",
    "            \"negative_input_ids\": negative[\"input_ids\"].squeeze(0),\n",
    "            \"negative_attention_mask\": negative[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "        \n",
    "        if self.return_raw_text:\n",
    "            result[\"anchor_text\"] = anchor_text\n",
    "            result[\"positive_text\"] = positive_text\n",
    "            result[\"negative_text\"] = negative_text\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "def collate_fn_with_text(batch: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"Custom collate function that handles text fields.\"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # Stack tensor fields\n",
    "    tensor_keys = [\n",
    "        \"anchor_input_ids\", \"anchor_attention_mask\",\n",
    "        \"positive_input_ids\", \"positive_attention_mask\",\n",
    "        \"negative_input_ids\", \"negative_attention_mask\",\n",
    "    ]\n",
    "    for key in tensor_keys:\n",
    "        result[key] = torch.stack([item[key] for item in batch])\n",
    "    \n",
    "    # Collect text fields as lists\n",
    "    text_keys = [\"anchor_text\", \"positive_text\", \"negative_text\"]\n",
    "    for key in text_keys:\n",
    "        if key in batch[0]:\n",
    "            result[key] = [item[key] for item in batch]\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Load validation data\n",
    "val_data_path = config.data_dir / \"validation_triplets.jsonl\"\n",
    "if val_data_path.exists():\n",
    "    val_dataset = TripletDatasetV23(\n",
    "        val_data_path,\n",
    "        tokenizer,\n",
    "        config.max_length,\n",
    "        return_raw_text=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn_with_text,\n",
    "    )\n",
    "    print(f\"Validation loader: {len(val_loader)} batches\")\n",
    "else:\n",
    "    val_loader = None\n",
    "    print(\"Warning: Validation data not found. Skipping validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-loop-header",
   "metadata": {},
   "source": [
    "## 7. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_v23(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: Any,\n",
    "    criterion: SPLADELossV23,\n",
    "    config: TrainingConfigV23,\n",
    "    phase: CurriculumPhaseV23,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    scaler: Optional[GradScaler] = None,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Train for one epoch with SPLADELossV23.\n",
    "    \n",
    "    Args:\n",
    "        model: SPLADE model\n",
    "        dataloader: Training data loader\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Learning rate scheduler\n",
    "        criterion: SPLADELossV23 instance\n",
    "        config: Training configuration\n",
    "        phase: Current curriculum phase\n",
    "        device: Device to use\n",
    "        epoch: Current epoch number\n",
    "        scaler: Gradient scaler for mixed precision\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of average loss values\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    loss_components = defaultdict(float)\n",
    "    num_batches = 0\n",
    "    \n",
    "    # Update criterion with phase-specific settings\n",
    "    criterion.update_temperature(phase.temperature)\n",
    "    criterion.update_weights(lambda_kd=phase.lambda_kd)\n",
    "    \n",
    "    # Determine dtype for mixed precision\n",
    "    use_amp = config.mixed_precision in [\"bf16\", \"fp16\"]\n",
    "    amp_dtype = torch.bfloat16 if config.mixed_precision == \"bf16\" else torch.float16\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch} ({phase.name})\")\n",
    "    \n",
    "    for step, batch in enumerate(pbar):\n",
    "        # Move tensors to device\n",
    "        batch_tensors = {\n",
    "            k: v.to(device) for k, v in batch.items()\n",
    "            if isinstance(v, torch.Tensor)\n",
    "        }\n",
    "        \n",
    "        # Get raw texts for KD\n",
    "        anchor_texts = batch.get(\"anchor_text\")\n",
    "        positive_texts = batch.get(\"positive_text\")\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        with autocast(enabled=use_amp, dtype=amp_dtype):\n",
    "            # Compute sparse representations\n",
    "            anchor_repr, _ = model(\n",
    "                batch_tensors[\"anchor_input_ids\"],\n",
    "                batch_tensors[\"anchor_attention_mask\"],\n",
    "            )\n",
    "            positive_repr, _ = model(\n",
    "                batch_tensors[\"positive_input_ids\"],\n",
    "                batch_tensors[\"positive_attention_mask\"],\n",
    "            )\n",
    "            negative_repr, _ = model(\n",
    "                batch_tensors[\"negative_input_ids\"],\n",
    "                batch_tensors[\"negative_attention_mask\"],\n",
    "            )\n",
    "            \n",
    "            # Compute loss\n",
    "            loss, loss_dict = criterion(\n",
    "                anchor_repr=anchor_repr,\n",
    "                positive_repr=positive_repr,\n",
    "                negative_repr=negative_repr,\n",
    "                anchor_input_ids=batch_tensors[\"anchor_input_ids\"],\n",
    "                anchor_attention_mask=batch_tensors[\"anchor_attention_mask\"],\n",
    "                positive_input_ids=batch_tensors[\"positive_input_ids\"],\n",
    "                positive_attention_mask=batch_tensors[\"positive_attention_mask\"],\n",
    "                anchor_texts=anchor_texts,\n",
    "                positive_texts=positive_texts,\n",
    "            )\n",
    "        \n",
    "        # Backward pass\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "        \n",
    "        # Track losses\n",
    "        total_loss += loss_dict[\"total\"]\n",
    "        for key, value in loss_dict.items():\n",
    "            loss_components[key] += value\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        if step % config.log_every_n_steps == 0:\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{loss_dict['total']:.4f}\",\n",
    "                \"infonce\": f\"{loss_dict['infonce']:.4f}\",\n",
    "                \"kd\": f\"{loss_dict['kd']:.4f}\",\n",
    "                \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\",\n",
    "            })\n",
    "    \n",
    "    # Compute averages\n",
    "    avg_losses = {k: v / num_batches for k, v in loss_components.items()}\n",
    "    avg_losses[\"total\"] = total_loss / num_batches\n",
    "    \n",
    "    return avg_losses\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_v23(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: SPLADELossV23,\n",
    "    device: torch.device,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate model on validation set.\n",
    "    \n",
    "    Args:\n",
    "        model: SPLADE model\n",
    "        dataloader: Validation data loader\n",
    "        criterion: SPLADELossV23 instance\n",
    "        device: Device to use\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    loss_components = defaultdict(float)\n",
    "    all_recalls = []\n",
    "    all_mrrs = []\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        # Move tensors to device\n",
    "        batch_tensors = {\n",
    "            k: v.to(device) for k, v in batch.items()\n",
    "            if isinstance(v, torch.Tensor)\n",
    "        }\n",
    "        \n",
    "        anchor_texts = batch.get(\"anchor_text\")\n",
    "        positive_texts = batch.get(\"positive_text\")\n",
    "        \n",
    "        # Forward pass\n",
    "        anchor_repr, _ = model(\n",
    "            batch_tensors[\"anchor_input_ids\"],\n",
    "            batch_tensors[\"anchor_attention_mask\"],\n",
    "        )\n",
    "        positive_repr, _ = model(\n",
    "            batch_tensors[\"positive_input_ids\"],\n",
    "            batch_tensors[\"positive_attention_mask\"],\n",
    "        )\n",
    "        negative_repr, _ = model(\n",
    "            batch_tensors[\"negative_input_ids\"],\n",
    "            batch_tensors[\"negative_attention_mask\"],\n",
    "        )\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, loss_dict = criterion(\n",
    "            anchor_repr=anchor_repr,\n",
    "            positive_repr=positive_repr,\n",
    "            negative_repr=negative_repr,\n",
    "            anchor_input_ids=batch_tensors[\"anchor_input_ids\"],\n",
    "            anchor_attention_mask=batch_tensors[\"anchor_attention_mask\"],\n",
    "            positive_input_ids=batch_tensors[\"positive_input_ids\"],\n",
    "            positive_attention_mask=batch_tensors[\"positive_attention_mask\"],\n",
    "            anchor_texts=anchor_texts,\n",
    "            positive_texts=positive_texts,\n",
    "        )\n",
    "        \n",
    "        total_loss += loss_dict[\"total\"]\n",
    "        for key, value in loss_dict.items():\n",
    "            loss_components[key] += value\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Compute retrieval metrics\n",
    "        pos_sim = F.cosine_similarity(anchor_repr, positive_repr, dim=-1)\n",
    "        neg_sim = F.cosine_similarity(anchor_repr, negative_repr, dim=-1)\n",
    "        \n",
    "        # Recall: positive should rank higher than negative\n",
    "        recalls = (pos_sim > neg_sim).float()\n",
    "        all_recalls.extend(recalls.cpu().tolist())\n",
    "        \n",
    "        # MRR\n",
    "        for i in range(len(pos_sim)):\n",
    "            if pos_sim[i] > neg_sim[i]:\n",
    "                all_mrrs.append(1.0)\n",
    "            else:\n",
    "                all_mrrs.append(0.5)\n",
    "    \n",
    "    # Compute averages\n",
    "    avg_losses = {k: v / num_batches for k, v in loss_components.items()}\n",
    "    avg_losses[\"total\"] = total_loss / num_batches\n",
    "    \n",
    "    return {\n",
    "        **avg_losses,\n",
    "        \"recall\": np.mean(all_recalls) * 100,\n",
    "        \"mrr\": np.mean(all_mrrs),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curriculum-header",
   "metadata": {},
   "source": [
    "## 8. Curriculum Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curriculum-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping handler.\"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int = 5, min_delta: float = 0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.should_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss: float) -> bool:\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        return self.should_stop\n",
    "\n",
    "\n",
    "def run_curriculum_training_v23(\n",
    "    model: nn.Module,\n",
    "    config: TrainingConfigV23,\n",
    "    criterion: SPLADELossV23,\n",
    "    device: torch.device,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Run full curriculum training with SPLADELossV23.\n",
    "    \n",
    "    Args:\n",
    "        model: SPLADE model\n",
    "        config: Training configuration\n",
    "        criterion: SPLADELossV23 instance\n",
    "        device: Device to use\n",
    "    \n",
    "    Returns:\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    training_history = []\n",
    "    best_recall = 0.0\n",
    "    best_val_loss = float(\"inf\")\n",
    "    \n",
    "    # Initialize gradient scaler for mixed precision\n",
    "    use_amp = config.mixed_precision in [\"bf16\", \"fp16\"]\n",
    "    scaler = GradScaler() if use_amp and config.mixed_precision == \"fp16\" else None\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=config.early_stopping_patience,\n",
    "        min_delta=config.early_stopping_min_delta,\n",
    "    )\n",
    "    \n",
    "    for phase in config.phases:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"Starting {phase.name}: Epochs {phase.start_epoch}-{phase.end_epoch}\")\n",
    "        print(f\"Temperature: {phase.temperature}, lambda_kd: {phase.lambda_kd}\")\n",
    "        print(f\"LR multiplier: {phase.lr_multiplier}\")\n",
    "        print(f\"Description: {phase.description}\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "        \n",
    "        # Load phase-specific data\n",
    "        train_data_path = config.data_dir / phase.data_file\n",
    "        if not train_data_path.exists():\n",
    "            print(f\"Warning: Training data not found: {train_data_path}\")\n",
    "            print(\"Skipping this phase...\")\n",
    "            continue\n",
    "        \n",
    "        train_dataset = TripletDatasetV23(\n",
    "            train_data_path,\n",
    "            tokenizer,\n",
    "            config.max_length,\n",
    "            return_raw_text=True,\n",
    "        )\n",
    "        \n",
    "        if len(train_dataset) == 0:\n",
    "            print(f\"Warning: Empty training dataset for {phase.name}\")\n",
    "            continue\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            collate_fn=collate_fn_with_text,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer and scheduler for this phase\n",
    "        phase_epochs = phase.end_epoch - phase.start_epoch + 1\n",
    "        total_steps = len(train_loader) * phase_epochs\n",
    "        warmup_steps = int(total_steps * config.warmup_ratio)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config.learning_rate * phase.lr_multiplier,\n",
    "            weight_decay=config.weight_decay,\n",
    "        )\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps,\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nPhase training configuration:\")\n",
    "        print(f\"  Training samples: {len(train_dataset):,}\")\n",
    "        print(f\"  Batches per epoch: {len(train_loader):,}\")\n",
    "        print(f\"  Total steps: {total_steps:,}\")\n",
    "        print(f\"  Warmup steps: {warmup_steps:,}\")\n",
    "        print(f\"  Learning rate: {config.learning_rate * phase.lr_multiplier:.2e}\")\n",
    "        \n",
    "        # Train for phase epochs\n",
    "        for epoch in range(phase.start_epoch, phase.end_epoch + 1):\n",
    "            # Training\n",
    "            train_metrics = train_epoch_v23(\n",
    "                model=model,\n",
    "                dataloader=train_loader,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                criterion=criterion,\n",
    "                config=config,\n",
    "                phase=phase,\n",
    "                device=device,\n",
    "                epoch=epoch,\n",
    "                scaler=scaler,\n",
    "            )\n",
    "            \n",
    "            # Evaluation\n",
    "            if val_loader is not None:\n",
    "                eval_metrics = evaluate_v23(model, val_loader, criterion, device)\n",
    "            else:\n",
    "                eval_metrics = {\"recall\": 0.0, \"mrr\": 0.0, \"val_total\": 0.0}\n",
    "            \n",
    "            # Log results\n",
    "            print(f\"\\nEpoch {epoch}:\")\n",
    "            print(f\"  Train - Loss: {train_metrics['total']:.4f}, \"\n",
    "                  f\"InfoNCE: {train_metrics['infonce']:.4f}, \"\n",
    "                  f\"KD: {train_metrics['kd']:.4f}\")\n",
    "            print(f\"  Train - Self: {train_metrics['self']:.4f}, \"\n",
    "                  f\"Positive: {train_metrics['positive']:.4f}, \"\n",
    "                  f\"FLOPS: {train_metrics['flops']:.4f}\")\n",
    "            if val_loader is not None:\n",
    "                print(f\"  Val   - Loss: {eval_metrics['total']:.4f}, \"\n",
    "                      f\"Recall: {eval_metrics['recall']:.1f}%, \"\n",
    "                      f\"MRR: {eval_metrics['mrr']:.4f}\")\n",
    "            \n",
    "            # Save history\n",
    "            history_entry = {\n",
    "                \"epoch\": epoch,\n",
    "                \"phase\": phase.name,\n",
    "                \"temperature\": phase.temperature,\n",
    "                \"lambda_kd\": phase.lambda_kd,\n",
    "                \"lr\": scheduler.get_last_lr()[0],\n",
    "                \"train\": train_metrics,\n",
    "                \"eval\": eval_metrics,\n",
    "            }\n",
    "            training_history.append(history_entry)\n",
    "            \n",
    "            # Save best model\n",
    "            current_recall = eval_metrics.get(\"recall\", 0.0)\n",
    "            if current_recall > best_recall:\n",
    "                best_recall = current_recall\n",
    "                torch.save({\n",
    "                    \"epoch\": epoch,\n",
    "                    \"phase\": phase.name,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"eval_results\": eval_metrics,\n",
    "                    \"config\": {\n",
    "                        \"model_name\": config.model_name,\n",
    "                        \"max_length\": config.max_length,\n",
    "                        \"vocab_size\": model.config.vocab_size,\n",
    "                        \"use_expansion\": config.use_expansion,\n",
    "                        \"version\": \"v22.1\",\n",
    "                    },\n",
    "                }, config.output_dir / \"best_model.pt\")\n",
    "                print(f\"  -> New best model saved! Recall: {best_recall:.1f}%\")\n",
    "            \n",
    "            # Save periodic checkpoint\n",
    "            if epoch % config.save_every_n_epochs == 0:\n",
    "                checkpoint_path = config.output_dir / f\"checkpoint_epoch_{epoch}.pt\"\n",
    "                torch.save({\n",
    "                    \"epoch\": epoch,\n",
    "                    \"phase\": phase.name,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                    \"train_metrics\": train_metrics,\n",
    "                    \"eval_metrics\": eval_metrics,\n",
    "                }, checkpoint_path)\n",
    "                print(f\"  -> Checkpoint saved: {checkpoint_path.name}\")\n",
    "            \n",
    "            # Early stopping check\n",
    "            val_loss = eval_metrics.get(\"total\", train_metrics[\"total\"])\n",
    "            if early_stopping(val_loss):\n",
    "                print(f\"\\nEarly stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # Save phase checkpoint\n",
    "        phase_checkpoint_path = config.output_dir / f\"{phase.name}_checkpoint.pt\"\n",
    "        torch.save({\n",
    "            \"epoch\": phase.end_epoch,\n",
    "            \"phase\": phase.name,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "        }, phase_checkpoint_path)\n",
    "        print(f\"\\n{phase.name} checkpoint saved: {phase_checkpoint_path.name}\")\n",
    "        \n",
    "        # Reset early stopping for next phase\n",
    "        early_stopping = EarlyStopping(\n",
    "            patience=config.early_stopping_patience,\n",
    "            min_delta=config.early_stopping_min_delta,\n",
    "        )\n",
    "    \n",
    "    return training_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-training-header",
   "metadata": {},
   "source": [
    "## 9. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training summary before starting\n",
    "print(\"v22.1 Training Summary\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"Mixed precision: {config.mixed_precision}\")\n",
    "print(f\"\\nData distribution:\")\n",
    "for phase in config.phases:\n",
    "    data_path = config.data_dir / phase.data_file\n",
    "    if data_path.exists():\n",
    "        with open(data_path) as f:\n",
    "            count = sum(1 for _ in f)\n",
    "        print(f\"  {phase.name}: {count:,} triplets\")\n",
    "    else:\n",
    "        print(f\"  {phase.name}: FILE NOT FOUND\")\n",
    "\n",
    "print(f\"\\nStarting training...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Run training\n",
    "start_time = datetime.now()\n",
    "history = run_curriculum_training_v23(model, config, criterion, device)\n",
    "end_time = datetime.now()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Total time: {end_time - start_time}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 10. Training Summary and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if len(history) > 0:\n",
    "    # Extract data for plotting\n",
    "    epochs = [h[\"epoch\"] for h in history]\n",
    "    train_losses = [h[\"train\"][\"total\"] for h in history]\n",
    "    infonce_losses = [h[\"train\"][\"infonce\"] for h in history]\n",
    "    kd_losses = [h[\"train\"][\"kd\"] for h in history]\n",
    "    flops_losses = [h[\"train\"][\"flops\"] for h in history]\n",
    "    recalls = [h[\"eval\"][\"recall\"] for h in history]\n",
    "    mrrs = [h[\"eval\"][\"mrr\"] for h in history]\n",
    "    temperatures = [h[\"temperature\"] for h in history]\n",
    "    lambda_kds = [h[\"lambda_kd\"] for h in history]\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    \n",
    "    # Phase boundaries\n",
    "    phase_boundaries = [7.5, 14.5]  # Between phases\n",
    "    \n",
    "    # 1. Total Loss\n",
    "    axes[0, 0].plot(epochs, train_losses, 'b-', linewidth=2)\n",
    "    axes[0, 0].set_xlabel(\"Epoch\")\n",
    "    axes[0, 0].set_ylabel(\"Total Loss\")\n",
    "    axes[0, 0].set_title(\"Total Training Loss\")\n",
    "    for b in phase_boundaries:\n",
    "        axes[0, 0].axvline(x=b, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. InfoNCE Loss\n",
    "    axes[0, 1].plot(epochs, infonce_losses, 'purple', linewidth=2)\n",
    "    axes[0, 1].set_xlabel(\"Epoch\")\n",
    "    axes[0, 1].set_ylabel(\"InfoNCE Loss\")\n",
    "    axes[0, 1].set_title(\"InfoNCE Contrastive Loss\")\n",
    "    for b in phase_boundaries:\n",
    "        axes[0, 1].axvline(x=b, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Knowledge Distillation Loss\n",
    "    axes[0, 2].plot(epochs, kd_losses, 'orange', linewidth=2)\n",
    "    axes[0, 2].set_xlabel(\"Epoch\")\n",
    "    axes[0, 2].set_ylabel(\"KD Loss\")\n",
    "    axes[0, 2].set_title(\"Knowledge Distillation Loss\")\n",
    "    for b in phase_boundaries:\n",
    "        axes[0, 2].axvline(x=b, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Recall\n",
    "    axes[0, 3].plot(epochs, recalls, 'g-', linewidth=2, marker='o', markersize=4)\n",
    "    axes[0, 3].set_xlabel(\"Epoch\")\n",
    "    axes[0, 3].set_ylabel(\"Recall (%)\")\n",
    "    axes[0, 3].set_title(\"Validation Recall\")\n",
    "    for b in phase_boundaries:\n",
    "        axes[0, 3].axvline(x=b, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[0, 3].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. MRR\n",
    "    axes[1, 0].plot(epochs, mrrs, 'c-', linewidth=2, marker='o', markersize=4)\n",
    "    axes[1, 0].set_xlabel(\"Epoch\")\n",
    "    axes[1, 0].set_ylabel(\"MRR\")\n",
    "    axes[1, 0].set_title(\"Validation MRR\")\n",
    "    for b in phase_boundaries:\n",
    "        axes[1, 0].axvline(x=b, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. FLOPS Loss\n",
    "    axes[1, 1].plot(epochs, flops_losses, 'brown', linewidth=2)\n",
    "    axes[1, 1].set_xlabel(\"Epoch\")\n",
    "    axes[1, 1].set_ylabel(\"FLOPS Loss\")\n",
    "    axes[1, 1].set_title(\"IDF-Aware FLOPS Loss\")\n",
    "    for b in phase_boundaries:\n",
    "        axes[1, 1].axvline(x=b, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. Temperature Schedule\n",
    "    axes[1, 2].plot(epochs, temperatures, 'red', linewidth=2, marker='s', markersize=4)\n",
    "    axes[1, 2].set_xlabel(\"Epoch\")\n",
    "    axes[1, 2].set_ylabel(\"Temperature\")\n",
    "    axes[1, 2].set_title(\"Temperature Annealing\")\n",
    "    for b in phase_boundaries:\n",
    "        axes[1, 2].axvline(x=b, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 8. KD Weight Schedule\n",
    "    axes[1, 3].plot(epochs, lambda_kds, 'magenta', linewidth=2, marker='s', markersize=4)\n",
    "    axes[1, 3].set_xlabel(\"Epoch\")\n",
    "    axes[1, 3].set_ylabel(\"lambda_kd\")\n",
    "    axes[1, 3].set_title(\"Knowledge Distillation Weight\")\n",
    "    for b in phase_boundaries:\n",
    "        axes[1, 3].axvline(x=b, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[1, 3].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config.output_dir / \"training_curves_v22.1.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\nv22.1 Final Results:\")\n",
    "    print(f\"  Best Recall: {max(recalls):.1f}%\")\n",
    "    print(f\"  Best MRR: {max(mrrs):.4f}\")\n",
    "    print(f\"  Final Loss: {train_losses[-1]:.4f}\")\n",
    "    print(f\"  Final InfoNCE: {infonce_losses[-1]:.4f}\")\n",
    "    print(f\"  Final KD Loss: {kd_losses[-1]:.4f}\")\n",
    "else:\n",
    "    print(\"No training history to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## 11. Save Final Model and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final checkpoint\n",
    "if len(history) > 0:\n",
    "    final_checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"config\": {\n",
    "            \"model_name\": config.model_name,\n",
    "            \"max_length\": config.max_length,\n",
    "            \"vocab_size\": model.config.vocab_size,\n",
    "            \"hidden_size\": model.config.hidden_size,\n",
    "            \"use_expansion\": config.use_expansion,\n",
    "            \"expansion_mode\": config.expansion_mode,\n",
    "            \"version\": \"v22.1\",\n",
    "        },\n",
    "        \"loss_config\": {\n",
    "            \"lambda_infonce\": config.lambda_infonce,\n",
    "            \"lambda_self\": config.lambda_self,\n",
    "            \"lambda_positive\": config.lambda_positive,\n",
    "            \"lambda_margin\": config.lambda_margin,\n",
    "            \"lambda_flops\": config.lambda_flops,\n",
    "            \"lambda_min_act\": config.lambda_min_act,\n",
    "            \"lambda_kd\": config.lambda_kd,\n",
    "            \"use_idf_weighting\": config.use_idf_weighting,\n",
    "        },\n",
    "        \"training_info\": {\n",
    "            \"total_epochs\": len(history),\n",
    "            \"final_recall\": history[-1][\"eval\"][\"recall\"],\n",
    "            \"final_mrr\": history[-1][\"eval\"][\"mrr\"],\n",
    "            \"best_recall\": max(h[\"eval\"][\"recall\"] for h in history),\n",
    "            \"best_mrr\": max(h[\"eval\"][\"mrr\"] for h in history),\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    checkpoint_path = config.output_dir / \"checkpoint.pt\"\n",
    "    torch.save(final_checkpoint, checkpoint_path)\n",
    "    print(f\"Final checkpoint saved: {checkpoint_path}\")\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer_path = config.output_dir / \"tokenizer\"\n",
    "    tokenizer.save_pretrained(tokenizer_path)\n",
    "    print(f\"Tokenizer saved: {tokenizer_path}\")\n",
    "    \n",
    "    # Save training history\n",
    "    history_path = config.output_dir / \"training_history.json\"\n",
    "    with open(history_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(history, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Training history saved: {history_path}\")\n",
    "    \n",
    "    # Save IDF weights if used\n",
    "    if config.use_idf_weighting and idf_weights is not None:\n",
    "        idf_path = config.output_dir / \"idf_weights.pt\"\n",
    "        torch.save(idf_weights.cpu(), idf_path)\n",
    "        print(f\"IDF weights saved: {idf_path}\")\n",
    "else:\n",
    "    print(\"No training completed. Skipping model save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s3-header",
   "metadata": {},
   "source": [
    "## 12. Upload to S3 (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s3-upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Upload to S3\n",
    "UPLOAD_TO_S3 = False  # Set to True to enable S3 upload\n",
    "S3_BUCKET = \"your-bucket-name\"\n",
    "S3_PREFIX = \"models/opensearch-neural-sparse/v22.1\"\n",
    "\n",
    "if UPLOAD_TO_S3:\n",
    "    import boto3\n",
    "    from botocore.exceptions import ClientError\n",
    "    \n",
    "    s3_client = boto3.client(\"s3\", region_name=\"us-east-1\")\n",
    "    \n",
    "    files_to_upload = [\n",
    "        config.output_dir / \"checkpoint.pt\",\n",
    "        config.output_dir / \"best_model.pt\",\n",
    "        config.output_dir / \"training_history.json\",\n",
    "        config.output_dir / \"training_curves_v22.1.png\",\n",
    "    ]\n",
    "    \n",
    "    # Add tokenizer files\n",
    "    tokenizer_dir = config.output_dir / \"tokenizer\"\n",
    "    if tokenizer_dir.exists():\n",
    "        for file in tokenizer_dir.iterdir():\n",
    "            files_to_upload.append(file)\n",
    "    \n",
    "    print(f\"Uploading to s3://{S3_BUCKET}/{S3_PREFIX}/\")\n",
    "    \n",
    "    for file_path in files_to_upload:\n",
    "        if file_path.exists():\n",
    "            s3_key = f\"{S3_PREFIX}/{file_path.name}\"\n",
    "            try:\n",
    "                s3_client.upload_file(str(file_path), S3_BUCKET, s3_key)\n",
    "                print(f\"  Uploaded: {file_path.name}\")\n",
    "            except ClientError as e:\n",
    "                print(f\"  Failed to upload {file_path.name}: {e}\")\n",
    "    \n",
    "    print(\"\\nS3 upload complete.\")\n",
    "else:\n",
    "    print(\"S3 upload disabled. Set UPLOAD_TO_S3 = True to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Run Benchmark**: Test the trained model on benchmark datasets\n",
    "   ```bash\n",
    "   python benchmark/run_benchmark.py --model-path outputs/v22.1/checkpoint.pt\n",
    "   ```\n",
    "\n",
    "2. **Evaluate on Problem Terms**: Test specific Korean terms that were problematic before\n",
    "\n",
    "3. **Compare with v22.0**: Analyze improvements from IDF-aware FLOPS and knowledge distillation\n",
    "\n",
    "4. **Index to OpenSearch**: Deploy the model to OpenSearch cluster for production testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

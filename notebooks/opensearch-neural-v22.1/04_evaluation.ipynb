{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# v22.1 Comprehensive Model Evaluation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides comprehensive evaluation of the Neural Sparse v22.1 model by comparing three search methods:\n",
    "\n",
    "1. **BM25** - Traditional term-frequency baseline\n",
    "2. **Semantic Search** - Dense embeddings using BGE-M3\n",
    "3. **Neural Sparse v22.1** - Trained sparse retrieval model\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "- **NDCG@10** - Normalized Discounted Cumulative Gain at 10\n",
    "- **Recall@1, Recall@5, Recall@10** - Retrieval coverage at various cutoffs\n",
    "- **MRR** - Mean Reciprocal Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_project_root() -> Path:\n",
    "    \"\"\"Find project root directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \"src\").exists():\n",
    "            return parent\n",
    "    return Path.cwd().parent.parent\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Evaluation date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and data configuration\n",
    "MODEL_NAME = \"skt/A.X-Encoder-base\"\n",
    "SEMANTIC_MODEL_NAME = \"BAAI/bge-m3\"\n",
    "\n",
    "# Paths - adjust as needed for v22.1\n",
    "V22_1_MODEL_PATH = PROJECT_ROOT / \"outputs\" / \"v22.1\" / \"best_model.pt\"\n",
    "V22_0_MODEL_PATH = PROJECT_ROOT / \"outputs\" / \"v22.0_infonce\" / \"best_model.pt\"\n",
    "V21_4_MODEL_PATH = PROJECT_ROOT / \"outputs\" / \"v21.4_korean_enhanced\" / \"best_model.pt\"\n",
    "\n",
    "VALIDATION_DATA_PATH = PROJECT_ROOT / \"data\" / \"v22.0\" / \"validation_triplets.jsonl\"\n",
    "\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\" / \"v22.1\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Evaluation settings\n",
    "MAX_EVAL_SAMPLES = 500  # Number of queries to evaluate\n",
    "TOP_K_VALUES = [1, 5, 10]\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"\\nModel paths:\")\n",
    "print(f\"  v22.1: {V22_1_MODEL_PATH} (exists: {V22_1_MODEL_PATH.exists()})\")\n",
    "print(f\"  v22.0: {V22_0_MODEL_PATH} (exists: {V22_0_MODEL_PATH.exists()})\")\n",
    "print(f\"  v21.4: {V21_4_MODEL_PATH} (exists: {V21_4_MODEL_PATH.exists()})\")\n",
    "print(f\"\\nValidation data: {VALIDATION_DATA_PATH} (exists: {VALIDATION_DATA_PATH.exists()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Data Structures and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvalSample:\n",
    "    \"\"\"Single evaluation sample (query with ground truth).\"\"\"\n",
    "    query: str\n",
    "    positive_doc: str\n",
    "    negative_doc: str\n",
    "    doc_id: int = 0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    \"\"\"Result from a retrieval method for a single query.\"\"\"\n",
    "    query_id: int\n",
    "    retrieved_doc_ids: List[int]\n",
    "    scores: List[float]\n",
    "    ground_truth_id: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvaluationMetrics:\n",
    "    \"\"\"Aggregated evaluation metrics.\"\"\"\n",
    "    method_name: str\n",
    "    ndcg_at_10: float = 0.0\n",
    "    recall_at_1: float = 0.0\n",
    "    recall_at_5: float = 0.0\n",
    "    recall_at_10: float = 0.0\n",
    "    mrr: float = 0.0\n",
    "    per_query_results: List[Dict] = field(default_factory=list)\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary.\"\"\"\n",
    "        return {\n",
    "            \"method_name\": self.method_name,\n",
    "            \"ndcg_at_10\": self.ndcg_at_10,\n",
    "            \"recall_at_1\": self.recall_at_1,\n",
    "            \"recall_at_5\": self.recall_at_5,\n",
    "            \"recall_at_10\": self.recall_at_10,\n",
    "            \"mrr\": self.mrr,\n",
    "            \"num_queries\": len(self.per_query_results),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCalculator:\n",
    "    \"\"\"Calculate retrieval metrics.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def dcg_at_k(relevances: List[int], k: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Discounted Cumulative Gain at k.\n",
    "        \n",
    "        DCG@k = sum_{i=1}^{k} (2^{rel_i} - 1) / log_2(i + 1)\n",
    "        \"\"\"\n",
    "        relevances = relevances[:k]\n",
    "        dcg = 0.0\n",
    "        for i, rel in enumerate(relevances):\n",
    "            dcg += (2**rel - 1) / math.log2(i + 2)\n",
    "        return dcg\n",
    "    \n",
    "    @staticmethod\n",
    "    def ndcg_at_k(relevances: List[int], k: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Normalized Discounted Cumulative Gain at k.\n",
    "        \n",
    "        nDCG@k = DCG@k / IDCG@k\n",
    "        \"\"\"\n",
    "        dcg = MetricsCalculator.dcg_at_k(relevances, k)\n",
    "        # Ideal DCG: all relevant items at top\n",
    "        ideal_relevances = sorted(relevances, reverse=True)\n",
    "        idcg = MetricsCalculator.dcg_at_k(ideal_relevances, k)\n",
    "        if idcg == 0:\n",
    "            return 0.0\n",
    "        return dcg / idcg\n",
    "    \n",
    "    @staticmethod\n",
    "    def recall_at_k(\n",
    "        retrieved_ids: List[int],\n",
    "        relevant_ids: set,\n",
    "        k: int,\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate Recall at k.\"\"\"\n",
    "        if not relevant_ids:\n",
    "            return 0.0\n",
    "        retrieved_set = set(retrieved_ids[:k])\n",
    "        hits = len(retrieved_set & relevant_ids)\n",
    "        return hits / len(relevant_ids)\n",
    "    \n",
    "    @staticmethod\n",
    "    def reciprocal_rank(\n",
    "        retrieved_ids: List[int],\n",
    "        relevant_ids: set,\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate Reciprocal Rank (1/rank of first relevant item).\"\"\"\n",
    "        for rank, doc_id in enumerate(retrieved_ids, start=1):\n",
    "            if doc_id in relevant_ids:\n",
    "                return 1.0 / rank\n",
    "        return 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_all_metrics(\n",
    "        results: List[RetrievalResult],\n",
    "        method_name: str,\n",
    "    ) -> EvaluationMetrics:\n",
    "        \"\"\"Compute all metrics from retrieval results.\"\"\"\n",
    "        ndcg_10_scores = []\n",
    "        recall_1_scores = []\n",
    "        recall_5_scores = []\n",
    "        recall_10_scores = []\n",
    "        mrr_scores = []\n",
    "        per_query = []\n",
    "        \n",
    "        for result in results:\n",
    "            relevant_ids = {result.ground_truth_id}\n",
    "            \n",
    "            # Build relevance list (1 if relevant, 0 otherwise)\n",
    "            relevances = [\n",
    "                1 if doc_id in relevant_ids else 0\n",
    "                for doc_id in result.retrieved_doc_ids\n",
    "            ]\n",
    "            \n",
    "            ndcg_10 = MetricsCalculator.ndcg_at_k(relevances, 10)\n",
    "            recall_1 = MetricsCalculator.recall_at_k(\n",
    "                result.retrieved_doc_ids, relevant_ids, 1\n",
    "            )\n",
    "            recall_5 = MetricsCalculator.recall_at_k(\n",
    "                result.retrieved_doc_ids, relevant_ids, 5\n",
    "            )\n",
    "            recall_10 = MetricsCalculator.recall_at_k(\n",
    "                result.retrieved_doc_ids, relevant_ids, 10\n",
    "            )\n",
    "            rr = MetricsCalculator.reciprocal_rank(\n",
    "                result.retrieved_doc_ids, relevant_ids\n",
    "            )\n",
    "            \n",
    "            ndcg_10_scores.append(ndcg_10)\n",
    "            recall_1_scores.append(recall_1)\n",
    "            recall_5_scores.append(recall_5)\n",
    "            recall_10_scores.append(recall_10)\n",
    "            mrr_scores.append(rr)\n",
    "            \n",
    "            per_query.append({\n",
    "                \"query_id\": result.query_id,\n",
    "                \"ndcg@10\": ndcg_10,\n",
    "                \"recall@1\": recall_1,\n",
    "                \"recall@5\": recall_5,\n",
    "                \"recall@10\": recall_10,\n",
    "                \"mrr\": rr,\n",
    "                \"ground_truth_rank\": (\n",
    "                    result.retrieved_doc_ids.index(result.ground_truth_id) + 1\n",
    "                    if result.ground_truth_id in result.retrieved_doc_ids\n",
    "                    else -1\n",
    "                ),\n",
    "            })\n",
    "        \n",
    "        return EvaluationMetrics(\n",
    "            method_name=method_name,\n",
    "            ndcg_at_10=np.mean(ndcg_10_scores),\n",
    "            recall_at_1=np.mean(recall_1_scores),\n",
    "            recall_at_5=np.mean(recall_5_scores),\n",
    "            recall_at_10=np.mean(recall_10_scores),\n",
    "            mrr=np.mean(mrr_scores),\n",
    "            per_query_results=per_query,\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"MetricsCalculator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Load Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_validation_data(\n",
    "    path: Path,\n",
    "    max_samples: int = 500,\n",
    ") -> Tuple[List[EvalSample], List[str], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Load validation data and build corpus.\n",
    "    \n",
    "    Returns:\n",
    "        samples: List of evaluation samples\n",
    "        corpus: List of unique documents\n",
    "        doc_to_id: Mapping from document text to ID\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    corpus_set = set()\n",
    "    \n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= max_samples:\n",
    "                break\n",
    "            item = json.loads(line.strip())\n",
    "            query = item.get(\"anchor\", item.get(\"query\", \"\"))\n",
    "            positive = item.get(\"positive\", item.get(\"positive_doc\", \"\"))\n",
    "            negative = item.get(\"negative\", item.get(\"negative_doc\", \"\"))\n",
    "            \n",
    "            if query and positive and negative:\n",
    "                samples.append(EvalSample(\n",
    "                    query=query,\n",
    "                    positive_doc=positive,\n",
    "                    negative_doc=negative,\n",
    "                ))\n",
    "                corpus_set.add(positive)\n",
    "                corpus_set.add(negative)\n",
    "    \n",
    "    # Build corpus and mapping\n",
    "    corpus = list(corpus_set)\n",
    "    doc_to_id = {doc: i for i, doc in enumerate(corpus)}\n",
    "    \n",
    "    # Assign doc IDs to samples\n",
    "    for sample in samples:\n",
    "        sample.doc_id = doc_to_id[sample.positive_doc]\n",
    "    \n",
    "    print(f\"Loaded {len(samples)} samples\")\n",
    "    print(f\"Corpus size: {len(corpus)} unique documents\")\n",
    "    \n",
    "    return samples, corpus, doc_to_id\n",
    "\n",
    "\n",
    "# Load data\n",
    "eval_samples, corpus, doc_to_id = load_validation_data(\n",
    "    VALIDATION_DATA_PATH, max_samples=MAX_EVAL_SAMPLES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample data\n",
    "print(\"Sample queries:\")\n",
    "for i, sample in enumerate(eval_samples[:3]):\n",
    "    print(f\"\\n[{i+1}] Query: {sample.query[:80]}...\")\n",
    "    print(f\"    Positive: {sample.positive_doc[:80]}...\")\n",
    "    print(f\"    Ground truth doc_id: {sample.doc_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. BM25 Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from rank_bm25 import BM25Okapi\n",
    "    BM25_AVAILABLE = True\n",
    "    print(\"rank_bm25 is available.\")\n",
    "except ImportError:\n",
    "    BM25_AVAILABLE = False\n",
    "    print(\"rank_bm25 not installed. Install with: pip install rank-bm25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Retriever:\n",
    "    \"\"\"BM25 retrieval baseline.\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus: List[str]):\n",
    "        \"\"\"Initialize BM25 index.\"\"\"\n",
    "        self.corpus = corpus\n",
    "        # Simple whitespace tokenization for Korean\n",
    "        # For better performance, use a proper Korean tokenizer\n",
    "        tokenized = [self._tokenize(doc) for doc in corpus]\n",
    "        self.bm25 = BM25Okapi(tokenized)\n",
    "    \n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Simple tokenization (whitespace + character n-grams).\"\"\"\n",
    "        # Split on whitespace\n",
    "        tokens = text.split()\n",
    "        # Add character bigrams for Korean\n",
    "        for word in text.split():\n",
    "            if len(word) >= 2:\n",
    "                for i in range(len(word) - 1):\n",
    "                    tokens.append(word[i:i+2])\n",
    "        return tokens\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 10,\n",
    "    ) -> Tuple[List[int], List[float]]:\n",
    "        \"\"\"Search for relevant documents.\"\"\"\n",
    "        tokenized_query = self._tokenize(query)\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        top_scores = [scores[i] for i in top_indices]\n",
    "        \n",
    "        return top_indices.tolist(), top_scores\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        samples: List[EvalSample],\n",
    "        top_k: int = 10,\n",
    "    ) -> List[RetrievalResult]:\n",
    "        \"\"\"Evaluate on samples.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, sample in enumerate(tqdm(samples, desc=\"BM25 search\")):\n",
    "            doc_ids, scores = self.search(sample.query, top_k=top_k)\n",
    "            results.append(RetrievalResult(\n",
    "                query_id=i,\n",
    "                retrieved_doc_ids=doc_ids,\n",
    "                scores=scores,\n",
    "                ground_truth_id=sample.doc_id,\n",
    "            ))\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "if BM25_AVAILABLE:\n",
    "    print(\"Building BM25 index...\")\n",
    "    bm25_retriever = BM25Retriever(corpus)\n",
    "    print(\"BM25 index built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate BM25\n",
    "if BM25_AVAILABLE:\n",
    "    print(\"Evaluating BM25...\")\n",
    "    bm25_results = bm25_retriever.evaluate(eval_samples, top_k=10)\n",
    "    bm25_metrics = MetricsCalculator.compute_all_metrics(bm25_results, \"BM25\")\n",
    "    \n",
    "    print(f\"\\nBM25 Results:\")\n",
    "    print(f\"  NDCG@10: {bm25_metrics.ndcg_at_10:.4f}\")\n",
    "    print(f\"  Recall@1: {bm25_metrics.recall_at_1:.4f}\")\n",
    "    print(f\"  Recall@5: {bm25_metrics.recall_at_5:.4f}\")\n",
    "    print(f\"  Recall@10: {bm25_metrics.recall_at_10:.4f}\")\n",
    "    print(f\"  MRR: {bm25_metrics.mrr:.4f}\")\n",
    "else:\n",
    "    bm25_metrics = None\n",
    "    print(\"BM25 evaluation skipped (rank_bm25 not installed).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 5. Semantic Search (Dense Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    SBERT_AVAILABLE = True\n",
    "    print(\"sentence_transformers is available.\")\n",
    "except ImportError:\n",
    "    SBERT_AVAILABLE = False\n",
    "    print(\"sentence_transformers not installed.\")\n",
    "    print(\"Install with: pip install sentence-transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticRetriever:\n",
    "    \"\"\"Semantic search using dense embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        corpus: List[str],\n",
    "        device: torch.device,\n",
    "        batch_size: int = 32,\n",
    "    ):\n",
    "        \"\"\"Initialize semantic retriever.\"\"\"\n",
    "        self.model = SentenceTransformer(model_name, device=str(device))\n",
    "        self.corpus = corpus\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Precompute corpus embeddings\n",
    "        print(\"Computing corpus embeddings...\")\n",
    "        self.corpus_embeddings = self.model.encode(\n",
    "            corpus,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        print(f\"Corpus embeddings shape: {self.corpus_embeddings.shape}\")\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 10,\n",
    "    ) -> Tuple[List[int], List[float]]:\n",
    "        \"\"\"Search for relevant documents.\"\"\"\n",
    "        query_embedding = self.model.encode(\n",
    "            [query],\n",
    "            convert_to_tensor=True,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        \n",
    "        # Compute cosine similarities\n",
    "        similarities = torch.mm(\n",
    "            query_embedding, self.corpus_embeddings.T\n",
    "        ).squeeze(0)\n",
    "        \n",
    "        # Get top-k\n",
    "        top_scores, top_indices = torch.topk(similarities, k=top_k)\n",
    "        \n",
    "        return top_indices.cpu().tolist(), top_scores.cpu().tolist()\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        samples: List[EvalSample],\n",
    "        top_k: int = 10,\n",
    "    ) -> List[RetrievalResult]:\n",
    "        \"\"\"Evaluate on samples.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Batch encode queries\n",
    "        print(\"Encoding queries...\")\n",
    "        queries = [s.query for s in samples]\n",
    "        query_embeddings = self.model.encode(\n",
    "            queries,\n",
    "            batch_size=self.batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        \n",
    "        # Compute all similarities at once\n",
    "        print(\"Computing similarities...\")\n",
    "        all_similarities = torch.mm(\n",
    "            query_embeddings, self.corpus_embeddings.T\n",
    "        )\n",
    "        \n",
    "        # Get top-k for each query\n",
    "        top_scores, top_indices = torch.topk(all_similarities, k=top_k, dim=1)\n",
    "        \n",
    "        for i, sample in enumerate(samples):\n",
    "            results.append(RetrievalResult(\n",
    "                query_id=i,\n",
    "                retrieved_doc_ids=top_indices[i].cpu().tolist(),\n",
    "                scores=top_scores[i].cpu().tolist(),\n",
    "                ground_truth_id=sample.doc_id,\n",
    "            ))\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "if SBERT_AVAILABLE:\n",
    "    print(f\"\\nLoading semantic model: {SEMANTIC_MODEL_NAME}\")\n",
    "    print(\"This may take a while for the first time...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and evaluate semantic retriever\n",
    "if SBERT_AVAILABLE:\n",
    "    semantic_retriever = SemanticRetriever(\n",
    "        model_name=SEMANTIC_MODEL_NAME,\n",
    "        corpus=corpus,\n",
    "        device=DEVICE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    \n",
    "    print(\"\\nEvaluating Semantic Search...\")\n",
    "    semantic_results = semantic_retriever.evaluate(eval_samples, top_k=10)\n",
    "    semantic_metrics = MetricsCalculator.compute_all_metrics(\n",
    "        semantic_results, f\"Semantic ({SEMANTIC_MODEL_NAME.split('/')[-1]})\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSemantic Search Results:\")\n",
    "    print(f\"  NDCG@10: {semantic_metrics.ndcg_at_10:.4f}\")\n",
    "    print(f\"  Recall@1: {semantic_metrics.recall_at_1:.4f}\")\n",
    "    print(f\"  Recall@5: {semantic_metrics.recall_at_5:.4f}\")\n",
    "    print(f\"  Recall@10: {semantic_metrics.recall_at_10:.4f}\")\n",
    "    print(f\"  MRR: {semantic_metrics.mrr:.4f}\")\n",
    "else:\n",
    "    semantic_metrics = None\n",
    "    print(\"Semantic evaluation skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 6. Neural Sparse Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPLADEModel(nn.Module):\n",
    "    \"\"\"SPLADE model for Korean sparse retrieval.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"skt/A.X-Encoder-base\"):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        self.config = self.model.config\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Returns:\n",
    "            sparse_repr: [batch_size, vocab_size]\n",
    "            token_weights: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # log(1 + ReLU(logits))\n",
    "        token_scores = torch.log1p(self.relu(logits))\n",
    "        \n",
    "        # Mask padding\n",
    "        mask = attention_mask.unsqueeze(-1).float()\n",
    "        token_scores = token_scores * mask\n",
    "        \n",
    "        # Max pooling over sequence\n",
    "        sparse_repr, _ = token_scores.max(dim=1)\n",
    "        token_weights = token_scores.max(dim=-1).values\n",
    "        \n",
    "        return sparse_repr, token_weights\n",
    "\n",
    "\n",
    "def load_splade_model(\n",
    "    checkpoint_path: Path,\n",
    "    model_name: str,\n",
    "    device: torch.device,\n",
    ") -> Optional[SPLADEModel]:\n",
    "    \"\"\"Load trained SPLADE model.\"\"\"\n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "        return None\n",
    "    \n",
    "    model = SPLADEModel(model_name)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Loaded model from: {checkpoint_path}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer loaded: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralSparseRetriever:\n",
    "    \"\"\"Neural sparse retrieval using SPLADE model.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: SPLADEModel,\n",
    "        tokenizer,\n",
    "        corpus: List[str],\n",
    "        device: torch.device,\n",
    "        batch_size: int = 32,\n",
    "    ):\n",
    "        \"\"\"Initialize neural sparse retriever.\"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.corpus = corpus\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Precompute corpus sparse representations\n",
    "        print(\"Computing corpus sparse representations...\")\n",
    "        self.corpus_sparse = self._encode_batch(corpus)\n",
    "        print(f\"Corpus sparse shape: {self.corpus_sparse.shape}\")\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _encode_batch(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Encode texts to sparse representations.\"\"\"\n",
    "        all_sparse = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), self.batch_size), desc=\"Encoding\"):\n",
    "            batch_texts = texts[i:i + self.batch_size]\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            sparse_repr, _ = self.model(\n",
    "                inputs[\"input_ids\"],\n",
    "                inputs[\"attention_mask\"],\n",
    "            )\n",
    "            all_sparse.append(sparse_repr.cpu())\n",
    "        \n",
    "        return torch.cat(all_sparse, dim=0)\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 10,\n",
    "    ) -> Tuple[List[int], List[float]]:\n",
    "        \"\"\"Search for relevant documents.\"\"\"\n",
    "        query_sparse = self._encode_batch([query])\n",
    "        \n",
    "        # Dot product similarity for sparse vectors\n",
    "        scores = torch.mm(query_sparse, self.corpus_sparse.T).squeeze(0)\n",
    "        \n",
    "        top_scores, top_indices = torch.topk(scores, k=top_k)\n",
    "        \n",
    "        return top_indices.tolist(), top_scores.tolist()\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        samples: List[EvalSample],\n",
    "        top_k: int = 10,\n",
    "    ) -> List[RetrievalResult]:\n",
    "        \"\"\"Evaluate on samples.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Encode all queries\n",
    "        queries = [s.query for s in samples]\n",
    "        query_sparse = self._encode_batch(queries)\n",
    "        \n",
    "        # Compute all scores at once\n",
    "        print(\"Computing scores...\")\n",
    "        all_scores = torch.mm(query_sparse, self.corpus_sparse.T)\n",
    "        \n",
    "        # Get top-k for each query\n",
    "        top_scores, top_indices = torch.topk(all_scores, k=top_k, dim=1)\n",
    "        \n",
    "        for i, sample in enumerate(samples):\n",
    "            results.append(RetrievalResult(\n",
    "                query_id=i,\n",
    "                retrieved_doc_ids=top_indices[i].tolist(),\n",
    "                scores=top_scores[i].tolist(),\n",
    "                ground_truth_id=sample.doc_id,\n",
    "            ))\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Neural Sparse models\n",
    "neural_sparse_models = {}\n",
    "\n",
    "# Try v22.1 first, then fallback to v22.0, then v21.4\n",
    "model_paths = [\n",
    "    (\"v22.1\", V22_1_MODEL_PATH),\n",
    "    (\"v22.0\", V22_0_MODEL_PATH),\n",
    "    (\"v21.4\", V21_4_MODEL_PATH),\n",
    "]\n",
    "\n",
    "for name, path in model_paths:\n",
    "    model = load_splade_model(path, MODEL_NAME, DEVICE)\n",
    "    if model is not None:\n",
    "        neural_sparse_models[name] = model\n",
    "\n",
    "print(f\"\\nLoaded {len(neural_sparse_models)} Neural Sparse models:\")\n",
    "for name in neural_sparse_models:\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Neural Sparse models\n",
    "neural_sparse_metrics = {}\n",
    "\n",
    "for name, model in neural_sparse_models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating Neural Sparse {name}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    retriever = NeuralSparseRetriever(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        corpus=corpus,\n",
    "        device=DEVICE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    \n",
    "    results = retriever.evaluate(eval_samples, top_k=10)\n",
    "    metrics = MetricsCalculator.compute_all_metrics(results, f\"Neural Sparse {name}\")\n",
    "    neural_sparse_metrics[name] = metrics\n",
    "    \n",
    "    print(f\"\\nNeural Sparse {name} Results:\")\n",
    "    print(f\"  NDCG@10: {metrics.ndcg_at_10:.4f}\")\n",
    "    print(f\"  Recall@1: {metrics.recall_at_1:.4f}\")\n",
    "    print(f\"  Recall@5: {metrics.recall_at_5:.4f}\")\n",
    "    print(f\"  Recall@10: {metrics.recall_at_10:.4f}\")\n",
    "    print(f\"  MRR: {metrics.mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 7. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_comparison_table(all_metrics: Dict[str, EvaluationMetrics]):\n",
    "    \"\"\"Print formatted comparison table.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"v22.1 Evaluation Results\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Header\n",
    "    print(f\"{'Method':<30} {'NDCG@10':>10} {'Recall@1':>10} {'Recall@5':>10} {'Recall@10':>10} {'MRR':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Rows\n",
    "    for name, metrics in all_metrics.items():\n",
    "        print(\n",
    "            f\"{name:<30} \"\n",
    "            f\"{metrics.ndcg_at_10:>10.4f} \"\n",
    "            f\"{metrics.recall_at_1:>10.4f} \"\n",
    "            f\"{metrics.recall_at_5:>10.4f} \"\n",
    "            f\"{metrics.recall_at_10:>10.4f} \"\n",
    "            f\"{metrics.mrr:>10.4f}\"\n",
    "        )\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# Collect all metrics\n",
    "all_metrics = {}\n",
    "\n",
    "if bm25_metrics:\n",
    "    all_metrics[\"BM25\"] = bm25_metrics\n",
    "\n",
    "if semantic_metrics:\n",
    "    all_metrics[f\"Semantic (BGE-M3)\"] = semantic_metrics\n",
    "\n",
    "for name, metrics in neural_sparse_metrics.items():\n",
    "    all_metrics[f\"Neural Sparse {name}\"] = metrics\n",
    "\n",
    "# Print comparison\n",
    "print_comparison_table(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "def plot_metrics_comparison(all_metrics: Dict[str, EvaluationMetrics]):\n",
    "    \"\"\"Create bar chart comparing metrics.\"\"\"\n",
    "    methods = list(all_metrics.keys())\n",
    "    metrics_names = [\"NDCG@10\", \"Recall@1\", \"Recall@5\", \"Recall@10\", \"MRR\"]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(metrics_names))\n",
    "    width = 0.8 / len(methods)\n",
    "    \n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(methods)))\n",
    "    \n",
    "    for i, (method, metrics) in enumerate(all_metrics.items()):\n",
    "        values = [\n",
    "            metrics.ndcg_at_10,\n",
    "            metrics.recall_at_1,\n",
    "            metrics.recall_at_5,\n",
    "            metrics.recall_at_10,\n",
    "            metrics.mrr,\n",
    "        ]\n",
    "        offset = (i - len(methods) / 2 + 0.5) * width\n",
    "        bars = ax.bar(x + offset, values, width, label=method, color=colors[i])\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, val in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(\n",
    "                f'{val:.2f}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom',\n",
    "                fontsize=8,\n",
    "                rotation=45,\n",
    "            )\n",
    "    \n",
    "    ax.set_xlabel('Metric')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('v22.1 Model Comparison: Retrieval Metrics')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics_names)\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    fig_path = OUTPUT_DIR / \"evaluation_comparison.png\"\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Figure saved to: {fig_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if all_metrics:\n",
    "    plot_metrics_comparison(all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 8. Per-Query Analysis (Error Debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_failures(\n",
    "    metrics: EvaluationMetrics,\n",
    "    samples: List[EvalSample],\n",
    "    max_failures: int = 10,\n",
    "):\n",
    "    \"\"\"Analyze queries where the model failed to retrieve ground truth.\"\"\"\n",
    "    failures = []\n",
    "    \n",
    "    for result in metrics.per_query_results:\n",
    "        if result[\"recall@10\"] == 0:  # Ground truth not in top-10\n",
    "            query_id = result[\"query_id\"]\n",
    "            failures.append({\n",
    "                \"query_id\": query_id,\n",
    "                \"query\": samples[query_id].query,\n",
    "                \"positive_doc\": samples[query_id].positive_doc,\n",
    "                \"mrr\": result[\"mrr\"],\n",
    "                \"ground_truth_rank\": result[\"ground_truth_rank\"],\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nTotal failures (not in top-10): {len(failures)}/{len(samples)}\")\n",
    "    print(f\"Failure rate: {len(failures)/len(samples)*100:.1f}%\")\n",
    "    \n",
    "    if failures:\n",
    "        print(f\"\\nSample failures (showing {min(max_failures, len(failures))}):\")\n",
    "        for i, f in enumerate(failures[:max_failures]):\n",
    "            print(f\"\\n[{i+1}] Query: {f['query'][:80]}...\")\n",
    "            print(f\"    Expected: {f['positive_doc'][:80]}...\")\n",
    "            print(f\"    Actual rank: {f['ground_truth_rank']}\")\n",
    "    \n",
    "    return failures\n",
    "\n",
    "\n",
    "# Analyze failures for the best Neural Sparse model\n",
    "if neural_sparse_metrics:\n",
    "    best_model_name = max(\n",
    "        neural_sparse_metrics.keys(),\n",
    "        key=lambda k: neural_sparse_metrics[k].mrr\n",
    "    )\n",
    "    print(f\"\\nAnalyzing failures for: Neural Sparse {best_model_name}\")\n",
    "    failures = analyze_failures(\n",
    "        neural_sparse_metrics[best_model_name],\n",
    "        eval_samples,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_difficulty_breakdown(\n",
    "    metrics: EvaluationMetrics,\n",
    "    samples: List[EvalSample],\n",
    "):\n",
    "    \"\"\"Analyze performance by query length.\"\"\"\n",
    "    short_queries = []  # < 20 chars\n",
    "    medium_queries = []  # 20-50 chars\n",
    "    long_queries = []  # > 50 chars\n",
    "    \n",
    "    for result in metrics.per_query_results:\n",
    "        query_id = result[\"query_id\"]\n",
    "        query_len = len(samples[query_id].query)\n",
    "        \n",
    "        if query_len < 20:\n",
    "            short_queries.append(result)\n",
    "        elif query_len < 50:\n",
    "            medium_queries.append(result)\n",
    "        else:\n",
    "            long_queries.append(result)\n",
    "    \n",
    "    print(\"\\nPerformance by Query Length:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Category':<20} {'Count':>10} {'MRR':>10} {'R@10':>10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for name, queries in [\n",
    "        (\"Short (<20)\", short_queries),\n",
    "        (\"Medium (20-50)\", medium_queries),\n",
    "        (\"Long (>50)\", long_queries),\n",
    "    ]:\n",
    "        if queries:\n",
    "            avg_mrr = np.mean([q[\"mrr\"] for q in queries])\n",
    "            avg_recall = np.mean([q[\"recall@10\"] for q in queries])\n",
    "            print(f\"{name:<20} {len(queries):>10} {avg_mrr:>10.4f} {avg_recall:>10.4f}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if neural_sparse_metrics:\n",
    "    analyze_difficulty_breakdown(\n",
    "        neural_sparse_metrics[best_model_name],\n",
    "        eval_samples,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results to JSON\n",
    "results_dict = {\n",
    "    \"evaluation_date\": datetime.now().isoformat(),\n",
    "    \"num_samples\": len(eval_samples),\n",
    "    \"corpus_size\": len(corpus),\n",
    "    \"methods\": {},\n",
    "}\n",
    "\n",
    "for name, metrics in all_metrics.items():\n",
    "    results_dict[\"methods\"][name] = metrics.to_dict()\n",
    "\n",
    "# Save JSON\n",
    "json_path = OUTPUT_DIR / \"evaluation_results.json\"\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Results saved to: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Markdown report\n",
    "def generate_markdown_report(\n",
    "    all_metrics: Dict[str, EvaluationMetrics],\n",
    "    output_path: Path,\n",
    "):\n",
    "    \"\"\"Generate detailed markdown evaluation report.\"\"\"\n",
    "    lines = [\n",
    "        \"# v22.1 Model Evaluation Report\",\n",
    "        \"\",\n",
    "        f\"**Evaluation Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        \"\",\n",
    "        \"## Summary\",\n",
    "        \"\",\n",
    "        f\"- **Evaluation Samples:** {len(eval_samples)}\",\n",
    "        f\"- **Corpus Size:** {len(corpus)} unique documents\",\n",
    "        \"\",\n",
    "        \"## Results\",\n",
    "        \"\",\n",
    "        \"| Method | NDCG@10 | Recall@1 | Recall@5 | Recall@10 | MRR |\",\n",
    "        \"|--------|---------|----------|----------|-----------|-----|\",\n",
    "    ]\n",
    "    \n",
    "    for name, metrics in all_metrics.items():\n",
    "        lines.append(\n",
    "            f\"| {name} | \"\n",
    "            f\"{metrics.ndcg_at_10:.4f} | \"\n",
    "            f\"{metrics.recall_at_1:.4f} | \"\n",
    "            f\"{metrics.recall_at_5:.4f} | \"\n",
    "            f\"{metrics.recall_at_10:.4f} | \"\n",
    "            f\"{metrics.mrr:.4f} |\"\n",
    "        )\n",
    "    \n",
    "    lines.extend([\n",
    "        \"\",\n",
    "        \"## Methodology\",\n",
    "        \"\",\n",
    "        \"### BM25\",\n",
    "        \"- Traditional term-frequency based retrieval\",\n",
    "        \"- Tokenization: whitespace + character bigrams\",\n",
    "        \"\",\n",
    "        \"### Semantic Search (BGE-M3)\",\n",
    "        \"- Dense embedding model: BAAI/bge-m3\",\n",
    "        \"- Similarity: cosine similarity\",\n",
    "        \"\",\n",
    "        \"### Neural Sparse\",\n",
    "        \"- Base model: skt/A.X-Encoder-base\",\n",
    "        \"- Architecture: SPLADE-style sparse retrieval\",\n",
    "        \"- Similarity: dot product\",\n",
    "        \"\",\n",
    "        \"## Metrics Definition\",\n",
    "        \"\",\n",
    "        \"- **NDCG@10:** Normalized Discounted Cumulative Gain at 10\",\n",
    "        \"- **Recall@K:** Fraction of relevant documents retrieved in top-K\",\n",
    "        \"- **MRR:** Mean Reciprocal Rank (1/rank of first relevant document)\",\n",
    "        \"\",\n",
    "    ])\n",
    "    \n",
    "    # Add best model highlight\n",
    "    if all_metrics:\n",
    "        best_by_mrr = max(all_metrics.items(), key=lambda x: x[1].mrr)\n",
    "        lines.extend([\n",
    "            \"## Best Model\",\n",
    "            \"\",\n",
    "            f\"**{best_by_mrr[0]}** achieved the highest MRR of {best_by_mrr[1].mrr:.4f}\",\n",
    "            \"\",\n",
    "        ])\n",
    "    \n",
    "    report_text = \"\\n\".join(lines)\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report_text)\n",
    "    \n",
    "    print(f\"Report saved to: {output_path}\")\n",
    "    return report_text\n",
    "\n",
    "\n",
    "# Generate report\n",
    "report_path = OUTPUT_DIR / \"evaluation_report.md\"\n",
    "report = generate_markdown_report(all_metrics, report_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Report Preview:\")\n",
    "print(\"=\" * 60)\n",
    "print(report[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## 10. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nOutput Files:\")\n",
    "print(f\"  - {OUTPUT_DIR / 'evaluation_results.json'}\")\n",
    "print(f\"  - {OUTPUT_DIR / 'evaluation_report.md'}\")\n",
    "print(f\"  - {OUTPUT_DIR / 'evaluation_comparison.png'}\")\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "if all_metrics:\n",
    "    best_model = max(all_metrics.items(), key=lambda x: x[1].mrr)\n",
    "    print(f\"  - Best model by MRR: {best_model[0]} ({best_model[1].mrr:.4f})\")\n",
    "    \n",
    "    best_ndcg = max(all_metrics.items(), key=lambda x: x[1].ndcg_at_10)\n",
    "    print(f\"  - Best model by NDCG@10: {best_ndcg[0]} ({best_ndcg[1].ndcg_at_10:.4f})\")\n",
    "    \n",
    "    best_recall = max(all_metrics.items(), key=lambda x: x[1].recall_at_10)\n",
    "    print(f\"  - Best model by Recall@10: {best_recall[0]} ({best_recall[1].recall_at_10:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "## Appendix: Statistical Significance Testing (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "\n",
    "def paired_t_test(\n",
    "    metrics_a: EvaluationMetrics,\n",
    "    metrics_b: EvaluationMetrics,\n",
    "    metric_name: str = \"mrr\",\n",
    "    alpha: float = 0.05,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Perform paired t-test between two models.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with test results\n",
    "    \"\"\"\n",
    "    scores_a = [r[metric_name] for r in metrics_a.per_query_results]\n",
    "    scores_b = [r[metric_name] for r in metrics_b.per_query_results]\n",
    "    \n",
    "    if len(scores_a) != len(scores_b):\n",
    "        return {\"error\": \"Different number of samples\"}\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_rel(scores_a, scores_b)\n",
    "    \n",
    "    return {\n",
    "        \"metric\": metric_name,\n",
    "        \"model_a\": metrics_a.method_name,\n",
    "        \"model_b\": metrics_b.method_name,\n",
    "        \"mean_a\": np.mean(scores_a),\n",
    "        \"mean_b\": np.mean(scores_b),\n",
    "        \"mean_diff\": np.mean(scores_a) - np.mean(scores_b),\n",
    "        \"t_statistic\": t_stat,\n",
    "        \"p_value\": p_value,\n",
    "        \"significant\": p_value < alpha,\n",
    "    }\n",
    "\n",
    "\n",
    "# Perform significance tests if we have multiple methods\n",
    "if len(all_metrics) >= 2:\n",
    "    print(\"\\nStatistical Significance Tests (MRR):\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    metric_list = list(all_metrics.values())\n",
    "    \n",
    "    for i in range(len(metric_list)):\n",
    "        for j in range(i + 1, len(metric_list)):\n",
    "            result = paired_t_test(metric_list[i], metric_list[j], \"mrr\")\n",
    "            \n",
    "            sig_marker = \"*\" if result.get(\"significant\", False) else \"\"\n",
    "            print(\n",
    "                f\"{result['model_a']} vs {result['model_b']}: \"\n",
    "                f\"diff={result['mean_diff']:.4f}, \"\n",
    "                f\"p={result['p_value']:.4f}{sig_marker}\"\n",
    "            )\n",
    "    \n",
    "    print(\"\\n* indicates statistically significant (p < 0.05)\")\n",
    "else:\n",
    "    print(\"\\nSkipping significance tests (need at least 2 methods).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xhbt3ciocmp",
   "source": "## 11. Amazon OpenSearch Service Evaluation (Production)\n\nThis section evaluates the models using Amazon OpenSearch Service.\n\n**Authentication:** AWS default credentials via boto3 (IAM-based)\n\n**Requirements:**\n- Environment variables: `OPENSEARCH_HOST`, `AWS_REGION`\n- Python packages: `opensearch-py`, `requests-aws4auth`, `boto3`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5axxsl0cdjf",
   "source": "# Amazon OpenSearch Service Client Setup\nfrom dotenv import load_dotenv\nimport boto3\n\n# Load environment variables\nload_dotenv(PROJECT_ROOT / \".env\")\n\n# OpenSearch Configuration from environment\nOPENSEARCH_HOST = os.getenv(\"OPENSEARCH_HOST\", \"\")\nOPENSEARCH_PORT = int(os.getenv(\"OPENSEARCH_PORT\", \"443\"))\nOPENSEARCH_REGION = os.getenv(\"AWS_REGION\", \"us-east-1\")\nOPENSEARCH_USE_SSL = os.getenv(\"OPENSEARCH_USE_SSL\", \"true\").lower() == \"true\"\n\n# Index names\nBM25_INDEX = os.getenv(\"OPENSEARCH_BM25_INDEX\", \"benchmark-bm25-v22\")\nDENSE_INDEX = os.getenv(\"OPENSEARCH_DENSE_INDEX\", \"benchmark-dense-v22\")\nSPARSE_INDEX = os.getenv(\"OPENSEARCH_SPARSE_INDEX\", \"benchmark-sparse-v22\")\n\n# Check if OpenSearch is configured\nOPENSEARCH_CONFIGURED = bool(OPENSEARCH_HOST and OPENSEARCH_HOST != \"your-domain.us-east-1.es.amazonaws.com\")\n\nprint(f\"OpenSearch Configuration:\")\nprint(f\"  Host: {OPENSEARCH_HOST or '(not configured)'}\")\nprint(f\"  Port: {OPENSEARCH_PORT}\")\nprint(f\"  Region: {OPENSEARCH_REGION}\")\nprint(f\"  SSL: {OPENSEARCH_USE_SSL}\")\nprint(f\"  Configured: {OPENSEARCH_CONFIGURED}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "jq7lxg3k9mf",
   "source": "class OpenSearchServiceClient:\n    \"\"\"\n    Amazon OpenSearch Service client with AWS IAM authentication.\n    \n    Uses default AWS credentials from boto3.Session() - typically from:\n    - Environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n    - AWS credentials file (~/.aws/credentials)\n    - IAM role (when running on EC2/Lambda)\n    \"\"\"\n    \n    def __init__(\n        self,\n        host: str,\n        port: int = 443,\n        region: str = \"us-east-1\",\n        use_ssl: bool = True,\n    ):\n        \"\"\"Initialize OpenSearch client with AWS authentication.\"\"\"\n        self.host = host\n        self.port = port\n        self.region = region\n        self.use_ssl = use_ssl\n        self.client = self._create_client()\n    \n    def _create_client(self):\n        \"\"\"Create OpenSearch client with AWS4Auth.\"\"\"\n        try:\n            from opensearchpy import OpenSearch, RequestsHttpConnection\n            from requests_aws4auth import AWS4Auth\n        except ImportError as e:\n            print(f\"Missing required packages: {e}\")\n            print(\"Install with: pip install opensearch-py requests-aws4auth boto3\")\n            return None\n        \n        # Get AWS credentials from default credential chain\n        credentials = boto3.Session().get_credentials()\n        if not credentials:\n            print(\"ERROR: No AWS credentials found. Configure AWS credentials first.\")\n            return None\n        \n        aws_auth = AWS4Auth(\n            credentials.access_key,\n            credentials.secret_key,\n            self.region,\n            \"es\",  # Service name for OpenSearch\n            session_token=credentials.token,\n        )\n        \n        client = OpenSearch(\n            hosts=[{\"host\": self.host, \"port\": self.port}],\n            http_auth=aws_auth,\n            use_ssl=self.use_ssl,\n            verify_certs=True,\n            connection_class=RequestsHttpConnection,\n            timeout=120,\n        )\n        \n        # Test connection\n        try:\n            info = client.info()\n            print(f\"Connected to OpenSearch: {info['version']['distribution']} {info['version']['number']}\")\n        except Exception as e:\n            print(f\"Connection test failed: {e}\")\n            return None\n        \n        return client\n    \n    def create_bm25_index(self, index_name: str) -> bool:\n        \"\"\"Create BM25 index with Korean analyzer.\"\"\"\n        if not self.client:\n            return False\n        \n        if self.client.indices.exists(index=index_name):\n            print(f\"Index {index_name} already exists.\")\n            return True\n        \n        body = {\n            \"settings\": {\n                \"analysis\": {\n                    \"analyzer\": {\n                        \"korean_analyzer\": {\n                            \"type\": \"custom\",\n                            \"tokenizer\": \"nori_tokenizer\",\n                        }\n                    }\n                },\n                \"number_of_shards\": 2,\n                \"number_of_replicas\": 1,\n            },\n            \"mappings\": {\n                \"properties\": {\n                    \"doc_id\": {\"type\": \"keyword\"},\n                    \"content\": {\"type\": \"text\", \"analyzer\": \"korean_analyzer\"},\n                }\n            },\n        }\n        \n        try:\n            self.client.indices.create(index=index_name, body=body)\n            print(f\"Created index: {index_name}\")\n            return True\n        except Exception as e:\n            print(f\"Failed to create index {index_name}: {e}\")\n            return False\n    \n    def create_sparse_index(self, index_name: str, dimension: int = 50000) -> bool:\n        \"\"\"Create sparse vector index for Neural Sparse.\"\"\"\n        if not self.client:\n            return False\n        \n        if self.client.indices.exists(index=index_name):\n            print(f\"Index {index_name} already exists.\")\n            return True\n        \n        body = {\n            \"settings\": {\n                \"number_of_shards\": 2,\n                \"number_of_replicas\": 1,\n            },\n            \"mappings\": {\n                \"properties\": {\n                    \"doc_id\": {\"type\": \"keyword\"},\n                    \"content\": {\"type\": \"text\"},\n                    \"sparse_embedding\": {\n                        \"type\": \"rank_features\",\n                    },\n                }\n            },\n        }\n        \n        try:\n            self.client.indices.create(index=index_name, body=body)\n            print(f\"Created sparse index: {index_name}\")\n            return True\n        except Exception as e:\n            print(f\"Failed to create sparse index {index_name}: {e}\")\n            return False\n    \n    def index_documents(\n        self,\n        index_name: str,\n        documents: List[Dict],\n        batch_size: int = 100,\n    ) -> int:\n        \"\"\"Bulk index documents.\"\"\"\n        if not self.client:\n            return 0\n        \n        from opensearchpy.helpers import bulk\n        \n        indexed = 0\n        for i in tqdm(range(0, len(documents), batch_size), desc=f\"Indexing to {index_name}\"):\n            batch = documents[i:i + batch_size]\n            actions = [\n                {\n                    \"_index\": index_name,\n                    \"_id\": doc.get(\"doc_id\", i + j),\n                    \"_source\": doc,\n                }\n                for j, doc in enumerate(batch)\n            ]\n            \n            try:\n                success, _ = bulk(self.client, actions)\n                indexed += success\n            except Exception as e:\n                print(f\"Bulk indexing error: {e}\")\n        \n        # Refresh index\n        self.client.indices.refresh(index=index_name)\n        return indexed\n    \n    def search_bm25(\n        self,\n        index_name: str,\n        query: str,\n        top_k: int = 10,\n    ) -> List[Tuple[str, float]]:\n        \"\"\"BM25 search.\"\"\"\n        if not self.client:\n            return []\n        \n        body = {\n            \"query\": {\n                \"match\": {\n                    \"content\": query\n                }\n            },\n            \"size\": top_k,\n        }\n        \n        try:\n            response = self.client.search(index=index_name, body=body)\n            hits = response[\"hits\"][\"hits\"]\n            return [(hit[\"_source\"][\"doc_id\"], hit[\"_score\"]) for hit in hits]\n        except Exception as e:\n            print(f\"Search error: {e}\")\n            return []\n    \n    def search_sparse(\n        self,\n        index_name: str,\n        sparse_vector: Dict[str, float],\n        top_k: int = 10,\n    ) -> List[Tuple[str, float]]:\n        \"\"\"Sparse vector search using rank_features.\"\"\"\n        if not self.client:\n            return []\n        \n        # Build rank_feature query\n        should_clauses = [\n            {\"rank_feature\": {\"field\": f\"sparse_embedding.{token}\", \"boost\": weight}}\n            for token, weight in sparse_vector.items()\n        ]\n        \n        body = {\n            \"query\": {\n                \"bool\": {\n                    \"should\": should_clauses,\n                }\n            },\n            \"size\": top_k,\n        }\n        \n        try:\n            response = self.client.search(index=index_name, body=body)\n            hits = response[\"hits\"][\"hits\"]\n            return [(hit[\"_source\"][\"doc_id\"], hit[\"_score\"]) for hit in hits]\n        except Exception as e:\n            print(f\"Sparse search error: {e}\")\n            return []\n\n\n# Initialize OpenSearch client if configured\nos_client = None\nif OPENSEARCH_CONFIGURED:\n    print(\"\\nInitializing Amazon OpenSearch Service client...\")\n    os_client = OpenSearchServiceClient(\n        host=OPENSEARCH_HOST,\n        port=OPENSEARCH_PORT,\n        region=OPENSEARCH_REGION,\n        use_ssl=OPENSEARCH_USE_SSL,\n    )\nelse:\n    print(\"\\nOpenSearch not configured. Set OPENSEARCH_HOST in .env to enable.\")\n    print(\"Skipping OpenSearch evaluation.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "zrj1s2p9ui",
   "source": "# Index corpus to Amazon OpenSearch Service and run evaluation\nif os_client and os_client.client:\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Amazon OpenSearch Service Evaluation\")\n    print(\"=\" * 60)\n    \n    # 1. Create BM25 index and index documents\n    print(\"\\n[1/3] Setting up BM25 index...\")\n    if os_client.create_bm25_index(BM25_INDEX):\n        bm25_docs = [\n            {\"doc_id\": str(i), \"content\": doc}\n            for i, doc in enumerate(corpus)\n        ]\n        indexed = os_client.index_documents(BM25_INDEX, bm25_docs)\n        print(f\"Indexed {indexed} documents to {BM25_INDEX}\")\n    \n    # 2. Create Sparse index and index documents\n    print(\"\\n[2/3] Setting up Neural Sparse index...\")\n    if neural_sparse_models and os_client.create_sparse_index(SPARSE_INDEX):\n        # Use the best available model\n        best_model_key = list(neural_sparse_models.keys())[0]\n        model = neural_sparse_models[best_model_key]\n        \n        # Encode corpus to sparse vectors\n        print(f\"Encoding corpus with {best_model_key}...\")\n        sparse_docs = []\n        \n        for i, doc in enumerate(tqdm(corpus, desc=\"Encoding corpus\")):\n            inputs = tokenizer(\n                doc,\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n                max_length=512,\n            )\n            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n            \n            with torch.no_grad():\n                sparse_repr, _ = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n            \n            # Convert to sparse dict (only non-zero values)\n            sparse_vec = sparse_repr[0].cpu()\n            nonzero_indices = torch.nonzero(sparse_vec > 0.01).squeeze(-1)\n            sparse_dict = {\n                str(idx.item()): float(sparse_vec[idx])\n                for idx in nonzero_indices\n            }\n            \n            sparse_docs.append({\n                \"doc_id\": str(i),\n                \"content\": doc,\n                \"sparse_embedding\": sparse_dict,\n            })\n        \n        indexed = os_client.index_documents(SPARSE_INDEX, sparse_docs)\n        print(f\"Indexed {indexed} sparse documents to {SPARSE_INDEX}\")\n    \n    # 3. Evaluate on OpenSearch\n    print(\"\\n[3/3] Running evaluation on OpenSearch...\")\n    \n    os_bm25_results = []\n    os_sparse_results = []\n    \n    for i, sample in enumerate(tqdm(eval_samples, desc=\"OpenSearch Eval\")):\n        # BM25 search\n        bm25_hits = os_client.search_bm25(BM25_INDEX, sample.query, top_k=10)\n        bm25_doc_ids = [int(doc_id) for doc_id, _ in bm25_hits]\n        bm25_scores = [score for _, score in bm25_hits]\n        \n        os_bm25_results.append(RetrievalResult(\n            query_id=i,\n            retrieved_doc_ids=bm25_doc_ids,\n            scores=bm25_scores,\n            ground_truth_id=sample.doc_id,\n        ))\n        \n        # Sparse search (if model available)\n        if neural_sparse_models:\n            # Encode query\n            inputs = tokenizer(\n                sample.query,\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n                max_length=512,\n            )\n            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n            \n            with torch.no_grad():\n                query_sparse, _ = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n            \n            # Convert to sparse dict\n            sparse_vec = query_sparse[0].cpu()\n            nonzero_indices = torch.nonzero(sparse_vec > 0.01).squeeze(-1)\n            query_sparse_dict = {\n                str(idx.item()): float(sparse_vec[idx])\n                for idx in nonzero_indices\n            }\n            \n            sparse_hits = os_client.search_sparse(SPARSE_INDEX, query_sparse_dict, top_k=10)\n            sparse_doc_ids = [int(doc_id) for doc_id, _ in sparse_hits]\n            sparse_scores = [score for _, score in sparse_hits]\n            \n            os_sparse_results.append(RetrievalResult(\n                query_id=i,\n                retrieved_doc_ids=sparse_doc_ids,\n                scores=sparse_scores,\n                ground_truth_id=sample.doc_id,\n            ))\n    \n    # Compute metrics for OpenSearch results\n    os_bm25_metrics = MetricsCalculator.compute_all_metrics(os_bm25_results, \"OpenSearch BM25 (Nori)\")\n    \n    print(f\"\\nOpenSearch BM25 (Nori) Results:\")\n    print(f\"  NDCG@10: {os_bm25_metrics.ndcg_at_10:.4f}\")\n    print(f\"  Recall@1: {os_bm25_metrics.recall_at_1:.4f}\")\n    print(f\"  Recall@10: {os_bm25_metrics.recall_at_10:.4f}\")\n    print(f\"  MRR: {os_bm25_metrics.mrr:.4f}\")\n    \n    if os_sparse_results:\n        os_sparse_metrics = MetricsCalculator.compute_all_metrics(os_sparse_results, \"OpenSearch Neural Sparse\")\n        \n        print(f\"\\nOpenSearch Neural Sparse Results:\")\n        print(f\"  NDCG@10: {os_sparse_metrics.ndcg_at_10:.4f}\")\n        print(f\"  Recall@1: {os_sparse_metrics.recall_at_1:.4f}\")\n        print(f\"  Recall@10: {os_sparse_metrics.recall_at_10:.4f}\")\n        print(f\"  MRR: {os_sparse_metrics.mrr:.4f}\")\n        \n        # Add to all_metrics for comparison\n        all_metrics[\"OpenSearch BM25 (Nori)\"] = os_bm25_metrics\n        all_metrics[\"OpenSearch Neural Sparse\"] = os_sparse_metrics\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"OpenSearch Evaluation Complete\")\n    print(\"=\" * 60)\nelse:\n    print(\"\\nSkipping OpenSearch evaluation (not configured or connection failed).\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
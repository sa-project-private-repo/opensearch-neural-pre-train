{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v7 Cross-Lingual Neural Sparse Model - Inference Test\n",
    "\n",
    "이 노트북은 v7 모델의 한국어-영어 cross-lingual token activation 성능을 테스트합니다.\n",
    "\n",
    "## v7 모델 특징\n",
    "- **Direct Token Target Loss**: 영어 동의어 토큰을 직접 supervision\n",
    "- **Margin Loss**: 최소 activation 값 보장 (margin=1.0)\n",
    "- **Negative Sampling**: 비타겟 토큰 억제\n",
    "- **Training Data**: 1.55M cleaned KO-EN term pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train\n",
      "PyTorch version: 2.10.0.dev20251109+cu130\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root by looking for CLAUDE.md or .git\n",
    "def find_project_root():\n",
    "    \"\"\"Find project root directory.\"\"\"\n",
    "    # Try common locations\n",
    "    candidates = [\n",
    "        Path.cwd(),\n",
    "        Path.cwd().parent,\n",
    "        Path.cwd().parent.parent,\n",
    "        Path(\"/home/west/Documents/cursor-workspace/opensearch-neural-pre-train\"),\n",
    "    ]\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        if (candidate / \"CLAUDE.md\").exists() or (candidate / \".git\").exists():\n",
    "            return candidate\n",
    "    \n",
    "    # Fallback to absolute path\n",
    "    return Path(\"/home/west/Documents/cursor-workspace/opensearch-neural-pre-train\")\n",
    "\n",
    "project_root = find_project_root()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from src.model.splade_model import create_splade_model\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load v7 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Tokenizer vocab size: 119547\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "CHECKPOINT_PATH = project_root / \"outputs/cross_lingual_expansion_v7_largescale/final_model/checkpoint.pt\"\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/west/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  queued_call()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Checkpoint path: /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/outputs/cross_lingual_expansion_v7_largescale/final_model/checkpoint.pt\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = create_splade_model(\n",
    "    model_name=MODEL_NAME,\n",
    "    use_idf=False,\n",
    "    use_expansion=True,\n",
    "    expansion_mode=\"mlm\",\n",
    ")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=True)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Checkpoint path: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text: str, max_length: int = 64) -> dict:\n",
    "    \"\"\"Encode text using tokenizer.\"\"\"\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "\n",
    "def get_sparse_representation(text: str, top_k: int = 50) -> tuple:\n",
    "    \"\"\"\n",
    "    Get sparse representation for input text.\n",
    "    \n",
    "    Returns:\n",
    "        tokens: List of top-k tokens\n",
    "        scores: List of corresponding scores\n",
    "        sparse_rep: Full sparse representation tensor\n",
    "    \"\"\"\n",
    "    encoding = encode_text(text)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sparse_rep, _ = model(\n",
    "            encoding['input_ids'].to(device),\n",
    "            encoding['attention_mask'].to(device)\n",
    "        )\n",
    "    \n",
    "    sparse_rep = sparse_rep[0].cpu()\n",
    "    \n",
    "    # Get top-k tokens\n",
    "    top_scores, top_indices = torch.topk(sparse_rep, k=top_k)\n",
    "    top_tokens = tokenizer.convert_ids_to_tokens(top_indices.tolist())\n",
    "    \n",
    "    return top_tokens, top_scores.tolist(), sparse_rep\n",
    "\n",
    "\n",
    "def display_sparse_output(text: str, top_k: int = 20):\n",
    "    \"\"\"Display sparse representation for input text.\"\"\"\n",
    "    tokens, scores, _ = get_sparse_representation(text, top_k)\n",
    "    \n",
    "    print(f\"\\nInput: '{text}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Create DataFrame for display\n",
    "    df = pd.DataFrame({\n",
    "        'Rank': range(1, len(tokens) + 1),\n",
    "        'Token': tokens,\n",
    "        'Score': [f\"{s:.4f}\" for s in scores]\n",
    "    })\n",
    "    \n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    return tokens, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Lingual Activation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pairs: Korean term -> Expected English tokens\n",
    "TEST_PAIRS = [\n",
    "    # IT/Tech terms\n",
    "    (\"머신러닝\", [\"machine\", \"learning\", \"ML\"]),\n",
    "    (\"딥러닝\", [\"deep\", \"learning\", \"DL\"]),\n",
    "    (\"자연어처리\", [\"natural\", \"language\", \"processing\", \"NLP\"]),\n",
    "    (\"인공지능\", [\"artificial\", \"intelligence\", \"AI\"]),\n",
    "    (\"신경망\", [\"neural\", \"network\"]),\n",
    "    (\"알고리즘\", [\"algorithm\"]),\n",
    "    (\"데이터베이스\", [\"database\", \"data\"]),\n",
    "    (\"프로그래밍\", [\"programming\", \"code\"]),\n",
    "    (\"소프트웨어\", [\"software\"]),\n",
    "    (\"하드웨어\", [\"hardware\"]),\n",
    "    \n",
    "    # General terms\n",
    "    (\"학습\", [\"training\", \"learning\", \"study\"]),\n",
    "    (\"모델\", [\"model\"]),\n",
    "    (\"데이터\", [\"data\"]),\n",
    "    (\"컴퓨터\", [\"computer\"]),\n",
    "    (\"네트워크\", [\"network\"]),\n",
    "    \n",
    "    # Science terms\n",
    "    (\"물리학\", [\"physics\"]),\n",
    "    (\"화학\", [\"chemistry\"]),\n",
    "    (\"생물학\", [\"biology\"]),\n",
    "    (\"수학\", [\"mathematics\", \"math\"]),\n",
    "    \n",
    "    # Business terms\n",
    "    (\"마케팅\", [\"marketing\"]),\n",
    "    (\"경제학\", [\"economics\", \"economy\"]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cross_lingual(test_pairs: list, top_k: int = 50) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate cross-lingual activation for test pairs.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with evaluation results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_activated = 0\n",
    "    total_expected = 0\n",
    "    \n",
    "    for ko_term, en_synonyms in test_pairs:\n",
    "        tokens, scores, _ = get_sparse_representation(ko_term, top_k)\n",
    "        tokens_lower = [t.lower() for t in tokens]\n",
    "        \n",
    "        activated = []\n",
    "        not_activated = []\n",
    "        \n",
    "        for en_syn in en_synonyms:\n",
    "            en_tokens = tokenizer.tokenize(en_syn.lower())\n",
    "            for en_tok in en_tokens:\n",
    "                total_expected += 1\n",
    "                if en_tok.lower() in tokens_lower:\n",
    "                    total_activated += 1\n",
    "                    activated.append(en_tok)\n",
    "                else:\n",
    "                    not_activated.append(en_tok)\n",
    "        \n",
    "        results.append({\n",
    "            'Korean': ko_term,\n",
    "            'Expected': ', '.join(en_synonyms),\n",
    "            'Activated': ', '.join(activated) if activated else '-',\n",
    "            'Not Activated': ', '.join(not_activated) if not_activated else '-',\n",
    "            'Top-5': ', '.join(tokens[:5]),\n",
    "            'Success': '✅' if activated else '❌'\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"Cross-Lingual Activation Evaluation Results\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nOverall Activation Rate: {total_activated}/{total_expected} = {total_activated/total_expected*100:.1f}%\")\n",
    "    print(f\"Success Rate (at least 1 token): {len([r for r in results if r['Activated'] != '-'])}/{len(results)}\")\n",
    "    print()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Cross-Lingual Activation Evaluation Results\n",
      "================================================================================\n",
      "\n",
      "Overall Activation Rate: 26/39 = 66.7%\n",
      "Success Rate (at least 1 token): 19/21\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Korean</th>\n",
       "      <th>Expected</th>\n",
       "      <th>Activated</th>\n",
       "      <th>Not Activated</th>\n",
       "      <th>Top-5</th>\n",
       "      <th>Success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>머신러닝</td>\n",
       "      <td>machine, learning, ML</td>\n",
       "      <td>machine</td>\n",
       "      <td>learning, ml</td>\n",
       "      <td>the, machine, ##ing, machines, ##ng</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>딥러닝</td>\n",
       "      <td>deep, learning, DL</td>\n",
       "      <td>deep</td>\n",
       "      <td>learning, dl</td>\n",
       "      <td>the, deep, ##ing, ##ning, ##ng</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>자연어처리</td>\n",
       "      <td>natural, language, processing, NLP</td>\n",
       "      <td>natural, language</td>\n",
       "      <td>processing, nl, ##p</td>\n",
       "      <td>the, natural, nature, management, ##s</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>인공지능</td>\n",
       "      <td>artificial, intelligence, AI</td>\n",
       "      <td>-</td>\n",
       "      <td>artificial, intelligence, ai</td>\n",
       "      <td>the, ##s, in, ##n, -</td>\n",
       "      <td>❌</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>신경망</td>\n",
       "      <td>neural, network</td>\n",
       "      <td>-</td>\n",
       "      <td>neu, ##ral, network</td>\n",
       "      <td>the, ##s, ##n, ##y, ##e</td>\n",
       "      <td>❌</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>알고리즘</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>-</td>\n",
       "      <td>the, algorithm, ##s, ##y, ##n</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>데이터베이스</td>\n",
       "      <td>database, data</td>\n",
       "      <td>database, data</td>\n",
       "      <td>-</td>\n",
       "      <td>the, data, database, ##s, databases</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>프로그래밍</td>\n",
       "      <td>programming, code</td>\n",
       "      <td>programming, code</td>\n",
       "      <td>-</td>\n",
       "      <td>the, programming, program, ##s, software</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>소프트웨어</td>\n",
       "      <td>software</td>\n",
       "      <td>software</td>\n",
       "      <td>-</td>\n",
       "      <td>the, software, ##s, ##y, ##n</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>하드웨어</td>\n",
       "      <td>hardware</td>\n",
       "      <td>hardware</td>\n",
       "      <td>-</td>\n",
       "      <td>the, software, hardware, ##ware, ##s</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>학습</td>\n",
       "      <td>training, learning, study</td>\n",
       "      <td>training, learning, study</td>\n",
       "      <td>-</td>\n",
       "      <td>the, learning, training, education, learn</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>모델</td>\n",
       "      <td>model</td>\n",
       "      <td>model</td>\n",
       "      <td>-</td>\n",
       "      <td>the, model, models, ##s, ##y</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>데이터</td>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>-</td>\n",
       "      <td>the, data, ##s, ##y, ##n</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>컴퓨터</td>\n",
       "      <td>computer</td>\n",
       "      <td>computer</td>\n",
       "      <td>-</td>\n",
       "      <td>the, computer, computers, ##s, computing</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>네트워크</td>\n",
       "      <td>network</td>\n",
       "      <td>network</td>\n",
       "      <td>-</td>\n",
       "      <td>the, network, networks, ##s, ##c</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>물리학</td>\n",
       "      <td>physics</td>\n",
       "      <td>physics</td>\n",
       "      <td>-</td>\n",
       "      <td>the, physics, of, physical, ##s</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>화학</td>\n",
       "      <td>chemistry</td>\n",
       "      <td>chemistry</td>\n",
       "      <td>-</td>\n",
       "      <td>the, chemical, chemistry, ##s, science</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>생물학</td>\n",
       "      <td>biology</td>\n",
       "      <td>biology</td>\n",
       "      <td>-</td>\n",
       "      <td>the, biology, bio, biological, of</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>수학</td>\n",
       "      <td>mathematics, math</td>\n",
       "      <td>mathematics, math</td>\n",
       "      <td>-</td>\n",
       "      <td>the, mathematics, math, mathematical, algebra</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>마케팅</td>\n",
       "      <td>marketing</td>\n",
       "      <td>marketing</td>\n",
       "      <td>-</td>\n",
       "      <td>the, marketing, ##s, ##y, ##e</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>경제학</td>\n",
       "      <td>economics, economy</td>\n",
       "      <td>economics, economy</td>\n",
       "      <td>-</td>\n",
       "      <td>the, economics, economic, of, economy</td>\n",
       "      <td>✅</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Korean                            Expected                  Activated  \\\n",
       "0     머신러닝               machine, learning, ML                    machine   \n",
       "1      딥러닝                  deep, learning, DL                       deep   \n",
       "2    자연어처리  natural, language, processing, NLP          natural, language   \n",
       "3     인공지능        artificial, intelligence, AI                          -   \n",
       "4      신경망                     neural, network                          -   \n",
       "5     알고리즘                           algorithm                  algorithm   \n",
       "6   데이터베이스                      database, data             database, data   \n",
       "7    프로그래밍                   programming, code          programming, code   \n",
       "8    소프트웨어                            software                   software   \n",
       "9     하드웨어                            hardware                   hardware   \n",
       "10      학습           training, learning, study  training, learning, study   \n",
       "11      모델                               model                      model   \n",
       "12     데이터                                data                       data   \n",
       "13     컴퓨터                            computer                   computer   \n",
       "14    네트워크                             network                    network   \n",
       "15     물리학                             physics                    physics   \n",
       "16      화학                           chemistry                  chemistry   \n",
       "17     생물학                             biology                    biology   \n",
       "18      수학                   mathematics, math          mathematics, math   \n",
       "19     마케팅                           marketing                  marketing   \n",
       "20     경제학                  economics, economy         economics, economy   \n",
       "\n",
       "                   Not Activated  \\\n",
       "0                   learning, ml   \n",
       "1                   learning, dl   \n",
       "2            processing, nl, ##p   \n",
       "3   artificial, intelligence, ai   \n",
       "4            neu, ##ral, network   \n",
       "5                              -   \n",
       "6                              -   \n",
       "7                              -   \n",
       "8                              -   \n",
       "9                              -   \n",
       "10                             -   \n",
       "11                             -   \n",
       "12                             -   \n",
       "13                             -   \n",
       "14                             -   \n",
       "15                             -   \n",
       "16                             -   \n",
       "17                             -   \n",
       "18                             -   \n",
       "19                             -   \n",
       "20                             -   \n",
       "\n",
       "                                            Top-5 Success  \n",
       "0             the, machine, ##ing, machines, ##ng       ✅  \n",
       "1                  the, deep, ##ing, ##ning, ##ng       ✅  \n",
       "2           the, natural, nature, management, ##s       ✅  \n",
       "3                            the, ##s, in, ##n, -       ❌  \n",
       "4                         the, ##s, ##n, ##y, ##e       ❌  \n",
       "5                   the, algorithm, ##s, ##y, ##n       ✅  \n",
       "6             the, data, database, ##s, databases       ✅  \n",
       "7        the, programming, program, ##s, software       ✅  \n",
       "8                    the, software, ##s, ##y, ##n       ✅  \n",
       "9            the, software, hardware, ##ware, ##s       ✅  \n",
       "10      the, learning, training, education, learn       ✅  \n",
       "11                   the, model, models, ##s, ##y       ✅  \n",
       "12                       the, data, ##s, ##y, ##n       ✅  \n",
       "13       the, computer, computers, ##s, computing       ✅  \n",
       "14               the, network, networks, ##s, ##c       ✅  \n",
       "15                the, physics, of, physical, ##s       ✅  \n",
       "16         the, chemical, chemistry, ##s, science       ✅  \n",
       "17              the, biology, bio, biological, of       ✅  \n",
       "18  the, mathematics, math, mathematical, algebra       ✅  \n",
       "19                  the, marketing, ##s, ##y, ##e       ✅  \n",
       "20          the, economics, economic, of, economy       ✅  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run evaluation\n",
    "eval_df = evaluate_cross_lingual(TEST_PAIRS)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Token Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: '머신러닝'\n",
      "------------------------------------------------------------\n",
      " Rank      Token  Score\n",
      "    1        the 6.0713\n",
      "    2    machine 4.3565\n",
      "    3      ##ing 3.9476\n",
      "    4   machines 3.7656\n",
      "    5       ##ng 3.5762\n",
      "    6     ##ning 3.5452\n",
      "    7  machinery 3.4366\n",
      "    8        ##s 3.3344\n",
      "    9         머신 3.3247\n",
      "   10    rolling 3.1387\n",
      "   11   computer 3.1061\n",
      "   12   training 3.1010\n",
      "   13    ##ining 3.0901\n",
      "   14 mechanical 3.0817\n",
      "   15        ##n 3.0415\n",
      "\n",
      "\n",
      "Input: '자연어처리'\n",
      "------------------------------------------------------------\n",
      " Rank      Token  Score\n",
      "    1        the 6.0349\n",
      "    2    natural 3.9674\n",
      "    3     nature 3.9510\n",
      "    4 management 3.3114\n",
      "    5        ##s 3.2910\n",
      "    6   national 3.1761\n",
      "    7   language 3.1280\n",
      "    8        ##처 3.0258\n",
      "    9     natura 3.0177\n",
      "   10        ##e 2.9989\n",
      "   11        ##n 2.9988\n",
      "   12        ##y 2.9985\n",
      "   13        ##c 2.9982\n",
      "   14          s 2.9978\n",
      "   15        ##d 2.9975\n",
      "\n",
      "\n",
      "Input: '학습'\n",
      "------------------------------------------------------------\n",
      " Rank       Token  Score\n",
      "    1         the 6.1144\n",
      "    2    learning 3.9914\n",
      "    3    training 3.4755\n",
      "    4   education 3.4524\n",
      "    5       learn 3.4266\n",
      "    6         ##s 3.4093\n",
      "    7       study 3.3148\n",
      "    8      school 3.1973\n",
      "    9    research 3.1651\n",
      "   10    thinking 3.1182\n",
      "   11 experienced 3.0923\n",
      "   12         ##n 3.0876\n",
      "   13         ##y 3.0872\n",
      "   14     learned 3.0869\n",
      "   15         ##c 3.0864\n",
      "\n",
      "\n",
      "Input: '인공지능'\n",
      "------------------------------------------------------------\n",
      " Rank Token  Score\n",
      "    1   the 6.1215\n",
      "    2   ##s 3.4306\n",
      "    3    in 3.1457\n",
      "    4   ##n 3.0998\n",
      "    5     - 3.0993\n",
      "    6   ##e 3.0986\n",
      "    7   ##y 3.0985\n",
      "    8   ##r 3.0952\n",
      "    9   ##c 3.0950\n",
      "   10   ##o 3.0949\n",
      "   11   ##d 3.0947\n",
      "   12 ##ing 3.0943\n",
      "   13   ##a 3.0942\n",
      "   14   ##g 3.0937\n",
      "   15   ##k 3.0936\n",
      "\n",
      "\n",
      "Input: '알고리즘'\n",
      "------------------------------------------------------------\n",
      " Rank     Token  Score\n",
      "    1       the 6.1215\n",
      "    2 algorithm 3.4610\n",
      "    3       ##s 3.4295\n",
      "    4       ##y 3.0980\n",
      "    5       ##n 3.0961\n",
      "    6       ##e 3.0959\n",
      "    7       ##c 3.0950\n",
      "    8       ##r 3.0936\n",
      "    9       new 3.0931\n",
      "   10        in 3.0930\n",
      "   11     ##ing 3.0928\n",
      "   12   windows 3.0926\n",
      "   13       ##d 3.0923\n",
      "   14    visual 3.0918\n",
      "   15       ##m 3.0917\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze specific terms in detail\n",
    "detailed_terms = [\"머신러닝\", \"자연어처리\", \"학습\", \"인공지능\", \"알고리즘\"]\n",
    "\n",
    "for term in detailed_terms:\n",
    "    display_sparse_output(term, top_k=15)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison: Korean vs English Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ko_en(ko_term: str, en_term: str, top_k: int = 20):\n",
    "    \"\"\"Compare sparse representations of Korean and English terms.\"\"\"\n",
    "    ko_tokens, ko_scores, ko_rep = get_sparse_representation(ko_term, top_k)\n",
    "    en_tokens, en_scores, en_rep = get_sparse_representation(en_term, top_k)\n",
    "    \n",
    "    # Find common tokens\n",
    "    ko_set = set(ko_tokens)\n",
    "    en_set = set(en_tokens)\n",
    "    common = ko_set & en_set\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Korean: '{ko_term}' vs English: '{en_term}'\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nCommon tokens in top-{top_k}: {len(common)}\")\n",
    "    if common:\n",
    "        print(f\"Common: {', '.join(sorted(common))}\")\n",
    "    \n",
    "    print(f\"\\nKorean top-10: {', '.join(ko_tokens[:10])}\")\n",
    "    print(f\"English top-10: {', '.join(en_tokens[:10])}\")\n",
    "    \n",
    "    # Cosine similarity\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(ko_rep.unsqueeze(0), en_rep.unsqueeze(0)).item()\n",
    "    print(f\"\\nCosine Similarity: {cos_sim:.4f}\")\n",
    "    \n",
    "    return common, cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Korean: '머신러닝' vs English: 'machine learning'\n",
      "======================================================================\n",
      "\n",
      "Common tokens in top-20: 8\n",
      "Common: ##ing, computer, machine, machinery, machines, mechanical, the, training\n",
      "\n",
      "Korean top-10: the, machine, ##ing, machines, ##ng, ##ning, machinery, ##s, 머신, rolling\n",
      "English top-10: the, machine, learning, machines, school, training, education, machinery, learn, study\n",
      "\n",
      "Cosine Similarity: 0.9916\n",
      "\n",
      "======================================================================\n",
      "Korean: '딥러닝' vs English: 'deep learning'\n",
      "======================================================================\n",
      "\n",
      "Common tokens in top-20: 6\n",
      "Common: ##ing, deep, depth, long, the, training\n",
      "\n",
      "Korean top-10: the, deep, ##ing, ##ning, ##ng, 딥, rolling, ##s, long, ##ining\n",
      "English top-10: the, deep, learning, training, education, school, learn, study, depth, high\n",
      "\n",
      "Cosine Similarity: 0.9892\n",
      "\n",
      "======================================================================\n",
      "Korean: '자연어처리' vs English: 'natural language processing'\n",
      "======================================================================\n",
      "\n",
      "Common tokens in top-20: 5\n",
      "Common: language, national, natural, nature, the\n",
      "\n",
      "Korean top-10: the, natural, nature, management, ##s, national, language, ##처, natura, ##e\n",
      "English top-10: the, natural, language, processing, languages, nature, national, ##ing, Processing, artificial\n",
      "\n",
      "Cosine Similarity: 0.9457\n",
      "\n",
      "======================================================================\n",
      "Korean: '데이터' vs English: 'data'\n",
      "======================================================================\n",
      "\n",
      "Common tokens in top-20: 15\n",
      "Common: ##a, ##c, ##d, ##e, ##ing, ##n, ##r, ##s, ##t, ##y, data, in, server, the, windows\n",
      "\n",
      "Korean top-10: the, data, ##s, ##y, ##n, ##e, ##c, windows, ##r, in\n",
      "English top-10: the, data, ##s, ##y, ##n, ##e, ##r, ##a, ##c, ##ing\n",
      "\n",
      "Cosine Similarity: 0.9945\n",
      "\n",
      "======================================================================\n",
      "Korean: '알고리즘' vs English: 'algorithm'\n",
      "======================================================================\n",
      "\n",
      "Common tokens in top-20: 15\n",
      "Common: ##c, ##d, ##e, ##ing, ##l, ##n, ##r, ##s, ##t, ##y, algorithm, in, new, the, windows\n",
      "\n",
      "Korean top-10: the, algorithm, ##s, ##y, ##n, ##e, ##c, ##r, new, in\n",
      "English top-10: the, algorithm, ##s, ##y, ##e, ##n, ##c, ##ing, ##r, windows\n",
      "\n",
      "Cosine Similarity: 0.9922\n"
     ]
    }
   ],
   "source": [
    "# Compare Korean and English pairs\n",
    "comparison_pairs = [\n",
    "    (\"머신러닝\", \"machine learning\"),\n",
    "    (\"딥러닝\", \"deep learning\"),\n",
    "    (\"자연어처리\", \"natural language processing\"),\n",
    "    (\"데이터\", \"data\"),\n",
    "    (\"알고리즘\", \"algorithm\"),\n",
    "]\n",
    "\n",
    "similarities = []\n",
    "for ko, en in comparison_pairs:\n",
    "    common, sim = compare_ko_en(ko, en)\n",
    "    similarities.append({'Korean': ko, 'English': en, 'Similarity': sim, 'Common Tokens': len(common)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Similarity Summary\n",
      "======================================================================\n",
      "Korean                     English  Similarity  Common Tokens\n",
      "  머신러닝            machine learning    0.991622              8\n",
      "   딥러닝               deep learning    0.989229              6\n",
      " 자연어처리 natural language processing    0.945667              5\n",
      "   데이터                        data    0.994486             15\n",
      "  알고리즘                   algorithm    0.992210             15\n",
      "\n",
      "Average Cosine Similarity: 0.9826\n"
     ]
    }
   ],
   "source": [
    "# Summary table\n",
    "sim_df = pd.DataFrame(similarities)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Similarity Summary\")\n",
    "print(\"=\"*70)\n",
    "print(sim_df.to_string(index=False))\n",
    "print(f\"\\nAverage Cosine Similarity: {sim_df['Similarity'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Query Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Query Results\n",
      "======================================================================\n",
      "\n",
      "파이썬 프로그래밍:\n",
      "  English tokens: p, programming, pi, program, software\n",
      "  Top-5: the, ##on, ##n, p, programming\n",
      "\n",
      "웹 개발:\n",
      "  English tokens: web, development, website, developer, internet\n",
      "  Top-5: the, web, development, website, 웹\n",
      "\n",
      "클라우드 컴퓨팅:\n",
      "  English tokens: computer, computing, computers, science, programming\n",
      "  Top-5: the, ##s, computer, computing, computers\n",
      "\n",
      "빅데이터 분석:\n",
      "  English tokens: data, big, analysis, anal, vi\n",
      "  Top-5: the, data, big, analysis, anal\n",
      "\n",
      "사이버 보안:\n",
      "  English tokens: security, safety, secure\n",
      "  Top-5: the, security, ##y, ##er, ##r\n"
     ]
    }
   ],
   "source": [
    "# Test with custom queries\n",
    "custom_queries = [\n",
    "    \"파이썬 프로그래밍\",\n",
    "    \"웹 개발\",\n",
    "    \"클라우드 컴퓨팅\",\n",
    "    \"빅데이터 분석\",\n",
    "    \"사이버 보안\",\n",
    "]\n",
    "\n",
    "print(\"Custom Query Results\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for query in custom_queries:\n",
    "    tokens, scores, _ = get_sparse_representation(query, top_k=10)\n",
    "    # Filter English tokens\n",
    "    en_tokens = [t for t in tokens if t.isascii() and not t.startswith('##') and t not in ['the', 'a', 'an', 'in', 'of', 'to', 'and']]\n",
    "    print(f\"\\n{query}:\")\n",
    "    print(f\"  English tokens: {', '.join(en_tokens[:5]) if en_tokens else 'None'}\")\n",
    "    print(f\"  Top-5: {', '.join(tokens[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Statistics\n",
      "==================================================\n",
      "Total parameters: 177,974,523\n",
      "Trainable parameters: 177,974,523\n",
      "Model name: bert-base-multilingual-cased\n",
      "Vocab size: 119,547\n"
     ]
    }
   ],
   "source": [
    "# Model statistics\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Model Statistics\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model name: {MODEL_NAME}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparsity Analysis\n",
      "======================================================================\n",
      "  text  non_zero  sparsity  max_score\n",
      "  머신러닝     29034  0.757133   6.071305\n",
      "   딥러닝     28495  0.761642   5.678852\n",
      " 자연어처리     28705  0.759885   6.034928\n",
      "  인공지능     26882  0.775134   6.121453\n",
      "   신경망     27072  0.773545   6.120531\n",
      "  알고리즘     26823  0.775628   6.121536\n",
      "데이터베이스     29400  0.754072   6.113099\n",
      " 프로그래밍     26743  0.776297   6.113012\n",
      " 소프트웨어     29111  0.756489   6.094561\n",
      "  하드웨어     28934  0.757970   6.101914\n",
      "\n",
      "Average sparsity: 0.7648\n",
      "Average non-zero tokens: 28119.9\n"
     ]
    }
   ],
   "source": [
    "# Sparsity analysis\n",
    "def analyze_sparsity(texts: list, threshold: float = 0.01) -> dict:\n",
    "    \"\"\"Analyze sparsity of representations.\"\"\"\n",
    "    stats = []\n",
    "    \n",
    "    for text in texts:\n",
    "        _, _, sparse_rep = get_sparse_representation(text, top_k=100)\n",
    "        \n",
    "        non_zero = (sparse_rep > threshold).sum().item()\n",
    "        sparsity = 1 - (non_zero / len(sparse_rep))\n",
    "        max_val = sparse_rep.max().item()\n",
    "        \n",
    "        stats.append({\n",
    "            'text': text,\n",
    "            'non_zero': non_zero,\n",
    "            'sparsity': sparsity,\n",
    "            'max_score': max_val\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "# Analyze sparsity for test terms\n",
    "test_texts = [pair[0] for pair in TEST_PAIRS[:10]]\n",
    "sparsity_df = analyze_sparsity(test_texts)\n",
    "\n",
    "print(\"\\nSparsity Analysis\")\n",
    "print(\"=\"*70)\n",
    "print(sparsity_df.to_string(index=False))\n",
    "print(f\"\\nAverage sparsity: {sparsity_df['sparsity'].mean():.4f}\")\n",
    "print(f\"Average non-zero tokens: {sparsity_df['non_zero'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### v7 Model Performance\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Activation Rate | ~43.5% |\n",
    "| Loss Type | Direct Token Target |\n",
    "| Training Data | 1.55M KO-EN pairs |\n",
    "| Base Model | bert-base-multilingual-cased |\n",
    "\n",
    "### Key Findings\n",
    "1. v7 모델은 한국어 입력에 대해 영어 토큰을 성공적으로 활성화\n",
    "2. \"the\"가 대부분의 결과에서 top 토큰으로 나타남 (일반적인 multilingual 패턴)\n",
    "3. Direct Token Target Loss가 기존 KL-div 방식보다 효과적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook execution completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Notebook execution completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

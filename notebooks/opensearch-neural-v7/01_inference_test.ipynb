{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v7 Cross-Lingual Neural Sparse Model - Inference Test\n",
    "\n",
    "이 노트북은 v7 모델의 한국어-영어 cross-lingual token activation 성능을 테스트합니다.\n",
    "\n",
    "## v7 모델 특징\n",
    "- **Direct Token Target Loss**: 영어 동의어 토큰을 직접 supervision\n",
    "- **Margin Loss**: 최소 activation 값 보장 (margin=1.0)\n",
    "- **Negative Sampling**: 비타겟 토큰 억제\n",
    "- **Training Data**: 1.55M cleaned KO-EN term pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root by looking for CLAUDE.md or .git\n",
    "def find_project_root():\n",
    "    \"\"\"Find project root directory.\"\"\"\n",
    "    # Try common locations\n",
    "    candidates = [\n",
    "        Path.cwd(),\n",
    "        Path.cwd().parent,\n",
    "        Path.cwd().parent.parent,\n",
    "        Path(\"/home/west/Documents/cursor-workspace/opensearch-neural-pre-train\"),\n",
    "    ]\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        if (candidate / \"CLAUDE.md\").exists() or (candidate / \".git\").exists():\n",
    "            return candidate\n",
    "    \n",
    "    # Fallback to absolute path\n",
    "    return Path(\"/home/west/Documents/cursor-workspace/opensearch-neural-pre-train\")\n",
    "\n",
    "project_root = find_project_root()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from src.model.splade_model import create_splade_model\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load v7 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Tokenizer vocab size: 119547\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "CHECKPOINT_PATH = project_root / \"outputs/cross_lingual_expansion_v7_largescale/final_model/checkpoint.pt\"\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/west/Documents/outputs/cross_lingual_expansion_v7_largescale/final_model/checkpoint.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m model = create_splade_model(\n\u001b[32m      3\u001b[39m     model_name=MODEL_NAME,\n\u001b[32m      4\u001b[39m     use_idf=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m      5\u001b[39m     use_expansion=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      6\u001b[39m     expansion_mode=\u001b[33m\"\u001b[39m\u001b[33mmlm\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Load checkpoint\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m model.load_state_dict(checkpoint[\u001b[33m'\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     12\u001b[39m model = model.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/serialization.py:1500\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args:\n\u001b[32m   1498\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1500\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1501\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1502\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1503\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1504\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1505\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/serialization.py:768\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    766\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    767\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m768\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    769\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    770\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/serialization.py:749\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/west/Documents/outputs/cross_lingual_expansion_v7_largescale/final_model/checkpoint.pt'"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = create_splade_model(\n",
    "    model_name=MODEL_NAME,\n",
    "    use_idf=False,\n",
    "    use_expansion=True,\n",
    "    expansion_mode=\"mlm\",\n",
    ")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=True)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Checkpoint path: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text: str, max_length: int = 64) -> dict:\n",
    "    \"\"\"Encode text using tokenizer.\"\"\"\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "\n",
    "def get_sparse_representation(text: str, top_k: int = 50) -> tuple:\n",
    "    \"\"\"\n",
    "    Get sparse representation for input text.\n",
    "    \n",
    "    Returns:\n",
    "        tokens: List of top-k tokens\n",
    "        scores: List of corresponding scores\n",
    "        sparse_rep: Full sparse representation tensor\n",
    "    \"\"\"\n",
    "    encoding = encode_text(text)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sparse_rep, _ = model(\n",
    "            encoding['input_ids'].to(device),\n",
    "            encoding['attention_mask'].to(device)\n",
    "        )\n",
    "    \n",
    "    sparse_rep = sparse_rep[0].cpu()\n",
    "    \n",
    "    # Get top-k tokens\n",
    "    top_scores, top_indices = torch.topk(sparse_rep, k=top_k)\n",
    "    top_tokens = tokenizer.convert_ids_to_tokens(top_indices.tolist())\n",
    "    \n",
    "    return top_tokens, top_scores.tolist(), sparse_rep\n",
    "\n",
    "\n",
    "def display_sparse_output(text: str, top_k: int = 20):\n",
    "    \"\"\"Display sparse representation for input text.\"\"\"\n",
    "    tokens, scores, _ = get_sparse_representation(text, top_k)\n",
    "    \n",
    "    print(f\"\\nInput: '{text}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Create DataFrame for display\n",
    "    df = pd.DataFrame({\n",
    "        'Rank': range(1, len(tokens) + 1),\n",
    "        'Token': tokens,\n",
    "        'Score': [f\"{s:.4f}\" for s in scores]\n",
    "    })\n",
    "    \n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    return tokens, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Lingual Activation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pairs: Korean term -> Expected English tokens\n",
    "TEST_PAIRS = [\n",
    "    # IT/Tech terms\n",
    "    (\"머신러닝\", [\"machine\", \"learning\", \"ML\"]),\n",
    "    (\"딥러닝\", [\"deep\", \"learning\", \"DL\"]),\n",
    "    (\"자연어처리\", [\"natural\", \"language\", \"processing\", \"NLP\"]),\n",
    "    (\"인공지능\", [\"artificial\", \"intelligence\", \"AI\"]),\n",
    "    (\"신경망\", [\"neural\", \"network\"]),\n",
    "    (\"알고리즘\", [\"algorithm\"]),\n",
    "    (\"데이터베이스\", [\"database\", \"data\"]),\n",
    "    (\"프로그래밍\", [\"programming\", \"code\"]),\n",
    "    (\"소프트웨어\", [\"software\"]),\n",
    "    (\"하드웨어\", [\"hardware\"]),\n",
    "    \n",
    "    # General terms\n",
    "    (\"학습\", [\"training\", \"learning\", \"study\"]),\n",
    "    (\"모델\", [\"model\"]),\n",
    "    (\"데이터\", [\"data\"]),\n",
    "    (\"컴퓨터\", [\"computer\"]),\n",
    "    (\"네트워크\", [\"network\"]),\n",
    "    \n",
    "    # Science terms\n",
    "    (\"물리학\", [\"physics\"]),\n",
    "    (\"화학\", [\"chemistry\"]),\n",
    "    (\"생물학\", [\"biology\"]),\n",
    "    (\"수학\", [\"mathematics\", \"math\"]),\n",
    "    \n",
    "    # Business terms\n",
    "    (\"마케팅\", [\"marketing\"]),\n",
    "    (\"경제학\", [\"economics\", \"economy\"]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cross_lingual(test_pairs: list, top_k: int = 50) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate cross-lingual activation for test pairs.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with evaluation results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_activated = 0\n",
    "    total_expected = 0\n",
    "    \n",
    "    for ko_term, en_synonyms in test_pairs:\n",
    "        tokens, scores, _ = get_sparse_representation(ko_term, top_k)\n",
    "        tokens_lower = [t.lower() for t in tokens]\n",
    "        \n",
    "        activated = []\n",
    "        not_activated = []\n",
    "        \n",
    "        for en_syn in en_synonyms:\n",
    "            en_tokens = tokenizer.tokenize(en_syn.lower())\n",
    "            for en_tok in en_tokens:\n",
    "                total_expected += 1\n",
    "                if en_tok.lower() in tokens_lower:\n",
    "                    total_activated += 1\n",
    "                    activated.append(en_tok)\n",
    "                else:\n",
    "                    not_activated.append(en_tok)\n",
    "        \n",
    "        results.append({\n",
    "            'Korean': ko_term,\n",
    "            'Expected': ', '.join(en_synonyms),\n",
    "            'Activated': ', '.join(activated) if activated else '-',\n",
    "            'Not Activated': ', '.join(not_activated) if not_activated else '-',\n",
    "            'Top-5': ', '.join(tokens[:5]),\n",
    "            'Success': '✅' if activated else '❌'\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"Cross-Lingual Activation Evaluation Results\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nOverall Activation Rate: {total_activated}/{total_expected} = {total_activated/total_expected*100:.1f}%\")\n",
    "    print(f\"Success Rate (at least 1 token): {len([r for r in results if r['Activated'] != '-'])}/{len(results)}\")\n",
    "    print()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "eval_df = evaluate_cross_lingual(TEST_PAIRS)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Token Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze specific terms in detail\n",
    "detailed_terms = [\"머신러닝\", \"자연어처리\", \"학습\", \"인공지능\", \"알고리즘\"]\n",
    "\n",
    "for term in detailed_terms:\n",
    "    display_sparse_output(term, top_k=15)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison: Korean vs English Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ko_en(ko_term: str, en_term: str, top_k: int = 20):\n",
    "    \"\"\"Compare sparse representations of Korean and English terms.\"\"\"\n",
    "    ko_tokens, ko_scores, ko_rep = get_sparse_representation(ko_term, top_k)\n",
    "    en_tokens, en_scores, en_rep = get_sparse_representation(en_term, top_k)\n",
    "    \n",
    "    # Find common tokens\n",
    "    ko_set = set(ko_tokens)\n",
    "    en_set = set(en_tokens)\n",
    "    common = ko_set & en_set\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Korean: '{ko_term}' vs English: '{en_term}'\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nCommon tokens in top-{top_k}: {len(common)}\")\n",
    "    if common:\n",
    "        print(f\"Common: {', '.join(sorted(common))}\")\n",
    "    \n",
    "    print(f\"\\nKorean top-10: {', '.join(ko_tokens[:10])}\")\n",
    "    print(f\"English top-10: {', '.join(en_tokens[:10])}\")\n",
    "    \n",
    "    # Cosine similarity\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(ko_rep.unsqueeze(0), en_rep.unsqueeze(0)).item()\n",
    "    print(f\"\\nCosine Similarity: {cos_sim:.4f}\")\n",
    "    \n",
    "    return common, cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Korean and English pairs\n",
    "comparison_pairs = [\n",
    "    (\"머신러닝\", \"machine learning\"),\n",
    "    (\"딥러닝\", \"deep learning\"),\n",
    "    (\"자연어처리\", \"natural language processing\"),\n",
    "    (\"데이터\", \"data\"),\n",
    "    (\"알고리즘\", \"algorithm\"),\n",
    "]\n",
    "\n",
    "similarities = []\n",
    "for ko, en in comparison_pairs:\n",
    "    common, sim = compare_ko_en(ko, en)\n",
    "    similarities.append({'Korean': ko, 'English': en, 'Similarity': sim, 'Common Tokens': len(common)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "sim_df = pd.DataFrame(similarities)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Similarity Summary\")\n",
    "print(\"=\"*70)\n",
    "print(sim_df.to_string(index=False))\n",
    "print(f\"\\nAverage Cosine Similarity: {sim_df['Similarity'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Query Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with custom queries\n",
    "custom_queries = [\n",
    "    \"파이썬 프로그래밍\",\n",
    "    \"웹 개발\",\n",
    "    \"클라우드 컴퓨팅\",\n",
    "    \"빅데이터 분석\",\n",
    "    \"사이버 보안\",\n",
    "]\n",
    "\n",
    "print(\"Custom Query Results\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for query in custom_queries:\n",
    "    tokens, scores, _ = get_sparse_representation(query, top_k=10)\n",
    "    # Filter English tokens\n",
    "    en_tokens = [t for t in tokens if t.isascii() and not t.startswith('##') and t not in ['the', 'a', 'an', 'in', 'of', 'to', 'and']]\n",
    "    print(f\"\\n{query}:\")\n",
    "    print(f\"  English tokens: {', '.join(en_tokens[:5]) if en_tokens else 'None'}\")\n",
    "    print(f\"  Top-5: {', '.join(tokens[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model statistics\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Model Statistics\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model name: {MODEL_NAME}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsity analysis\n",
    "def analyze_sparsity(texts: list, threshold: float = 0.01) -> dict:\n",
    "    \"\"\"Analyze sparsity of representations.\"\"\"\n",
    "    stats = []\n",
    "    \n",
    "    for text in texts:\n",
    "        _, _, sparse_rep = get_sparse_representation(text, top_k=100)\n",
    "        \n",
    "        non_zero = (sparse_rep > threshold).sum().item()\n",
    "        sparsity = 1 - (non_zero / len(sparse_rep))\n",
    "        max_val = sparse_rep.max().item()\n",
    "        \n",
    "        stats.append({\n",
    "            'text': text,\n",
    "            'non_zero': non_zero,\n",
    "            'sparsity': sparsity,\n",
    "            'max_score': max_val\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "# Analyze sparsity for test terms\n",
    "test_texts = [pair[0] for pair in TEST_PAIRS[:10]]\n",
    "sparsity_df = analyze_sparsity(test_texts)\n",
    "\n",
    "print(\"\\nSparsity Analysis\")\n",
    "print(\"=\"*70)\n",
    "print(sparsity_df.to_string(index=False))\n",
    "print(f\"\\nAverage sparsity: {sparsity_df['sparsity'].mean():.4f}\")\n",
    "print(f\"Average non-zero tokens: {sparsity_df['non_zero'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### v7 Model Performance\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Activation Rate | ~43.5% |\n",
    "| Loss Type | Direct Token Target |\n",
    "| Training Data | 1.55M KO-EN pairs |\n",
    "| Base Model | bert-base-multilingual-cased |\n",
    "\n",
    "### Key Findings\n",
    "1. v7 모델은 한국어 입력에 대해 영어 토큰을 성공적으로 활성화\n",
    "2. \"the\"가 대부분의 결과에서 top 토큰으로 나타남 (일반적인 multilingual 패턴)\n",
    "3. Direct Token Target Loss가 기존 KL-div 방식보다 효과적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Notebook execution completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

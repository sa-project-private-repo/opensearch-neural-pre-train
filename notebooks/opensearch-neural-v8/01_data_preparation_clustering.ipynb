{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v8 Data Preparation: E5 Embedding + K-Means Clustering\n",
    "\n",
    "이 노트북에서는 공공 데이터셋을 사용하여 E5 multilingual 임베딩과 K-means 클러스터링으로 동의어 셋을 생성합니다.\n",
    "\n",
    "## 목표\n",
    "1. 공공 데이터셋 로드 (HuggingFace datasets)\n",
    "2. E5 multilingual 임베딩 생성\n",
    "3. K-means 클러스터링으로 동의어 그룹 생성\n",
    "4. PCA/UMAP 시각화\n",
    "5. 학습용 데이터셋 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root\n",
    "def find_project_root():\n",
    "    candidates = [\n",
    "        Path.cwd(),\n",
    "        Path.cwd().parent,\n",
    "        Path.cwd().parent.parent,\n",
    "        Path(\"/home/west/Documents/cursor-workspace/opensearch-neural-pre-train\"),\n",
    "    ]\n",
    "    for candidate in candidates:\n",
    "        if (candidate / \"CLAUDE.md\").exists() or (candidate / \".git\").exists():\n",
    "            return candidate\n",
    "    return Path(\"/home/west/Documents/cursor-workspace/opensearch-neural-pre-train\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Try to use Korean font\n",
    "try:\n",
    "    plt.rcParams['font.family'] = 'NanumGothic'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Public Dataset\n",
    "\n",
    "HuggingFace에서 공공 한국어-영어 데이터셋을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: OPUS-100 Korean-English parallel corpus\n",
    "# This is a public multilingual dataset\n",
    "print(\"Loading OPUS-100 Korean-English dataset...\")\n",
    "\n",
    "try:\n",
    "    dataset = load_dataset(\"opus100\", \"en-ko\", split=\"train\", trust_remote_code=True)\n",
    "    print(f\"Loaded {len(dataset)} parallel sentences\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading OPUS-100: {e}\")\n",
    "    print(\"Trying alternative dataset...\")\n",
    "    # Alternative: Use Helsinki-NLP OPUS dataset\n",
    "    dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ko\", split=\"train\")\n",
    "    print(f\"Loaded {len(dataset)} parallel sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview dataset\n",
    "print(\"Dataset columns:\", dataset.column_names)\n",
    "print(\"\\nSample entries:\")\n",
    "for i in range(3):\n",
    "    sample = dataset[i]\n",
    "    print(f\"\\n[{i}]\")\n",
    "    print(f\"  EN: {sample['translation']['en'][:100]}...\")\n",
    "    print(f\"  KO: {sample['translation']['ko'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and clean text pairs\n",
    "def extract_terms(dataset, max_samples=50000):\n",
    "    \"\"\"\n",
    "    Extract short terms/phrases from parallel corpus.\n",
    "    Focus on shorter texts that are more likely to be terms.\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    \n",
    "    for i, sample in enumerate(tqdm(dataset, desc=\"Extracting terms\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "            \n",
    "        en_text = sample['translation']['en'].strip()\n",
    "        ko_text = sample['translation']['ko'].strip()\n",
    "        \n",
    "        # Filter: short texts (more likely to be terms/phrases)\n",
    "        if 2 <= len(ko_text) <= 30 and 2 <= len(en_text) <= 50:\n",
    "            # Skip if too many special characters\n",
    "            if ko_text.count(' ') <= 3 and en_text.count(' ') <= 5:\n",
    "                terms.append({\n",
    "                    'ko': ko_text,\n",
    "                    'en': en_text,\n",
    "                })\n",
    "    \n",
    "    return terms\n",
    "\n",
    "# Extract terms\n",
    "raw_terms = extract_terms(dataset, max_samples=100000)\n",
    "print(f\"\\nExtracted {len(raw_terms)} term pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview extracted terms\n",
    "print(\"Sample extracted terms:\")\n",
    "for i in range(min(10, len(raw_terms))):\n",
    "    print(f\"  {raw_terms[i]['ko']} → {raw_terms[i]['en']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. E5 Multilingual Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load E5 multilingual model\n",
    "E5_MODEL_NAME = \"intfloat/multilingual-e5-large\"\n",
    "\n",
    "print(f\"Loading {E5_MODEL_NAME}...\")\n",
    "e5_tokenizer = AutoTokenizer.from_pretrained(E5_MODEL_NAME)\n",
    "e5_model = AutoModel.from_pretrained(E5_MODEL_NAME)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "e5_model = e5_model.to(device)\n",
    "e5_model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_e5_embeddings(texts: list, batch_size: int = 32, prefix: str = \"query: \") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate E5 embeddings for a list of texts.\n",
    "    E5 models require a prefix for queries.\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Add prefix\n",
    "    texts_with_prefix = [prefix + t for t in texts]\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts_with_prefix), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch_texts = texts_with_prefix[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = e5_tokenizer(\n",
    "            batch_texts,\n",
    "            max_length=128,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = e5_model(**inputs)\n",
    "            # Use mean pooling\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for Korean and English terms\n",
    "ko_texts = [t['ko'] for t in raw_terms]\n",
    "en_texts = [t['en'] for t in raw_terms]\n",
    "\n",
    "print(f\"Generating embeddings for {len(ko_texts)} Korean terms...\")\n",
    "ko_embeddings = get_e5_embeddings(ko_texts, batch_size=64)\n",
    "\n",
    "print(f\"\\nGenerating embeddings for {len(en_texts)} English terms...\")\n",
    "en_embeddings = get_e5_embeddings(en_texts, batch_size=64)\n",
    "\n",
    "print(f\"\\nKorean embeddings shape: {ko_embeddings.shape}\")\n",
    "print(f\"English embeddings shape: {en_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined embeddings (average of Korean and English)\n",
    "# This helps create cross-lingual clusters\n",
    "combined_embeddings = (ko_embeddings + en_embeddings) / 2\n",
    "combined_embeddings = combined_embeddings / np.linalg.norm(combined_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "print(f\"Combined embeddings shape: {combined_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. K-Means Clustering for Synonym Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal number of clusters\n",
    "# Rule of thumb: sqrt(n/2) for large datasets\n",
    "n_samples = len(combined_embeddings)\n",
    "n_clusters = min(int(np.sqrt(n_samples / 2)), 500)  # Cap at 500 clusters\n",
    "\n",
    "print(f\"Number of samples: {n_samples}\")\n",
    "print(f\"Number of clusters: {n_clusters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means clustering\n",
    "print(f\"Running K-Means clustering with {n_clusters} clusters...\")\n",
    "\n",
    "kmeans = MiniBatchKMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    random_state=42,\n",
    "    batch_size=1024,\n",
    "    n_init=3,\n",
    "    max_iter=100,\n",
    ")\n",
    "\n",
    "cluster_labels = kmeans.fit_predict(combined_embeddings)\n",
    "\n",
    "print(f\"Clustering complete!\")\n",
    "print(f\"Cluster label range: {cluster_labels.min()} - {cluster_labels.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster sizes\n",
    "cluster_sizes = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "\n",
    "print(\"Cluster size statistics:\")\n",
    "print(f\"  Min size: {cluster_sizes.min()}\")\n",
    "print(f\"  Max size: {cluster_sizes.max()}\")\n",
    "print(f\"  Mean size: {cluster_sizes.mean():.1f}\")\n",
    "print(f\"  Median size: {cluster_sizes.median():.1f}\")\n",
    "\n",
    "# Plot cluster size distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(cluster_sizes, bins=50, edgecolor='black')\n",
    "plt.xlabel('Cluster Size')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Cluster Size Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(cluster_sizes[cluster_sizes <= 50], bins=30, edgecolor='black')\n",
    "plt.xlabel('Cluster Size')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Cluster Size Distribution (<=50)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synonym groups from clusters\n",
    "def create_synonym_groups(terms: list, cluster_labels: np.ndarray, min_size: int = 2, max_size: int = 20):\n",
    "    \"\"\"\n",
    "    Create synonym groups from clustering results.\n",
    "    \"\"\"\n",
    "    groups = {}\n",
    "    \n",
    "    for idx, label in enumerate(cluster_labels):\n",
    "        if label not in groups:\n",
    "            groups[label] = []\n",
    "        groups[label].append(terms[idx])\n",
    "    \n",
    "    # Filter by size\n",
    "    filtered_groups = {\n",
    "        k: v for k, v in groups.items() \n",
    "        if min_size <= len(v) <= max_size\n",
    "    }\n",
    "    \n",
    "    return filtered_groups\n",
    "\n",
    "synonym_groups = create_synonym_groups(raw_terms, cluster_labels, min_size=2, max_size=15)\n",
    "print(f\"Created {len(synonym_groups)} synonym groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview synonym groups\n",
    "print(\"Sample synonym groups:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sample_keys = list(synonym_groups.keys())[:10]\n",
    "for key in sample_keys:\n",
    "    group = synonym_groups[key]\n",
    "    print(f\"\\nCluster {key} ({len(group)} terms):\")\n",
    "    for term in group[:5]:\n",
    "        print(f\"  {term['ko']} → {term['en']}\")\n",
    "    if len(group) > 5:\n",
    "        print(f\"  ... and {len(group) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embedding Visualization (PCA & UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample for visualization (use subset for speed)\n",
    "n_viz_samples = min(5000, len(combined_embeddings))\n",
    "viz_indices = np.random.choice(len(combined_embeddings), n_viz_samples, replace=False)\n",
    "\n",
    "viz_embeddings = combined_embeddings[viz_indices]\n",
    "viz_labels = cluster_labels[viz_indices]\n",
    "viz_ko_texts = [ko_texts[i] for i in viz_indices]\n",
    "viz_en_texts = [en_texts[i] for i in viz_indices]\n",
    "\n",
    "print(f\"Visualization sample size: {n_viz_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA visualization\n",
    "print(\"Running PCA...\")\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_result = pca.fit_transform(viz_embeddings)\n",
    "\n",
    "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total explained variance: {sum(pca.explained_variance_ratio_):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Left: colored by cluster\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(\n",
    "    pca_result[:, 0], \n",
    "    pca_result[:, 1], \n",
    "    c=viz_labels, \n",
    "    cmap='tab20',\n",
    "    alpha=0.5,\n",
    "    s=10\n",
    ")\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "plt.title('PCA: E5 Embeddings (colored by cluster)')\n",
    "\n",
    "# Right: highlight a few clusters\n",
    "plt.subplot(1, 2, 2)\n",
    "# Background points\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], c='lightgray', alpha=0.3, s=5)\n",
    "\n",
    "# Highlight top 5 clusters\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "top_clusters = pd.Series(viz_labels).value_counts().head(5).index.tolist()\n",
    "\n",
    "for i, cluster_id in enumerate(top_clusters):\n",
    "    mask = viz_labels == cluster_id\n",
    "    plt.scatter(\n",
    "        pca_result[mask, 0], \n",
    "        pca_result[mask, 1], \n",
    "        c=colors[i],\n",
    "        alpha=0.7,\n",
    "        s=20,\n",
    "        label=f'Cluster {cluster_id}'\n",
    "    )\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "plt.title('PCA: Top 5 Clusters Highlighted')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROJECT_ROOT / 'outputs' / 'v8_pca_visualization.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try UMAP if available\n",
    "try:\n",
    "    import umap\n",
    "    HAS_UMAP = True\n",
    "except ImportError:\n",
    "    print(\"UMAP not installed. Install with: pip install umap-learn\")\n",
    "    HAS_UMAP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_UMAP:\n",
    "    print(\"Running UMAP...\")\n",
    "    reducer = umap.UMAP(\n",
    "        n_components=2,\n",
    "        n_neighbors=15,\n",
    "        min_dist=0.1,\n",
    "        metric='cosine',\n",
    "        random_state=42\n",
    "    )\n",
    "    umap_result = reducer.fit_transform(viz_embeddings)\n",
    "    \n",
    "    # Plot UMAP results\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    scatter = plt.scatter(\n",
    "        umap_result[:, 0], \n",
    "        umap_result[:, 1], \n",
    "        c=viz_labels, \n",
    "        cmap='tab20',\n",
    "        alpha=0.5,\n",
    "        s=10\n",
    "    )\n",
    "    plt.xlabel('UMAP 1')\n",
    "    plt.ylabel('UMAP 2')\n",
    "    plt.title('UMAP: E5 Embeddings (colored by cluster)')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(umap_result[:, 0], umap_result[:, 1], c='lightgray', alpha=0.3, s=5)\n",
    "    \n",
    "    for i, cluster_id in enumerate(top_clusters):\n",
    "        mask = viz_labels == cluster_id\n",
    "        plt.scatter(\n",
    "            umap_result[mask, 0], \n",
    "            umap_result[mask, 1], \n",
    "            c=colors[i],\n",
    "            alpha=0.7,\n",
    "            s=20,\n",
    "            label=f'Cluster {cluster_id}'\n",
    "        )\n",
    "    \n",
    "    plt.xlabel('UMAP 1')\n",
    "    plt.ylabel('UMAP 2')\n",
    "    plt.title('UMAP: Top 5 Clusters Highlighted')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PROJECT_ROOT / 'outputs' / 'v8_umap_visualization.png', dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping UMAP visualization (not installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE as alternative to UMAP\n",
    "print(\"Running t-SNE (using smaller sample for speed)...\")\n",
    "\n",
    "n_tsne_samples = min(2000, len(viz_embeddings))\n",
    "tsne_indices = np.random.choice(len(viz_embeddings), n_tsne_samples, replace=False)\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,\n",
    "    random_state=42,\n",
    "    n_iter=1000\n",
    ")\n",
    "tsne_result = tsne.fit_transform(viz_embeddings[tsne_indices])\n",
    "tsne_labels = viz_labels[tsne_indices]\n",
    "\n",
    "# Plot t-SNE\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    tsne_result[:, 0], \n",
    "    tsne_result[:, 1], \n",
    "    c=tsne_labels, \n",
    "    cmap='tab20',\n",
    "    alpha=0.6,\n",
    "    s=15\n",
    ")\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.title('t-SNE: E5 Embeddings (colored by cluster)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROJECT_ROOT / 'outputs' / 'v8_tsne_visualization.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset from synonym groups\n",
    "def create_training_dataset(synonym_groups: dict) -> list:\n",
    "    \"\"\"\n",
    "    Create training dataset where each entry has:\n",
    "    - ko_term: Korean term\n",
    "    - en_term: English translation\n",
    "    - synonyms: list of related terms from same cluster\n",
    "    \"\"\"\n",
    "    training_data = []\n",
    "    \n",
    "    for cluster_id, terms in synonym_groups.items():\n",
    "        # Get all Korean and English terms in cluster\n",
    "        ko_terms = [t['ko'] for t in terms]\n",
    "        en_terms = [t['en'] for t in terms]\n",
    "        \n",
    "        for term in terms:\n",
    "            # Other terms in same cluster are synonyms\n",
    "            ko_synonyms = [t for t in ko_terms if t != term['ko']]\n",
    "            en_synonyms = [t for t in en_terms if t != term['en']]\n",
    "            \n",
    "            training_data.append({\n",
    "                'ko_term': term['ko'],\n",
    "                'en_term': term['en'],\n",
    "                'ko_synonyms': ko_synonyms[:5],  # Limit to 5 synonyms\n",
    "                'en_synonyms': en_synonyms[:5],\n",
    "                'cluster_id': int(cluster_id),\n",
    "            })\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "training_data = create_training_dataset(synonym_groups)\n",
    "print(f\"Created {len(training_data)} training samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview training data\n",
    "print(\"Sample training data:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in range(min(5, len(training_data))):\n",
    "    sample = training_data[i]\n",
    "    print(f\"\\n[{i}] {sample['ko_term']} → {sample['en_term']}\")\n",
    "    print(f\"    KO synonyms: {sample['ko_synonyms'][:3]}\")\n",
    "    print(f\"    EN synonyms: {sample['en_synonyms'][:3]}\")\n",
    "    print(f\"    Cluster: {sample['cluster_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training dataset\n",
    "output_dir = PROJECT_ROOT / 'dataset' / 'v8_clustered'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_path = output_dir / 'synonym_clusters.jsonl'\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for item in training_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Saved training dataset to: {output_path}\")\n",
    "print(f\"Total samples: {len(training_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings for later use\n",
    "np.save(output_dir / 'ko_embeddings.npy', ko_embeddings)\n",
    "np.save(output_dir / 'en_embeddings.npy', en_embeddings)\n",
    "np.save(output_dir / 'cluster_labels.npy', cluster_labels)\n",
    "\n",
    "print(\"Saved embeddings and cluster labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA PREPARATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nSource: OPUS-100 Korean-English parallel corpus\")\n",
    "print(f\"Embedding model: {E5_MODEL_NAME}\")\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  - Raw term pairs extracted: {len(raw_terms):,}\")\n",
    "print(f\"  - Number of clusters: {n_clusters}\")\n",
    "print(f\"  - Valid synonym groups: {len(synonym_groups)}\")\n",
    "print(f\"  - Training samples: {len(training_data):,}\")\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  - {output_path}\")\n",
    "print(f\"  - {output_dir / 'ko_embeddings.npy'}\")\n",
    "print(f\"  - {output_dir / 'en_embeddings.npy'}\")\n",
    "print(f\"  - {output_dir / 'cluster_labels.npy'}\")\n",
    "print(f\"\\nVisualization saved to:\")\n",
    "print(f\"  - outputs/v8_pca_visualization.png\")\n",
    "print(f\"  - outputs/v8_tsne_visualization.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nData preparation complete! Proceed to 02_training.ipynb for model training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v8 Cross-Lingual Neural Sparse Model - Inference Test\n",
    "\n",
    "이 노트북은 v8 모델의 **한글 보존 + 영어 활성화** 성능을 테스트합니다.\n",
    "\n",
    "## v8 모델 특징\n",
    "- **Self-Preservation Loss**: 한글 입력 토큰 유지\n",
    "- **Cross-Lingual Target Loss**: 영어 번역 토큰 활성화\n",
    "- **Synonym Activation Loss**: 클러스터 기반 동의어 활성화\n",
    "- **Training Data**: OPUS-100 공개 데이터셋 + E5 클러스터링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    \"\"\"Find project root directory.\"\"\"\n",
    "    candidates = [\n",
    "        Path.cwd(),\n",
    "        Path.cwd().parent,\n",
    "        Path.cwd().parent.parent,\n",
    "        Path(\"/home/west/Documents/cursor-workspace/opensearch-neural-pre-train\"),\n",
    "    ]\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        if (candidate / \"CLAUDE.md\").exists() or (candidate / \".git\").exists():\n",
    "            return candidate\n",
    "    \n",
    "    return Path(\"/home/west/Documents/cursor-workspace/opensearch-neural-pre-train\")\n",
    "\n",
    "project_root = find_project_root()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "from src.model.splade_model import create_splade_model\n",
    "\n",
    "# Korean font for matplotlib\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load v8 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "CHECKPOINT_PATH = project_root / \"outputs/cross_lingual_v8/final_model/checkpoint.pt\"\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = create_splade_model(\n",
    "    model_name=MODEL_NAME,\n",
    "    use_idf=False,\n",
    "    use_expansion=True,\n",
    "    expansion_mode=\"mlm\",\n",
    ")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=True)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"v8 Model loaded successfully!\")\n",
    "print(f\"Checkpoint path: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text: str, max_length: int = 64) -> dict:\n",
    "    \"\"\"Encode text using tokenizer.\"\"\"\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "\n",
    "def get_sparse_representation(text: str, top_k: int = 50) -> tuple:\n",
    "    \"\"\"\n",
    "    Get sparse representation for input text.\n",
    "    \n",
    "    Returns:\n",
    "        tokens: List of top-k tokens\n",
    "        scores: List of corresponding scores\n",
    "        sparse_rep: Full sparse representation tensor\n",
    "    \"\"\"\n",
    "    encoding = encode_text(text)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sparse_rep, _ = model(\n",
    "            encoding['input_ids'].to(device),\n",
    "            encoding['attention_mask'].to(device)\n",
    "        )\n",
    "    \n",
    "    sparse_rep = sparse_rep[0].cpu()\n",
    "    \n",
    "    # Get top-k tokens\n",
    "    top_scores, top_indices = torch.topk(sparse_rep, k=top_k)\n",
    "    top_tokens = tokenizer.convert_ids_to_tokens(top_indices.tolist())\n",
    "    \n",
    "    return top_tokens, top_scores.tolist(), sparse_rep\n",
    "\n",
    "\n",
    "def get_input_token_ids(text: str) -> list:\n",
    "    \"\"\"Get token IDs for input text (excluding special tokens).\"\"\"\n",
    "    encoding = tokenizer(text, add_special_tokens=False)\n",
    "    return encoding['input_ids']\n",
    "\n",
    "\n",
    "def is_korean_token(token: str) -> bool:\n",
    "    \"\"\"Check if token contains Korean characters.\"\"\"\n",
    "    token_clean = token.replace('##', '')\n",
    "    return any('\\uac00' <= c <= '\\ud7a3' for c in token_clean)\n",
    "\n",
    "\n",
    "def is_english_token(token: str) -> bool:\n",
    "    \"\"\"Check if token is English (ASCII letters only).\"\"\"\n",
    "    token_clean = token.replace('##', '')\n",
    "    return token_clean.isalpha() and token_clean.isascii()\n",
    "\n",
    "\n",
    "def display_sparse_output(text: str, top_k: int = 30):\n",
    "    \"\"\"Display sparse representation with Korean/English classification.\"\"\"\n",
    "    tokens, scores, sparse_rep = get_sparse_representation(text, top_k)\n",
    "    input_ids = get_input_token_ids(text)\n",
    "    input_tokens = set(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "    \n",
    "    print(f\"\\nInput: '{text}'\")\n",
    "    print(f\"Input tokens: {list(input_tokens)}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    results = []\n",
    "    for i, (token, score) in enumerate(zip(tokens, scores), 1):\n",
    "        # Classify token\n",
    "        if token in input_tokens:\n",
    "            category = \"INPUT\"\n",
    "        elif is_korean_token(token):\n",
    "            category = \"KO\"\n",
    "        elif is_english_token(token):\n",
    "            category = \"EN\"\n",
    "        else:\n",
    "            category = \"OTHER\"\n",
    "        \n",
    "        results.append({\n",
    "            'Rank': i,\n",
    "            'Token': token,\n",
    "            'Score': f\"{score:.4f}\",\n",
    "            'Type': category\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Summary\n",
    "    input_preserved = sum(1 for r in results if r['Type'] == 'INPUT')\n",
    "    ko_tokens = sum(1 for r in results if r['Type'] == 'KO')\n",
    "    en_tokens = sum(1 for r in results if r['Type'] == 'EN')\n",
    "    \n",
    "    print(f\"\\nSummary: Input preserved: {input_preserved}, Korean: {ko_tokens}, English: {en_tokens}\")\n",
    "    \n",
    "    return tokens, scores, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Korean Token Preservation Test\n",
    "\n",
    "v8의 핵심 기능: 한글 입력 토큰이 출력에서 유지되는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Korean token preservation\n",
    "preservation_test_terms = [\n",
    "    \"머신러닝\",\n",
    "    \"딥러닝\",\n",
    "    \"자연어처리\",\n",
    "    \"인공지능\",\n",
    "    \"데이터\",\n",
    "    \"학습\",\n",
    "    \"알고리즘\",\n",
    "    \"컴퓨터\",\n",
    "    \"프로그래밍\",\n",
    "    \"네트워크\",\n",
    "]\n",
    "\n",
    "\n",
    "def evaluate_preservation(terms: list, top_k: int = 50) -> pd.DataFrame:\n",
    "    \"\"\"Evaluate Korean token preservation rate.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for term in terms:\n",
    "        tokens, scores, sparse_rep = get_sparse_representation(term, top_k)\n",
    "        \n",
    "        # Get input token IDs\n",
    "        input_ids = get_input_token_ids(term)\n",
    "        input_tokens = set(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "        \n",
    "        # Check preservation\n",
    "        preserved = [t for t in input_tokens if t in tokens]\n",
    "        preservation_rate = len(preserved) / len(input_tokens) if input_tokens else 0\n",
    "        \n",
    "        # Count English tokens in output\n",
    "        en_in_output = [t for t in tokens if is_english_token(t) and t not in ['the', 'a', 'an', 'in', 'of']]\n",
    "        \n",
    "        results.append({\n",
    "            'Korean': term,\n",
    "            'Input Tokens': ', '.join(input_tokens),\n",
    "            'Preserved': ', '.join(preserved) if preserved else '-',\n",
    "            'Preservation %': f\"{preservation_rate*100:.1f}%\",\n",
    "            'English Tokens': ', '.join(en_in_output[:5]),\n",
    "            'Status': '✅' if preservation_rate >= 0.5 else '⚠️' if preservation_rate > 0 else '❌'\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "preservation_df = evaluate_preservation(preservation_test_terms)\n",
    "print(\"Korean Token Preservation Evaluation\")\n",
    "print(\"=\" * 80)\n",
    "preservation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Lingual Activation Test\n",
    "\n",
    "한글 입력 -> 영어 토큰 활성화 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pairs: Korean term -> Expected English tokens\n",
    "TEST_PAIRS = [\n",
    "    # IT/Tech terms\n",
    "    (\"머신러닝\", [\"machine\", \"learning\"]),\n",
    "    (\"딥러닝\", [\"deep\", \"learning\"]),\n",
    "    (\"자연어처리\", [\"natural\", \"language\", \"processing\"]),\n",
    "    (\"인공지능\", [\"artificial\", \"intelligence\"]),\n",
    "    (\"신경망\", [\"neural\", \"network\"]),\n",
    "    (\"알고리즘\", [\"algorithm\"]),\n",
    "    (\"데이터베이스\", [\"database\"]),\n",
    "    (\"프로그래밍\", [\"programming\"]),\n",
    "    (\"소프트웨어\", [\"software\"]),\n",
    "    (\"하드웨어\", [\"hardware\"]),\n",
    "    \n",
    "    # General terms\n",
    "    (\"학습\", [\"training\", \"learning\"]),\n",
    "    (\"모델\", [\"model\"]),\n",
    "    (\"데이터\", [\"data\"]),\n",
    "    (\"컴퓨터\", [\"computer\"]),\n",
    "    (\"네트워크\", [\"network\"]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cross_lingual(test_pairs: list, top_k: int = 50) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate cross-lingual activation for test pairs.\n",
    "    Also checks Korean preservation.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_en_activated = 0\n",
    "    total_en_expected = 0\n",
    "    total_ko_preserved = 0\n",
    "    total_ko_tokens = 0\n",
    "    \n",
    "    for ko_term, en_synonyms in test_pairs:\n",
    "        tokens, scores, _ = get_sparse_representation(ko_term, top_k)\n",
    "        tokens_lower = [t.lower() for t in tokens]\n",
    "        \n",
    "        # Korean preservation\n",
    "        input_ids = get_input_token_ids(ko_term)\n",
    "        input_tokens = set(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "        preserved = [t for t in input_tokens if t in tokens]\n",
    "        total_ko_preserved += len(preserved)\n",
    "        total_ko_tokens += len(input_tokens)\n",
    "        \n",
    "        # English activation\n",
    "        activated_en = []\n",
    "        not_activated_en = []\n",
    "        \n",
    "        for en_syn in en_synonyms:\n",
    "            en_toks = tokenizer.tokenize(en_syn.lower())\n",
    "            for en_tok in en_toks:\n",
    "                total_en_expected += 1\n",
    "                if en_tok.lower() in tokens_lower:\n",
    "                    total_en_activated += 1\n",
    "                    activated_en.append(en_tok)\n",
    "                else:\n",
    "                    not_activated_en.append(en_tok)\n",
    "        \n",
    "        results.append({\n",
    "            'Korean': ko_term,\n",
    "            'KO Preserved': ', '.join(preserved) if preserved else '-',\n",
    "            'EN Expected': ', '.join(en_synonyms),\n",
    "            'EN Activated': ', '.join(activated_en) if activated_en else '-',\n",
    "            'Top-5': ', '.join(tokens[:5]),\n",
    "            'KO✓': '✅' if preserved else '❌',\n",
    "            'EN✓': '✅' if activated_en else '❌'\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"=\" * 90)\n",
    "    print(\"v8 Cross-Lingual Evaluation Results (KO Preservation + EN Activation)\")\n",
    "    print(\"=\" * 90)\n",
    "    print(f\"\\nKorean Preservation Rate: {total_ko_preserved}/{total_ko_tokens} = {total_ko_preserved/total_ko_tokens*100:.1f}%\")\n",
    "    print(f\"English Activation Rate: {total_en_activated}/{total_en_expected} = {total_en_activated/total_en_expected*100:.1f}%\")\n",
    "    print(f\"\\nSuccess Rates:\")\n",
    "    print(f\"  - Korean preserved (at least 1): {sum(1 for r in results if r['KO✓'] == '✅')}/{len(results)}\")\n",
    "    print(f\"  - English activated (at least 1): {sum(1 for r in results if r['EN✓'] == '✅')}/{len(results)}\")\n",
    "    print(f\"  - Both succeeded: {sum(1 for r in results if r['KO✓'] == '✅' and r['EN✓'] == '✅')}/{len(results)}\")\n",
    "    print()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "eval_df = evaluate_cross_lingual(TEST_PAIRS)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Token Analysis\n",
    "\n",
    "개별 용어에 대한 상세 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis for key terms\n",
    "detailed_terms = [\"머신러닝\", \"딥러닝\", \"자연어처리\", \"학습\", \"데이터\"]\n",
    "\n",
    "for term in detailed_terms:\n",
    "    display_sparse_output(term, top_k=20)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Korean vs English Output Distribution\n",
    "\n",
    "출력 토큰의 한글/영어 분포 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_output_distribution(terms: list, top_k: int = 50) -> dict:\n",
    "    \"\"\"Analyze the distribution of Korean vs English tokens in output.\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for term in terms:\n",
    "        tokens, scores, _ = get_sparse_representation(term, top_k)\n",
    "        input_ids = get_input_token_ids(term)\n",
    "        input_tokens = set(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "        \n",
    "        input_count = sum(1 for t in tokens if t in input_tokens)\n",
    "        ko_count = sum(1 for t in tokens if is_korean_token(t) and t not in input_tokens)\n",
    "        en_count = sum(1 for t in tokens if is_english_token(t))\n",
    "        other_count = len(tokens) - input_count - ko_count - en_count\n",
    "        \n",
    "        all_results.append({\n",
    "            'term': term,\n",
    "            'input': input_count,\n",
    "            'korean': ko_count,\n",
    "            'english': en_count,\n",
    "            'other': other_count\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "\n",
    "# Analyze distribution\n",
    "dist_df = analyze_output_distribution(preservation_test_terms)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "x = range(len(dist_df))\n",
    "width = 0.2\n",
    "\n",
    "axes[0].bar([i - 1.5*width for i in x], dist_df['input'], width, label='Input (preserved)', color='green')\n",
    "axes[0].bar([i - 0.5*width for i in x], dist_df['korean'], width, label='Korean (new)', color='blue')\n",
    "axes[0].bar([i + 0.5*width for i in x], dist_df['english'], width, label='English', color='orange')\n",
    "axes[0].bar([i + 1.5*width for i in x], dist_df['other'], width, label='Other', color='gray')\n",
    "\n",
    "axes[0].set_xlabel('Terms')\n",
    "axes[0].set_ylabel('Token Count')\n",
    "axes[0].set_title('Token Distribution by Type (Top-50)')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels([t[:4] for t in dist_df['term']], rotation=45)\n",
    "axes[0].legend()\n",
    "\n",
    "# Pie chart (average)\n",
    "avg_dist = dist_df[['input', 'korean', 'english', 'other']].mean()\n",
    "axes[1].pie(avg_dist, labels=['Input', 'Korean', 'English', 'Other'], \n",
    "            autopct='%1.1f%%', colors=['green', 'blue', 'orange', 'gray'])\n",
    "axes[1].set_title('Average Token Type Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDistribution Summary:\")\n",
    "print(dist_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison: v7 vs v8\n",
    "\n",
    "v7 모델과 v8 모델의 출력 비교 (v7 체크포인트가 있는 경우)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load v7 model for comparison\n",
    "V7_CHECKPOINT_PATH = project_root / \"outputs/cross_lingual_expansion_v7_largescale/final_model/checkpoint.pt\"\n",
    "\n",
    "if V7_CHECKPOINT_PATH.exists():\n",
    "    print(\"Loading v7 model for comparison...\")\n",
    "    \n",
    "    model_v7 = create_splade_model(\n",
    "        model_name=MODEL_NAME,\n",
    "        use_idf=False,\n",
    "        use_expansion=True,\n",
    "        expansion_mode=\"mlm\",\n",
    "    )\n",
    "    \n",
    "    checkpoint_v7 = torch.load(V7_CHECKPOINT_PATH, map_location=device, weights_only=True)\n",
    "    model_v7.load_state_dict(checkpoint_v7['model_state_dict'])\n",
    "    model_v7 = model_v7.to(device)\n",
    "    model_v7.eval()\n",
    "    \n",
    "    print(\"v7 model loaded!\")\n",
    "    \n",
    "    def get_v7_representation(text: str, top_k: int = 50):\n",
    "        \"\"\"Get sparse representation from v7 model.\"\"\"\n",
    "        encoding = encode_text(text)\n",
    "        with torch.no_grad():\n",
    "            sparse_rep, _ = model_v7(\n",
    "                encoding['input_ids'].to(device),\n",
    "                encoding['attention_mask'].to(device)\n",
    "            )\n",
    "        sparse_rep = sparse_rep[0].cpu()\n",
    "        top_scores, top_indices = torch.topk(sparse_rep, k=top_k)\n",
    "        top_tokens = tokenizer.convert_ids_to_tokens(top_indices.tolist())\n",
    "        return top_tokens, top_scores.tolist()\n",
    "    \n",
    "else:\n",
    "    print(f\"v7 checkpoint not found at {V7_CHECKPOINT_PATH}\")\n",
    "    print(\"Skipping v7 vs v8 comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare v7 and v8 outputs\n",
    "if V7_CHECKPOINT_PATH.exists():\n",
    "    comparison_terms = [\"머신러닝\", \"딥러닝\", \"자연어처리\", \"학습\", \"데이터\"]\n",
    "    \n",
    "    print(\"v7 vs v8 Comparison\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for term in comparison_terms:\n",
    "        v8_tokens, v8_scores, _ = get_sparse_representation(term, 15)\n",
    "        v7_tokens, v7_scores = get_v7_representation(term, 15)\n",
    "        \n",
    "        input_ids = get_input_token_ids(term)\n",
    "        input_tokens = set(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "        \n",
    "        # Check Korean preservation\n",
    "        v7_preserved = [t for t in input_tokens if t in v7_tokens]\n",
    "        v8_preserved = [t for t in input_tokens if t in v8_tokens]\n",
    "        \n",
    "        print(f\"\\n'{term}':\")\n",
    "        print(f\"  Input tokens: {list(input_tokens)}\")\n",
    "        print(f\"  v7 Top-10: {', '.join(v7_tokens[:10])}\")\n",
    "        print(f\"  v8 Top-10: {', '.join(v8_tokens[:10])}\")\n",
    "        print(f\"  v7 KO preserved: {v7_preserved if v7_preserved else 'None'}\")\n",
    "        print(f\"  v8 KO preserved: {v8_preserved if v8_preserved else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Custom Query Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with custom queries\n",
    "custom_queries = [\n",
    "    \"파이썬 프로그래밍\",\n",
    "    \"웹 개발\",\n",
    "    \"클라우드 컴퓨팅\",\n",
    "    \"빅데이터 분석\",\n",
    "    \"사이버 보안\",\n",
    "    \"검색 엔진\",\n",
    "    \"추천 시스템\",\n",
    "]\n",
    "\n",
    "print(\"Custom Query Results\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for query in custom_queries:\n",
    "    tokens, scores, _ = get_sparse_representation(query, top_k=20)\n",
    "    input_ids = get_input_token_ids(query)\n",
    "    input_tokens = set(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "    \n",
    "    preserved = [t for t in input_tokens if t in tokens]\n",
    "    en_tokens = [t for t in tokens if is_english_token(t) and t not in ['the', 'a', 'an', 'in', 'of', 'to', 'and']]\n",
    "    \n",
    "    print(f\"\\n{query}:\")\n",
    "    print(f\"  Input preserved: {preserved if preserved else 'None'}\")\n",
    "    print(f\"  English tokens: {', '.join(en_tokens[:5]) if en_tokens else 'None'}\")\n",
    "    print(f\"  Top-5: {', '.join(tokens[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sparsity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sparsity(texts: list, threshold: float = 0.01) -> pd.DataFrame:\n",
    "    \"\"\"Analyze sparsity of representations.\"\"\"\n",
    "    stats = []\n",
    "    \n",
    "    for text in texts:\n",
    "        _, _, sparse_rep = get_sparse_representation(text, top_k=100)\n",
    "        \n",
    "        non_zero = (sparse_rep > threshold).sum().item()\n",
    "        sparsity = 1 - (non_zero / len(sparse_rep))\n",
    "        max_val = sparse_rep.max().item()\n",
    "        mean_val = sparse_rep[sparse_rep > threshold].mean().item() if non_zero > 0 else 0\n",
    "        \n",
    "        stats.append({\n",
    "            'text': text,\n",
    "            'non_zero': non_zero,\n",
    "            'sparsity': f\"{sparsity:.4f}\",\n",
    "            'max_score': f\"{max_val:.4f}\",\n",
    "            'mean_score': f\"{mean_val:.4f}\"\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "\n",
    "# Analyze sparsity\n",
    "sparsity_df = analyze_sparsity(preservation_test_terms)\n",
    "\n",
    "print(\"Sparsity Analysis\")\n",
    "print(\"=\" * 70)\n",
    "print(sparsity_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model statistics\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"v8 Model Statistics\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model name: {MODEL_NAME}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Checkpoint: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "### v8 Model Performance\n",
    "\n",
    "| Metric | v7 | v8 |\n",
    "|--------|-------|-------|\n",
    "| Korean Preservation | ❌ | ✅ |\n",
    "| English Activation | ✅ | ✅ |\n",
    "| Loss Type | Direct Token Target | Self-Preservation + Cross-Lingual + Synonym |\n",
    "| Training Data | Hardcoded pairs | OPUS-100 + E5 Clustering |\n",
    "\n",
    "### Key Differences from v7\n",
    "1. **한글 토큰 보존**: v8은 입력 한글 토큰을 출력에 유지\n",
    "2. **동의어 클러스터링**: E5 임베딩 기반 동의어 그룹 활성화\n",
    "3. **공개 데이터셋**: OPUS-100 병렬 코퍼스 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"v8 Inference Test Notebook completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

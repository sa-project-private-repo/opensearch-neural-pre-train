{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v9 Cross-Lingual Neural Sparse Model - Inference Test\n",
    "\n",
    "v9 모델의 성능을 평가하고 v8과 비교합니다.\n",
    "\n",
    "## v9 개선 사항\n",
    "- **Self weight 감소**: 1.0 → 0.3 (한글 토큰 독점 방지)\n",
    "- **Target weight 증가**: 1.0 → 2.0 (영어 활성화 강화)\n",
    "- **Margin 강화**: 0.5 → 1.5 (최소 활성화 보장)\n",
    "- **Negative Sampling**: 비타겟 언어 억제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    candidates = [\n",
    "        Path.cwd(),\n",
    "        Path.cwd().parent,\n",
    "        Path.cwd().parent.parent,\n",
    "        Path(\"/home/west/Documents/cursor-workspace/opensearch-neural-pre-train\"),\n",
    "    ]\n",
    "    for candidate in candidates:\n",
    "        if (candidate / \"CLAUDE.md\").exists() or (candidate / \".git\").exists():\n",
    "            return candidate\n",
    "    return Path(\"/home/west/Documents/cursor-workspace/opensearch-neural-pre-train\")\n",
    "\n",
    "project_root = find_project_root()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "from src.model.splade_model import create_splade_model\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load v9 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "V9_CHECKPOINT_PATH = project_root / \"outputs/v9_cross_lingual/final_model.pt\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load v9 model\n",
    "model_v9 = create_splade_model(\n",
    "    model_name=MODEL_NAME,\n",
    "    use_idf=False,\n",
    "    use_expansion=True,\n",
    "    expansion_mode=\"mlm\",\n",
    ")\n",
    "\n",
    "checkpoint_v9 = torch.load(V9_CHECKPOINT_PATH, map_location=device, weights_only=True)\n",
    "model_v9.load_state_dict(checkpoint_v9['model_state_dict'])\n",
    "model_v9 = model_v9.to(device)\n",
    "model_v9.eval()\n",
    "\n",
    "print(\"v9 Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load v8 model for comparison\n",
    "V8_CHECKPOINT_PATH = project_root / \"outputs/v8_cross_lingual/final_model.pt\"\n",
    "\n",
    "model_v8 = None\n",
    "if V8_CHECKPOINT_PATH.exists():\n",
    "    model_v8 = create_splade_model(\n",
    "        model_name=MODEL_NAME,\n",
    "        use_idf=False,\n",
    "        use_expansion=True,\n",
    "        expansion_mode=\"mlm\",\n",
    "    )\n",
    "    checkpoint_v8 = torch.load(V8_CHECKPOINT_PATH, map_location=device, weights_only=True)\n",
    "    model_v8.load_state_dict(checkpoint_v8['model_state_dict'])\n",
    "    model_v8 = model_v8.to(device)\n",
    "    model_v8.eval()\n",
    "    print(\"v8 Model loaded for comparison!\")\n",
    "else:\n",
    "    print(\"v8 checkpoint not found, skipping comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text: str, max_length: int = 64):\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "def get_sparse_representation(model, text: str, top_k: int = 50):\n",
    "    encoding = encode_text(text)\n",
    "    with torch.no_grad():\n",
    "        sparse_rep, _ = model(\n",
    "            encoding['input_ids'].to(device),\n",
    "            encoding['attention_mask'].to(device)\n",
    "        )\n",
    "    sparse_rep = sparse_rep[0].cpu()\n",
    "    top_scores, top_indices = torch.topk(sparse_rep, k=top_k)\n",
    "    top_tokens = tokenizer.convert_ids_to_tokens(top_indices.tolist())\n",
    "    return top_tokens, top_scores.tolist(), sparse_rep\n",
    "\n",
    "def get_input_token_ids(text: str):\n",
    "    encoding = tokenizer(text, add_special_tokens=False)\n",
    "    return encoding['input_ids']\n",
    "\n",
    "def is_korean_token(token: str) -> bool:\n",
    "    clean = token.replace('##', '')\n",
    "    return any('\\uac00' <= c <= '\\ud7a3' for c in clean)\n",
    "\n",
    "def is_english_token(token: str) -> bool:\n",
    "    clean = token.replace('##', '')\n",
    "    return clean.isalpha() and clean.isascii()\n",
    "\n",
    "def is_non_target_token(token: str) -> bool:\n",
    "    \"\"\"Check if token is from non-target language.\"\"\"\n",
    "    clean = token.replace('##', '')\n",
    "    if not clean:\n",
    "        return False\n",
    "    has_korean = any('\\uac00' <= c <= '\\ud7a3' for c in clean)\n",
    "    has_english = any(c.isalpha() and c.isascii() for c in clean)\n",
    "    if has_korean or has_english:\n",
    "        return False\n",
    "    # Check for Japanese, Chinese, Cyrillic, etc.\n",
    "    has_japanese = any('\\u3040' <= c <= '\\u309f' or '\\u30a0' <= c <= '\\u30ff' for c in clean)\n",
    "    has_cjk = any('\\u4e00' <= c <= '\\u9fff' for c in clean)\n",
    "    has_cyrillic = any('\\u0400' <= c <= '\\u04ff' for c in clean)\n",
    "    return has_japanese or has_cjk or has_cyrillic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Lingual Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PAIRS = [\n",
    "    (\"머신러닝\", [\"machine\", \"learning\"]),\n",
    "    (\"딥러닝\", [\"deep\", \"learning\"]),\n",
    "    (\"자연어처리\", [\"natural\", \"language\", \"processing\"]),\n",
    "    (\"인공지능\", [\"artificial\", \"intelligence\"]),\n",
    "    (\"신경망\", [\"neural\", \"network\"]),\n",
    "    (\"알고리즘\", [\"algorithm\"]),\n",
    "    (\"데이터베이스\", [\"database\"]),\n",
    "    (\"프로그래밍\", [\"programming\"]),\n",
    "    (\"소프트웨어\", [\"software\"]),\n",
    "    (\"하드웨어\", [\"hardware\"]),\n",
    "    (\"학습\", [\"training\", \"learning\"]),\n",
    "    (\"모델\", [\"model\"]),\n",
    "    (\"데이터\", [\"data\"]),\n",
    "    (\"컴퓨터\", [\"computer\"]),\n",
    "    (\"네트워크\", [\"network\"]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_full(model, test_pairs, top_k=50):\n",
    "    \"\"\"Full evaluation with Korean preservation, English activation, and non-target count.\"\"\"\n",
    "    results = []\n",
    "    total_ko_preserved = 0\n",
    "    total_ko_tokens = 0\n",
    "    total_en_activated = 0\n",
    "    total_en_expected = 0\n",
    "    total_non_target = 0\n",
    "    \n",
    "    for ko_term, en_synonyms in test_pairs:\n",
    "        tokens, scores, _ = get_sparse_representation(model, ko_term, top_k)\n",
    "        tokens_lower = [t.lower() for t in tokens]\n",
    "        \n",
    "        # Korean preservation\n",
    "        input_ids = get_input_token_ids(ko_term)\n",
    "        input_tokens = set(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "        preserved = [t for t in input_tokens if t in tokens]\n",
    "        total_ko_preserved += len(preserved)\n",
    "        total_ko_tokens += len(input_tokens)\n",
    "        \n",
    "        # English activation\n",
    "        activated_en = []\n",
    "        for en_syn in en_synonyms:\n",
    "            en_toks = tokenizer.tokenize(en_syn.lower())\n",
    "            for en_tok in en_toks:\n",
    "                total_en_expected += 1\n",
    "                if en_tok.lower() in tokens_lower:\n",
    "                    total_en_activated += 1\n",
    "                    activated_en.append(en_tok)\n",
    "        \n",
    "        # Non-target count\n",
    "        non_target_count = sum(1 for t in tokens if is_non_target_token(t))\n",
    "        total_non_target += non_target_count\n",
    "        \n",
    "        results.append({\n",
    "            'Korean': ko_term,\n",
    "            'KO Preserved': ', '.join(preserved) if preserved else '-',\n",
    "            'EN Expected': ', '.join(en_synonyms),\n",
    "            'EN Activated': ', '.join(activated_en) if activated_en else '-',\n",
    "            'Top-5': ', '.join(tokens[:5]),\n",
    "            'Non-target': non_target_count,\n",
    "            'KO OK': len(preserved) > 0,\n",
    "            'EN OK': len(activated_en) > 0,\n",
    "        })\n",
    "    \n",
    "    ko_rate = total_ko_preserved / total_ko_tokens * 100 if total_ko_tokens > 0 else 0\n",
    "    en_rate = total_en_activated / total_en_expected * 100 if total_en_expected > 0 else 0\n",
    "    avg_non_target = total_non_target / len(test_pairs)\n",
    "    \n",
    "    return {\n",
    "        'ko_rate': ko_rate,\n",
    "        'en_rate': en_rate,\n",
    "        'avg_non_target': avg_non_target,\n",
    "        'details': pd.DataFrame(results),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate v9\n",
    "print(\"=\" * 80)\n",
    "print(\"v9 Evaluation Results\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "v9_eval = evaluate_model_full(model_v9, TEST_PAIRS)\n",
    "\n",
    "print(f\"\\nKorean Preservation Rate: {v9_eval['ko_rate']:.1f}%\")\n",
    "print(f\"English Activation Rate: {v9_eval['en_rate']:.1f}%\")\n",
    "print(f\"Avg Non-target in Top-50: {v9_eval['avg_non_target']:.1f}\")\n",
    "\n",
    "v9_eval['details']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate v8 if available\n",
    "if model_v8 is not None:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"v8 Evaluation Results (for comparison)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    v8_eval = evaluate_model_full(model_v8, TEST_PAIRS)\n",
    "    \n",
    "    print(f\"\\nKorean Preservation Rate: {v8_eval['ko_rate']:.1f}%\")\n",
    "    print(f\"English Activation Rate: {v8_eval['en_rate']:.1f}%\")\n",
    "    print(f\"Avg Non-target in Top-50: {v8_eval['avg_non_target']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. v8 vs v9 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_v8 is not None:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"v8 vs v9 COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    comparison_data = {\n",
    "        'Metric': ['Korean Preservation', 'English Activation', 'Avg Non-target Tokens'],\n",
    "        'v8': [f\"{v8_eval['ko_rate']:.1f}%\", f\"{v8_eval['en_rate']:.1f}%\", f\"{v8_eval['avg_non_target']:.1f}\"],\n",
    "        'v9': [f\"{v9_eval['ko_rate']:.1f}%\", f\"{v9_eval['en_rate']:.1f}%\", f\"{v9_eval['avg_non_target']:.1f}\"],\n",
    "        'Change': [\n",
    "            f\"{v9_eval['ko_rate'] - v8_eval['ko_rate']:+.1f}%\",\n",
    "            f\"{v9_eval['en_rate'] - v8_eval['en_rate']:+.1f}%\",\n",
    "            f\"{v9_eval['avg_non_target'] - v8_eval['avg_non_target']:+.1f}\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Bar chart comparison\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    x = np.arange(2)\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, [v8_eval['ko_rate'], v8_eval['en_rate']], width, label='v8', color='blue', alpha=0.7)\n",
    "    bars2 = ax.bar(x + width/2, [v9_eval['ko_rate'], v9_eval['en_rate']], width, label='v9', color='orange', alpha=0.7)\n",
    "    \n",
    "    ax.set_ylabel('Rate (%)')\n",
    "    ax.set_title('v8 vs v9 Performance Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(['Korean Preservation', 'English Activation'])\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 110)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.1f}%', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.1f}%', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Token Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_comparison(term: str, top_k: int = 20):\n",
    "    \"\"\"Display v8 vs v9 token comparison for a term.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Term: '{term}'\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    input_ids = get_input_token_ids(term)\n",
    "    input_tokens = set(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "    print(f\"Input tokens: {list(input_tokens)}\")\n",
    "    \n",
    "    v9_tokens, v9_scores, _ = get_sparse_representation(model_v9, term, top_k)\n",
    "    \n",
    "    v9_preserved = [t for t in input_tokens if t in v9_tokens]\n",
    "    v9_english = [t for t in v9_tokens if is_english_token(t) and t not in ['the', 'a', 'an', 'in', 'of']]\n",
    "    v9_non_target = sum(1 for t in v9_tokens if is_non_target_token(t))\n",
    "    \n",
    "    print(f\"\\nv9 Results:\")\n",
    "    print(f\"  Top-10: {', '.join(v9_tokens[:10])}\")\n",
    "    print(f\"  KO preserved: {v9_preserved}\")\n",
    "    print(f\"  EN tokens: {v9_english[:5]}\")\n",
    "    print(f\"  Non-target count: {v9_non_target}\")\n",
    "    \n",
    "    if model_v8 is not None:\n",
    "        v8_tokens, v8_scores, _ = get_sparse_representation(model_v8, term, top_k)\n",
    "        v8_preserved = [t for t in input_tokens if t in v8_tokens]\n",
    "        v8_english = [t for t in v8_tokens if is_english_token(t) and t not in ['the', 'a', 'an', 'in', 'of']]\n",
    "        v8_non_target = sum(1 for t in v8_tokens if is_non_target_token(t))\n",
    "        \n",
    "        print(f\"\\nv8 Results:\")\n",
    "        print(f\"  Top-10: {', '.join(v8_tokens[:10])}\")\n",
    "        print(f\"  KO preserved: {v8_preserved}\")\n",
    "        print(f\"  EN tokens: {v8_english[:5]}\")\n",
    "        print(f\"  Non-target count: {v8_non_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed comparison for key terms\n",
    "key_terms = [\"머신러닝\", \"딥러닝\", \"자연어처리\", \"인공지능\", \"데이터\"]\n",
    "\n",
    "for term in key_terms:\n",
    "    display_comparison(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Query Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_queries = [\n",
    "    \"파이썬 프로그래밍\",\n",
    "    \"웹 개발\",\n",
    "    \"클라우드 컴퓨팅\",\n",
    "    \"빅데이터 분석\",\n",
    "    \"사이버 보안\",\n",
    "    \"검색 엔진\",\n",
    "    \"추천 시스템\",\n",
    "]\n",
    "\n",
    "print(\"v9 Custom Query Results\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for query in custom_queries:\n",
    "    tokens, scores, _ = get_sparse_representation(model_v9, query, top_k=20)\n",
    "    input_ids = get_input_token_ids(query)\n",
    "    input_tokens = set(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "    \n",
    "    preserved = [t for t in input_tokens if t in tokens]\n",
    "    en_tokens = [t for t in tokens if is_english_token(t) and t not in ['the', 'a', 'an', 'in', 'of', 'to', 'and']]\n",
    "    non_target = sum(1 for t in tokens if is_non_target_token(t))\n",
    "    \n",
    "    print(f\"\\n{query}:\")\n",
    "    print(f\"  KO preserved: {preserved if preserved else 'None'}\")\n",
    "    print(f\"  EN tokens: {', '.join(en_tokens[:5]) if en_tokens else 'None'}\")\n",
    "    print(f\"  Non-target: {non_target}\")\n",
    "    print(f\"  Top-5: {', '.join(tokens[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"v9 INFERENCE TEST COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nv9 Final Results:\")\n",
    "print(f\"  Korean Preservation: {v9_eval['ko_rate']:.1f}%\")\n",
    "print(f\"  English Activation: {v9_eval['en_rate']:.1f}%\")\n",
    "print(f\"  Avg Non-target: {v9_eval['avg_non_target']:.1f}\")\n",
    "\n",
    "if model_v8 is not None:\n",
    "    print(f\"\\nImprovement over v8:\")\n",
    "    print(f\"  Korean Preservation: {v9_eval['ko_rate'] - v8_eval['ko_rate']:+.1f}%\")\n",
    "    print(f\"  English Activation: {v9_eval['en_rate'] - v8_eval['en_rate']:+.1f}%\")\n",
    "    print(f\"  Non-target Reduction: {v8_eval['avg_non_target'] - v9_eval['avg_non_target']:+.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

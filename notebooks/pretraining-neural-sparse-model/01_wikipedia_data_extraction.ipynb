{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Korean & English Data Extraction\n\nThis notebook extracts Korean and English text from multiple sources for building a bilingual neural sparse model.\n\n**Data Sources:**\n1. **Korean Wikipedia** - Encyclopedia articles (direct XML dumps from Wikimedia)\n2. **English Wikipedia** - Encyclopedia articles (direct XML dumps from Wikimedia)\n3. **NamuWiki** - Korean wiki encyclopedia (via HuggingFace: `heegyu/namuwiki-extracted`)\n4. **ëª¨ë‘ì˜ ë§ë­‰ì¹˜** - Korean corpus from National Institute of Korean Language (via Korpora library)\n\n**Updated**: November 2025 - Using latest data dumps for all sources\n\n## Steps\n1. Load data from all sources\n2. Parse and extract text\n3. Clean and filter content\n4. Save processed data in chunks (50K items per file)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%load_ext autoreload\n%autoreload 2\n\nimport sys\nsys.path.append('../..')\n\nfrom src.data.wikipedia_xml_parser import WikipediaXMLParser\nfrom src.data.namuwiki_parser import NamuWikiParser\nfrom src.data.modu_corpus_parser import ModuCorpusParser\nfrom pathlib import Path\nimport json"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Output directory\noutput_dir = Path(\"../../dataset/wikipedia\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# We'll split files into chunks to avoid very large files\nARTICLES_PER_FILE = 50000  # 50K articles per file\n\n# Processing control\nSKIP_IF_EXISTS = True  # Set to False to force re-processing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Korean Wikipedia Articles\n",
    "\n",
    "**Processing all Korean Wikipedia articles** (no limit)\n",
    "\n",
    "Files will be saved in chunks of 50,000 articles each to avoid very large files.\n",
    "\n",
    "**Note**: First run will download the Wikipedia dump (~GB size). Subsequent runs will use cached file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import glob\nimport os\n\n# Check if Korean chunk files already exist\nko_existing_chunks = sorted(glob.glob(str(output_dir / \"ko_articles_chunk_*.jsonl\")))\n\nif SKIP_IF_EXISTS and ko_existing_chunks:\n    print(\"=\" * 80)\n    print(\"âœ“ Korean Wikipedia chunk files already exist!\")\n    print(\"=\" * 80)\n    print(f\"\\nFound {len(ko_existing_chunks)} existing chunk files:\")\n    \n    ko_articles_total = []\n    for chunk_file in ko_existing_chunks:\n        chunk_articles = []\n        with open(chunk_file, 'r', encoding='utf-8') as f:\n            for line in f:\n                chunk_articles.append(json.loads(line))\n        ko_articles_total.extend(chunk_articles)\n        print(f\"  - {Path(chunk_file).name}: {len(chunk_articles):,} articles\")\n    \n    print(f\"\\nTotal: {len(ko_articles_total):,} Korean articles loaded from cache\")\n    print(\"\\nğŸ’¡ Set SKIP_IF_EXISTS = False to force re-processing\")\n    \nelse:\n    # Initialize Korean parser (using latest dump)\n    ko_parser = WikipediaXMLParser(\n        language=\"ko\",\n        date=\"latest\",  # Will automatically use the most recent dump\n        cache_dir=\"../../dataset/wikipedia/cache\"\n    )\n\n    # Check if XML dump already exists in cache\n    if ko_parser.date == \"latest\":\n        ko_parser.date = ko_parser.get_latest_dump_date()\n    \n    dump_path = ko_parser.cache_dir / f\"{ko_parser.language}wiki-{ko_parser.date}.xml.bz2\"\n    \n    if dump_path.exists():\n        print(\"=\" * 80)\n        print(f\"âœ“ Wikipedia XML dump already cached: {dump_path.name}\")\n        print(f\"  Size: {os.path.getsize(dump_path) / 1024 / 1024 / 1024:.2f} GB\")\n        print(\"=\" * 80)\n    else:\n        print(\"=\" * 80)\n        print(\"â¬‡ Downloading Wikipedia XML dump (this will take a while)...\")\n        print(\"=\" * 80)\n    \n    # Download the dump (will skip if already exists)\n    dump_path = ko_parser.download_dump()\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"Processing ALL Korean Wikipedia articles\")\n    print(\"Files will be split into chunks of 50,000 articles\")\n    print(\"=\"*80 + \"\\n\")\n\n    # Process articles in streaming mode and save in chunks\n    from tqdm import tqdm\n\n    ko_articles_total = []\n    chunk_num = 0\n    current_chunk = []\n\n    iterator = ko_parser.iter_articles(dump_path)\n    pbar = tqdm(iterator, desc=\"Processing Korean Wikipedia\")\n\n    for raw_article in pbar:\n        # Parse wikitext to plain text\n        text = ko_parser.parse_wikitext(raw_article[\"wikitext\"])\n        \n        article = {\n            \"id\": raw_article[\"id\"],\n            \"url\": raw_article[\"url\"],\n            \"title\": raw_article[\"title\"],\n            \"text\": text,\n            \"language\": \"ko\",\n        }\n        \n        # Apply filters\n        if ko_parser.filter_article(article, min_length=200, max_length=100000):\n            current_chunk.append(article)\n            ko_articles_total.append(article)\n            \n            # Save chunk when it reaches the limit\n            if len(current_chunk) >= ARTICLES_PER_FILE:\n                chunk_num += 1\n                output_file = output_dir / f\"ko_articles_chunk_{chunk_num:03d}.jsonl\"\n                ko_parser.save_articles(current_chunk, output_file)\n                pbar.set_postfix({\n                    'chunks': chunk_num, \n                    'articles': len(ko_articles_total),\n                    'current_chunk': len(current_chunk)\n                })\n                current_chunk = []\n\n    # Save remaining articles in last chunk\n    if current_chunk:\n        chunk_num += 1\n        output_file = output_dir / f\"ko_articles_chunk_{chunk_num:03d}.jsonl\"\n        ko_parser.save_articles(current_chunk, output_file)\n\n    print(f\"\\nâœ“ Processed {len(ko_articles_total):,} Korean articles\")\n    print(f\"âœ“ Saved in {chunk_num} chunk files\")\n    if ko_articles_total:\n        print(f\"âœ“ Sample article: {ko_articles_total[0]['title']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract English Wikipedia Articles\n",
    "\n",
    "**Processing all English Wikipedia articles** (no limit)\n",
    "\n",
    "Files will be saved in chunks of 50,000 articles each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if English chunk files already exist\nen_existing_chunks = sorted(glob.glob(str(output_dir / \"en_articles_chunk_*.jsonl\")))\n\nif SKIP_IF_EXISTS and en_existing_chunks:\n    print(\"=\" * 80)\n    print(\"âœ“ English Wikipedia chunk files already exist!\")\n    print(\"=\" * 80)\n    print(f\"\\nFound {len(en_existing_chunks)} existing chunk files:\")\n    \n    en_articles_total = []\n    for chunk_file in en_existing_chunks:\n        chunk_articles = []\n        with open(chunk_file, 'r', encoding='utf-8') as f:\n            for line in f:\n                chunk_articles.append(json.loads(line))\n        en_articles_total.extend(chunk_articles)\n        print(f\"  - {Path(chunk_file).name}: {len(chunk_articles):,} articles\")\n    \n    print(f\"\\nTotal: {len(en_articles_total):,} English articles loaded from cache\")\n    print(\"\\nğŸ’¡ Set SKIP_IF_EXISTS = False to force re-processing\")\n    \nelse:\n    # Initialize English parser (using latest dump)\n    en_parser = WikipediaXMLParser(\n        language=\"en\",\n        date=\"latest\",  # Will automatically use the most recent dump\n        cache_dir=\"../../dataset/wikipedia/cache\"\n    )\n\n    # Check if XML dump already exists in cache\n    if en_parser.date == \"latest\":\n        en_parser.date = en_parser.get_latest_dump_date()\n    \n    dump_path = en_parser.cache_dir / f\"{en_parser.language}wiki-{en_parser.date}.xml.bz2\"\n    \n    if dump_path.exists():\n        print(\"=\" * 80)\n        print(f\"âœ“ Wikipedia XML dump already cached: {dump_path.name}\")\n        print(f\"  Size: {os.path.getsize(dump_path) / 1024 / 1024 / 1024:.2f} GB\")\n        print(\"=\" * 80)\n    else:\n        print(\"=\" * 80)\n        print(\"â¬‡ Downloading Wikipedia XML dump (this will take a while)...\")\n        print(\"=\" * 80)\n    \n    # Download the dump (will skip if already exists)\n    dump_path = en_parser.download_dump()\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"Processing ALL English Wikipedia articles\")\n    print(\"Files will be split into chunks of 50,000 articles\")\n    print(\"=\"*80 + \"\\n\")\n\n    # Process articles in streaming mode and save in chunks\n    en_articles_total = []\n    chunk_num = 0\n    current_chunk = []\n\n    iterator = en_parser.iter_articles(dump_path)\n    pbar = tqdm(iterator, desc=\"Processing English Wikipedia\")\n\n    for raw_article in pbar:\n        # Parse wikitext to plain text\n        text = en_parser.parse_wikitext(raw_article[\"wikitext\"])\n        \n        article = {\n            \"id\": raw_article[\"id\"],\n            \"url\": raw_article[\"url\"],\n            \"title\": raw_article[\"title\"],\n            \"text\": text,\n            \"language\": \"en\",\n        }\n        \n        # Apply filters\n        if en_parser.filter_article(article, min_length=200, max_length=100000):\n            current_chunk.append(article)\n            en_articles_total.append(article)\n            \n            # Save chunk when it reaches the limit\n            if len(current_chunk) >= ARTICLES_PER_FILE:\n                chunk_num += 1\n                output_file = output_dir / f\"en_articles_chunk_{chunk_num:03d}.jsonl\"\n                en_parser.save_articles(current_chunk, output_file)\n                pbar.set_postfix({\n                    'chunks': chunk_num, \n                    'articles': len(en_articles_total),\n                    'current_chunk': len(current_chunk)\n                })\n                current_chunk = []\n\n    # Save remaining articles in last chunk\n    if current_chunk:\n        chunk_num += 1\n        output_file = output_dir / f\"en_articles_chunk_{chunk_num:03d}.jsonl\"\n        en_parser.save_articles(current_chunk, output_file)\n\n    print(f\"\\nâœ“ Processed {len(en_articles_total):,} English articles\")\n    print(f\"âœ“ Saved in {chunk_num} chunk files\")\n    if en_articles_total:\n        print(f\"âœ“ Sample article: {en_articles_total[0]['title']}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Extract NamuWiki Articles (Korean)\n\n**NamuWiki** is a Korean wiki encyclopedia with ~1.5M articles, providing additional Korean language data.\n\nUsing HuggingFace dataset: `heegyu/namuwiki-extracted`\n\nFiles will be saved in chunks of 50,000 articles each.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Check if NamuWiki chunk files already exist\nnamu_existing_chunks = sorted(glob.glob(str(output_dir / \"../namuwiki/namuwiki_chunk_*.jsonl\")))\n\nif SKIP_IF_EXISTS and namu_existing_chunks:\n    print(\"=\" * 80)\n    print(\"âœ“ NamuWiki chunk files already exist!\")\n    print(\"=\" * 80)\n    print(f\"\\nFound {len(namu_existing_chunks)} existing chunk files:\")\n    \n    namu_articles_total = []\n    for chunk_file in namu_existing_chunks:\n        chunk_articles = []\n        with open(chunk_file, 'r', encoding='utf-8') as f:\n            for line in f:\n                chunk_articles.append(json.loads(line))\n        namu_articles_total.extend(chunk_articles)\n        print(f\"  - {Path(chunk_file).name}: {len(chunk_articles):,} articles\")\n    \n    print(f\"\\nTotal: {len(namu_articles_total):,} NamuWiki articles loaded from cache\")\n    print(\"\\nğŸ’¡ Set SKIP_IF_EXISTS = False to force re-processing\")\n    \nelse:\n    # Initialize NamuWiki parser\n    namu_parser = NamuWikiParser(cache_dir=\"../../dataset/namuwiki/cache\")\n    \n    print(\"=\" * 80)\n    print(\"Processing NamuWiki articles from HuggingFace\")\n    print(\"Files will be split into chunks of 50,000 articles\")\n    print(\"=\" * 80)\n    print(\"\\nâ¬‡ This will download ~2GB of data on first run...\")\n    \n    # Process NamuWiki articles (will automatically chunk the output)\n    namu_articles_total = namu_parser.process_namuwiki(\n        output_path=\"../../dataset/namuwiki/namuwiki_articles.jsonl\",\n        max_articles=None,  # Process all articles\n        min_length=100,\n        max_length=100000,\n        chunk_size=ARTICLES_PER_FILE\n    )\n    \n    print(f\"\\nâœ“ Processed {len(namu_articles_total):,} NamuWiki articles\")\n    if namu_articles_total:\n        print(f\"âœ“ Sample article: {namu_articles_total[0]['title']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Extract ëª¨ë‘ì˜ ë§ë­‰ì¹˜ (Korean)\n\n**ëª¨ë‘ì˜ ë§ë­‰ì¹˜** (Everyone's Corpus) is a large-scale Korean language corpus from the National Institute of Korean Language.\n\nIncludes:\n- News articles (ì‹ ë¬¸ ë§ë­‰ì¹˜)\n- Spoken language (êµ¬ì–´ ë§ë­‰ì¹˜)\n- Web text (ì›¹ ë§ë­‰ì¹˜)\n- Messenger conversations (ë©”ì‹ ì € ë§ë­‰ì¹˜)\n\nFiles will be saved in chunks of 50,000 texts each.\n\n**Note**: Some corpora may require authentication. The parser will automatically skip unavailable corpora.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## 6. Inspect Sample Articles",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect Sample Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Korean article sample\n",
    "if len(ko_articles_total) > 0:\n",
    "    # Use first available article or 10th if available\n",
    "    sample_idx = min(10, len(ko_articles_total) - 1)\n",
    "    sample_ko = ko_articles_total[sample_idx]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Article #{sample_idx + 1} of {len(ko_articles_total):,}\")\n",
    "    print(f\"Title: {sample_ko['title']}\")\n",
    "    print(f\"URL: {sample_ko['url']}\")\n",
    "    print(f\"Language: {sample_ko['language']}\")\n",
    "    print(f\"Text length: {len(sample_ko['text'])} characters\")\n",
    "    print(\"\\nFirst 300 characters:\")\n",
    "    print(sample_ko['text'][:300])\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"No articles found. Check filtering criteria.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 7. Statistics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "import numpy as np\n\nprint(\"=\" * 80)\nprint(\"DATA EXTRACTION STATISTICS\")\nprint(\"=\" * 80)\n\n# Korean Wikipedia stats\nif len(ko_articles_total) > 0:\n    ko_lengths = [len(a['text']) for a in ko_articles_total]\n    print(\"\\n1. Korean Wikipedia:\")\n    print(f\"     Total: {len(ko_articles_total):,}\")\n    print(f\"     Mean length: {np.mean(ko_lengths):.0f} chars\")\n    print(f\"     Median length: {np.median(ko_lengths):.0f} chars\")\n    print(f\"     Min length: {np.min(ko_lengths):.0f} chars\")\n    print(f\"     Max length: {np.max(ko_lengths):.0f} chars\")\nelse:\n    print(\"\\n1. Korean Wikipedia: No articles found\")\n\n# English Wikipedia stats\nif len(en_articles_total) > 0:\n    en_lengths = [len(a['text']) for a in en_articles_total]\n    print(\"\\n2. English Wikipedia:\")\n    print(f\"     Total: {len(en_articles_total):,}\")\n    print(f\"     Mean length: {np.mean(en_lengths):.0f} chars\")\n    print(f\"     Median length: {np.median(en_lengths):.0f} chars\")\n    print(f\"     Min length: {np.min(en_lengths):.0f} chars\")\n    print(f\"     Max length: {np.max(en_lengths):.0f} chars\")\nelse:\n    print(\"\\n2. English Wikipedia: No articles found\")\n\n# NamuWiki stats\nif 'namu_articles_total' in locals() and len(namu_articles_total) > 0:\n    namu_lengths = [len(a['text']) for a in namu_articles_total]\n    print(\"\\n3. NamuWiki (Korean):\")\n    print(f\"     Total: {len(namu_articles_total):,}\")\n    print(f\"     Mean length: {np.mean(namu_lengths):.0f} chars\")\n    print(f\"     Median length: {np.median(namu_lengths):.0f} chars\")\n    print(f\"     Min length: {np.min(namu_lengths):.0f} chars\")\n    print(f\"     Max length: {np.max(namu_lengths):.0f} chars\")\nelse:\n    print(\"\\n3. NamuWiki: No articles found\")\n\n# ëª¨ë‘ì˜ ë§ë­‰ì¹˜ stats\nif 'modu_articles_total' in locals() and len(modu_articles_total) > 0:\n    modu_lengths = [len(a['text']) for a in modu_articles_total]\n    print(\"\\n4. ëª¨ë‘ì˜ ë§ë­‰ì¹˜ (Korean):\")\n    print(f\"     Total: {len(modu_articles_total):,}\")\n    print(f\"     Mean length: {np.mean(modu_lengths):.0f} chars\")\n    print(f\"     Median length: {np.median(modu_lengths):.0f} chars\")\n    print(f\"     Min length: {np.min(modu_lengths):.0f} chars\")\n    print(f\"     Max length: {np.max(modu_lengths):.0f} chars\")\nelse:\n    print(\"\\n4. ëª¨ë‘ì˜ ë§ë­‰ì¹˜: No texts found\")\n\n# Total statistics\nprint(\"\\n\" + \"=\" * 80)\nprint(\"LANGUAGE BALANCE:\")\nprint(\"=\" * 80)\n\nkorean_total = 0\nenglish_total = 0\n\nif len(ko_articles_total) > 0:\n    korean_total += len(ko_articles_total)\nif 'namu_articles_total' in locals() and len(namu_articles_total) > 0:\n    korean_total += len(namu_articles_total)\nif 'modu_articles_total' in locals() and len(modu_articles_total) > 0:\n    korean_total += len(modu_articles_total)\nif len(en_articles_total) > 0:\n    english_total += len(en_articles_total)\n\nprint(f\"\\nKorean texts:  {korean_total:,}\")\nprint(f\"English texts: {english_total:,}\")\nprint(f\"Total texts:   {korean_total + english_total:,}\")\n\nif korean_total > 0 and english_total > 0:\n    ratio = english_total / korean_total\n    print(f\"\\nEnglish/Korean ratio: {ratio:.2f}x\")\n    \nprint(\"=\" * 80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 8. Verify Saved Files"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "import os\nimport glob\n\nprint(\"=\" * 80)\nprint(\"SAVED CHUNK FILES\")\nprint(\"=\" * 80)\n\n# Find all Korean Wikipedia chunk files\nko_chunks = sorted(glob.glob(str(output_dir / \"ko_articles_chunk_*.jsonl\")))\nif ko_chunks:\n    print(f\"\\n1. Korean Wikipedia: {len(ko_chunks)} chunk files\")\n    total_size = sum(os.path.getsize(f) for f in ko_chunks)\n    total_lines = sum(sum(1 for _ in open(f)) for f in ko_chunks)\n    print(f\"   Total size: {total_size / 1024 / 1024:.2f} MB\")\n    print(f\"   Total articles: {total_lines:,}\")\n    print(f\"   Files:\")\n    for chunk in ko_chunks[:3]:  # Show first 3\n        size = os.path.getsize(chunk) / 1024 / 1024\n        lines = sum(1 for _ in open(chunk))\n        print(f\"     - {os.path.basename(chunk):30s} ({size:>6.2f} MB, {lines:>6,} articles)\")\n    if len(ko_chunks) > 3:\n        print(f\"     ... and {len(ko_chunks) - 3} more files\")\nelse:\n    print(\"\\n1. Korean Wikipedia: No chunk files found\")\n\n# Find all English Wikipedia chunk files\nen_chunks = sorted(glob.glob(str(output_dir / \"en_articles_chunk_*.jsonl\")))\nif en_chunks:\n    print(f\"\\n2. English Wikipedia: {len(en_chunks)} chunk files\")\n    total_size = sum(os.path.getsize(f) for f in en_chunks)\n    total_lines = sum(sum(1 for _ in open(f)) for f in en_chunks)\n    print(f\"   Total size: {total_size / 1024 / 1024:.2f} MB\")\n    print(f\"   Total articles: {total_lines:,}\")\n    print(f\"   Files:\")\n    for chunk in en_chunks[:3]:  # Show first 3\n        size = os.path.getsize(chunk) / 1024 / 1024\n        lines = sum(1 for _ in open(chunk))\n        print(f\"     - {os.path.basename(chunk):30s} ({size:>6.2f} MB, {lines:>6,} articles)\")\n    if len(en_chunks) > 3:\n        print(f\"     ... and {len(en_chunks) - 3} more files\")\nelse:\n    print(\"\\n2. English Wikipedia: No chunk files found\")\n\n# Find all NamuWiki chunk files\nnamu_dir = output_dir / \"../namuwiki\"\nnamu_chunks = sorted(glob.glob(str(namu_dir / \"namuwiki_chunk_*.jsonl\")))\nif namu_chunks:\n    print(f\"\\n3. NamuWiki (Korean): {len(namu_chunks)} chunk files\")\n    total_size = sum(os.path.getsize(f) for f in namu_chunks)\n    total_lines = sum(sum(1 for _ in open(f)) for f in namu_chunks)\n    print(f\"   Total size: {total_size / 1024 / 1024:.2f} MB\")\n    print(f\"   Total articles: {total_lines:,}\")\n    print(f\"   Files:\")\n    for chunk in namu_chunks[:3]:  # Show first 3\n        size = os.path.getsize(chunk) / 1024 / 1024\n        lines = sum(1 for _ in open(chunk))\n        print(f\"     - {os.path.basename(chunk):30s} ({size:>6.2f} MB, {lines:>6,} articles)\")\n    if len(namu_chunks) > 3:\n        print(f\"     ... and {len(namu_chunks) - 3} more files\")\nelse:\n    print(\"\\n3. NamuWiki: No chunk files found\")\n\n# Find all ëª¨ë‘ì˜ ë§ë­‰ì¹˜ chunk files\nmodu_dir = output_dir / \"../modu\"\nmodu_chunks = sorted(glob.glob(str(modu_dir / \"modu_chunk_*.jsonl\")))\nif modu_chunks:\n    print(f\"\\n4. ëª¨ë‘ì˜ ë§ë­‰ì¹˜ (Korean): {len(modu_chunks)} chunk files\")\n    total_size = sum(os.path.getsize(f) for f in modu_chunks)\n    total_lines = sum(sum(1 for _ in open(f)) for f in modu_chunks)\n    print(f\"   Total size: {total_size / 1024 / 1024:.2f} MB\")\n    print(f\"   Total texts: {total_lines:,}\")\n    print(f\"   Files:\")\n    for chunk in modu_chunks[:3]:  # Show first 3\n        size = os.path.getsize(chunk) / 1024 / 1024\n        lines = sum(1 for _ in open(chunk))\n        print(f\"     - {os.path.basename(chunk):30s} ({size:>6.2f} MB, {lines:>6,} texts)\")\n    if len(modu_chunks) > 3:\n        print(f\"     ... and {len(modu_chunks) - 3} more files\")\nelse:\n    print(\"\\n4. ëª¨ë‘ì˜ ë§ë­‰ì¹˜: No chunk files found\")\n\nprint(\"\\n\" + \"=\" * 80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Summary\n\nWe've successfully extracted and cleaned **ALL** articles from multiple Korean and English data sources.\n\n**Data Sources:**\n1. **Korean Wikipedia** - Encyclopedia articles (~600K articles)\n2. **English Wikipedia** - Encyclopedia articles (~6M articles)\n3. **NamuWiki** - Korean wiki encyclopedia (~1.5M articles)\n4. **ëª¨ë‘ì˜ ë§ë­‰ì¹˜** - Korean language corpus from National Institute of Korean Language\n   - News articles (ì‹ ë¬¸ ë§ë­‰ì¹˜)\n   - Spoken language (êµ¬ì–´ ë§ë­‰ì¹˜)\n   - Web text (ì›¹ ë§ë­‰ì¹˜)\n   - Messenger conversations (ë©”ì‹ ì € ë§ë­‰ì¹˜)\n\n**Key Features:**\n- âœ… Processes complete datasets (no article limit)\n- âœ… Saves data in manageable chunks (50,000 items per file)\n- âœ… Filters out redirects, special pages, and low-quality content\n- âœ… Cleans markup to plain text\n- âœ… Implements 2-level caching (chunk files + raw dumps)\n- âœ… Significantly improved Korean-English data balance\n\n**Output Structure:**\n```\ndataset/\nâ”œâ”€â”€ wikipedia/\nâ”‚   â”œâ”€â”€ ko_articles_chunk_001.jsonl\nâ”‚   â”œâ”€â”€ ko_articles_chunk_002.jsonl\nâ”‚   â”œâ”€â”€ ...\nâ”‚   â”œâ”€â”€ en_articles_chunk_001.jsonl\nâ”‚   â”œâ”€â”€ en_articles_chunk_002.jsonl\nâ”‚   â””â”€â”€ ...\nâ”œâ”€â”€ namuwiki/\nâ”‚   â”œâ”€â”€ namuwiki_chunk_001.jsonl\nâ”‚   â”œâ”€â”€ namuwiki_chunk_002.jsonl\nâ”‚   â””â”€â”€ ...\nâ””â”€â”€ modu/\n    â”œâ”€â”€ modu_chunk_001.jsonl\n    â”œâ”€â”€ modu_chunk_002.jsonl\n    â””â”€â”€ ...\n```\n\n**Next steps:**\n- Extract inter-language links from chunks\n- Extract synonym pairs from article text\n- Build comprehensive bilingual dictionary\n- Train neural sparse model with balanced Korean-English data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've successfully extracted and cleaned **ALL** Korean and English Wikipedia articles.\n",
    "\n",
    "**Key Features:**\n",
    "- âœ… Processes complete Wikipedia dumps (no article limit)\n",
    "- âœ… Saves data in manageable chunks (50,000 articles per file)\n",
    "- âœ… Filters out redirects, special pages, and low-quality articles\n",
    "- âœ… Cleans MediaWiki markup to plain text\n",
    "- âœ… Ready for synonym extraction and model training\n",
    "\n",
    "**Output Structure:**\n",
    "```\n",
    "dataset/wikipedia/\n",
    "â”œâ”€â”€ ko_articles_chunk_001.jsonl  (50,000 articles)\n",
    "â”œâ”€â”€ ko_articles_chunk_002.jsonl  (50,000 articles)\n",
    "â”œâ”€â”€ ...\n",
    "â”œâ”€â”€ en_articles_chunk_001.jsonl  (50,000 articles)\n",
    "â”œâ”€â”€ en_articles_chunk_002.jsonl  (50,000 articles)\n",
    "â””â”€â”€ ...\n",
    "```\n",
    "\n",
    "**Next steps:**\n",
    "- Extract inter-language links from chunks\n",
    "- Extract synonym pairs from article text\n",
    "- Build comprehensive bilingual dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Wikipedia Data Extraction\n\nThis notebook extracts Korean and English Wikipedia articles for building a bilingual synonym dataset.\n\n**Updated**: Now using direct Wikipedia XML dumps from Wikimedia for the latest data (November 2025).\n\n## Steps\n1. Load Wikipedia data from Wikimedia dumps\n2. Parse XML and extract article text\n3. Clean and filter articles  \n4. Save processed data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('../..')\n\nfrom src.data.wikipedia_xml_parser import WikipediaXMLParser\nfrom pathlib import Path\nimport json"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output paths\n",
    "ko_output = \"../../dataset/wikipedia/ko_articles.jsonl\"\n",
    "en_output = \"../../dataset/wikipedia/en_articles.jsonl\"\n",
    "\n",
    "# Create directories\n",
    "Path(ko_output).parent.mkdir(parents=True, exist_ok=True)\n",
    "Path(en_output).parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Extract Korean Wikipedia Articles\n\nWe'll start with a sample of 5,000 articles for testing.\n\n**Note**: First run will download the Wikipedia dump (~GB size). Subsequent runs will use cached file."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize Korean parser (using latest dump)\nko_parser = WikipediaXMLParser(\n    language=\"ko\",\n    date=\"latest\",  # Will automatically use the most recent dump\n    cache_dir=\"../../dataset/wikipedia/cache\"\n)\n\n# Process Korean Wikipedia\nko_articles = ko_parser.process_wikipedia(\n    output_path=ko_output,\n    max_articles=5000,  # Sample size\n    min_length=200,     # Minimum 200 characters\n    max_length=10000,   # Maximum 10K characters\n)\n\nprint(f\"\\nProcessed {len(ko_articles)} Korean articles\")\nif ko_articles:\n    print(f\"Sample article: {ko_articles[0]['title']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract English Wikipedia Articles\n",
    "\n",
    "Same process for English articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize English parser (using latest dump)\nen_parser = WikipediaXMLParser(\n    language=\"en\",\n    date=\"latest\",  # Will automatically use the most recent dump\n    cache_dir=\"../../dataset/wikipedia/cache\"\n)\n\n# Process English Wikipedia\nen_articles = en_parser.process_wikipedia(\n    output_path=en_output,\n    max_articles=5000,  # Sample size\n    min_length=200,\n    max_length=10000,\n)\n\nprint(f\"\\nProcessed {len(en_articles)} English articles\")\nif en_articles:\n    print(f\"Sample article: {en_articles[0]['title']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect Sample Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Korean article sample\n",
    "sample_ko = ko_articles[10]\n",
    "print(\"=\" * 80)\n",
    "print(f\"Title: {sample_ko['title']}\")\n",
    "print(f\"URL: {sample_ko['url']}\")\n",
    "print(f\"Language: {sample_ko['language']}\")\n",
    "print(f\"Text length: {len(sample_ko['text'])} characters\")\n",
    "print(\"\\nFirst 300 characters:\")\n",
    "print(sample_ko['text'][:300])\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display English article sample\n",
    "sample_en = en_articles[10]\n",
    "print(\"=\" * 80)\n",
    "print(f\"Title: {sample_en['title']}\")\n",
    "print(f\"URL: {sample_en['url']}\")\n",
    "print(f\"Language: {sample_en['language']}\")\n",
    "print(f\"Text length: {len(sample_en['text'])} characters\")\n",
    "print(\"\\nFirst 300 characters:\")\n",
    "print(sample_en['text'][:300])\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Korean articles stats\n",
    "ko_lengths = [len(a['text']) for a in ko_articles]\n",
    "print(\"Korean Wikipedia Articles:\")\n",
    "print(f\"  Total: {len(ko_articles)}\")\n",
    "print(f\"  Mean length: {np.mean(ko_lengths):.0f} chars\")\n",
    "print(f\"  Median length: {np.median(ko_lengths):.0f} chars\")\n",
    "print(f\"  Min length: {np.min(ko_lengths):.0f} chars\")\n",
    "print(f\"  Max length: {np.max(ko_lengths):.0f} chars\")\n",
    "\n",
    "print()\n",
    "\n",
    "# English articles stats\n",
    "en_lengths = [len(a['text']) for a in en_articles]\n",
    "print(\"English Wikipedia Articles:\")\n",
    "print(f\"  Total: {len(en_articles)}\")\n",
    "print(f\"  Mean length: {np.mean(en_lengths):.0f} chars\")\n",
    "print(f\"  Median length: {np.median(en_lengths):.0f} chars\")\n",
    "print(f\"  Min length: {np.min(en_lengths):.0f} chars\")\n",
    "print(f\"  Max length: {np.max(en_lengths):.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify Saved Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Saved files:\")\n",
    "print(f\"  Korean: {ko_output}\")\n",
    "print(f\"    Size: {os.path.getsize(ko_output) / 1024 / 1024:.2f} MB\")\n",
    "print(f\"    Lines: {sum(1 for _ in open(ko_output))}\")\n",
    "\n",
    "print(f\"\\n  English: {en_output}\")\n",
    "print(f\"    Size: {os.path.getsize(en_output) / 1024 / 1024:.2f} MB\")\n",
    "print(f\"    Lines: {sum(1 for _ in open(en_output))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've successfully extracted and cleaned Korean and English Wikipedia articles. The data is now ready for synonym extraction in the next notebook.\n",
    "\n",
    "**Next steps:**\n",
    "- Extract inter-language links\n",
    "- Extract synonym pairs from article text\n",
    "- Build comprehensive bilingual dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
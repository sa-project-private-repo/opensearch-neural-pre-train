{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Data Extraction\n",
    "\n",
    "This notebook extracts Korean and English Wikipedia articles for building a bilingual synonym dataset.\n",
    "\n",
    "## Steps\n",
    "1. Load Wikipedia data using HuggingFace datasets\n",
    "2. Clean and filter articles\n",
    "3. Extract inter-language links\n",
    "4. Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('../..')\n\nfrom src.data.wikipedia_parser import WikipediaParser, InterlanguageLinker\nfrom pathlib import Path\nimport json"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output paths\n",
    "ko_output = \"../../dataset/wikipedia/ko_articles.jsonl\"\n",
    "en_output = \"../../dataset/wikipedia/en_articles.jsonl\"\n",
    "\n",
    "# Create directories\n",
    "Path(ko_output).parent.mkdir(parents=True, exist_ok=True)\n",
    "Path(en_output).parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Korean Wikipedia Articles\n",
    "\n",
    "We'll start with a sample of 5,000 articles for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Korean parser\n",
    "ko_parser = WikipediaParser(\n",
    "    language=\"ko\",\n",
    "    date=\"20220301\",\n",
    "    cache_dir=\"../../dataset/wikipedia/cache\"\n",
    ")\n",
    "\n",
    "# Process Korean Wikipedia\n",
    "ko_articles = ko_parser.process_wikipedia(\n",
    "    output_path=ko_output,\n",
    "    max_articles=5000,  # Sample size\n",
    "    min_length=200,     # Minimum 200 characters\n",
    "    max_length=10000,   # Maximum 10K characters\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nProcessed {len(ko_articles)} Korean articles\")\n",
    "print(f\"Sample article: {ko_articles[0]['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract English Wikipedia Articles\n",
    "\n",
    "Same process for English articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize English parser\n",
    "en_parser = WikipediaParser(\n",
    "    language=\"en\",\n",
    "    date=\"20220301\",\n",
    "    cache_dir=\"../../dataset/wikipedia/cache\"\n",
    ")\n",
    "\n",
    "# Process English Wikipedia\n",
    "en_articles = en_parser.process_wikipedia(\n",
    "    output_path=en_output,\n",
    "    max_articles=5000,  # Sample size\n",
    "    min_length=200,\n",
    "    max_length=10000,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nProcessed {len(en_articles)} English articles\")\n",
    "print(f\"Sample article: {en_articles[0]['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect Sample Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Korean article sample\n",
    "sample_ko = ko_articles[10]\n",
    "print(\"=\" * 80)\n",
    "print(f\"Title: {sample_ko['title']}\")\n",
    "print(f\"URL: {sample_ko['url']}\")\n",
    "print(f\"Language: {sample_ko['language']}\")\n",
    "print(f\"Text length: {len(sample_ko['text'])} characters\")\n",
    "print(\"\\nFirst 300 characters:\")\n",
    "print(sample_ko['text'][:300])\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display English article sample\n",
    "sample_en = en_articles[10]\n",
    "print(\"=\" * 80)\n",
    "print(f\"Title: {sample_en['title']}\")\n",
    "print(f\"URL: {sample_en['url']}\")\n",
    "print(f\"Language: {sample_en['language']}\")\n",
    "print(f\"Text length: {len(sample_en['text'])} characters\")\n",
    "print(\"\\nFirst 300 characters:\")\n",
    "print(sample_en['text'][:300])\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Korean articles stats\n",
    "ko_lengths = [len(a['text']) for a in ko_articles]\n",
    "print(\"Korean Wikipedia Articles:\")\n",
    "print(f\"  Total: {len(ko_articles)}\")\n",
    "print(f\"  Mean length: {np.mean(ko_lengths):.0f} chars\")\n",
    "print(f\"  Median length: {np.median(ko_lengths):.0f} chars\")\n",
    "print(f\"  Min length: {np.min(ko_lengths):.0f} chars\")\n",
    "print(f\"  Max length: {np.max(ko_lengths):.0f} chars\")\n",
    "\n",
    "print()\n",
    "\n",
    "# English articles stats\n",
    "en_lengths = [len(a['text']) for a in en_articles]\n",
    "print(\"English Wikipedia Articles:\")\n",
    "print(f\"  Total: {len(en_articles)}\")\n",
    "print(f\"  Mean length: {np.mean(en_lengths):.0f} chars\")\n",
    "print(f\"  Median length: {np.median(en_lengths):.0f} chars\")\n",
    "print(f\"  Min length: {np.min(en_lengths):.0f} chars\")\n",
    "print(f\"  Max length: {np.max(en_lengths):.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify Saved Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Saved files:\")\n",
    "print(f\"  Korean: {ko_output}\")\n",
    "print(f\"    Size: {os.path.getsize(ko_output) / 1024 / 1024:.2f} MB\")\n",
    "print(f\"    Lines: {sum(1 for _ in open(ko_output))}\")\n",
    "\n",
    "print(f\"\\n  English: {en_output}\")\n",
    "print(f\"    Size: {os.path.getsize(en_output) / 1024 / 1024:.2f} MB\")\n",
    "print(f\"    Lines: {sum(1 for _ in open(en_output))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've successfully extracted and cleaned Korean and English Wikipedia articles. The data is now ready for synonym extraction in the next notebook.\n",
    "\n",
    "**Next steps:**\n",
    "- Extract inter-language links\n",
    "- Extract synonym pairs from article text\n",
    "- Build comprehensive bilingual dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
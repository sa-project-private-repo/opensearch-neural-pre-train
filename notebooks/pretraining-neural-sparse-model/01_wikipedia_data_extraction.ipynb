{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Korean & English Data Extraction\n",
    "\n",
    "This notebook extracts Korean and English text from multiple sources for building a bilingual neural sparse model.\n",
    "\n",
    "**Data Sources:**\n",
    "1. **Korean Wikipedia** - Encyclopedia articles (direct XML dumps from Wikimedia)\n",
    "2. **English Wikipedia** - Encyclopedia articles (direct XML dumps from Wikimedia)\n",
    "3. **NamuWiki** - Korean wiki encyclopedia (via HuggingFace: `heegyu/namuwiki-extracted`)\n",
    "4. **Î™®ÎëêÏùò ÎßêÎ≠âÏπò** - Korean corpus from National Institute of Korean Language (via Korpora library)\n",
    "\n",
    "**Updated**: November 2025 - Using latest data dumps for all sources\n",
    "\n",
    "## Steps\n",
    "1. Load data from all sources\n",
    "2. Parse and extract text\n",
    "3. Clean and filter content\n",
    "4. Save processed data in chunks (50K items per file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.data.wikipedia_xml_parser import WikipediaXMLParser\n",
    "from src.data.namuwiki_parser import NamuWikiParser\n",
    "from src.data.modu_corpus_parser import ModuCorpusParser\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory\n",
    "output_dir = Path(\"../../dataset/wikipedia\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# We'll split files into chunks to avoid very large files\n",
    "ARTICLES_PER_FILE = 50000  # 50K articles per file\n",
    "\n",
    "# Processing control\n",
    "SKIP_IF_EXISTS = True  # Set to False to force re-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Korean Wikipedia Articles\n",
    "\n",
    "**Processing all Korean Wikipedia articles** (no limit)\n",
    "\n",
    "Files will be saved in chunks of 50,000 articles each to avoid very large files.\n",
    "\n",
    "**Note**: First run will download the Wikipedia dump (~GB size). Subsequent runs will use cached file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "‚úì Wikipedia XML dump already cached: kowiki-20251101.xml.bz2\n",
      "  Size: 1.18 GB\n",
      "================================================================================\n",
      "Using cached dump: ../../dataset/wikipedia/cache/kowiki-20251101.xml.bz2\n",
      "\n",
      "================================================================================\n",
      "Processing ALL Korean Wikipedia articles\n",
      "Files will be split into chunks of 50,000 articles\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Korean Wikipedia: 5it [00:00, 34.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Wikipedia dump: ../../dataset/wikipedia/cache/kowiki-20251101.xml.bz2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Korean Wikipedia: 55918it [03:37, 92.89it/s, chunks=1, articles=5e+4, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/ko_articles_chunk_001.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Korean Wikipedia: 112795it [05:58, 189.69it/s, chunks=2, articles=1e+5, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/ko_articles_chunk_002.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Korean Wikipedia: 171915it [08:11, 287.61it/s, chunks=3, articles=150000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/ko_articles_chunk_003.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Korean Wikipedia: 233567it [10:24, 234.61it/s, chunks=4, articles=2e+5, current_chunk=5e+4]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/ko_articles_chunk_004.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Korean Wikipedia: 298401it [12:21, 665.95it/s, chunks=5, articles=250000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/ko_articles_chunk_005.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Korean Wikipedia: 364911it [14:21, 344.84it/s, chunks=6, articles=3e+5, current_chunk=5e+4]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/ko_articles_chunk_006.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Korean Wikipedia: 430050it [16:25, 391.38it/s, chunks=7, articles=350000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/ko_articles_chunk_007.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Korean Wikipedia: 503996it [18:19, 744.29it/s, chunks=8, articles=4e+5, current_chunk=5e+4]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/ko_articles_chunk_008.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Korean Wikipedia: 575761it [20:43, 505.94it/s, chunks=9, articles=450000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/ko_articles_chunk_009.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Korean Wikipedia: 647753it [22:20, 473.89it/s, chunks=10, articles=5e+5, current_chunk=5e+4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/ko_articles_chunk_010.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Korean Wikipedia: 711968it [24:30, 103.70it/s, chunks=11, articles=550000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/ko_articles_chunk_011.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Korean Wikipedia: 726340it [25:25, 476.17it/s, chunks=11, articles=550000, current_chunk=5e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 11999 articles to ../../dataset/wikipedia/ko_articles_chunk_012.jsonl\n",
      "\n",
      "‚úì Processed 561,999 Korean articles\n",
      "‚úì Saved in 12 chunk files\n",
      "‚úì Sample article: ÏßÄÎØ∏ Ïπ¥ÌÑ∞\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Check if Korean chunk files already exist\n",
    "ko_existing_chunks = sorted(glob.glob(str(output_dir / \"ko_articles_chunk_*.jsonl\")))\n",
    "\n",
    "if SKIP_IF_EXISTS and ko_existing_chunks:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úì Korean Wikipedia chunk files already exist!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nFound {len(ko_existing_chunks)} existing chunk files:\")\n",
    "    \n",
    "    ko_articles_total = []\n",
    "    for chunk_file in ko_existing_chunks:\n",
    "        chunk_articles = []\n",
    "        with open(chunk_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                chunk_articles.append(json.loads(line))\n",
    "        ko_articles_total.extend(chunk_articles)\n",
    "        print(f\"  - {Path(chunk_file).name}: {len(chunk_articles):,} articles\")\n",
    "    \n",
    "    print(f\"\\nTotal: {len(ko_articles_total):,} Korean articles loaded from cache\")\n",
    "    print(\"\\nüí° Set SKIP_IF_EXISTS = False to force re-processing\")\n",
    "    \n",
    "else:\n",
    "    # Initialize Korean parser (using latest dump)\n",
    "    ko_parser = WikipediaXMLParser(\n",
    "        language=\"ko\",\n",
    "        date=\"latest\",  # Will automatically use the most recent dump\n",
    "        cache_dir=\"../../dataset/wikipedia/cache\"\n",
    "    )\n",
    "\n",
    "    # Check if XML dump already exists in cache\n",
    "    if ko_parser.date == \"latest\":\n",
    "        ko_parser.date = ko_parser.get_latest_dump_date()\n",
    "    \n",
    "    dump_path = ko_parser.cache_dir / f\"{ko_parser.language}wiki-{ko_parser.date}.xml.bz2\"\n",
    "    \n",
    "    if dump_path.exists():\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"‚úì Wikipedia XML dump already cached: {dump_path.name}\")\n",
    "        print(f\"  Size: {os.path.getsize(dump_path) / 1024 / 1024 / 1024:.2f} GB\")\n",
    "        print(\"=\" * 80)\n",
    "    else:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"‚¨á Downloading Wikipedia XML dump (this will take a while)...\")\n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    # Download the dump (will skip if already exists)\n",
    "    dump_path = ko_parser.download_dump()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Processing ALL Korean Wikipedia articles\")\n",
    "    print(\"Files will be split into chunks of 50,000 articles\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Process articles in streaming mode and save in chunks\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    ko_articles_total = []\n",
    "    chunk_num = 0\n",
    "    current_chunk = []\n",
    "\n",
    "    iterator = ko_parser.iter_articles(dump_path)\n",
    "    pbar = tqdm(iterator, desc=\"Processing Korean Wikipedia\")\n",
    "\n",
    "    for raw_article in pbar:\n",
    "        # Parse wikitext to plain text\n",
    "        text = ko_parser.parse_wikitext(raw_article[\"wikitext\"])\n",
    "        \n",
    "        article = {\n",
    "            \"id\": raw_article[\"id\"],\n",
    "            \"url\": raw_article[\"url\"],\n",
    "            \"title\": raw_article[\"title\"],\n",
    "            \"text\": text,\n",
    "            \"language\": \"ko\",\n",
    "        }\n",
    "        \n",
    "        # Apply filters\n",
    "        if ko_parser.filter_article(article, min_length=200, max_length=100000):\n",
    "            current_chunk.append(article)\n",
    "            ko_articles_total.append(article)\n",
    "            \n",
    "            # Save chunk when it reaches the limit\n",
    "            if len(current_chunk) >= ARTICLES_PER_FILE:\n",
    "                chunk_num += 1\n",
    "                output_file = output_dir / f\"ko_articles_chunk_{chunk_num:03d}.jsonl\"\n",
    "                ko_parser.save_articles(current_chunk, output_file)\n",
    "                pbar.set_postfix({\n",
    "                    'chunks': chunk_num, \n",
    "                    'articles': len(ko_articles_total),\n",
    "                    'current_chunk': len(current_chunk)\n",
    "                })\n",
    "                current_chunk = []\n",
    "\n",
    "    # Save remaining articles in last chunk\n",
    "    if current_chunk:\n",
    "        chunk_num += 1\n",
    "        output_file = output_dir / f\"ko_articles_chunk_{chunk_num:03d}.jsonl\"\n",
    "        ko_parser.save_articles(current_chunk, output_file)\n",
    "\n",
    "    print(f\"\\n‚úì Processed {len(ko_articles_total):,} Korean articles\")\n",
    "    print(f\"‚úì Saved in {chunk_num} chunk files\")\n",
    "    if ko_articles_total:\n",
    "        print(f\"‚úì Sample article: {ko_articles_total[0]['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract English Wikipedia Articles\n",
    "\n",
    "**Processing all English Wikipedia articles** (no limit)\n",
    "\n",
    "Files will be saved in chunks of 50,000 articles each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "‚úì Wikipedia XML dump already cached: enwiki-20251101.xml.bz2\n",
      "  Size: 23.95 GB\n",
      "================================================================================\n",
      "Using cached dump: ../../dataset/wikipedia/cache/enwiki-20251101.xml.bz2\n",
      "\n",
      "================================================================================\n",
      "Processing ALL English Wikipedia articles\n",
      "Files will be split into chunks of 50,000 articles\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 4it [00:00, 32.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Wikipedia dump: ../../dataset/wikipedia/cache/enwiki-20251101.xml.bz2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 52133it [14:24, 13.99it/s, chunks=1, articles=5e+4, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_001.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 103028it [21:14, 16.70it/s, chunks=2, articles=1e+5, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_002.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 155331it [30:44, 118.92it/s, chunks=3, articles=150000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_003.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 208003it [39:13, 21.25it/s, chunks=4, articles=2e+5, current_chunk=5e+4]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_004.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 260763it [47:36, 40.54it/s, chunks=5, articles=250000, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_005.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 312907it [55:24, 30.83it/s, chunks=6, articles=3e+5, current_chunk=5e+4]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_006.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 365104it [1:03:01, 49.13it/s, chunks=7, articles=350000, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_007.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 417086it [1:10:23, 36.88it/s, chunks=8, articles=4e+5, current_chunk=5e+4]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_008.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 469091it [1:17:28, 44.02it/s, chunks=9, articles=450000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_009.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 521185it [1:25:00, 51.17it/s, chunks=10, articles=5e+5, current_chunk=5e+4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_010.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 573258it [1:32:01, 51.41it/s, chunks=11, articles=550000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_011.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 625841it [1:39:02, 64.28it/s, chunks=12, articles=6e+5, current_chunk=5e+4]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_012.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 677738it [1:45:16, 25.58it/s, chunks=13, articles=650000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_013.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 729864it [1:51:03, 75.76it/s, chunks=14, articles=7e+5, current_chunk=5e+4]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_014.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 781850it [1:57:17, 55.90it/s, chunks=15, articles=750000, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_015.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 834061it [2:03:43, 68.30it/s, chunks=16, articles=8e+5, current_chunk=5e+4]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_016.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 886160it [2:09:46, 78.70it/s, chunks=17, articles=850000, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_017.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 938235it [2:15:32, 66.06it/s, chunks=18, articles=9e+5, current_chunk=5e+4]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_018.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 990441it [2:21:00, 69.41it/s, chunks=19, articles=950000, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_019.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 1042656it [2:26:17, 89.54it/s, chunks=20, articles=1e+6, current_chunk=5e+4]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_020.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 1094922it [2:31:13, 85.13it/s, chunks=21, articles=1050000, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_021.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 1147216it [2:35:52, 63.15it/s, chunks=22, articles=1.1e+6, current_chunk=5e+4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_022.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 1199601it [2:39:58, 91.17it/s, chunks=23, articles=1150000, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_023.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 1251920it [2:44:41, 69.33it/s, chunks=24, articles=1.2e+6, current_chunk=5e+4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_024.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 1304091it [2:49:15, 122.24it/s, chunks=25, articles=1250000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_025.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 1356224it [2:53:41, 125.40it/s, chunks=26, articles=1.3e+6, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_026.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 1408706it [2:58:16, 117.49it/s, chunks=27, articles=1350000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_027.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 1460976it [3:02:28, 81.32it/s, chunks=28, articles=1.4e+6, current_chunk=5e+4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_028.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 1514136it [3:06:49, 409.76it/s, chunks=29, articles=1450000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_029.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 1567006it [3:11:31, 107.79it/s, chunks=30, articles=1.5e+6, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_030.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 1620499it [3:15:46, 141.15it/s, chunks=31, articles=1550000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_031.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 1673470it [3:19:50, 116.16it/s, chunks=32, articles=1.6e+6, current_chunk=5e+4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_032.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 1725149it [3:22:54, 116.91it/s, chunks=33, articles=1650000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_033.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 1777797it [3:26:05, 226.86it/s, chunks=34, articles=1.7e+6, current_chunk=5e+4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_034.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 1831761it [3:29:36, 145.13it/s, chunks=35, articles=1750000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_035.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 1884598it [3:33:47, 120.24it/s, chunks=36, articles=1.8e+6, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_036.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 1938639it [3:37:30, 113.87it/s, chunks=37, articles=1850000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_037.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 1992525it [3:40:48, 227.07it/s, chunks=38, articles=1.9e+6, current_chunk=5e+4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_038.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 2051609it [3:43:51, 107.13it/s, chunks=39, articles=1950000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_039.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 2105071it [3:47:47, 120.33it/s, chunks=40, articles=2e+6, current_chunk=5e+4]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_040.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 2158087it [3:51:29, 98.39it/s, chunks=41, articles=2050000, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_041.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 2211897it [3:55:17, 73.74it/s, chunks=42, articles=2.1e+6, current_chunk=5e+4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_042.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 2267430it [3:58:25, 99.78it/s, chunks=43, articles=2150000, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_043.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 2324755it [4:01:47, 147.83it/s, chunks=44, articles=2.2e+6, current_chunk=5e+4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_044.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 2384066it [4:05:31, 132.54it/s, chunks=45, articles=2250000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_045.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 2437084it [4:08:58, 83.72it/s, chunks=46, articles=2.3e+6, current_chunk=5e+4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_046.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 2492974it [4:12:35, 121.94it/s, chunks=47, articles=2350000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_047.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 2547273it [4:15:37, 109.31it/s, chunks=48, articles=2.4e+6, current_chunk=5e+4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_048.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 2601610it [4:18:27, 166.03it/s, chunks=49, articles=2450000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_049.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 2656523it [4:21:59, 106.16it/s, chunks=50, articles=2.5e+6, current_chunk=5e+4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_050.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 2710965it [4:25:34, 167.43it/s, chunks=51, articles=2550000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_051.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 2766329it [4:28:41, 165.92it/s, chunks=52, articles=2.6e+6, current_chunk=5e+4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_052.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 2819320it [4:32:13, 143.83it/s, chunks=53, articles=2650000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_053.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 2872286it [4:35:44, 134.87it/s, chunks=54, articles=2.7e+6, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_054.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 2927240it [4:39:14, 119.43it/s, chunks=55, articles=2750000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_055.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 2980262it [4:42:55, 149.21it/s, chunks=56, articles=2.8e+6, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_056.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 3034135it [4:46:06, 103.24it/s, chunks=57, articles=2850000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_057.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 3089026it [4:49:45, 129.39it/s, chunks=58, articles=2.9e+6, current_chunk=5e+4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_058.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 3143393it [4:53:15, 338.60it/s, chunks=59, articles=2950000, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_059.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 3200304it [4:56:46, 150.54it/s, chunks=60, articles=3e+6, current_chunk=5e+4]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_060.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 3253137it [4:59:59, 151.29it/s, chunks=61, articles=3050000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_061.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 3307767it [5:03:35, 133.04it/s, chunks=62, articles=3.1e+6, current_chunk=5e+4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_062.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 3360597it [5:07:17, 219.86it/s, chunks=63, articles=3150000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_063.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 3413953it [5:11:15, 53.66it/s, chunks=64, articles=3.2e+6, current_chunk=5e+4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_064.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 3467126it [5:15:16, 121.27it/s, chunks=65, articles=3250000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_065.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 3520612it [5:18:31, 223.77it/s, chunks=66, articles=3.3e+6, current_chunk=5e+4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_066.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 3574627it [5:21:56, 277.79it/s, chunks=67, articles=3350000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_067.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 3628057it [5:25:22, 143.79it/s, chunks=68, articles=3.4e+6, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_068.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 3681312it [5:28:51, 143.76it/s, chunks=69, articles=3450000, current_chunk=5e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_069.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 3735730it [5:32:18, 103.99it/s, chunks=70, articles=3.5e+6, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_070.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 3789090it [5:35:29, 493.66it/s, chunks=71, articles=3550000, current_chunk=5e+4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_071.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 3842265it [5:38:46, 149.42it/s, chunks=72, articles=3.6e+6, current_chunk=5e+4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 articles to ../../dataset/wikipedia/en_articles_chunk_072.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English Wikipedia: 3864632it [5:40:12, 296.09it/s, chunks=72, articles=3.6e+6, current_chunk=5e+4]"
     ]
    }
   ],
   "source": [
    "# Check if English chunk files already exist\n",
    "en_existing_chunks = sorted(glob.glob(str(output_dir / \"en_articles_chunk_*.jsonl\")))\n",
    "\n",
    "if SKIP_IF_EXISTS and en_existing_chunks:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úì English Wikipedia chunk files already exist!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nFound {len(en_existing_chunks)} existing chunk files:\")\n",
    "    \n",
    "    en_articles_total = []\n",
    "    for chunk_file in en_existing_chunks:\n",
    "        chunk_articles = []\n",
    "        with open(chunk_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                chunk_articles.append(json.loads(line))\n",
    "        en_articles_total.extend(chunk_articles)\n",
    "        print(f\"  - {Path(chunk_file).name}: {len(chunk_articles):,} articles\")\n",
    "    \n",
    "    print(f\"\\nTotal: {len(en_articles_total):,} English articles loaded from cache\")\n",
    "    print(\"\\nüí° Set SKIP_IF_EXISTS = False to force re-processing\")\n",
    "    \n",
    "else:\n",
    "    # Initialize English parser (using latest dump)\n",
    "    en_parser = WikipediaXMLParser(\n",
    "        language=\"en\",\n",
    "        date=\"latest\",  # Will automatically use the most recent dump\n",
    "        cache_dir=\"../../dataset/wikipedia/cache\"\n",
    "    )\n",
    "\n",
    "    # Check if XML dump already exists in cache\n",
    "    if en_parser.date == \"latest\":\n",
    "        en_parser.date = en_parser.get_latest_dump_date()\n",
    "    \n",
    "    dump_path = en_parser.cache_dir / f\"{en_parser.language}wiki-{en_parser.date}.xml.bz2\"\n",
    "    \n",
    "    if dump_path.exists():\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"‚úì Wikipedia XML dump already cached: {dump_path.name}\")\n",
    "        print(f\"  Size: {os.path.getsize(dump_path) / 1024 / 1024 / 1024:.2f} GB\")\n",
    "        print(\"=\" * 80)\n",
    "    else:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"‚¨á Downloading Wikipedia XML dump (this will take a while)...\")\n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    # Download the dump (will skip if already exists)\n",
    "    dump_path = en_parser.download_dump()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Processing ALL English Wikipedia articles\")\n",
    "    print(\"Files will be split into chunks of 50,000 articles\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Process articles in streaming mode and save in chunks\n",
    "    en_articles_total = []\n",
    "    chunk_num = 0\n",
    "    current_chunk = []\n",
    "\n",
    "    iterator = en_parser.iter_articles(dump_path)\n",
    "    pbar = tqdm(iterator, desc=\"Processing English Wikipedia\")\n",
    "\n",
    "    for raw_article in pbar:\n",
    "        # Parse wikitext to plain text\n",
    "        text = en_parser.parse_wikitext(raw_article[\"wikitext\"])\n",
    "        \n",
    "        article = {\n",
    "            \"id\": raw_article[\"id\"],\n",
    "            \"url\": raw_article[\"url\"],\n",
    "            \"title\": raw_article[\"title\"],\n",
    "            \"text\": text,\n",
    "            \"language\": \"en\",\n",
    "        }\n",
    "        \n",
    "        # Apply filters\n",
    "        if en_parser.filter_article(article, min_length=200, max_length=100000):\n",
    "            current_chunk.append(article)\n",
    "            en_articles_total.append(article)\n",
    "            \n",
    "            # Save chunk when it reaches the limit\n",
    "            if len(current_chunk) >= ARTICLES_PER_FILE:\n",
    "                chunk_num += 1\n",
    "                output_file = output_dir / f\"en_articles_chunk_{chunk_num:03d}.jsonl\"\n",
    "                en_parser.save_articles(current_chunk, output_file)\n",
    "                pbar.set_postfix({\n",
    "                    'chunks': chunk_num, \n",
    "                    'articles': len(en_articles_total),\n",
    "                    'current_chunk': len(current_chunk)\n",
    "                })\n",
    "                current_chunk = []\n",
    "\n",
    "    # Save remaining articles in last chunk\n",
    "    if current_chunk:\n",
    "        chunk_num += 1\n",
    "        output_file = output_dir / f\"en_articles_chunk_{chunk_num:03d}.jsonl\"\n",
    "        en_parser.save_articles(current_chunk, output_file)\n",
    "\n",
    "    print(f\"\\n‚úì Processed {len(en_articles_total):,} English articles\")\n",
    "    print(f\"‚úì Saved in {chunk_num} chunk files\")\n",
    "    if en_articles_total:\n",
    "        print(f\"‚úì Sample article: {en_articles_total[0]['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract NamuWiki Articles (Korean)\n",
    "\n",
    "**NamuWiki** is a Korean wiki encyclopedia with ~1.5M articles, providing additional Korean language data.\n",
    "\n",
    "Using HuggingFace dataset: `heegyu/namuwiki-extracted`\n",
    "\n",
    "Files will be saved in chunks of 50,000 articles each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if NamuWiki chunk files already exist\n",
    "namu_existing_chunks = sorted(glob.glob(str(output_dir / \"../namuwiki/namuwiki_chunk_*.jsonl\")))\n",
    "\n",
    "if SKIP_IF_EXISTS and namu_existing_chunks:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úì NamuWiki chunk files already exist!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nFound {len(namu_existing_chunks)} existing chunk files:\")\n",
    "    \n",
    "    namu_articles_total = []\n",
    "    for chunk_file in namu_existing_chunks:\n",
    "        chunk_articles = []\n",
    "        with open(chunk_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                chunk_articles.append(json.loads(line))\n",
    "        namu_articles_total.extend(chunk_articles)\n",
    "        print(f\"  - {Path(chunk_file).name}: {len(chunk_articles):,} articles\")\n",
    "    \n",
    "    print(f\"\\nTotal: {len(namu_articles_total):,} NamuWiki articles loaded from cache\")\n",
    "    print(\"\\nüí° Set SKIP_IF_EXISTS = False to force re-processing\")\n",
    "    \n",
    "else:\n",
    "    # Initialize NamuWiki parser\n",
    "    namu_parser = NamuWikiParser(cache_dir=\"../../dataset/namuwiki/cache\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"Processing NamuWiki articles from HuggingFace\")\n",
    "    print(\"Files will be split into chunks of 50,000 articles\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n‚¨á This will download ~2GB of data on first run...\")\n",
    "    \n",
    "    # Process NamuWiki articles (will automatically chunk the output)\n",
    "    namu_articles_total = namu_parser.process_namuwiki(\n",
    "        output_path=\"../../dataset/namuwiki/namuwiki_articles.jsonl\",\n",
    "        max_articles=None,  # Process all articles\n",
    "        min_length=100,\n",
    "        max_length=100000,\n",
    "        chunk_size=ARTICLES_PER_FILE\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úì Processed {len(namu_articles_total):,} NamuWiki articles\")\n",
    "    if namu_articles_total:\n",
    "        print(f\"‚úì Sample article: {namu_articles_total[0]['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract Î™®ÎëêÏùò ÎßêÎ≠âÏπò (Korean)\n",
    "\n",
    "**Î™®ÎëêÏùò ÎßêÎ≠âÏπò** (Everyone's Corpus) is a large-scale Korean language corpus from the National Institute of Korean Language.\n",
    "\n",
    "Includes:\n",
    "- News articles (Ïã†Î¨∏ ÎßêÎ≠âÏπò)\n",
    "- Spoken language (Íµ¨Ïñ¥ ÎßêÎ≠âÏπò)\n",
    "- Web text (Ïõπ ÎßêÎ≠âÏπò)\n",
    "- Messenger conversations (Î©îÏã†Ï†Ä ÎßêÎ≠âÏπò)\n",
    "\n",
    "Files will be saved in chunks of 50,000 texts each.\n",
    "\n",
    "**Note**: Some corpora may require authentication. The parser will automatically skip unavailable corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inspect Sample Articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Korean article sample\n",
    "if len(ko_articles_total) > 0:\n",
    "    # Use first available article or 10th if available\n",
    "    sample_idx = min(10, len(ko_articles_total) - 1)\n",
    "    sample_ko = ko_articles_total[sample_idx]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Article #{sample_idx + 1} of {len(ko_articles_total):,}\")\n",
    "    print(f\"Title: {sample_ko['title']}\")\n",
    "    print(f\"URL: {sample_ko['url']}\")\n",
    "    print(f\"Language: {sample_ko['language']}\")\n",
    "    print(f\"Text length: {len(sample_ko['text'])} characters\")\n",
    "    print(\"\\nFirst 300 characters:\")\n",
    "    print(sample_ko['text'][:300])\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"No articles found. Check filtering criteria.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA EXTRACTION STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Korean Wikipedia stats\n",
    "if len(ko_articles_total) > 0:\n",
    "    ko_lengths = [len(a['text']) for a in ko_articles_total]\n",
    "    print(\"\\n1. Korean Wikipedia:\")\n",
    "    print(f\"     Total: {len(ko_articles_total):,}\")\n",
    "    print(f\"     Mean length: {np.mean(ko_lengths):.0f} chars\")\n",
    "    print(f\"     Median length: {np.median(ko_lengths):.0f} chars\")\n",
    "    print(f\"     Min length: {np.min(ko_lengths):.0f} chars\")\n",
    "    print(f\"     Max length: {np.max(ko_lengths):.0f} chars\")\n",
    "else:\n",
    "    print(\"\\n1. Korean Wikipedia: No articles found\")\n",
    "\n",
    "# English Wikipedia stats\n",
    "if len(en_articles_total) > 0:\n",
    "    en_lengths = [len(a['text']) for a in en_articles_total]\n",
    "    print(\"\\n2. English Wikipedia:\")\n",
    "    print(f\"     Total: {len(en_articles_total):,}\")\n",
    "    print(f\"     Mean length: {np.mean(en_lengths):.0f} chars\")\n",
    "    print(f\"     Median length: {np.median(en_lengths):.0f} chars\")\n",
    "    print(f\"     Min length: {np.min(en_lengths):.0f} chars\")\n",
    "    print(f\"     Max length: {np.max(en_lengths):.0f} chars\")\n",
    "else:\n",
    "    print(\"\\n2. English Wikipedia: No articles found\")\n",
    "\n",
    "# NamuWiki stats\n",
    "if 'namu_articles_total' in locals() and len(namu_articles_total) > 0:\n",
    "    namu_lengths = [len(a['text']) for a in namu_articles_total]\n",
    "    print(\"\\n3. NamuWiki (Korean):\")\n",
    "    print(f\"     Total: {len(namu_articles_total):,}\")\n",
    "    print(f\"     Mean length: {np.mean(namu_lengths):.0f} chars\")\n",
    "    print(f\"     Median length: {np.median(namu_lengths):.0f} chars\")\n",
    "    print(f\"     Min length: {np.min(namu_lengths):.0f} chars\")\n",
    "    print(f\"     Max length: {np.max(namu_lengths):.0f} chars\")\n",
    "else:\n",
    "    print(\"\\n3. NamuWiki: No articles found\")\n",
    "\n",
    "# Î™®ÎëêÏùò ÎßêÎ≠âÏπò stats\n",
    "if 'modu_articles_total' in locals() and len(modu_articles_total) > 0:\n",
    "    modu_lengths = [len(a['text']) for a in modu_articles_total]\n",
    "    print(\"\\n4. Î™®ÎëêÏùò ÎßêÎ≠âÏπò (Korean):\")\n",
    "    print(f\"     Total: {len(modu_articles_total):,}\")\n",
    "    print(f\"     Mean length: {np.mean(modu_lengths):.0f} chars\")\n",
    "    print(f\"     Median length: {np.median(modu_lengths):.0f} chars\")\n",
    "    print(f\"     Min length: {np.min(modu_lengths):.0f} chars\")\n",
    "    print(f\"     Max length: {np.max(modu_lengths):.0f} chars\")\n",
    "else:\n",
    "    print(\"\\n4. Î™®ÎëêÏùò ÎßêÎ≠âÏπò: No texts found\")\n",
    "\n",
    "# Total statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LANGUAGE BALANCE:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "korean_total = 0\n",
    "english_total = 0\n",
    "\n",
    "if len(ko_articles_total) > 0:\n",
    "    korean_total += len(ko_articles_total)\n",
    "if 'namu_articles_total' in locals() and len(namu_articles_total) > 0:\n",
    "    korean_total += len(namu_articles_total)\n",
    "if 'modu_articles_total' in locals() and len(modu_articles_total) > 0:\n",
    "    korean_total += len(modu_articles_total)\n",
    "if len(en_articles_total) > 0:\n",
    "    english_total += len(en_articles_total)\n",
    "\n",
    "print(f\"\\nKorean texts:  {korean_total:,}\")\n",
    "print(f\"English texts: {english_total:,}\")\n",
    "print(f\"Total texts:   {korean_total + english_total:,}\")\n",
    "\n",
    "if korean_total > 0 and english_total > 0:\n",
    "    ratio = english_total / korean_total\n",
    "    print(f\"\\nEnglish/Korean ratio: {ratio:.2f}x\")\n",
    "    \n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Verify Saved Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVED CHUNK FILES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find all Korean Wikipedia chunk files\n",
    "ko_chunks = sorted(glob.glob(str(output_dir / \"ko_articles_chunk_*.jsonl\")))\n",
    "if ko_chunks:\n",
    "    print(f\"\\n1. Korean Wikipedia: {len(ko_chunks)} chunk files\")\n",
    "    total_size = sum(os.path.getsize(f) for f in ko_chunks)\n",
    "    total_lines = sum(sum(1 for _ in open(f)) for f in ko_chunks)\n",
    "    print(f\"   Total size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"   Total articles: {total_lines:,}\")\n",
    "    print(f\"   Files:\")\n",
    "    for chunk in ko_chunks[:3]:  # Show first 3\n",
    "        size = os.path.getsize(chunk) / 1024 / 1024\n",
    "        lines = sum(1 for _ in open(chunk))\n",
    "        print(f\"     - {os.path.basename(chunk):30s} ({size:>6.2f} MB, {lines:>6,} articles)\")\n",
    "    if len(ko_chunks) > 3:\n",
    "        print(f\"     ... and {len(ko_chunks) - 3} more files\")\n",
    "else:\n",
    "    print(\"\\n1. Korean Wikipedia: No chunk files found\")\n",
    "\n",
    "# Find all English Wikipedia chunk files\n",
    "en_chunks = sorted(glob.glob(str(output_dir / \"en_articles_chunk_*.jsonl\")))\n",
    "if en_chunks:\n",
    "    print(f\"\\n2. English Wikipedia: {len(en_chunks)} chunk files\")\n",
    "    total_size = sum(os.path.getsize(f) for f in en_chunks)\n",
    "    total_lines = sum(sum(1 for _ in open(f)) for f in en_chunks)\n",
    "    print(f\"   Total size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"   Total articles: {total_lines:,}\")\n",
    "    print(f\"   Files:\")\n",
    "    for chunk in en_chunks[:3]:  # Show first 3\n",
    "        size = os.path.getsize(chunk) / 1024 / 1024\n",
    "        lines = sum(1 for _ in open(chunk))\n",
    "        print(f\"     - {os.path.basename(chunk):30s} ({size:>6.2f} MB, {lines:>6,} articles)\")\n",
    "    if len(en_chunks) > 3:\n",
    "        print(f\"     ... and {len(en_chunks) - 3} more files\")\n",
    "else:\n",
    "    print(\"\\n2. English Wikipedia: No chunk files found\")\n",
    "\n",
    "# Find all NamuWiki chunk files\n",
    "namu_dir = output_dir / \"../namuwiki\"\n",
    "namu_chunks = sorted(glob.glob(str(namu_dir / \"namuwiki_chunk_*.jsonl\")))\n",
    "if namu_chunks:\n",
    "    print(f\"\\n3. NamuWiki (Korean): {len(namu_chunks)} chunk files\")\n",
    "    total_size = sum(os.path.getsize(f) for f in namu_chunks)\n",
    "    total_lines = sum(sum(1 for _ in open(f)) for f in namu_chunks)\n",
    "    print(f\"   Total size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"   Total articles: {total_lines:,}\")\n",
    "    print(f\"   Files:\")\n",
    "    for chunk in namu_chunks[:3]:  # Show first 3\n",
    "        size = os.path.getsize(chunk) / 1024 / 1024\n",
    "        lines = sum(1 for _ in open(chunk))\n",
    "        print(f\"     - {os.path.basename(chunk):30s} ({size:>6.2f} MB, {lines:>6,} articles)\")\n",
    "    if len(namu_chunks) > 3:\n",
    "        print(f\"     ... and {len(namu_chunks) - 3} more files\")\n",
    "else:\n",
    "    print(\"\\n3. NamuWiki: No chunk files found\")\n",
    "\n",
    "# Find all Î™®ÎëêÏùò ÎßêÎ≠âÏπò chunk files\n",
    "modu_dir = output_dir / \"../modu\"\n",
    "modu_chunks = sorted(glob.glob(str(modu_dir / \"modu_chunk_*.jsonl\")))\n",
    "if modu_chunks:\n",
    "    print(f\"\\n4. Î™®ÎëêÏùò ÎßêÎ≠âÏπò (Korean): {len(modu_chunks)} chunk files\")\n",
    "    total_size = sum(os.path.getsize(f) for f in modu_chunks)\n",
    "    total_lines = sum(sum(1 for _ in open(f)) for f in modu_chunks)\n",
    "    print(f\"   Total size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"   Total texts: {total_lines:,}\")\n",
    "    print(f\"   Files:\")\n",
    "    for chunk in modu_chunks[:3]:  # Show first 3\n",
    "        size = os.path.getsize(chunk) / 1024 / 1024\n",
    "        lines = sum(1 for _ in open(chunk))\n",
    "        print(f\"     - {os.path.basename(chunk):30s} ({size:>6.2f} MB, {lines:>6,} texts)\")\n",
    "    if len(modu_chunks) > 3:\n",
    "        print(f\"     ... and {len(modu_chunks) - 3} more files\")\n",
    "else:\n",
    "    print(\"\\n4. Î™®ÎëêÏùò ÎßêÎ≠âÏπò: No chunk files found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've successfully extracted and cleaned **ALL** articles from multiple Korean and English data sources.\n",
    "\n",
    "**Data Sources:**\n",
    "1. **Korean Wikipedia** - Encyclopedia articles (~600K articles)\n",
    "2. **English Wikipedia** - Encyclopedia articles (~6M articles)\n",
    "3. **NamuWiki** - Korean wiki encyclopedia (~1.5M articles)\n",
    "4. **Î™®ÎëêÏùò ÎßêÎ≠âÏπò** - Korean language corpus from National Institute of Korean Language\n",
    "   - News articles (Ïã†Î¨∏ ÎßêÎ≠âÏπò)\n",
    "   - Spoken language (Íµ¨Ïñ¥ ÎßêÎ≠âÏπò)\n",
    "   - Web text (Ïõπ ÎßêÎ≠âÏπò)\n",
    "   - Messenger conversations (Î©îÏã†Ï†Ä ÎßêÎ≠âÏπò)\n",
    "\n",
    "**Key Features:**\n",
    "- ‚úÖ Processes complete datasets (no article limit)\n",
    "- ‚úÖ Saves data in manageable chunks (50,000 items per file)\n",
    "- ‚úÖ Filters out redirects, special pages, and low-quality content\n",
    "- ‚úÖ Cleans markup to plain text\n",
    "- ‚úÖ Implements 2-level caching (chunk files + raw dumps)\n",
    "- ‚úÖ Significantly improved Korean-English data balance\n",
    "\n",
    "**Output Structure:**\n",
    "```\n",
    "dataset/\n",
    "‚îú‚îÄ‚îÄ wikipedia/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ ko_articles_chunk_001.jsonl\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ ko_articles_chunk_002.jsonl\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ ...\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ en_articles_chunk_001.jsonl\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ en_articles_chunk_002.jsonl\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ namuwiki/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ namuwiki_chunk_001.jsonl\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ namuwiki_chunk_002.jsonl\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ modu/\n",
    "    ‚îú‚îÄ‚îÄ modu_chunk_001.jsonl\n",
    "    ‚îú‚îÄ‚îÄ modu_chunk_002.jsonl\n",
    "    ‚îî‚îÄ‚îÄ ...\n",
    "```\n",
    "\n",
    "**Next steps:**\n",
    "- Extract inter-language links from chunks\n",
    "- Extract synonym pairs from article text\n",
    "- Build comprehensive bilingual dictionary\n",
    "- Train neural sparse model with balanced Korean-English data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've successfully extracted and cleaned **ALL** Korean and English Wikipedia articles.\n",
    "\n",
    "**Key Features:**\n",
    "- ‚úÖ Processes complete Wikipedia dumps (no article limit)\n",
    "- ‚úÖ Saves data in manageable chunks (50,000 articles per file)\n",
    "- ‚úÖ Filters out redirects, special pages, and low-quality articles\n",
    "- ‚úÖ Cleans MediaWiki markup to plain text\n",
    "- ‚úÖ Ready for synonym extraction and model training\n",
    "\n",
    "**Output Structure:**\n",
    "```\n",
    "dataset/wikipedia/\n",
    "‚îú‚îÄ‚îÄ ko_articles_chunk_001.jsonl  (50,000 articles)\n",
    "‚îú‚îÄ‚îÄ ko_articles_chunk_002.jsonl  (50,000 articles)\n",
    "‚îú‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ en_articles_chunk_001.jsonl  (50,000 articles)\n",
    "‚îú‚îÄ‚îÄ en_articles_chunk_002.jsonl  (50,000 articles)\n",
    "‚îî‚îÄ‚îÄ ...\n",
    "```\n",
    "\n",
    "**Next steps:**\n",
    "- Extract inter-language links from chunks\n",
    "- Extract synonym pairs from article text\n",
    "- Build comprehensive bilingual dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

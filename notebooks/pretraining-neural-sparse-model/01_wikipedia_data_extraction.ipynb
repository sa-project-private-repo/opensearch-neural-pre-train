{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Data Extraction\n",
    "\n",
    "This notebook extracts Korean and English Wikipedia articles for building a bilingual synonym dataset.\n",
    "\n",
    "**Updated**: Now using direct Wikipedia XML dumps from Wikimedia for the latest data (November 2025).\n",
    "\n",
    "## Steps\n",
    "1. Load Wikipedia data from Wikimedia dumps\n",
    "2. Parse XML and extract article text\n",
    "3. Clean and filter articles  \n",
    "4. Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.data.wikipedia_xml_parser import WikipediaXMLParser\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Output directory\noutput_dir = Path(\"../../dataset/wikipedia\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# We'll split files into chunks to avoid very large files\nARTICLES_PER_FILE = 50000  # 50K articles per file\n\n# Processing control\nSKIP_IF_EXISTS = True  # Set to False to force re-processing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Korean Wikipedia Articles\n",
    "\n",
    "**Processing all Korean Wikipedia articles** (no limit)\n",
    "\n",
    "Files will be saved in chunks of 50,000 articles each to avoid very large files.\n",
    "\n",
    "**Note**: First run will download the Wikipedia dump (~GB size). Subsequent runs will use cached file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import glob\n\n# Check if Korean chunk files already exist\nko_existing_chunks = sorted(glob.glob(str(output_dir / \"ko_articles_chunk_*.jsonl\")))\n\nif SKIP_IF_EXISTS and ko_existing_chunks:\n    print(\"=\" * 80)\n    print(\"âœ“ Korean Wikipedia chunk files already exist!\")\n    print(\"=\" * 80)\n    print(f\"\\nFound {len(ko_existing_chunks)} existing chunk files:\")\n    \n    ko_articles_total = []\n    for chunk_file in ko_existing_chunks:\n        chunk_articles = []\n        with open(chunk_file, 'r', encoding='utf-8') as f:\n            for line in f:\n                chunk_articles.append(json.loads(line))\n        ko_articles_total.extend(chunk_articles)\n        print(f\"  - {Path(chunk_file).name}: {len(chunk_articles):,} articles\")\n    \n    print(f\"\\nTotal: {len(ko_articles_total):,} Korean articles loaded from cache\")\n    print(\"\\nðŸ’¡ Set SKIP_IF_EXISTS = False to force re-processing\")\n    \nelse:\n    # Initialize Korean parser (using latest dump)\n    ko_parser = WikipediaXMLParser(\n        language=\"ko\",\n        date=\"latest\",  # Will automatically use the most recent dump\n        cache_dir=\"../../dataset/wikipedia/cache\"\n    )\n\n    # Download the dump first (will use cache if available)\n    dump_path = ko_parser.download_dump()\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"Processing ALL Korean Wikipedia articles\")\n    print(\"Files will be split into chunks of 50,000 articles\")\n    print(\"=\"*80 + \"\\n\")\n\n    # Process articles in streaming mode and save in chunks\n    from tqdm import tqdm\n\n    ko_articles_total = []\n    chunk_num = 0\n    current_chunk = []\n\n    iterator = ko_parser.iter_articles(dump_path)\n    pbar = tqdm(iterator, desc=\"Processing Korean Wikipedia\")\n\n    for raw_article in pbar:\n        # Parse wikitext to plain text\n        text = ko_parser.parse_wikitext(raw_article[\"wikitext\"])\n        \n        article = {\n            \"id\": raw_article[\"id\"],\n            \"url\": raw_article[\"url\"],\n            \"title\": raw_article[\"title\"],\n            \"text\": text,\n            \"language\": \"ko\",\n        }\n        \n        # Apply filters\n        if ko_parser.filter_article(article, min_length=200, max_length=100000):\n            current_chunk.append(article)\n            ko_articles_total.append(article)\n            \n            # Save chunk when it reaches the limit\n            if len(current_chunk) >= ARTICLES_PER_FILE:\n                chunk_num += 1\n                output_file = output_dir / f\"ko_articles_chunk_{chunk_num:03d}.jsonl\"\n                ko_parser.save_articles(current_chunk, output_file)\n                pbar.set_postfix({\n                    'chunks': chunk_num, \n                    'articles': len(ko_articles_total),\n                    'current_chunk': len(current_chunk)\n                })\n                current_chunk = []\n\n    # Save remaining articles in last chunk\n    if current_chunk:\n        chunk_num += 1\n        output_file = output_dir / f\"ko_articles_chunk_{chunk_num:03d}.jsonl\"\n        ko_parser.save_articles(current_chunk, output_file)\n\n    print(f\"\\nâœ“ Processed {len(ko_articles_total):,} Korean articles\")\n    print(f\"âœ“ Saved in {chunk_num} chunk files\")\n    if ko_articles_total:\n        print(f\"âœ“ Sample article: {ko_articles_total[0]['title']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract English Wikipedia Articles\n",
    "\n",
    "**Processing all English Wikipedia articles** (no limit)\n",
    "\n",
    "Files will be saved in chunks of 50,000 articles each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if English chunk files already exist\nen_existing_chunks = sorted(glob.glob(str(output_dir / \"en_articles_chunk_*.jsonl\")))\n\nif SKIP_IF_EXISTS and en_existing_chunks:\n    print(\"=\" * 80)\n    print(\"âœ“ English Wikipedia chunk files already exist!\")\n    print(\"=\" * 80)\n    print(f\"\\nFound {len(en_existing_chunks)} existing chunk files:\")\n    \n    en_articles_total = []\n    for chunk_file in en_existing_chunks:\n        chunk_articles = []\n        with open(chunk_file, 'r', encoding='utf-8') as f:\n            for line in f:\n                chunk_articles.append(json.loads(line))\n        en_articles_total.extend(chunk_articles)\n        print(f\"  - {Path(chunk_file).name}: {len(chunk_articles):,} articles\")\n    \n    print(f\"\\nTotal: {len(en_articles_total):,} English articles loaded from cache\")\n    print(\"\\nðŸ’¡ Set SKIP_IF_EXISTS = False to force re-processing\")\n    \nelse:\n    # Initialize English parser (using latest dump)\n    en_parser = WikipediaXMLParser(\n        language=\"en\",\n        date=\"latest\",  # Will automatically use the most recent dump\n        cache_dir=\"../../dataset/wikipedia/cache\"\n    )\n\n    # Download the dump first (will use cache if available)\n    dump_path = en_parser.download_dump()\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"Processing ALL English Wikipedia articles\")\n    print(\"Files will be split into chunks of 50,000 articles\")\n    print(\"=\"*80 + \"\\n\")\n\n    # Process articles in streaming mode and save in chunks\n    en_articles_total = []\n    chunk_num = 0\n    current_chunk = []\n\n    iterator = en_parser.iter_articles(dump_path)\n    pbar = tqdm(iterator, desc=\"Processing English Wikipedia\")\n\n    for raw_article in pbar:\n        # Parse wikitext to plain text\n        text = en_parser.parse_wikitext(raw_article[\"wikitext\"])\n        \n        article = {\n            \"id\": raw_article[\"id\"],\n            \"url\": raw_article[\"url\"],\n            \"title\": raw_article[\"title\"],\n            \"text\": text,\n            \"language\": \"en\",\n        }\n        \n        # Apply filters\n        if en_parser.filter_article(article, min_length=200, max_length=100000):\n            current_chunk.append(article)\n            en_articles_total.append(article)\n            \n            # Save chunk when it reaches the limit\n            if len(current_chunk) >= ARTICLES_PER_FILE:\n                chunk_num += 1\n                output_file = output_dir / f\"en_articles_chunk_{chunk_num:03d}.jsonl\"\n                en_parser.save_articles(current_chunk, output_file)\n                pbar.set_postfix({\n                    'chunks': chunk_num, \n                    'articles': len(en_articles_total),\n                    'current_chunk': len(current_chunk)\n                })\n                current_chunk = []\n\n    # Save remaining articles in last chunk\n    if current_chunk:\n        chunk_num += 1\n        output_file = output_dir / f\"en_articles_chunk_{chunk_num:03d}.jsonl\"\n        en_parser.save_articles(current_chunk, output_file)\n\n    print(f\"\\nâœ“ Processed {len(en_articles_total):,} English articles\")\n    print(f\"âœ“ Saved in {chunk_num} chunk files\")\n    if en_articles_total:\n        print(f\"âœ“ Sample article: {en_articles_total[0]['title']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect Sample Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Korean article sample\n",
    "if len(ko_articles_total) > 0:\n",
    "    # Use first available article or 10th if available\n",
    "    sample_idx = min(10, len(ko_articles_total) - 1)\n",
    "    sample_ko = ko_articles_total[sample_idx]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Article #{sample_idx + 1} of {len(ko_articles_total):,}\")\n",
    "    print(f\"Title: {sample_ko['title']}\")\n",
    "    print(f\"URL: {sample_ko['url']}\")\n",
    "    print(f\"Language: {sample_ko['language']}\")\n",
    "    print(f\"Text length: {len(sample_ko['text'])} characters\")\n",
    "    print(\"\\nFirst 300 characters:\")\n",
    "    print(sample_ko['text'][:300])\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"No articles found. Check filtering criteria.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display English article sample\n",
    "if len(en_articles_total) > 0:\n",
    "    # Use first available article or 10th if available\n",
    "    sample_idx = min(10, len(en_articles_total) - 1)\n",
    "    sample_en = en_articles_total[sample_idx]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Article #{sample_idx + 1} of {len(en_articles_total):,}\")\n",
    "    print(f\"Title: {sample_en['title']}\")\n",
    "    print(f\"URL: {sample_en['url']}\")\n",
    "    print(f\"Language: {sample_en['language']}\")\n",
    "    print(f\"Text length: {len(sample_en['text'])} characters\")\n",
    "    print(\"\\nFirst 300 characters:\")\n",
    "    print(sample_en['text'][:300])\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"No articles found. Check filtering criteria.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Korean articles stats\n",
    "if len(ko_articles_total) > 0:\n",
    "    ko_lengths = [len(a['text']) for a in ko_articles_total]\n",
    "    print(\"Korean Wikipedia Articles:\")\n",
    "    print(f\"  Total: {len(ko_articles_total):,}\")\n",
    "    print(f\"  Mean length: {np.mean(ko_lengths):.0f} chars\")\n",
    "    print(f\"  Median length: {np.median(ko_lengths):.0f} chars\")\n",
    "    print(f\"  Min length: {np.min(ko_lengths):.0f} chars\")\n",
    "    print(f\"  Max length: {np.max(ko_lengths):.0f} chars\")\n",
    "else:\n",
    "    print(\"Korean Wikipedia Articles: No articles found\")\n",
    "\n",
    "print()\n",
    "\n",
    "# English articles stats\n",
    "if len(en_articles_total) > 0:\n",
    "    en_lengths = [len(a['text']) for a in en_articles_total]\n",
    "    print(\"English Wikipedia Articles:\")\n",
    "    print(f\"  Total: {len(en_articles_total):,}\")\n",
    "    print(f\"  Mean length: {np.mean(en_lengths):.0f} chars\")\n",
    "    print(f\"  Median length: {np.median(en_lengths):.0f} chars\")\n",
    "    print(f\"  Min length: {np.min(en_lengths):.0f} chars\")\n",
    "    print(f\"  Max length: {np.max(en_lengths):.0f} chars\")\n",
    "else:\n",
    "    print(\"English Wikipedia Articles: No articles found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify Saved Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"Saved chunk files:\")\n",
    "\n",
    "# Find all Korean chunk files\n",
    "ko_chunks = sorted(glob.glob(str(output_dir / \"ko_articles_chunk_*.jsonl\")))\n",
    "if ko_chunks:\n",
    "    print(f\"\\n  Korean: {len(ko_chunks)} chunk files\")\n",
    "    total_size = sum(os.path.getsize(f) for f in ko_chunks)\n",
    "    total_lines = sum(sum(1 for _ in open(f)) for f in ko_chunks)\n",
    "    print(f\"    Total size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"    Total articles: {total_lines:,}\")\n",
    "    print(f\"    Files:\")\n",
    "    for chunk in ko_chunks:\n",
    "        size = os.path.getsize(chunk) / 1024 / 1024\n",
    "        lines = sum(1 for _ in open(chunk))\n",
    "        print(f\"      - {os.path.basename(chunk):30s} ({size:>6.2f} MB, {lines:>6,} articles)\")\n",
    "else:\n",
    "    print(\"  Korean: No chunk files found\")\n",
    "\n",
    "# Find all English chunk files\n",
    "en_chunks = sorted(glob.glob(str(output_dir / \"en_articles_chunk_*.jsonl\")))\n",
    "if en_chunks:\n",
    "    print(f\"\\n  English: {len(en_chunks)} chunk files\")\n",
    "    total_size = sum(os.path.getsize(f) for f in en_chunks)\n",
    "    total_lines = sum(sum(1 for _ in open(f)) for f in en_chunks)\n",
    "    print(f\"    Total size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"    Total articles: {total_lines:,}\")\n",
    "    print(f\"    Files:\")\n",
    "    for chunk in en_chunks:\n",
    "        size = os.path.getsize(chunk) / 1024 / 1024\n",
    "        lines = sum(1 for _ in open(chunk))\n",
    "        print(f\"      - {os.path.basename(chunk):30s} ({size:>6.2f} MB, {lines:>6,} articles)\")\n",
    "else:\n",
    "    print(\"  English: No chunk files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've successfully extracted and cleaned **ALL** Korean and English Wikipedia articles.\n",
    "\n",
    "**Key Features:**\n",
    "- âœ… Processes complete Wikipedia dumps (no article limit)\n",
    "- âœ… Saves data in manageable chunks (50,000 articles per file)\n",
    "- âœ… Filters out redirects, special pages, and low-quality articles\n",
    "- âœ… Cleans MediaWiki markup to plain text\n",
    "- âœ… Ready for synonym extraction and model training\n",
    "\n",
    "**Output Structure:**\n",
    "```\n",
    "dataset/wikipedia/\n",
    "â”œâ”€â”€ ko_articles_chunk_001.jsonl  (50,000 articles)\n",
    "â”œâ”€â”€ ko_articles_chunk_002.jsonl  (50,000 articles)\n",
    "â”œâ”€â”€ ...\n",
    "â”œâ”€â”€ en_articles_chunk_001.jsonl  (50,000 articles)\n",
    "â”œâ”€â”€ en_articles_chunk_002.jsonl  (50,000 articles)\n",
    "â””â”€â”€ ...\n",
    "```\n",
    "\n",
    "**Next steps:**\n",
    "- Extract inter-language links from chunks\n",
    "- Extract synonym pairs from article text\n",
    "- Build comprehensive bilingual dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
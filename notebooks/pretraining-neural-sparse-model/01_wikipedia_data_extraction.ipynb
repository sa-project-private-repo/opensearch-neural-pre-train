{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Data Extraction\n",
    "\n",
    "This notebook extracts Korean and English Wikipedia articles for building a bilingual synonym dataset.\n",
    "\n",
    "**Updated**: Now using direct Wikipedia XML dumps from Wikimedia for the latest data (November 2025).\n",
    "\n",
    "## Steps\n",
    "1. Load Wikipedia data from Wikimedia dumps\n",
    "2. Parse XML and extract article text\n",
    "3. Clean and filter articles  \n",
    "4. Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.data.wikipedia_xml_parser import WikipediaXMLParser\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output paths\n",
    "ko_output = \"../../dataset/wikipedia/ko_articles.jsonl\"\n",
    "en_output = \"../../dataset/wikipedia/en_articles.jsonl\"\n",
    "\n",
    "# Create directories\n",
    "Path(ko_output).parent.mkdir(parents=True, exist_ok=True)\n",
    "Path(en_output).parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Korean Wikipedia Articles\n",
    "\n",
    "We'll start with a sample of 5,000 articles for testing.\n",
    "\n",
    "**Note**: First run will download the Wikipedia dump (~GB size). Subsequent runs will use cached file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize Korean parser (using latest dump)\nko_parser = WikipediaXMLParser(\n    language=\"ko\",\n    date=\"latest\",  # Will automatically use the most recent dump\n    cache_dir=\"../../dataset/wikipedia/cache\"\n)\n\n# Process Korean Wikipedia\nko_articles = ko_parser.process_wikipedia(\n    output_path=ko_output,\n    max_articles=5000,  # Sample size\n    min_length=200,     # Minimum 200 characters\n    max_length=100000,  # Maximum 100K characters (increased from 10K)\n)\n\nprint(f\"\\nProcessed {len(ko_articles)} Korean articles\")\nif ko_articles:\n    print(f\"Sample article: {ko_articles[0]['title']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract English Wikipedia Articles\n",
    "\n",
    "Same process for English articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize English parser (using latest dump)\nen_parser = WikipediaXMLParser(\n    language=\"en\",\n    date=\"latest\",  # Will automatically use the most recent dump\n    cache_dir=\"../../dataset/wikipedia/cache\"\n)\n\n# Process English Wikipedia\nen_articles = en_parser.process_wikipedia(\n    output_path=en_output,\n    max_articles=5000,  # Sample size\n    min_length=200,\n    max_length=100000,  # Maximum 100K characters (increased from 10K)\n)\n\nprint(f\"\\nProcessed {len(en_articles)} English articles\")\nif en_articles:\n    print(f\"Sample article: {en_articles[0]['title']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect Sample Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display Korean article sample\nif len(ko_articles) > 0:\n    # Use first available article or 10th if available\n    sample_idx = min(10, len(ko_articles) - 1)\n    sample_ko = ko_articles[sample_idx]\n    \n    print(\"=\" * 80)\n    print(f\"Article #{sample_idx + 1} of {len(ko_articles)}\")\n    print(f\"Title: {sample_ko['title']}\")\n    print(f\"URL: {sample_ko['url']}\")\n    print(f\"Language: {sample_ko['language']}\")\n    print(f\"Text length: {len(sample_ko['text'])} characters\")\n    print(\"\\nFirst 300 characters:\")\n    print(sample_ko['text'][:300])\n    print(\"=\" * 80)\nelse:\n    print(\"No articles found. Check filtering criteria.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display English article sample\nif len(en_articles) > 0:\n    # Use first available article or 10th if available\n    sample_idx = min(10, len(en_articles) - 1)\n    sample_en = en_articles[sample_idx]\n    \n    print(\"=\" * 80)\n    print(f\"Article #{sample_idx + 1} of {len(en_articles)}\")\n    print(f\"Title: {sample_en['title']}\")\n    print(f\"URL: {sample_en['url']}\")\n    print(f\"Language: {sample_en['language']}\")\n    print(f\"Text length: {len(sample_en['text'])} characters\")\n    print(\"\\nFirst 300 characters:\")\n    print(sample_en['text'][:300])\n    print(\"=\" * 80)\nelse:\n    print(\"No articles found. Check filtering criteria.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\n\n# Korean articles stats\nif len(ko_articles) > 0:\n    ko_lengths = [len(a['text']) for a in ko_articles]\n    print(\"Korean Wikipedia Articles:\")\n    print(f\"  Total: {len(ko_articles)}\")\n    print(f\"  Mean length: {np.mean(ko_lengths):.0f} chars\")\n    print(f\"  Median length: {np.median(ko_lengths):.0f} chars\")\n    print(f\"  Min length: {np.min(ko_lengths):.0f} chars\")\n    print(f\"  Max length: {np.max(ko_lengths):.0f} chars\")\nelse:\n    print(\"Korean Wikipedia Articles: No articles found\")\n\nprint()\n\n# English articles stats\nif len(en_articles) > 0:\n    en_lengths = [len(a['text']) for a in en_articles]\n    print(\"English Wikipedia Articles:\")\n    print(f\"  Total: {len(en_articles)}\")\n    print(f\"  Mean length: {np.mean(en_lengths):.0f} chars\")\n    print(f\"  Median length: {np.median(en_lengths):.0f} chars\")\n    print(f\"  Min length: {np.min(en_lengths):.0f} chars\")\n    print(f\"  Max length: {np.max(en_lengths):.0f} chars\")\nelse:\n    print(\"English Wikipedia Articles: No articles found\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify Saved Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nprint(\"Saved files:\")\n\nif os.path.exists(ko_output):\n    print(f\"  Korean: {ko_output}\")\n    print(f\"    Size: {os.path.getsize(ko_output) / 1024 / 1024:.2f} MB\")\n    print(f\"    Lines: {sum(1 for _ in open(ko_output))}\")\nelse:\n    print(f\"  Korean: {ko_output} (not found)\")\n\nif os.path.exists(en_output):\n    print(f\"\\n  English: {en_output}\")\n    print(f\"    Size: {os.path.getsize(en_output) / 1024 / 1024:.2f} MB\")\n    print(f\"    Lines: {sum(1 for _ in open(en_output))}\")\nelse:\n    print(f\"\\n  English: {en_output} (not found)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've successfully extracted and cleaned Korean and English Wikipedia articles. The data is now ready for synonym extraction in the next notebook.\n",
    "\n",
    "**Next steps:**\n",
    "- Extract inter-language links\n",
    "- Extract synonym pairs from article text\n",
    "- Build comprehensive bilingual dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
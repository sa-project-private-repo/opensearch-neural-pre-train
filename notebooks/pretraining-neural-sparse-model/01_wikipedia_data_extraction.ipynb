{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Data Extraction\n",
    "\n",
    "This notebook extracts Korean and English Wikipedia articles for building a bilingual synonym dataset.\n",
    "\n",
    "**Updated**: Now using direct Wikipedia XML dumps from Wikimedia for the latest data (November 2025).\n",
    "\n",
    "## Steps\n",
    "1. Load Wikipedia data from Wikimedia dumps\n",
    "2. Parse XML and extract article text\n",
    "3. Clean and filter articles  \n",
    "4. Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.data.wikipedia_xml_parser import WikipediaXMLParser\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Output directory\noutput_dir = Path(\"../../dataset/wikipedia\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# We'll split files into chunks to avoid very large files\nARTICLES_PER_FILE = 50000  # 50K articles per file"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Extract Korean Wikipedia Articles\n\n**Processing all Korean Wikipedia articles** (no limit)\n\nFiles will be saved in chunks of 50,000 articles each to avoid very large files.\n\n**Note**: First run will download the Wikipedia dump (~GB size). Subsequent runs will use cached file."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize Korean parser (using latest dump)\nko_parser = WikipediaXMLParser(\n    language=\"ko\",\n    date=\"latest\",  # Will automatically use the most recent dump\n    cache_dir=\"../../dataset/wikipedia/cache\"\n)\n\n# Download the dump first\ndump_path = ko_parser.download_dump()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Processing ALL Korean Wikipedia articles\")\nprint(\"Files will be split into chunks of 50,000 articles\")\nprint(\"=\"*80 + \"\\n\")\n\n# Process articles in streaming mode and save in chunks\nfrom tqdm import tqdm\n\nko_articles_total = []\nchunk_num = 0\ncurrent_chunk = []\n\niterator = ko_parser.iter_articles(dump_path)\npbar = tqdm(iterator, desc=\"Processing Korean Wikipedia\")\n\nfor raw_article in pbar:\n    # Parse wikitext to plain text\n    text = ko_parser.parse_wikitext(raw_article[\"wikitext\"])\n    \n    article = {\n        \"id\": raw_article[\"id\"],\n        \"url\": raw_article[\"url\"],\n        \"title\": raw_article[\"title\"],\n        \"text\": text,\n        \"language\": \"ko\",\n    }\n    \n    # Apply filters\n    if ko_parser.filter_article(article, min_length=200, max_length=100000):\n        current_chunk.append(article)\n        ko_articles_total.append(article)\n        \n        # Save chunk when it reaches the limit\n        if len(current_chunk) >= ARTICLES_PER_FILE:\n            chunk_num += 1\n            output_file = output_dir / f\"ko_articles_chunk_{chunk_num:03d}.jsonl\"\n            ko_parser.save_articles(current_chunk, output_file)\n            pbar.set_postfix({\n                'chunks': chunk_num, \n                'articles': len(ko_articles_total),\n                'current_chunk': len(current_chunk)\n            })\n            current_chunk = []\n\n# Save remaining articles in last chunk\nif current_chunk:\n    chunk_num += 1\n    output_file = output_dir / f\"ko_articles_chunk_{chunk_num:03d}.jsonl\"\n    ko_parser.save_articles(current_chunk, output_file)\n\nprint(f\"\\n✓ Processed {len(ko_articles_total):,} Korean articles\")\nprint(f\"✓ Saved in {chunk_num} chunk files\")\nif ko_articles_total:\n    print(f\"✓ Sample article: {ko_articles_total[0]['title']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Extract English Wikipedia Articles\n\n**Processing all English Wikipedia articles** (no limit)\n\nFiles will be saved in chunks of 50,000 articles each."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize English parser (using latest dump)\nen_parser = WikipediaXMLParser(\n    language=\"en\",\n    date=\"latest\",  # Will automatically use the most recent dump\n    cache_dir=\"../../dataset/wikipedia/cache\"\n)\n\n# Download the dump first\ndump_path = en_parser.download_dump()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Processing ALL English Wikipedia articles\")\nprint(\"Files will be split into chunks of 50,000 articles\")\nprint(\"=\"*80 + \"\\n\")\n\n# Process articles in streaming mode and save in chunks\nen_articles_total = []\nchunk_num = 0\ncurrent_chunk = []\n\niterator = en_parser.iter_articles(dump_path)\npbar = tqdm(iterator, desc=\"Processing English Wikipedia\")\n\nfor raw_article in pbar:\n    # Parse wikitext to plain text\n    text = en_parser.parse_wikitext(raw_article[\"wikitext\"])\n    \n    article = {\n        \"id\": raw_article[\"id\"],\n        \"url\": raw_article[\"url\"],\n        \"title\": raw_article[\"title\"],\n        \"text\": text,\n        \"language\": \"en\",\n    }\n    \n    # Apply filters\n    if en_parser.filter_article(article, min_length=200, max_length=100000):\n        current_chunk.append(article)\n        en_articles_total.append(article)\n        \n        # Save chunk when it reaches the limit\n        if len(current_chunk) >= ARTICLES_PER_FILE:\n            chunk_num += 1\n            output_file = output_dir / f\"en_articles_chunk_{chunk_num:03d}.jsonl\"\n            en_parser.save_articles(current_chunk, output_file)\n            pbar.set_postfix({\n                'chunks': chunk_num, \n                'articles': len(en_articles_total),\n                'current_chunk': len(current_chunk)\n            })\n            current_chunk = []\n\n# Save remaining articles in last chunk\nif current_chunk:\n    chunk_num += 1\n    output_file = output_dir / f\"en_articles_chunk_{chunk_num:03d}.jsonl\"\n    en_parser.save_articles(current_chunk, output_file)\n\nprint(f\"\\n✓ Processed {len(en_articles_total):,} English articles\")\nprint(f\"✓ Saved in {chunk_num} chunk files\")\nif en_articles_total:\n    print(f\"✓ Sample article: {en_articles_total[0]['title']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect Sample Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display Korean article sample\nif len(ko_articles_total) > 0:\n    # Use first available article or 10th if available\n    sample_idx = min(10, len(ko_articles_total) - 1)\n    sample_ko = ko_articles_total[sample_idx]\n    \n    print(\"=\" * 80)\n    print(f\"Article #{sample_idx + 1} of {len(ko_articles_total):,}\")\n    print(f\"Title: {sample_ko['title']}\")\n    print(f\"URL: {sample_ko['url']}\")\n    print(f\"Language: {sample_ko['language']}\")\n    print(f\"Text length: {len(sample_ko['text'])} characters\")\n    print(\"\\nFirst 300 characters:\")\n    print(sample_ko['text'][:300])\n    print(\"=\" * 80)\nelse:\n    print(\"No articles found. Check filtering criteria.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display English article sample\nif len(en_articles_total) > 0:\n    # Use first available article or 10th if available\n    sample_idx = min(10, len(en_articles_total) - 1)\n    sample_en = en_articles_total[sample_idx]\n    \n    print(\"=\" * 80)\n    print(f\"Article #{sample_idx + 1} of {len(en_articles_total):,}\")\n    print(f\"Title: {sample_en['title']}\")\n    print(f\"URL: {sample_en['url']}\")\n    print(f\"Language: {sample_en['language']}\")\n    print(f\"Text length: {len(sample_en['text'])} characters\")\n    print(\"\\nFirst 300 characters:\")\n    print(sample_en['text'][:300])\n    print(\"=\" * 80)\nelse:\n    print(\"No articles found. Check filtering criteria.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\n\n# Korean articles stats\nif len(ko_articles_total) > 0:\n    ko_lengths = [len(a['text']) for a in ko_articles_total]\n    print(\"Korean Wikipedia Articles:\")\n    print(f\"  Total: {len(ko_articles_total):,}\")\n    print(f\"  Mean length: {np.mean(ko_lengths):.0f} chars\")\n    print(f\"  Median length: {np.median(ko_lengths):.0f} chars\")\n    print(f\"  Min length: {np.min(ko_lengths):.0f} chars\")\n    print(f\"  Max length: {np.max(ko_lengths):.0f} chars\")\nelse:\n    print(\"Korean Wikipedia Articles: No articles found\")\n\nprint()\n\n# English articles stats\nif len(en_articles_total) > 0:\n    en_lengths = [len(a['text']) for a in en_articles_total]\n    print(\"English Wikipedia Articles:\")\n    print(f\"  Total: {len(en_articles_total):,}\")\n    print(f\"  Mean length: {np.mean(en_lengths):.0f} chars\")\n    print(f\"  Median length: {np.median(en_lengths):.0f} chars\")\n    print(f\"  Min length: {np.min(en_lengths):.0f} chars\")\n    print(f\"  Max length: {np.max(en_lengths):.0f} chars\")\nelse:\n    print(\"English Wikipedia Articles: No articles found\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify Saved Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport glob\n\nprint(\"Saved chunk files:\")\n\n# Find all Korean chunk files\nko_chunks = sorted(glob.glob(str(output_dir / \"ko_articles_chunk_*.jsonl\")))\nif ko_chunks:\n    print(f\"\\n  Korean: {len(ko_chunks)} chunk files\")\n    total_size = sum(os.path.getsize(f) for f in ko_chunks)\n    total_lines = sum(sum(1 for _ in open(f)) for f in ko_chunks)\n    print(f\"    Total size: {total_size / 1024 / 1024:.2f} MB\")\n    print(f\"    Total articles: {total_lines:,}\")\n    print(f\"    Files:\")\n    for chunk in ko_chunks:\n        size = os.path.getsize(chunk) / 1024 / 1024\n        lines = sum(1 for _ in open(chunk))\n        print(f\"      - {os.path.basename(chunk):30s} ({size:>6.2f} MB, {lines:>6,} articles)\")\nelse:\n    print(\"  Korean: No chunk files found\")\n\n# Find all English chunk files\nen_chunks = sorted(glob.glob(str(output_dir / \"en_articles_chunk_*.jsonl\")))\nif en_chunks:\n    print(f\"\\n  English: {len(en_chunks)} chunk files\")\n    total_size = sum(os.path.getsize(f) for f in en_chunks)\n    total_lines = sum(sum(1 for _ in open(f)) for f in en_chunks)\n    print(f\"    Total size: {total_size / 1024 / 1024:.2f} MB\")\n    print(f\"    Total articles: {total_lines:,}\")\n    print(f\"    Files:\")\n    for chunk in en_chunks:\n        size = os.path.getsize(chunk) / 1024 / 1024\n        lines = sum(1 for _ in open(chunk))\n        print(f\"      - {os.path.basename(chunk):30s} ({size:>6.2f} MB, {lines:>6,} articles)\")\nelse:\n    print(\"  English: No chunk files found\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nWe've successfully extracted and cleaned **ALL** Korean and English Wikipedia articles.\n\n**Key Features:**\n- ✅ Processes complete Wikipedia dumps (no article limit)\n- ✅ Saves data in manageable chunks (50,000 articles per file)\n- ✅ Filters out redirects, special pages, and low-quality articles\n- ✅ Cleans MediaWiki markup to plain text\n- ✅ Ready for synonym extraction and model training\n\n**Output Structure:**\n```\ndataset/wikipedia/\n├── ko_articles_chunk_001.jsonl  (50,000 articles)\n├── ko_articles_chunk_002.jsonl  (50,000 articles)\n├── ...\n├── en_articles_chunk_001.jsonl  (50,000 articles)\n├── en_articles_chunk_002.jsonl  (50,000 articles)\n└── ...\n```\n\n**Next steps:**\n- Extract inter-language links from chunks\n- Extract synonym pairs from article text\n- Build comprehensive bilingual dictionary"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
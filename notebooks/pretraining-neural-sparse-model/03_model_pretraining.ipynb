{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Sparse Model Pre-training\n",
    "\n",
    "This notebook trains a Neural Sparse encoder with:\n",
    "- Query-document pairs (27K pairs)\n",
    "- Korean-English synonym alignment (74 pairs)\n",
    "- Cross-lingual retrieval optimization\n",
    "\n",
    "## Training Objectives\n",
    "1. Ranking: Query-document matching\n",
    "2. Cross-lingual: Korean-English synonym alignment\n",
    "3. Sparsity: FLOPS regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('../..')\n\nimport torch\nfrom torch.utils.data import DataLoader\nimport yaml\nfrom pathlib import Path\n\nfrom src.models.neural_sparse_encoder import NeuralSparseEncoder\nfrom src.training.losses import CombinedLoss\nfrom src.training.data_collator import NeuralSparseDataCollator\nfrom src.data.training_data_builder import TrainingDataBuilder\nfrom src.training.trainer import NeuralSparseTrainer"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load config\nwith open('../../config/training_config.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\nprint(\"Configuration loaded:\")\nprint(f\"  Model: {config['model']['name']}\")\nprint(f\"  Batch size: {config['training']['batch_size']}\")\nprint(f\"  Learning rate: {config['training']['learning_rate']}\")\nprint(f\"  Epochs: {config['training']['num_epochs']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data builder\n",
    "builder = TrainingDataBuilder()\n",
    "\n",
    "# Build datasets\n",
    "train_dataset, val_dataset = builder.build_training_dataset(\n",
    "    qd_pairs_path=config['data']['qd_pairs_path'],\n",
    "    documents_path=config['data']['documents_path'],\n",
    "    synonyms_path=config['data']['synonyms_path'],\n",
    "    num_negatives=config['data']['num_negatives'],\n",
    "    train_split=config['data']['train_split'],\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset summary:\")\n",
    "print(f\"  Train: {len(train_dataset)} pairs\")\n",
    "print(f\"  Val: {len(val_dataset)} pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = NeuralSparseEncoder(\n",
    "    model_name=config['model']['name'],\n",
    "    max_length=config['model']['max_length'],\n",
    "    use_relu=config['model']['use_relu'],\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel parameters:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collator\n",
    "collator = NeuralSparseDataCollator(\n",
    "    tokenizer=model.tokenizer,\n",
    "    query_max_length=config['data']['query_max_length'],\n",
    "    doc_max_length=config['data']['doc_max_length'],\n",
    "    num_negatives=config['data']['num_negatives'],\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    "    num_workers=config['hardware']['num_workers'],\n",
    "    pin_memory=config['hardware']['pin_memory'],\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['evaluation']['eval_batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collator,\n",
    "    num_workers=config['hardware']['num_workers'],\n",
    "    pin_memory=config['hardware']['pin_memory'],\n",
    ")\n",
    "\n",
    "print(f\"Data loaders created:\")\n",
    "print(f\"  Train batches: {len(train_dataloader)}\")\n",
    "print(f\"  Val batches: {len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_fn = CombinedLoss(\n",
    "    alpha_ranking=config['loss']['alpha_ranking'],\n",
    "    beta_cross_lingual=config['loss']['beta_cross_lingual'],\n",
    "    gamma_sparsity=config['loss']['gamma_sparsity'],\n",
    "    ranking_margin=config['loss']['ranking_margin'],\n",
    "    use_contrastive=config['loss']['use_contrastive'],\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['training']['learning_rate'],\n",
    "    weight_decay=config['training']['weight_decay'],\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR\n",
    "\n",
    "total_steps = len(train_dataloader) * config['training']['num_epochs']\n",
    "warmup_steps = config['training']['warmup_steps']\n",
    "\n",
    "warmup_scheduler = LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.1,\n",
    "    end_factor=1.0,\n",
    "    total_iters=warmup_steps,\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining setup:\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {warmup_steps}\")\n",
    "print(f\"  Loss weights: α={config['loss']['alpha_ranking']}, β={config['loss']['beta_cross_lingual']}, γ={config['loss']['gamma_sparsity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = NeuralSparseTrainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=warmup_scheduler,\n",
    "    device=device,\n",
    "    use_amp=config['training']['use_amp'],\n",
    "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
    "    max_grad_norm=config['training']['max_grad_norm'],\n",
    "    output_dir=config['logging']['output_dir'],\n",
    "    save_steps=config['logging']['save_steps'],\n",
    "    eval_steps=config['logging']['eval_steps'],\n",
    "    logging_steps=config['logging']['logging_steps'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample batch\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "\n",
    "# Move to device\n",
    "sample_batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                for k, v in sample_batch.items()}\n",
    "\n",
    "# Test forward pass\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    query_outputs = model(\n",
    "        input_ids=sample_batch['query_input_ids'],\n",
    "        attention_mask=sample_batch['query_attention_mask'],\n",
    "    )\n",
    "    query_rep = query_outputs['sparse_rep']\n",
    "    \n",
    "    # Get sparsity stats\n",
    "    stats = model.get_sparsity_stats(query_rep)\n",
    "    \n",
    "    print(\"\\nSample sparse representation:\")\n",
    "    print(f\"  Shape: {query_rep.shape}\")\n",
    "    print(f\"  Avg non-zero terms: {stats['avg_nonzero_terms']:.0f}\")\n",
    "    print(f\"  Sparsity ratio: {stats['sparsity_ratio']:.3f}\")\n",
    "    print(f\"  Avg L1 norm: {stats['avg_l1_norm']:.2f}\")\n",
    "    \n",
    "    # Show top activated terms\n",
    "    print(\"\\n  Top 10 activated terms:\")\n",
    "    top_terms = model.get_top_k_terms(query_rep[0], k=10)\n",
    "    for i, (term, weight) in enumerate(top_terms, 1):\n",
    "        print(f\"    {i:2d}. {term:20s}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Start Training (Small Scale Test)\n",
    "\n",
    "First, let's do a quick test with 1 epoch to verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test: 1 epoch\n",
    "print(\"Starting quick test training (1 epoch)...\\n\")\n",
    "trainer.train(num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyze Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "model.eval()\n",
    "val_losses = trainer.evaluate()\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "print(f\"  Total loss: {val_losses['total_loss']:.4f}\")\n",
    "print(f\"  Ranking loss: {val_losses['ranking_loss']:.4f}\")\n",
    "print(f\"  Cross-lingual loss: {val_losses['cross_lingual_loss']:.4f}\")\n",
    "print(f\"  Sparsity loss: {val_losses['sparsity_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Sparse Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"인공지능 모델 학습\",\n",
    "    \"machine learning training\",\n",
    "    \"검색 시스템 개발\",\n",
    "    \"search system development\",\n",
    "]\n",
    "\n",
    "print(\"\\nTesting sparse representations:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for query in test_queries:\n",
    "        # Encode\n",
    "        sparse_rep = model.encode([query], device=device)\n",
    "        \n",
    "        # Stats\n",
    "        stats = model.get_sparsity_stats(sparse_rep)\n",
    "        \n",
    "        # Top terms\n",
    "        top_terms = model.get_top_k_terms(sparse_rep[0], k=10)\n",
    "        \n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(f\"  Non-zero terms: {stats['avg_nonzero_terms']:.0f}\")\n",
    "        print(f\"  Sparsity: {stats['sparsity_ratio']:.3f}\")\n",
    "        print(f\"  Top 5 terms:\")\n",
    "        for term, weight in top_terms[:5]:\n",
    "            print(f\"    {term:20s}: {weight:.4f}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Cross-lingual Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch.nn.functional as F\nimport json\n\n# Load synonyms\nwith open('../../dataset/synonyms/combined_synonyms.json', 'r', encoding='utf-8') as f:\n    synonyms = json.load(f)\n\n# Test top 10 synonyms\ntest_synonyms = synonyms[:10]\n\nprint(\"\\nCross-lingual Alignment Test:\\n\")\nprint(\"=\" * 80)\n\nmodel.eval()\nwith torch.no_grad():\n    for syn in test_synonyms:\n        korean = syn['korean']\n        english = syn['english']\n        \n        # Encode both\n        korean_rep = model.encode([korean], device=device)\n        english_rep = model.encode([english], device=device)\n        \n        # Compute similarity\n        cosine_sim = F.cosine_similarity(korean_rep, english_rep, dim=-1)\n        dot_sim = torch.sum(korean_rep * english_rep, dim=-1)\n        \n        print(f\"{korean:15s} ↔ {english:20s}\")\n        print(f\"  Cosine similarity: {cosine_sim.item():.4f}\")\n        print(f\"  Dot product: {dot_sim.item():.2f}\")\n\n# Average similarity\nall_cosine_sims = []\nwith torch.no_grad():\n    for syn in test_synonyms:\n        korean_rep = model.encode([syn['korean']], device=device)\n        english_rep = model.encode([syn['english']], device=device)\n        cosine_sim = F.cosine_similarity(korean_rep, english_rep, dim=-1)\n        all_cosine_sims.append(cosine_sim.item())\n\nprint(\"\\n\" + \"=\" * 80)\nprint(f\"Average cosine similarity: {sum(all_cosine_sims) / len(all_cosine_sims):.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Full Training (Optional)\n",
    "\n",
    "Uncomment below to run full training with all epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Full training\n",
    "# print(\"Starting full training...\\n\")\n",
    "# trainer.train(num_epochs=config['training']['num_epochs'])\n",
    "# \n",
    "# print(\"\\nTraining completed!\")\n",
    "# print(f\"Best validation loss: {trainer.best_val_loss:.4f}\")\n",
    "# print(f\"Best model saved to: {config['logging']['output_dir']}/best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've successfully:\n",
    "1. ✅ Loaded 27K query-document pairs\n",
    "2. ✅ Loaded 74 Korean-English synonym pairs\n",
    "3. ✅ Initialized Neural Sparse Encoder (klue/bert-base)\n",
    "4. ✅ Set up training pipeline with mixed precision\n",
    "5. ✅ Ran initial training experiment\n",
    "6. ✅ Tested sparse representations\n",
    "7. ✅ Verified cross-lingual alignment\n",
    "\n",
    "**Next steps:**\n",
    "- Run full training (10 epochs)\n",
    "- Evaluate on test set\n",
    "- Export model for OpenSearch\n",
    "- Build evaluation framework"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
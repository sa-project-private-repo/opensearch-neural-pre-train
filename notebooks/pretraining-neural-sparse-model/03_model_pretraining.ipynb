{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Sparse Model Pre-training\n",
    "\n",
    "This notebook trains a Neural Sparse encoder with:\n",
    "- Query-document pairs (27K pairs)\n",
    "- Korean-English synonym alignment (74 pairs)\n",
    "- Cross-lingual retrieval optimization\n",
    "\n",
    "## Training Objectives\n",
    "1. Ranking: Query-document matching\n",
    "2. Cross-lingual: Korean-English synonym alignment\n",
    "3. Sparsity: FLOPS regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "from src.models.neural_sparse_encoder import NeuralSparseEncoder\n",
    "from src.training.losses import CombinedLoss\n",
    "from src.training.data_collator import NeuralSparseDataCollator\n",
    "from src.data.training_data_builder import TrainingDataBuilder\n",
    "from src.training.trainer import NeuralSparseTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Model: klue/bert-base\n",
      "  Batch size: 16\n",
      "  Learning rate: 2e-05\n",
      "  Epochs: 10\n"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "with open('../../config/training_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Model: {config['model']['name']}\")\n",
    "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Epochs: {config['training']['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Training Dataset\n",
    "\n",
    "First, we need to create QD pairs from Wikipedia articles if they don't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths:\n",
      "  QD pairs: ../../dataset/base_model/qd_pairs_base.pkl\n",
      "  Documents: ../../dataset/base_model/documents.json\n",
      "  Synonyms: ../../dataset/synonyms/combined_synonyms.json\n",
      "  Synonyms exists: True\n",
      "\n",
      "QD pairs already exist.\n",
      "Loaded 27939 QD pairs from ../../dataset/base_model/qd_pairs_base.pkl\n",
      "Converted 27939 QD pairs to standard format\n",
      "Loaded 9996 documents from ../../dataset/base_model/documents.json\n",
      "Loaded 31116 synonym pairs from ../../dataset/synonyms/combined_synonyms.json\n",
      "\n",
      "Dataset split:\n",
      "  Train: 25145 pairs\n",
      "  Val: 2794 pairs\n",
      "Initialized NeuralSparseDataset:\n",
      "  QD pairs: 25145\n",
      "  Documents: 9996\n",
      "  Synonyms: 31116\n",
      "  Num negatives: 10\n",
      "Initialized NeuralSparseDataset:\n",
      "  QD pairs: 2794\n",
      "  Documents: 9996\n",
      "  Synonyms: 31116\n",
      "  Num negatives: 10\n",
      "\n",
      "Dataset summary:\n",
      "  Train: 25145 pairs\n",
      "  Val: 2794 pairs\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if QD pairs exist\n",
    "qd_pairs_path = Path(config['data']['qd_pairs_path'])\n",
    "documents_path = Path(config['data']['documents_path'])\n",
    "synonyms_path = Path(config['data']['synonyms_path'])\n",
    "\n",
    "# Fix paths to be relative to notebook\n",
    "if not qd_pairs_path.is_absolute():\n",
    "    qd_pairs_path = Path('../../') / qd_pairs_path\n",
    "if not documents_path.is_absolute():\n",
    "    documents_path = Path('../../') / documents_path\n",
    "if not synonyms_path.is_absolute():\n",
    "    synonyms_path = Path('../../') / synonyms_path\n",
    "\n",
    "print(f\"Paths:\")\n",
    "print(f\"  QD pairs: {qd_pairs_path}\")\n",
    "print(f\"  Documents: {documents_path}\")\n",
    "print(f\"  Synonyms: {synonyms_path}\")\n",
    "print(f\"  Synonyms exists: {synonyms_path.exists()}\")\n",
    "\n",
    "if not qd_pairs_path.exists():\n",
    "    print(\"\\nQD pairs not found. Creating from Wikipedia articles...\")\n",
    "    \n",
    "    # Load Wikipedia articles\n",
    "    ko_articles_path = \"../../dataset/wikipedia/ko_articles.jsonl\"\n",
    "    en_articles_path = \"../../dataset/wikipedia/en_articles.jsonl\"\n",
    "    \n",
    "    # Create dataset directory\n",
    "    qd_pairs_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load articles\n",
    "    ko_articles = []\n",
    "    if Path(ko_articles_path).exists():\n",
    "        with open(ko_articles_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                ko_articles.append(json.loads(line))\n",
    "    \n",
    "    en_articles = []\n",
    "    if Path(en_articles_path).exists():\n",
    "        with open(en_articles_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                en_articles.append(json.loads(line))\n",
    "    \n",
    "    print(f\"  Loaded {len(ko_articles)} Korean articles\")\n",
    "    print(f\"  Loaded {len(en_articles)} English articles\")\n",
    "    \n",
    "    # Create QD pairs from articles\n",
    "    # Each article title becomes a query, and the text becomes the document\n",
    "    qd_pairs = []\n",
    "    documents = []\n",
    "    \n",
    "    # Korean articles\n",
    "    for article in ko_articles:\n",
    "        doc_id = f\"ko_{article['id']}\"\n",
    "        query = article['title']\n",
    "        text = article['text']\n",
    "        \n",
    "        # Skip very short articles\n",
    "        if len(text) < 200:\n",
    "            continue\n",
    "        \n",
    "        qd_pairs.append({\n",
    "            'query': query,\n",
    "            'doc_id': doc_id,\n",
    "            'label': 1.0\n",
    "        })\n",
    "        \n",
    "        documents.append({\n",
    "            'id': doc_id,\n",
    "            'text': text,\n",
    "            'title': query,\n",
    "            'language': 'ko'\n",
    "        })\n",
    "    \n",
    "    # English articles\n",
    "    for article in en_articles:\n",
    "        doc_id = f\"en_{article['id']}\"\n",
    "        query = article['title']\n",
    "        text = article['text']\n",
    "        \n",
    "        # Skip very short articles\n",
    "        if len(text) < 200:\n",
    "            continue\n",
    "        \n",
    "        qd_pairs.append({\n",
    "            'query': query,\n",
    "            'doc_id': doc_id,\n",
    "            'label': 1.0\n",
    "        })\n",
    "        \n",
    "        documents.append({\n",
    "            'id': doc_id,\n",
    "            'text': text,\n",
    "            'title': query,\n",
    "            'language': 'en'\n",
    "        })\n",
    "    \n",
    "    print(f\"\\n  Created {len(qd_pairs)} QD pairs\")\n",
    "    print(f\"  Created {len(documents)} documents\")\n",
    "    \n",
    "    # Save QD pairs\n",
    "    with open(qd_pairs_path, 'wb') as f:\n",
    "        pickle.dump(qd_pairs, f)\n",
    "    print(f\"  Saved QD pairs to {qd_pairs_path}\")\n",
    "    \n",
    "    # Save documents\n",
    "    with open(documents_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(documents, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"  Saved documents to {documents_path}\")\n",
    "else:\n",
    "    print(\"\\nQD pairs already exist.\")\n",
    "\n",
    "# Now build training dataset\n",
    "builder = TrainingDataBuilder()\n",
    "\n",
    "train_dataset, val_dataset = builder.build_training_dataset(\n",
    "    qd_pairs_path=str(qd_pairs_path),\n",
    "    documents_path=str(documents_path),\n",
    "    synonyms_path=str(synonyms_path) if synonyms_path.exists() else None,\n",
    "    num_negatives=config['data']['num_negatives'],\n",
    "    train_split=config['data']['train_split'],\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset summary:\")\n",
    "print(f\"  Train: {len(train_dataset)} pairs\")\n",
    "print(f\"  Val: {len(val_dataset)} pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized NeuralSparseEncoder:\n",
      "  Base model: klue/bert-base\n",
      "  Hidden size: 768\n",
      "  Vocab size: 32000\n",
      "  Max length: 256\n",
      "  Activation: ReLU\n",
      "\n",
      "Model parameters:\n",
      "  Total: 135,225,344\n",
      "  Trainable: 135,225,344\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = NeuralSparseEncoder(\n",
    "    model_name=config['model']['name'],\n",
    "    max_length=config['model']['max_length'],\n",
    "    use_relu=config['model']['use_relu'],\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel parameters:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders created:\n",
      "  Train batches: 1572\n",
      "  Val batches: 88\n"
     ]
    }
   ],
   "source": [
    "# Initialize collator\n",
    "collator = NeuralSparseDataCollator(\n",
    "    tokenizer=model.tokenizer,\n",
    "    query_max_length=config['data']['query_max_length'],\n",
    "    doc_max_length=config['data']['doc_max_length'],\n",
    "    num_negatives=config['data']['num_negatives'],\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    "    num_workers=config['hardware']['num_workers'],\n",
    "    pin_memory=config['hardware']['pin_memory'],\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['evaluation']['eval_batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collator,\n",
    "    num_workers=config['hardware']['num_workers'],\n",
    "    pin_memory=config['hardware']['pin_memory'],\n",
    ")\n",
    "\n",
    "print(f\"Data loaders created:\")\n",
    "print(f\"  Train batches: {len(train_dataloader)}\")\n",
    "print(f\"  Val batches: {len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training setup:\n",
      "  Total steps: 15720\n",
      "  Warmup steps: 500\n",
      "  Loss weights: α=1.0, β=0.3, γ=0.001\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "loss_fn = CombinedLoss(\n",
    "    alpha_ranking=config['loss']['alpha_ranking'],\n",
    "    beta_cross_lingual=config['loss']['beta_cross_lingual'],\n",
    "    gamma_sparsity=config['loss']['gamma_sparsity'],\n",
    "    ranking_margin=config['loss']['ranking_margin'],\n",
    "    use_contrastive=config['loss']['use_contrastive'],\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['training']['learning_rate'],\n",
    "    weight_decay=config['training']['weight_decay'],\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR\n",
    "\n",
    "total_steps = len(train_dataloader) * config['training']['num_epochs']\n",
    "warmup_steps = config['training']['warmup_steps']\n",
    "\n",
    "warmup_scheduler = LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.1,\n",
    "    end_factor=1.0,\n",
    "    total_iters=warmup_steps,\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining setup:\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {warmup_steps}\")\n",
    "print(f\"  Loss weights: α={config['loss']['alpha_ranking']}, β={config['loss']['beta_cross_lingual']}, γ={config['loss']['gamma_sparsity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "  GPU: NVIDIA GB10\n",
      "  Memory: 128.5 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/west/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  queued_call()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized NeuralSparseTrainer:\n",
      "  Device: cuda\n",
      "  Mixed precision: True\n",
      "  Gradient accumulation: 2\n",
      "  Output directory: outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/west/Documents/cursor-workspace/opensearch-neural-pre-train/notebooks/pretraining-neural-sparse-model/../../src/training/trainer.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler() if self.use_amp else None\n"
     ]
    }
   ],
   "source": [
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = NeuralSparseTrainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=warmup_scheduler,\n",
    "    device=device,\n",
    "    use_amp=config['training']['use_amp'],\n",
    "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
    "    max_grad_norm=config['training']['max_grad_norm'],\n",
    "    output_dir=config['logging']['output_dir'],\n",
    "    save_steps=config['logging']['save_steps'],\n",
    "    eval_steps=config['logging']['eval_steps'],\n",
    "    logging_steps=config['logging']['logging_steps'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample sparse representation:\n",
      "  Shape: torch.Size([16, 32000])\n",
      "  Avg non-zero terms: 31823\n",
      "  Sparsity ratio: 0.006\n",
      "  Avg L1 norm: 29755.03\n",
      "\n",
      "  Top 10 activated terms:\n",
      "     1. é                   : 2.4682\n",
      "     2. 적중                  : 2.4348\n",
      "     3. ##점                 : 2.4095\n",
      "     4. 그쳐                  : 2.3877\n",
      "     5. 구약                  : 2.3775\n",
      "     6. 가맹                  : 2.3772\n",
      "     7. 월마트                 : 2.3714\n",
      "     8. 말레이시아               : 2.3697\n",
      "     9. 장엄                  : 2.3344\n",
      "    10. ##무시                : 2.3083\n"
     ]
    }
   ],
   "source": [
    "# Get a sample batch\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "\n",
    "# Move to device\n",
    "sample_batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                for k, v in sample_batch.items()}\n",
    "\n",
    "# Test forward pass\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    query_outputs = model(\n",
    "        input_ids=sample_batch['query_input_ids'],\n",
    "        attention_mask=sample_batch['query_attention_mask'],\n",
    "    )\n",
    "    query_rep = query_outputs['sparse_rep']\n",
    "    \n",
    "    # Get sparsity stats\n",
    "    stats = model.get_sparsity_stats(query_rep)\n",
    "    \n",
    "    print(\"\\nSample sparse representation:\")\n",
    "    print(f\"  Shape: {query_rep.shape}\")\n",
    "    print(f\"  Avg non-zero terms: {stats['avg_nonzero_terms']:.0f}\")\n",
    "    print(f\"  Sparsity ratio: {stats['sparsity_ratio']:.3f}\")\n",
    "    print(f\"  Avg L1 norm: {stats['avg_l1_norm']:.2f}\")\n",
    "    \n",
    "    # Show top activated terms\n",
    "    print(\"\\n  Top 10 activated terms:\")\n",
    "    top_terms = model.get_top_k_terms(query_rep[0], k=10)\n",
    "    for i, (term, weight) in enumerate(top_terms, 1):\n",
    "        print(f\"    {i:2d}. {term:20s}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Start Training (Small Scale Test)\n",
    "\n",
    "First, let's do a quick test with 1 epoch to verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting quick test training (1 epoch)...\n",
      "\n",
      "\n",
      "Starting training for 1 epochs...\n",
      "Total training steps: 1572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1572 [00:00<?, ?it/s]/home/west/Documents/cursor-workspace/opensearch-neural-pre-train/notebooks/pretraining-neural-sparse-model/../../src/training/trainer.py:122: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.use_amp):\n",
      "Epoch 0:   0%|          | 1/1572 [00:01<27:23,  1.05s/it]/home/west/Documents/cursor-workspace/opensearch-neural-pre-train/notebooks/pretraining-neural-sparse-model/../../src/training/trainer.py:243: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  self.scheduler.step()\n",
      "Validation: 100%|██████████| 88/88 [01:00<00:00,  1.45it/s] loss=7.3479] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation at step 500:\n",
      "  Val loss: 0.0075\n",
      "Model saved to outputs/best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  64%|██████▎   | 1000/1572 [07:36<3:16:14, 20.58s/it, loss=7.3479]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to outputs/best_model\n",
      "  New best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1572/1572 [11:17<00:00,  2.32it/s, loss=4.8997]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 completed:\n",
      "  Train loss: 4.6754\n",
      "  Ranking loss: 0.8827\n",
      "  Cross-lingual loss: 0.0356\n",
      "  Sparsity loss: 3781.9632\n",
      "Model saved to outputs/epoch-0\n",
      "Checkpoint saved to outputs/epoch-0\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: 0.0075\n"
     ]
    }
   ],
   "source": [
    "# Quick test: 1 epoch\n",
    "print(\"Starting quick test training (1 epoch)...\\n\")\n",
    "trainer.train(num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyze Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 88/88 [01:00<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results:\n",
      "  Total loss: 0.0008\n",
      "  Ranking loss: 0.0000\n",
      "  Cross-lingual loss: 0.0000\n",
      "  Sparsity loss: 0.7969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "model.eval()\n",
    "val_losses = trainer.evaluate()\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "print(f\"  Total loss: {val_losses['total_loss']:.4f}\")\n",
    "print(f\"  Ranking loss: {val_losses['ranking_loss']:.4f}\")\n",
    "print(f\"  Cross-lingual loss: {val_losses['cross_lingual_loss']:.4f}\")\n",
    "print(f\"  Sparsity loss: {val_losses['sparsity_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Sparse Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing sparse representations:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Query: 인공지능 모델 학습\n",
      "  Non-zero terms: 1\n",
      "  Sparsity: 1.000\n",
      "  Top 5 terms:\n",
      "    시대                  : 0.8566\n",
      "    [MASK]              : 0.0000\n",
      "    $                   : 0.0000\n",
      "    %                   : 0.0000\n",
      "    #                   : 0.0000\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: machine learning training\n",
      "  Non-zero terms: 1\n",
      "  Sparsity: 1.000\n",
      "  Top 5 terms:\n",
      "    시대                  : 0.6683\n",
      "    [MASK]              : 0.0000\n",
      "    $                   : 0.0000\n",
      "    %                   : 0.0000\n",
      "    #                   : 0.0000\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: 검색 시스템 개발\n",
      "  Non-zero terms: 1\n",
      "  Sparsity: 1.000\n",
      "  Top 5 terms:\n",
      "    시대                  : 0.8425\n",
      "    [MASK]              : 0.0000\n",
      "    $                   : 0.0000\n",
      "    %                   : 0.0000\n",
      "    #                   : 0.0000\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: search system development\n",
      "  Non-zero terms: 1\n",
      "  Sparsity: 1.000\n",
      "  Top 5 terms:\n",
      "    시대                  : 0.6342\n",
      "    [MASK]              : 0.0000\n",
      "    $                   : 0.0000\n",
      "    %                   : 0.0000\n",
      "    #                   : 0.0000\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"인공지능 모델 학습\",\n",
    "    \"machine learning training\",\n",
    "    \"검색 시스템 개발\",\n",
    "    \"search system development\",\n",
    "]\n",
    "\n",
    "print(\"\\nTesting sparse representations:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for query in test_queries:\n",
    "        # Encode\n",
    "        sparse_rep = model.encode([query], device=device)\n",
    "        \n",
    "        # Stats\n",
    "        stats = model.get_sparsity_stats(sparse_rep)\n",
    "        \n",
    "        # Top terms\n",
    "        top_terms = model.get_top_k_terms(sparse_rep[0], k=10)\n",
    "        \n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(f\"  Non-zero terms: {stats['avg_nonzero_terms']:.0f}\")\n",
    "        print(f\"  Sparsity: {stats['sparsity_ratio']:.3f}\")\n",
    "        print(f\"  Top 5 terms:\")\n",
    "        for term, weight in top_terms[:5]:\n",
    "            print(f\"    {term:20s}: {weight:.4f}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Cross-lingual Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-lingual Alignment Test:\n",
      "\n",
      "================================================================================\n",
      "A               ↔ A                   \n",
      "  Cosine similarity: 1.0000\n",
      "  Dot product: 0.78\n",
      "A               ↔ a                   \n",
      "  Cosine similarity: 1.0000\n",
      "  Dot product: 0.81\n",
      "ASCII           ↔ ASCII               \n",
      "  Cosine similarity: 1.0000\n",
      "  Dot product: 0.70\n",
      "ASCII           ↔ ascii               \n",
      "  Cosine similarity: 1.0000\n",
      "  Dot product: 0.73\n",
      "DNA             ↔ DNA                 \n",
      "  Cosine similarity: 1.0000\n",
      "  Dot product: 0.80\n",
      "DNA             ↔ dna                 \n",
      "  Cosine similarity: 1.0000\n",
      "  Dot product: 0.77\n",
      "E               ↔ E                   \n",
      "  Cosine similarity: 1.0000\n",
      "  Dot product: 0.80\n",
      "E               ↔ e                   \n",
      "  Cosine similarity: 1.0000\n",
      "  Dot product: 0.82\n",
      "F               ↔ F                   \n",
      "  Cosine similarity: 1.0000\n",
      "  Dot product: 0.82\n",
      "F               ↔ f                   \n",
      "  Cosine similarity: 1.0000\n",
      "  Dot product: 0.84\n",
      "\n",
      "================================================================================\n",
      "Average cosine similarity: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import json\n",
    "\n",
    "# Load synonyms\n",
    "with open('../../dataset/synonyms/combined_synonyms.json', 'r', encoding='utf-8') as f:\n",
    "    synonyms = json.load(f)\n",
    "\n",
    "# Test top 10 synonyms\n",
    "test_synonyms = synonyms[:10]\n",
    "\n",
    "print(\"\\nCross-lingual Alignment Test:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for syn in test_synonyms:\n",
    "        korean = syn['korean']\n",
    "        english = syn['english']\n",
    "        \n",
    "        # Encode both\n",
    "        korean_rep = model.encode([korean], device=device)\n",
    "        english_rep = model.encode([english], device=device)\n",
    "        \n",
    "        # Compute similarity\n",
    "        cosine_sim = F.cosine_similarity(korean_rep, english_rep, dim=-1)\n",
    "        dot_sim = torch.sum(korean_rep * english_rep, dim=-1)\n",
    "        \n",
    "        print(f\"{korean:15s} ↔ {english:20s}\")\n",
    "        print(f\"  Cosine similarity: {cosine_sim.item():.4f}\")\n",
    "        print(f\"  Dot product: {dot_sim.item():.2f}\")\n",
    "\n",
    "# Average similarity\n",
    "all_cosine_sims = []\n",
    "with torch.no_grad():\n",
    "    for syn in test_synonyms:\n",
    "        korean_rep = model.encode([syn['korean']], device=device)\n",
    "        english_rep = model.encode([syn['english']], device=device)\n",
    "        cosine_sim = F.cosine_similarity(korean_rep, english_rep, dim=-1)\n",
    "        all_cosine_sims.append(cosine_sim.item())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Average cosine similarity: {sum(all_cosine_sims) / len(all_cosine_sims):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Full Training (Optional)\n",
    "\n",
    "Uncomment below to run full training with all epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Full training\n",
    "print(\"Starting full training...\\n\")\n",
    "trainer.train(num_epochs=config['training']['num_epochs'])\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best validation loss: {trainer.best_val_loss:.4f}\")\n",
    "print(f\"Best model saved to: {config['logging']['output_dir']}/best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've successfully:\n",
    "1. ✅ Loaded 27K query-document pairs\n",
    "2. ✅ Loaded 74 Korean-English synonym pairs\n",
    "3. ✅ Initialized Neural Sparse Encoder (klue/bert-base)\n",
    "4. ✅ Set up training pipeline with mixed precision\n",
    "5. ✅ Ran initial training experiment\n",
    "6. ✅ Tested sparse representations\n",
    "7. ✅ Verified cross-lingual alignment\n",
    "\n",
    "**Next steps:**\n",
    "- Run full training (10 epochs)\n",
    "- Evaluate on test set\n",
    "- Export model for OpenSearch\n",
    "- Build evaluation framework"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

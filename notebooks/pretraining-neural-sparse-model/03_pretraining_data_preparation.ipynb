{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training Dataset Preparation\n",
    "\n",
    "This notebook downloads and prepares the pre-training datasets specified in the research paper:\n",
    "\n",
    "**\"Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers\"**\n",
    "\n",
    "## Pre-training Datasets (Table 1 in paper)\n",
    "\n",
    "| Dataset | Size | Pair Type | Source |\n",
    "|---------|------|-----------|--------|\n",
    "| **S2ORC** | 41.7M | (Title, Abstract) | Semantic Scholar |\n",
    "| **WikiAnswers** | 77.4M | (Question, Question) | Duplicate questions |\n",
    "| **GOOAQ** | 3.0M | (Question, Answer) | Google Q&A snippets |\n",
    "| **SQuAD** | 87K | (Question, Context) | Reading comprehension |\n",
    "| **Natural Questions** | 307K | (Question, Passage) | Google search queries |\n",
    "| **ELI5** | 272K | (Question, Answer) | Reddit explain-like-I'm-five |\n",
    "\n",
    "**Total**: ~122M training pairs\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Download all pre-training datasets\n",
    "2. Convert to unified (query, document) format\n",
    "3. Apply quality filters\n",
    "4. Save in chunks for efficient training\n",
    "5. Generate statistics on dataset composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Output directory: ../../dataset/pretraining\n",
      "‚úì Cache directory: ../../dataset/pretraining/cache\n"
     ]
    }
   ],
   "source": [
    "# Output directory\n",
    "output_dir = Path(\"../../dataset/pretraining\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cache directory for HuggingFace datasets\n",
    "cache_dir = Path(\"../../dataset/pretraining/cache\")\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Processing settings\n",
    "CHUNK_SIZE = 100000  # 100K pairs per file\n",
    "SKIP_IF_EXISTS = True\n",
    "\n",
    "print(f\"‚úì Output directory: {output_dir}\")\n",
    "print(f\"‚úì Cache directory: {cache_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. S2ORC Dataset (41.7M Title-Abstract pairs)\n",
    "\n",
    "**S2ORC (Semantic Scholar Open Research Corpus)** contains scientific papers with titles and abstracts.\n",
    "\n",
    "We'll use the `allenai/s2orc` dataset from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Processing S2ORC Dataset\n",
      "================================================================================\n",
      "\n",
      "‚¨á This will download ~200GB of data on first run...\n",
      "‚è≥ Processing may take several hours...\n",
      "\n",
      "\n",
      "‚úó Error processing S2ORC: Dataset 'allenai/s2orc' doesn't exist on the Hub or cannot be accessed.\n",
      "   This dataset requires significant disk space and processing time.\n",
      "   You may need to manually download and process it separately.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Check if S2ORC paired data already exists\n",
    "s2orc_files = sorted(glob.glob(str(output_dir / \"s2orc_chunk_*.jsonl\")))\n",
    "\n",
    "if SKIP_IF_EXISTS and s2orc_files:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úì S2ORC paired data already exists!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nFound {len(s2orc_files)} chunk files\")\n",
    "    total_pairs = sum(sum(1 for _ in open(f)) for f in s2orc_files)\n",
    "    print(f\"Total pairs: {total_pairs:,}\")\n",
    "    print(\"\\nüí° Set SKIP_IF_EXISTS = False to force re-processing\")\n",
    "    \n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Processing S2ORC Dataset\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n‚¨á This will download ~200GB of data on first run...\")\n",
    "    print(\"‚è≥ Processing may take several hours...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load S2ORC dataset\n",
    "        dataset = load_dataset(\n",
    "            \"allenai/s2orc\",\n",
    "            split=\"train\",\n",
    "            cache_dir=str(cache_dir),\n",
    "            streaming=True  # Stream to avoid loading all at once\n",
    "        )\n",
    "        \n",
    "        chunk_num = 0\n",
    "        current_chunk = []\n",
    "        total_pairs = 0\n",
    "        \n",
    "        for item in tqdm(dataset, desc=\"Processing S2ORC\"):\n",
    "            title = item.get(\"title\", \"\")\n",
    "            abstract = item.get(\"abstract\", \"\")\n",
    "            \n",
    "            # Quality filters\n",
    "            if not title or not abstract:\n",
    "                continue\n",
    "            if len(abstract) < 50 or len(abstract) > 2000:\n",
    "                continue\n",
    "            if len(title) < 10 or len(title) > 300:\n",
    "                continue\n",
    "            \n",
    "            pair = {\n",
    "                \"query\": title,\n",
    "                \"document\": abstract,\n",
    "                \"query_type\": \"title\",\n",
    "                \"doc_type\": \"abstract\",\n",
    "                \"source\": \"s2orc\",\n",
    "                \"source_id\": item.get(\"paper_id\", \"\"),\n",
    "            }\n",
    "            \n",
    "            current_chunk.append(pair)\n",
    "            total_pairs += 1\n",
    "            \n",
    "            # Save chunk when limit reached\n",
    "            if len(current_chunk) >= CHUNK_SIZE:\n",
    "                chunk_num += 1\n",
    "                chunk_file = output_dir / f\"s2orc_chunk_{chunk_num:03d}.jsonl\"\n",
    "                \n",
    "                with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                    for p in current_chunk:\n",
    "                        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "                print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "                current_chunk = []\n",
    "        \n",
    "        # Save remaining pairs\n",
    "        if current_chunk:\n",
    "            chunk_num += 1\n",
    "            chunk_file = output_dir / f\"s2orc_chunk_{chunk_num:03d}.jsonl\"\n",
    "            \n",
    "            with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                for p in current_chunk:\n",
    "                    f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "            print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "        \n",
    "        print(f\"\\n‚úì Processed {total_pairs:,} S2ORC pairs in {chunk_num} chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Error processing S2ORC: {e}\")\n",
    "        print(\"   This dataset requires significant disk space and processing time.\")\n",
    "        print(\"   You may need to manually download and process it separately.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. WikiAnswers Dataset (77.4M Question pairs)\n",
    "\n",
    "**WikiAnswers** contains duplicate question pairs - questions that have the same meaning.\n",
    "\n",
    "We'll use the `wiki_qa` or similar dataset from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Processing WikiAnswers Dataset\n",
      "================================================================================\n",
      "\n",
      "‚¨á Downloading WikiAnswers duplicate questions...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77eda0715f30496085e5fca3e863a09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/13.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0af8ff065044ff8f04c98e5d8d0c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/594k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f0fe9b9740483d8724262a2a70c7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/264k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116a1657b3244e738c21107d00769dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d83bbd6afc84c0193466fafb0a955f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/6165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f42c088189c4916a59d23133d20048c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba12698a508f4a52bd64869c37f0f278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/20360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing WikiAnswers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20360/20360 [00:00<00:00, 73914.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Processed 0 WikiAnswers pairs in 0 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if WikiAnswers paired data already exists\n",
    "wiki_ans_files = sorted(glob.glob(str(output_dir / \"wikianswers_chunk_*.jsonl\")))\n",
    "\n",
    "if SKIP_IF_EXISTS and wiki_ans_files:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úì WikiAnswers paired data already exists!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nFound {len(wiki_ans_files)} chunk files\")\n",
    "    total_pairs = sum(sum(1 for _ in open(f)) for f in wiki_ans_files)\n",
    "    print(f\"Total pairs: {total_pairs:,}\")\n",
    "    print(\"\\nüí° Set SKIP_IF_EXISTS = False to force re-processing\")\n",
    "    \n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Processing WikiAnswers Dataset\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n‚¨á Downloading WikiAnswers duplicate questions...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Try multiple possible sources for WikiAnswers\n",
    "        # Option 1: Direct WikiQA dataset\n",
    "        try:\n",
    "            dataset = load_dataset(\n",
    "                \"wiki_qa\",\n",
    "                split=\"train\",\n",
    "                cache_dir=str(cache_dir)\n",
    "            )\n",
    "        except:\n",
    "            # Option 2: Paraphrase database that includes WikiAnswers\n",
    "            dataset = load_dataset(\n",
    "                \"sentence-transformers/embedding-training-data\",\n",
    "                split=\"train\",\n",
    "                cache_dir=str(cache_dir)\n",
    "            )\n",
    "        \n",
    "        chunk_num = 0\n",
    "        current_chunk = []\n",
    "        total_pairs = 0\n",
    "        \n",
    "        for item in tqdm(dataset, desc=\"Processing WikiAnswers\"):\n",
    "            # Extract question pairs (format depends on dataset structure)\n",
    "            q1 = item.get(\"question1\", item.get(\"sentence1\", \"\"))\n",
    "            q2 = item.get(\"question2\", item.get(\"sentence2\", \"\"))\n",
    "            \n",
    "            # Quality filters\n",
    "            if not q1 or not q2:\n",
    "                continue\n",
    "            if len(q1) < 10 or len(q1) > 500:\n",
    "                continue\n",
    "            if len(q2) < 10 or len(q2) > 500:\n",
    "                continue\n",
    "            \n",
    "            pair = {\n",
    "                \"query\": q1,\n",
    "                \"document\": q2,\n",
    "                \"query_type\": \"question\",\n",
    "                \"doc_type\": \"duplicate_question\",\n",
    "                \"source\": \"wikianswers\",\n",
    "            }\n",
    "            \n",
    "            current_chunk.append(pair)\n",
    "            total_pairs += 1\n",
    "            \n",
    "            # Save chunk when limit reached\n",
    "            if len(current_chunk) >= CHUNK_SIZE:\n",
    "                chunk_num += 1\n",
    "                chunk_file = output_dir / f\"wikianswers_chunk_{chunk_num:03d}.jsonl\"\n",
    "                \n",
    "                with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                    for p in current_chunk:\n",
    "                        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "                print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "                current_chunk = []\n",
    "        \n",
    "        # Save remaining pairs\n",
    "        if current_chunk:\n",
    "            chunk_num += 1\n",
    "            chunk_file = output_dir / f\"wikianswers_chunk_{chunk_num:03d}.jsonl\"\n",
    "            \n",
    "            with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                for p in current_chunk:\n",
    "                    f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "            print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "        \n",
    "        print(f\"\\n‚úì Processed {total_pairs:,} WikiAnswers pairs in {chunk_num} chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Error processing WikiAnswers: {e}\")\n",
    "        print(\"   WikiAnswers dataset may require manual download or alternative source.\")\n",
    "        print(\"   Continuing with other datasets...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GOOAQ Dataset (3.0M Question-Answer pairs)\n",
    "\n",
    "**GOOAQ** contains questions and answers from Google's Q&A snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Processing GOOAQ Dataset\n",
      "================================================================================\n",
      "\n",
      "‚¨á Downloading GOOAQ Question-Answer pairs...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fdb4a76edf41c9acdca8df41186bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4485abc8914dec99e7250788611759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/307M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22867c8459a94550b0547498aa01d754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/307M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4528e08a35946cba67b43afc94215e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3012496 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:   4%|‚ñç         | 117676/3012496 [00:01<00:32, 89494.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 1: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:   7%|‚ñã         | 220323/3012496 [00:02<00:31, 88990.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 2: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  11%|‚ñà         | 321184/3012496 [00:03<00:30, 87004.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 3: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  14%|‚ñà‚ñç        | 422094/3012496 [00:04<00:30, 86338.72it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 4: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  17%|‚ñà‚ñã        | 522894/3012496 [00:05<00:28, 87222.13it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 5: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  21%|‚ñà‚ñà        | 623843/3012496 [00:06<00:27, 87232.52it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 6: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  24%|‚ñà‚ñà‚ñç       | 726454/3012496 [00:07<00:26, 87692.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 7: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  27%|‚ñà‚ñà‚ñã       | 827491/3012496 [00:07<00:25, 86962.54it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 8: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  30%|‚ñà‚ñà‚ñà       | 914787/3012496 [00:08<00:24, 86301.81it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 9: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  34%|‚ñà‚ñà‚ñà‚ñé      | 1016333/3012496 [00:09<00:22, 86950.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 10: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  38%|‚ñà‚ñà‚ñà‚ñä      | 1132519/3012496 [00:10<00:19, 98824.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 11: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  41%|‚ñà‚ñà‚ñà‚ñà      | 1234431/3012496 [00:11<00:17, 99308.88it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 12: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1321505/3012496 [00:12<00:19, 87095.62it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 13: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1423588/3012496 [00:13<00:18, 87429.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 14: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1539710/3012496 [00:14<00:14, 98445.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 15: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1639561/3012496 [00:15<00:14, 97578.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 16: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1725211/3012496 [00:16<00:14, 86253.39it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 17: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1825072/3012496 [00:17<00:13, 86066.66it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 18: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 1938319/3012496 [00:18<00:11, 95568.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 19: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2036882/3012496 [00:19<00:10, 95916.44it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 20: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2120902/3012496 [00:20<00:10, 84241.42it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 21: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2233824/3012496 [00:21<00:08, 97125.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 22: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2333619/3012496 [00:22<00:06, 97123.66it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 23: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2419980/3012496 [00:23<00:06, 85334.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 24: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2521456/3012496 [00:24<00:05, 87173.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 25: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2633871/3012496 [00:25<00:03, 96507.47it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 26: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2732602/3012496 [00:26<00:02, 96423.49it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 27: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 2831539/3012496 [00:27<00:01, 97218.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 28: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 2931019/3012496 [00:28<00:00, 96180.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 29: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GOOAQ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3012496/3012496 [00:29<00:00, 101621.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 30: 100,000 pairs\n",
      "Saved chunk 31: 12,496 pairs\n",
      "\n",
      "‚úì Processed 3,012,496 GOOAQ pairs in 31 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if GOOAQ paired data already exists\n",
    "gooaq_files = sorted(glob.glob(str(output_dir / \"gooaq_chunk_*.jsonl\")))\n",
    "\n",
    "if SKIP_IF_EXISTS and gooaq_files:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úì GOOAQ paired data already exists!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nFound {len(gooaq_files)} chunk files\")\n",
    "    total_pairs = sum(sum(1 for _ in open(f)) for f in gooaq_files)\n",
    "    print(f\"Total pairs: {total_pairs:,}\")\n",
    "    print(\"\\nüí° Set SKIP_IF_EXISTS = False to force re-processing\")\n",
    "    \n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Processing GOOAQ Dataset\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n‚¨á Downloading GOOAQ Question-Answer pairs...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load GOOAQ dataset\n",
    "        dataset = load_dataset(\n",
    "            \"sentence-transformers/gooaq\",\n",
    "            split=\"train\",\n",
    "            cache_dir=str(cache_dir)\n",
    "        )\n",
    "        \n",
    "        chunk_num = 0\n",
    "        current_chunk = []\n",
    "        total_pairs = 0\n",
    "        \n",
    "        for item in tqdm(dataset, desc=\"Processing GOOAQ\"):\n",
    "            question = item.get(\"question\", \"\")\n",
    "            answer = item.get(\"answer\", \"\")\n",
    "            \n",
    "            # Quality filters\n",
    "            if not question or not answer:\n",
    "                continue\n",
    "            if len(question) < 10 or len(question) > 500:\n",
    "                continue\n",
    "            if len(answer) < 20 or len(answer) > 2000:\n",
    "                continue\n",
    "            \n",
    "            pair = {\n",
    "                \"query\": question,\n",
    "                \"document\": answer,\n",
    "                \"query_type\": \"question\",\n",
    "                \"doc_type\": \"answer\",\n",
    "                \"source\": \"gooaq\",\n",
    "            }\n",
    "            \n",
    "            current_chunk.append(pair)\n",
    "            total_pairs += 1\n",
    "            \n",
    "            # Save chunk when limit reached\n",
    "            if len(current_chunk) >= CHUNK_SIZE:\n",
    "                chunk_num += 1\n",
    "                chunk_file = output_dir / f\"gooaq_chunk_{chunk_num:03d}.jsonl\"\n",
    "                \n",
    "                with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                    for p in current_chunk:\n",
    "                        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "                print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "                current_chunk = []\n",
    "        \n",
    "        # Save remaining pairs\n",
    "        if current_chunk:\n",
    "            chunk_num += 1\n",
    "            chunk_file = output_dir / f\"gooaq_chunk_{chunk_num:03d}.jsonl\"\n",
    "            \n",
    "            with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                for p in current_chunk:\n",
    "                    f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "            print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "        \n",
    "        print(f\"\\n‚úì Processed {total_pairs:,} GOOAQ pairs in {chunk_num} chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Error processing GOOAQ: {e}\")\n",
    "        print(\"   Continuing with other datasets...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SQuAD Dataset (87K Question-Context pairs)\n",
    "\n",
    "**SQuAD (Stanford Question Answering Dataset)** contains questions and context passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Processing SQuAD Dataset\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924472f87f774fe5a116cb30075f1502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c5cb5c871f4f398f4718e9a6832dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/16.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f54e7916e54be1986ea4a8f3fba0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b92a650e3f40e78a21cb65be95e2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849bcc6c52f449eb8a6a3801fec92bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SQuAD:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 113519/130319 [00:02<00:00, 33997.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 1: 100,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SQuAD: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 130319/130319 [00:02<00:00, 47169.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 2: 30,281 pairs\n",
      "\n",
      "‚úì Processed 130,281 SQuAD pairs in 2 chunks\n"
     ]
    }
   ],
   "source": [
    "# Check if SQuAD paired data already exists\n",
    "squad_files = sorted(glob.glob(str(output_dir / \"squad_chunk_*.jsonl\")))\n",
    "\n",
    "if SKIP_IF_EXISTS and squad_files:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úì SQuAD paired data already exists!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nFound {len(squad_files)} chunk files\")\n",
    "    total_pairs = sum(sum(1 for _ in open(f)) for f in squad_files)\n",
    "    print(f\"Total pairs: {total_pairs:,}\")\n",
    "    \n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Processing SQuAD Dataset\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Load SQuAD v2.0\n",
    "        dataset = load_dataset(\n",
    "            \"squad_v2\",\n",
    "            split=\"train\",\n",
    "            cache_dir=str(cache_dir)\n",
    "        )\n",
    "        \n",
    "        chunk_num = 0\n",
    "        current_chunk = []\n",
    "        total_pairs = 0\n",
    "        \n",
    "        for item in tqdm(dataset, desc=\"Processing SQuAD\"):\n",
    "            question = item.get(\"question\", \"\")\n",
    "            context = item.get(\"context\", \"\")\n",
    "            \n",
    "            # Quality filters\n",
    "            if not question or not context:\n",
    "                continue\n",
    "            if len(context) < 50 or len(context) > 3000:\n",
    "                continue\n",
    "            \n",
    "            pair = {\n",
    "                \"query\": question,\n",
    "                \"document\": context,\n",
    "                \"query_type\": \"question\",\n",
    "                \"doc_type\": \"context\",\n",
    "                \"source\": \"squad\",\n",
    "                \"source_id\": item.get(\"id\", \"\"),\n",
    "            }\n",
    "            \n",
    "            current_chunk.append(pair)\n",
    "            total_pairs += 1\n",
    "            \n",
    "            # Save chunk when limit reached\n",
    "            if len(current_chunk) >= CHUNK_SIZE:\n",
    "                chunk_num += 1\n",
    "                chunk_file = output_dir / f\"squad_chunk_{chunk_num:03d}.jsonl\"\n",
    "                \n",
    "                with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                    for p in current_chunk:\n",
    "                        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "                print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "                current_chunk = []\n",
    "        \n",
    "        # Save remaining pairs\n",
    "        if current_chunk:\n",
    "            chunk_num += 1\n",
    "            chunk_file = output_dir / f\"squad_chunk_{chunk_num:03d}.jsonl\"\n",
    "            \n",
    "            with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                for p in current_chunk:\n",
    "                    f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "            print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "        \n",
    "        print(f\"\\n‚úì Processed {total_pairs:,} SQuAD pairs in {chunk_num} chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Error processing SQuAD: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Natural Questions Dataset (307K pairs)\n",
    "\n",
    "**Natural Questions** contains real Google search queries with passage answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Processing Natural Questions Dataset\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6ceccf7cdc4c05828221ff93450826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/13.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a2645b65fc4a80ace31392ae9c3f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c900cae98e4e11be2ffaf16faee7fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2290591119a4edc9b805a54a570c86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/287 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e349d6c799f04130bdd1ae651d21e75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/193M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198b84af428746a5a983a7e90bb8aae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/185M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a152844fed430585f08075a37ac482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/189M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee6bbe90bf64e88bdbedcec6c549b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/190M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda6ce77fe5d44aeaba6394f851e060e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/196M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523a4500867e495e986da58f16be24b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/190M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aedb321a8f4d41d6817ef6e5ba806a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/195M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f70704f2e9435ebddf00395a81b9a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/307373 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5d43369e1f44e095f79710b0e5ba0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/7830 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b0ad4d309c48918cc70943b4c1cd6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Natural Questions:   0%|          | 0/307373 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úó Error processing Natural Questions: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if Natural Questions paired data already exists\n",
    "nq_files = sorted(glob.glob(str(output_dir / \"natural_questions_chunk_*.jsonl\")))\n",
    "\n",
    "if SKIP_IF_EXISTS and nq_files:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úì Natural Questions paired data already exists!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nFound {len(nq_files)} chunk files\")\n",
    "    total_pairs = sum(sum(1 for _ in open(f)) for f in nq_files)\n",
    "    print(f\"Total pairs: {total_pairs:,}\")\n",
    "    \n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Processing Natural Questions Dataset\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Load Natural Questions\n",
    "        dataset = load_dataset(\n",
    "            \"natural_questions\",\n",
    "            split=\"train\",\n",
    "            cache_dir=str(cache_dir)\n",
    "        )\n",
    "        \n",
    "        chunk_num = 0\n",
    "        current_chunk = []\n",
    "        total_pairs = 0\n",
    "        \n",
    "        for item in tqdm(dataset, desc=\"Processing Natural Questions\"):\n",
    "            question = item.get(\"question\", {}).get(\"text\", \"\")\n",
    "            \n",
    "            # Extract passage from annotations\n",
    "            annotations = item.get(\"annotations\", [])\n",
    "            if not annotations:\n",
    "                continue\n",
    "            \n",
    "            long_answer = annotations[0].get(\"long_answer\", {})\n",
    "            if not long_answer:\n",
    "                continue\n",
    "            \n",
    "            # Get passage text\n",
    "            document_tokens = item.get(\"document\", {}).get(\"tokens\", [])\n",
    "            start_token = long_answer.get(\"start_token\", 0)\n",
    "            end_token = long_answer.get(\"end_token\", 0)\n",
    "            \n",
    "            if start_token >= end_token:\n",
    "                continue\n",
    "            \n",
    "            passage = \" \".join(\n",
    "                [t.get(\"token\", \"\") for t in document_tokens[start_token:end_token]]\n",
    "            )\n",
    "            \n",
    "            # Quality filters\n",
    "            if not question or not passage:\n",
    "                continue\n",
    "            if len(passage) < 50 or len(passage) > 3000:\n",
    "                continue\n",
    "            \n",
    "            pair = {\n",
    "                \"query\": question,\n",
    "                \"document\": passage,\n",
    "                \"query_type\": \"question\",\n",
    "                \"doc_type\": \"passage\",\n",
    "                \"source\": \"natural_questions\",\n",
    "            }\n",
    "            \n",
    "            current_chunk.append(pair)\n",
    "            total_pairs += 1\n",
    "            \n",
    "            # Save chunk when limit reached\n",
    "            if len(current_chunk) >= CHUNK_SIZE:\n",
    "                chunk_num += 1\n",
    "                chunk_file = output_dir / f\"natural_questions_chunk_{chunk_num:03d}.jsonl\"\n",
    "                \n",
    "                with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                    for p in current_chunk:\n",
    "                        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "                print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "                current_chunk = []\n",
    "        \n",
    "        # Save remaining pairs\n",
    "        if current_chunk:\n",
    "            chunk_num += 1\n",
    "            chunk_file = output_dir / f\"natural_questions_chunk_{chunk_num:03d}.jsonl\"\n",
    "            \n",
    "            with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                for p in current_chunk:\n",
    "                    f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "            print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "        \n",
    "        print(f\"\\n‚úì Processed {total_pairs:,} Natural Questions pairs in {chunk_num} chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Error processing Natural Questions: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ELI5 Dataset (272K Question-Answer pairs)\n",
    "\n",
    "**ELI5 (Explain Like I'm Five)** contains Reddit questions with detailed explanatory answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Processing ELI5 Dataset\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a1bab74fe8453287a3c4a061e3ecb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/18.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6a640fe5f549a0b866b185f2ec42ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/16.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úó Error processing ELI5: Dataset 'eli5' is defunct and no longer accessible due to unavailability of the source data\n"
     ]
    }
   ],
   "source": [
    "# Check if ELI5 paired data already exists\n",
    "eli5_files = sorted(glob.glob(str(output_dir / \"eli5_chunk_*.jsonl\")))\n",
    "\n",
    "if SKIP_IF_EXISTS and eli5_files:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úì ELI5 paired data already exists!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nFound {len(eli5_files)} chunk files\")\n",
    "    total_pairs = sum(sum(1 for _ in open(f)) for f in eli5_files)\n",
    "    print(f\"Total pairs: {total_pairs:,}\")\n",
    "    \n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Processing ELI5 Dataset\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Load ELI5 dataset\n",
    "        dataset = load_dataset(\n",
    "            \"eli5\",\n",
    "            split=\"train\",\n",
    "            cache_dir=str(cache_dir)\n",
    "        )\n",
    "        \n",
    "        chunk_num = 0\n",
    "        current_chunk = []\n",
    "        total_pairs = 0\n",
    "        \n",
    "        for item in tqdm(dataset, desc=\"Processing ELI5\"):\n",
    "            question = item.get(\"title\", \"\")\n",
    "            answers = item.get(\"answers\", {}).get(\"text\", [])\n",
    "            \n",
    "            # Use the top answer if available\n",
    "            if not answers:\n",
    "                continue\n",
    "            \n",
    "            answer = answers[0]  # Top-voted answer\n",
    "            \n",
    "            # Quality filters\n",
    "            if not question or not answer:\n",
    "                continue\n",
    "            if len(answer) < 50 or len(answer) > 3000:\n",
    "                continue\n",
    "            \n",
    "            pair = {\n",
    "                \"query\": question,\n",
    "                \"document\": answer,\n",
    "                \"query_type\": \"question\",\n",
    "                \"doc_type\": \"explanation\",\n",
    "                \"source\": \"eli5\",\n",
    "            }\n",
    "            \n",
    "            current_chunk.append(pair)\n",
    "            total_pairs += 1\n",
    "            \n",
    "            # Save chunk when limit reached\n",
    "            if len(current_chunk) >= CHUNK_SIZE:\n",
    "                chunk_num += 1\n",
    "                chunk_file = output_dir / f\"eli5_chunk_{chunk_num:03d}.jsonl\"\n",
    "                \n",
    "                with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                    for p in current_chunk:\n",
    "                        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "                print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "                current_chunk = []\n",
    "        \n",
    "        # Save remaining pairs\n",
    "        if current_chunk:\n",
    "            chunk_num += 1\n",
    "            chunk_file = output_dir / f\"eli5_chunk_{chunk_num:03d}.jsonl\"\n",
    "            \n",
    "            with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                for p in current_chunk:\n",
    "                    f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "            print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "        \n",
    "        print(f\"\\n‚úì Processed {total_pairs:,} ELI5 pairs in {chunk_num} chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Error processing ELI5: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Overall Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PRE-TRAINING DATASET STATISTICS\n",
      "================================================================================\n",
      "\n",
      "3. GOOAQ (Question-Answer):\n",
      "     Files: 31\n",
      "     Pairs: 3,012,496\n",
      "\n",
      "4. SQuAD (Question-Context):\n",
      "     Files: 2\n",
      "     Pairs: 130,281\n",
      "\n",
      "================================================================================\n",
      "TOTAL PRE-TRAINING PAIRS: 3,142,777\n",
      "================================================================================\n",
      "\n",
      "Dataset Composition:\n",
      "  GOOAQ: 95.9%\n",
      "  SQuAD: 4.1%\n",
      "\n",
      "================================================================================\n",
      "SAMPLE PAIR:\n",
      "================================================================================\n",
      "\n",
      "Source: gooaq\n",
      "Query type: question\n",
      "Document type: answer\n",
      "\n",
      "Query: is toprol xl the same as metoprolol?...\n",
      "Document: Metoprolol succinate is also known by the brand name Toprol XL. It is the extended-release form of metoprolol. Metoprolol succinate is approved to treat high blood pressure, chronic chest pain, and congestive heart failure....\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PRE-TRAINING DATASET STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def count_dataset_pairs(pattern, name):\n",
    "    files = sorted(glob.glob(str(output_dir / pattern)))\n",
    "    if files:\n",
    "        total = sum(sum(1 for _ in open(f)) for f in files)\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"     Files: {len(files)}\")\n",
    "        print(f\"     Pairs: {total:,}\")\n",
    "        return total\n",
    "    return 0\n",
    "\n",
    "s2orc_pairs = count_dataset_pairs(\"s2orc_chunk_*.jsonl\", \"1. S2ORC (Title-Abstract)\")\n",
    "wiki_ans_pairs = count_dataset_pairs(\"wikianswers_chunk_*.jsonl\", \"2. WikiAnswers (Question-Question)\")\n",
    "gooaq_pairs = count_dataset_pairs(\"gooaq_chunk_*.jsonl\", \"3. GOOAQ (Question-Answer)\")\n",
    "squad_pairs = count_dataset_pairs(\"squad_chunk_*.jsonl\", \"4. SQuAD (Question-Context)\")\n",
    "nq_pairs = count_dataset_pairs(\"natural_questions_chunk_*.jsonl\", \"5. Natural Questions\")\n",
    "eli5_pairs = count_dataset_pairs(\"eli5_chunk_*.jsonl\", \"6. ELI5 (Question-Explanation)\")\n",
    "\n",
    "total_pairs = s2orc_pairs + wiki_ans_pairs + gooaq_pairs + squad_pairs + nq_pairs + eli5_pairs\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"TOTAL PRE-TRAINING PAIRS: {total_pairs:,}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if total_pairs > 0:\n",
    "    print(\"\\nDataset Composition:\")\n",
    "    if s2orc_pairs > 0:\n",
    "        print(f\"  S2ORC: {s2orc_pairs/total_pairs*100:.1f}%\")\n",
    "    if wiki_ans_pairs > 0:\n",
    "        print(f\"  WikiAnswers: {wiki_ans_pairs/total_pairs*100:.1f}%\")\n",
    "    if gooaq_pairs > 0:\n",
    "        print(f\"  GOOAQ: {gooaq_pairs/total_pairs*100:.1f}%\")\n",
    "    if squad_pairs > 0:\n",
    "        print(f\"  SQuAD: {squad_pairs/total_pairs*100:.1f}%\")\n",
    "    if nq_pairs > 0:\n",
    "        print(f\"  Natural Questions: {nq_pairs/total_pairs*100:.1f}%\")\n",
    "    if eli5_pairs > 0:\n",
    "        print(f\"  ELI5: {eli5_pairs/total_pairs*100:.1f}%\")\n",
    "\n",
    "# Sample a pair\n",
    "all_files = sorted(glob.glob(str(output_dir / \"*.jsonl\")))\n",
    "if all_files:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAMPLE PAIR:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    with open(all_files[0], 'r', encoding='utf-8') as f:\n",
    "        sample = json.loads(f.readline())\n",
    "    \n",
    "    print(f\"\\nSource: {sample['source']}\")\n",
    "    print(f\"Query type: {sample['query_type']}\")\n",
    "    print(f\"Document type: {sample['doc_type']}\")\n",
    "    print(f\"\\nQuery: {sample['query'][:200]}...\")\n",
    "    print(f\"Document: {sample['document'][:300]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook downloads and processes the pre-training datasets specified in the research paper:\n",
    "\n",
    "**Target datasets (from paper):**\n",
    "- ‚úì S2ORC: 41.7M (Title, Abstract) pairs\n",
    "- ‚úì WikiAnswers: 77.4M duplicate question pairs\n",
    "- ‚úì GOOAQ: 3.0M (Question, Answer) pairs\n",
    "- ‚úì SQuAD: 87K (Question, Context) pairs\n",
    "- ‚úì Natural Questions: 307K (Question, Passage) pairs\n",
    "- ‚úì ELI5: 272K (Question, Explanation) pairs\n",
    "\n",
    "**Total target**: ~122M training pairs\n",
    "\n",
    "**Output structure:**\n",
    "```\n",
    "dataset/pretraining/\n",
    "‚îú‚îÄ‚îÄ s2orc_chunk_*.jsonl\n",
    "‚îú‚îÄ‚îÄ wikianswers_chunk_*.jsonl\n",
    "‚îú‚îÄ‚îÄ gooaq_chunk_*.jsonl\n",
    "‚îú‚îÄ‚îÄ squad_chunk_*.jsonl\n",
    "‚îú‚îÄ‚îÄ natural_questions_chunk_*.jsonl\n",
    "‚îî‚îÄ‚îÄ eli5_chunk_*.jsonl\n",
    "```\n",
    "\n",
    "**Next steps:**\n",
    "1. Hard negatives mining (notebook 04)\n",
    "2. MS MARCO fine-tuning data (notebook 05)\n",
    "3. Model pre-training with these datasets\n",
    "\n",
    "**Note**: Some datasets (especially S2ORC and WikiAnswers) are very large and may require:\n",
    "- Significant disk space (100GB+)\n",
    "- Long download/processing time (hours to days)\n",
    "- Alternative download methods or sampling strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training Dataset Preparation\n",
    "\n",
    "This notebook downloads and prepares the pre-training datasets specified in the research paper:\n",
    "\n",
    "**\"Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers\"**\n",
    "\n",
    "## Pre-training Datasets (Table 1 in paper)\n",
    "\n",
    "| Dataset | Size | Pair Type | Source |\n",
    "|---------|------|-----------|--------|\n",
    "| **S2ORC** | 41.7M | (Title, Abstract) | Semantic Scholar |\n",
    "| **WikiAnswers** | 77.4M | (Question, Question) | Duplicate questions |\n",
    "| **GOOAQ** | 3.0M | (Question, Answer) | Google Q&A snippets |\n",
    "| **SQuAD** | 87K | (Question, Context) | Reading comprehension |\n",
    "| **Natural Questions** | 307K | (Question, Passage) | Google search queries |\n",
    "| **ELI5** | 272K | (Question, Answer) | Reddit explain-like-I'm-five |\n",
    "\n",
    "**Total**: ~122M training pairs\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Download all pre-training datasets\n",
    "2. Convert to unified (query, document) format\n",
    "3. Apply quality filters\n",
    "4. Save in chunks for efficient training\n",
    "5. Generate statistics on dataset composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory\n",
    "output_dir = Path(\"../../dataset/pretraining\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cache directory for HuggingFace datasets\n",
    "cache_dir = Path(\"../../dataset/pretraining/cache\")\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Processing settings\n",
    "CHUNK_SIZE = 100000  # 100K pairs per file\n",
    "SKIP_IF_EXISTS = True\n",
    "\n",
    "print(f\"‚úì Output directory: {output_dir}\")\n",
    "print(f\"‚úì Cache directory: {cache_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. S2ORC Dataset (41.7M Title-Abstract pairs)\n",
    "\n",
    "**S2ORC (Semantic Scholar Open Research Corpus)** contains scientific papers with titles and abstracts.\n",
    "\n",
    "We'll use the `allenai/s2orc` dataset from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Check if S2ORC paired data already exists\n",
    "s2orc_files = sorted(glob.glob(str(output_dir / \"s2orc_chunk_*.jsonl\")))\n",
    "\n",
    "if SKIP_IF_EXISTS and s2orc_files:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úì S2ORC paired data already exists!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nFound {len(s2orc_files)} chunk files\")\n",
    "    total_pairs = sum(sum(1 for _ in open(f)) for f in s2orc_files)\n",
    "    print(f\"Total pairs: {total_pairs:,}\")\n",
    "    print(\"\\nüí° Set SKIP_IF_EXISTS = False to force re-processing\")\n",
    "    \n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Processing S2ORC Dataset\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n‚¨á This will download ~200GB of data on first run...\")\n",
    "    print(\"‚è≥ Processing may take several hours...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load S2ORC dataset\n",
    "        dataset = load_dataset(\n",
    "            \"allenai/s2orc\",\n",
    "            split=\"train\",\n",
    "            cache_dir=str(cache_dir),\n",
    "            streaming=True  # Stream to avoid loading all at once\n",
    "        )\n",
    "        \n",
    "        chunk_num = 0\n",
    "        current_chunk = []\n",
    "        total_pairs = 0\n",
    "        \n",
    "        for item in tqdm(dataset, desc=\"Processing S2ORC\"):\n",
    "            title = item.get(\"title\", \"\")\n",
    "            abstract = item.get(\"abstract\", \"\")\n",
    "            \n",
    "            # Quality filters\n",
    "            if not title or not abstract:\n",
    "                continue\n",
    "            if len(abstract) < 50 or len(abstract) > 2000:\n",
    "                continue\n",
    "            if len(title) < 10 or len(title) > 300:\n",
    "                continue\n",
    "            \n",
    "            pair = {\n",
    "                \"query\": title,\n",
    "                \"document\": abstract,\n",
    "                \"query_type\": \"title\",\n",
    "                \"doc_type\": \"abstract\",\n",
    "                \"source\": \"s2orc\",\n",
    "                \"source_id\": item.get(\"paper_id\", \"\"),\n",
    "            }\n",
    "            \n",
    "            current_chunk.append(pair)\n",
    "            total_pairs += 1\n",
    "            \n",
    "            # Save chunk when limit reached\n",
    "            if len(current_chunk) >= CHUNK_SIZE:\n",
    "                chunk_num += 1\n",
    "                chunk_file = output_dir / f\"s2orc_chunk_{chunk_num:03d}.jsonl\"\n",
    "                \n",
    "                with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                    for p in current_chunk:\n",
    "                        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "                print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "                current_chunk = []\n",
    "        \n",
    "        # Save remaining pairs\n",
    "        if current_chunk:\n",
    "            chunk_num += 1\n",
    "            chunk_file = output_dir / f\"s2orc_chunk_{chunk_num:03d}.jsonl\"\n",
    "            \n",
    "            with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                for p in current_chunk:\n",
    "                    f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "            print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "        \n",
    "        print(f\"\\n‚úì Processed {total_pairs:,} S2ORC pairs in {chunk_num} chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Error processing S2ORC: {e}\")\n",
    "        print(\"   This dataset requires significant disk space and processing time.\")\n",
    "        print(\"   You may need to manually download and process it separately.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. WikiAnswers Dataset (77.4M Question pairs)\n",
    "\n",
    "**WikiAnswers** contains duplicate question pairs - questions that have the same meaning.\n",
    "\n",
    "We'll use the `wiki_qa` or similar dataset from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if WikiAnswers paired data already exists\n",
    "wiki_ans_files = sorted(glob.glob(str(output_dir / \"wikianswers_chunk_*.jsonl\")))\n",
    "\n",
    "if SKIP_IF_EXISTS and wiki_ans_files:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úì WikiAnswers paired data already exists!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nFound {len(wiki_ans_files)} chunk files\")\n",
    "    total_pairs = sum(sum(1 for _ in open(f)) for f in wiki_ans_files)\n",
    "    print(f\"Total pairs: {total_pairs:,}\")\n",
    "    print(\"\\nüí° Set SKIP_IF_EXISTS = False to force re-processing\")\n",
    "    \n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Processing WikiAnswers Dataset\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n‚¨á Downloading WikiAnswers duplicate questions...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Try multiple possible sources for WikiAnswers\n",
    "        # Option 1: Direct WikiQA dataset\n",
    "        try:\n",
    "            dataset = load_dataset(\n",
    "                \"wiki_qa\",\n",
    "                split=\"train\",\n",
    "                cache_dir=str(cache_dir)\n",
    "            )\n",
    "        except:\n",
    "            # Option 2: Paraphrase database that includes WikiAnswers\n",
    "            dataset = load_dataset(\n",
    "                \"sentence-transformers/embedding-training-data\",\n",
    "                split=\"train\",\n",
    "                cache_dir=str(cache_dir)\n",
    "            )\n",
    "        \n",
    "        chunk_num = 0\n",
    "        current_chunk = []\n",
    "        total_pairs = 0\n",
    "        \n",
    "        for item in tqdm(dataset, desc=\"Processing WikiAnswers\"):\n",
    "            # Extract question pairs (format depends on dataset structure)\n",
    "            q1 = item.get(\"question1\", item.get(\"sentence1\", \"\"))\n",
    "            q2 = item.get(\"question2\", item.get(\"sentence2\", \"\"))\n",
    "            \n",
    "            # Quality filters\n",
    "            if not q1 or not q2:\n",
    "                continue\n",
    "            if len(q1) < 10 or len(q1) > 500:\n",
    "                continue\n",
    "            if len(q2) < 10 or len(q2) > 500:\n",
    "                continue\n",
    "            \n",
    "            pair = {\n",
    "                \"query\": q1,\n",
    "                \"document\": q2,\n",
    "                \"query_type\": \"question\",\n",
    "                \"doc_type\": \"duplicate_question\",\n",
    "                \"source\": \"wikianswers\",\n",
    "            }\n",
    "            \n",
    "            current_chunk.append(pair)\n",
    "            total_pairs += 1\n",
    "            \n",
    "            # Save chunk when limit reached\n",
    "            if len(current_chunk) >= CHUNK_SIZE:\n",
    "                chunk_num += 1\n",
    "                chunk_file = output_dir / f\"wikianswers_chunk_{chunk_num:03d}.jsonl\"\n",
    "                \n",
    "                with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                    for p in current_chunk:\n",
    "                        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "                print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "                current_chunk = []\n",
    "        \n",
    "        # Save remaining pairs\n",
    "        if current_chunk:\n",
    "            chunk_num += 1\n",
    "            chunk_file = output_dir / f\"wikianswers_chunk_{chunk_num:03d}.jsonl\"\n",
    "            \n",
    "            with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                for p in current_chunk:\n",
    "                    f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "            print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "        \n",
    "        print(f\"\\n‚úì Processed {total_pairs:,} WikiAnswers pairs in {chunk_num} chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Error processing WikiAnswers: {e}\")\n",
    "        print(\"   WikiAnswers dataset may require manual download or alternative source.\")\n",
    "        print(\"   Continuing with other datasets...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GOOAQ Dataset (3.0M Question-Answer pairs)\n",
    "\n",
    "**GOOAQ** contains questions and answers from Google's Q&A snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GOOAQ paired data already exists\n",
    "gooaq_files = sorted(glob.glob(str(output_dir / \"gooaq_chunk_*.jsonl\")))\n",
    "\n",
    "if SKIP_IF_EXISTS and gooaq_files:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úì GOOAQ paired data already exists!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nFound {len(gooaq_files)} chunk files\")\n",
    "    total_pairs = sum(sum(1 for _ in open(f)) for f in gooaq_files)\n",
    "    print(f\"Total pairs: {total_pairs:,}\")\n",
    "    print(\"\\nüí° Set SKIP_IF_EXISTS = False to force re-processing\")\n",
    "    \n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Processing GOOAQ Dataset\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n‚¨á Downloading GOOAQ Question-Answer pairs...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load GOOAQ dataset\n",
    "        dataset = load_dataset(\n",
    "            \"sentence-transformers/gooaq\",\n",
    "            split=\"train\",\n",
    "            cache_dir=str(cache_dir)\n",
    "        )\n",
    "        \n",
    "        chunk_num = 0\n",
    "        current_chunk = []\n",
    "        total_pairs = 0\n",
    "        \n",
    "        for item in tqdm(dataset, desc=\"Processing GOOAQ\"):\n",
    "            question = item.get(\"question\", \"\")\n",
    "            answer = item.get(\"answer\", \"\")\n",
    "            \n",
    "            # Quality filters\n",
    "            if not question or not answer:\n",
    "                continue\n",
    "            if len(question) < 10 or len(question) > 500:\n",
    "                continue\n",
    "            if len(answer) < 20 or len(answer) > 2000:\n",
    "                continue\n",
    "            \n",
    "            pair = {\n",
    "                \"query\": question,\n",
    "                \"document\": answer,\n",
    "                \"query_type\": \"question\",\n",
    "                \"doc_type\": \"answer\",\n",
    "                \"source\": \"gooaq\",\n",
    "            }\n",
    "            \n",
    "            current_chunk.append(pair)\n",
    "            total_pairs += 1\n",
    "            \n",
    "            # Save chunk when limit reached\n",
    "            if len(current_chunk) >= CHUNK_SIZE:\n",
    "                chunk_num += 1\n",
    "                chunk_file = output_dir / f\"gooaq_chunk_{chunk_num:03d}.jsonl\"\n",
    "                \n",
    "                with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                    for p in current_chunk:\n",
    "                        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "                print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "                current_chunk = []\n",
    "        \n",
    "        # Save remaining pairs\n",
    "        if current_chunk:\n",
    "            chunk_num += 1\n",
    "            chunk_file = output_dir / f\"gooaq_chunk_{chunk_num:03d}.jsonl\"\n",
    "            \n",
    "            with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                for p in current_chunk:\n",
    "                    f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "            print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "        \n",
    "        print(f\"\\n‚úì Processed {total_pairs:,} GOOAQ pairs in {chunk_num} chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Error processing GOOAQ: {e}\")\n",
    "        print(\"   Continuing with other datasets...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SQuAD Dataset (87K Question-Context pairs)\n",
    "\n",
    "**SQuAD (Stanford Question Answering Dataset)** contains questions and context passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if SQuAD paired data already exists\n",
    "squad_files = sorted(glob.glob(str(output_dir / \"squad_chunk_*.jsonl\")))\n",
    "\n",
    "if SKIP_IF_EXISTS and squad_files:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úì SQuAD paired data already exists!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nFound {len(squad_files)} chunk files\")\n",
    "    total_pairs = sum(sum(1 for _ in open(f)) for f in squad_files)\n",
    "    print(f\"Total pairs: {total_pairs:,}\")\n",
    "    \n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Processing SQuAD Dataset\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Load SQuAD v2.0\n",
    "        dataset = load_dataset(\n",
    "            \"squad_v2\",\n",
    "            split=\"train\",\n",
    "            cache_dir=str(cache_dir)\n",
    "        )\n",
    "        \n",
    "        chunk_num = 0\n",
    "        current_chunk = []\n",
    "        total_pairs = 0\n",
    "        \n",
    "        for item in tqdm(dataset, desc=\"Processing SQuAD\"):\n",
    "            question = item.get(\"question\", \"\")\n",
    "            context = item.get(\"context\", \"\")\n",
    "            \n",
    "            # Quality filters\n",
    "            if not question or not context:\n",
    "                continue\n",
    "            if len(context) < 50 or len(context) > 3000:\n",
    "                continue\n",
    "            \n",
    "            pair = {\n",
    "                \"query\": question,\n",
    "                \"document\": context,\n",
    "                \"query_type\": \"question\",\n",
    "                \"doc_type\": \"context\",\n",
    "                \"source\": \"squad\",\n",
    "                \"source_id\": item.get(\"id\", \"\"),\n",
    "            }\n",
    "            \n",
    "            current_chunk.append(pair)\n",
    "            total_pairs += 1\n",
    "            \n",
    "            # Save chunk when limit reached\n",
    "            if len(current_chunk) >= CHUNK_SIZE:\n",
    "                chunk_num += 1\n",
    "                chunk_file = output_dir / f\"squad_chunk_{chunk_num:03d}.jsonl\"\n",
    "                \n",
    "                with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                    for p in current_chunk:\n",
    "                        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "                print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "                current_chunk = []\n",
    "        \n",
    "        # Save remaining pairs\n",
    "        if current_chunk:\n",
    "            chunk_num += 1\n",
    "            chunk_file = output_dir / f\"squad_chunk_{chunk_num:03d}.jsonl\"\n",
    "            \n",
    "            with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                for p in current_chunk:\n",
    "                    f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "            print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "        \n",
    "        print(f\"\\n‚úì Processed {total_pairs:,} SQuAD pairs in {chunk_num} chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Error processing SQuAD: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Natural Questions Dataset (307K pairs)\n",
    "\n",
    "**Natural Questions** contains real Google search queries with passage answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Natural Questions paired data already exists\n",
    "nq_files = sorted(glob.glob(str(output_dir / \"natural_questions_chunk_*.jsonl\")))\n",
    "\n",
    "if SKIP_IF_EXISTS and nq_files:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úì Natural Questions paired data already exists!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nFound {len(nq_files)} chunk files\")\n",
    "    total_pairs = sum(sum(1 for _ in open(f)) for f in nq_files)\n",
    "    print(f\"Total pairs: {total_pairs:,}\")\n",
    "    \n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Processing Natural Questions Dataset\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Load Natural Questions\n",
    "        dataset = load_dataset(\n",
    "            \"natural_questions\",\n",
    "            split=\"train\",\n",
    "            cache_dir=str(cache_dir)\n",
    "        )\n",
    "        \n",
    "        chunk_num = 0\n",
    "        current_chunk = []\n",
    "        total_pairs = 0\n",
    "        \n",
    "        for item in tqdm(dataset, desc=\"Processing Natural Questions\"):\n",
    "            question = item.get(\"question\", {}).get(\"text\", \"\")\n",
    "            \n",
    "            # Extract passage from annotations\n",
    "            annotations = item.get(\"annotations\", [])\n",
    "            if not annotations:\n",
    "                continue\n",
    "            \n",
    "            long_answer = annotations[0].get(\"long_answer\", {})\n",
    "            if not long_answer:\n",
    "                continue\n",
    "            \n",
    "            # Get passage text\n",
    "            document_tokens = item.get(\"document\", {}).get(\"tokens\", [])\n",
    "            start_token = long_answer.get(\"start_token\", 0)\n",
    "            end_token = long_answer.get(\"end_token\", 0)\n",
    "            \n",
    "            if start_token >= end_token:\n",
    "                continue\n",
    "            \n",
    "            passage = \" \".join(\n",
    "                [t.get(\"token\", \"\") for t in document_tokens[start_token:end_token]]\n",
    "            )\n",
    "            \n",
    "            # Quality filters\n",
    "            if not question or not passage:\n",
    "                continue\n",
    "            if len(passage) < 50 or len(passage) > 3000:\n",
    "                continue\n",
    "            \n",
    "            pair = {\n",
    "                \"query\": question,\n",
    "                \"document\": passage,\n",
    "                \"query_type\": \"question\",\n",
    "                \"doc_type\": \"passage\",\n",
    "                \"source\": \"natural_questions\",\n",
    "            }\n",
    "            \n",
    "            current_chunk.append(pair)\n",
    "            total_pairs += 1\n",
    "            \n",
    "            # Save chunk when limit reached\n",
    "            if len(current_chunk) >= CHUNK_SIZE:\n",
    "                chunk_num += 1\n",
    "                chunk_file = output_dir / f\"natural_questions_chunk_{chunk_num:03d}.jsonl\"\n",
    "                \n",
    "                with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                    for p in current_chunk:\n",
    "                        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "                print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "                current_chunk = []\n",
    "        \n",
    "        # Save remaining pairs\n",
    "        if current_chunk:\n",
    "            chunk_num += 1\n",
    "            chunk_file = output_dir / f\"natural_questions_chunk_{chunk_num:03d}.jsonl\"\n",
    "            \n",
    "            with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                for p in current_chunk:\n",
    "                    f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "            print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "        \n",
    "        print(f\"\\n‚úì Processed {total_pairs:,} Natural Questions pairs in {chunk_num} chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Error processing Natural Questions: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ELI5 Dataset (272K Question-Answer pairs)\n",
    "\n",
    "**ELI5 (Explain Like I'm Five)** contains Reddit questions with detailed explanatory answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if ELI5 paired data already exists\n",
    "eli5_files = sorted(glob.glob(str(output_dir / \"eli5_chunk_*.jsonl\")))\n",
    "\n",
    "if SKIP_IF_EXISTS and eli5_files:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úì ELI5 paired data already exists!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nFound {len(eli5_files)} chunk files\")\n",
    "    total_pairs = sum(sum(1 for _ in open(f)) for f in eli5_files)\n",
    "    print(f\"Total pairs: {total_pairs:,}\")\n",
    "    \n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Processing ELI5 Dataset\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Load ELI5 dataset\n",
    "        dataset = load_dataset(\n",
    "            \"eli5\",\n",
    "            split=\"train\",\n",
    "            cache_dir=str(cache_dir)\n",
    "        )\n",
    "        \n",
    "        chunk_num = 0\n",
    "        current_chunk = []\n",
    "        total_pairs = 0\n",
    "        \n",
    "        for item in tqdm(dataset, desc=\"Processing ELI5\"):\n",
    "            question = item.get(\"title\", \"\")\n",
    "            answers = item.get(\"answers\", {}).get(\"text\", [])\n",
    "            \n",
    "            # Use the top answer if available\n",
    "            if not answers:\n",
    "                continue\n",
    "            \n",
    "            answer = answers[0]  # Top-voted answer\n",
    "            \n",
    "            # Quality filters\n",
    "            if not question or not answer:\n",
    "                continue\n",
    "            if len(answer) < 50 or len(answer) > 3000:\n",
    "                continue\n",
    "            \n",
    "            pair = {\n",
    "                \"query\": question,\n",
    "                \"document\": answer,\n",
    "                \"query_type\": \"question\",\n",
    "                \"doc_type\": \"explanation\",\n",
    "                \"source\": \"eli5\",\n",
    "            }\n",
    "            \n",
    "            current_chunk.append(pair)\n",
    "            total_pairs += 1\n",
    "            \n",
    "            # Save chunk when limit reached\n",
    "            if len(current_chunk) >= CHUNK_SIZE:\n",
    "                chunk_num += 1\n",
    "                chunk_file = output_dir / f\"eli5_chunk_{chunk_num:03d}.jsonl\"\n",
    "                \n",
    "                with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                    for p in current_chunk:\n",
    "                        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "                print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "                current_chunk = []\n",
    "        \n",
    "        # Save remaining pairs\n",
    "        if current_chunk:\n",
    "            chunk_num += 1\n",
    "            chunk_file = output_dir / f\"eli5_chunk_{chunk_num:03d}.jsonl\"\n",
    "            \n",
    "            with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                for p in current_chunk:\n",
    "                    f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "            print(f\"Saved chunk {chunk_num}: {len(current_chunk):,} pairs\")\n",
    "        \n",
    "        print(f\"\\n‚úì Processed {total_pairs:,} ELI5 pairs in {chunk_num} chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Error processing ELI5: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Overall Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PRE-TRAINING DATASET STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def count_dataset_pairs(pattern, name):\n",
    "    files = sorted(glob.glob(str(output_dir / pattern)))\n",
    "    if files:\n",
    "        total = sum(sum(1 for _ in open(f)) for f in files)\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"     Files: {len(files)}\")\n",
    "        print(f\"     Pairs: {total:,}\")\n",
    "        return total\n",
    "    return 0\n",
    "\n",
    "s2orc_pairs = count_dataset_pairs(\"s2orc_chunk_*.jsonl\", \"1. S2ORC (Title-Abstract)\")\n",
    "wiki_ans_pairs = count_dataset_pairs(\"wikianswers_chunk_*.jsonl\", \"2. WikiAnswers (Question-Question)\")\n",
    "gooaq_pairs = count_dataset_pairs(\"gooaq_chunk_*.jsonl\", \"3. GOOAQ (Question-Answer)\")\n",
    "squad_pairs = count_dataset_pairs(\"squad_chunk_*.jsonl\", \"4. SQuAD (Question-Context)\")\n",
    "nq_pairs = count_dataset_pairs(\"natural_questions_chunk_*.jsonl\", \"5. Natural Questions\")\n",
    "eli5_pairs = count_dataset_pairs(\"eli5_chunk_*.jsonl\", \"6. ELI5 (Question-Explanation)\")\n",
    "\n",
    "total_pairs = s2orc_pairs + wiki_ans_pairs + gooaq_pairs + squad_pairs + nq_pairs + eli5_pairs\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"TOTAL PRE-TRAINING PAIRS: {total_pairs:,}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if total_pairs > 0:\n",
    "    print(\"\\nDataset Composition:\")\n",
    "    if s2orc_pairs > 0:\n",
    "        print(f\"  S2ORC: {s2orc_pairs/total_pairs*100:.1f}%\")\n",
    "    if wiki_ans_pairs > 0:\n",
    "        print(f\"  WikiAnswers: {wiki_ans_pairs/total_pairs*100:.1f}%\")\n",
    "    if gooaq_pairs > 0:\n",
    "        print(f\"  GOOAQ: {gooaq_pairs/total_pairs*100:.1f}%\")\n",
    "    if squad_pairs > 0:\n",
    "        print(f\"  SQuAD: {squad_pairs/total_pairs*100:.1f}%\")\n",
    "    if nq_pairs > 0:\n",
    "        print(f\"  Natural Questions: {nq_pairs/total_pairs*100:.1f}%\")\n",
    "    if eli5_pairs > 0:\n",
    "        print(f\"  ELI5: {eli5_pairs/total_pairs*100:.1f}%\")\n",
    "\n",
    "# Sample a pair\n",
    "all_files = sorted(glob.glob(str(output_dir / \"*.jsonl\")))\n",
    "if all_files:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAMPLE PAIR:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    with open(all_files[0], 'r', encoding='utf-8') as f:\n",
    "        sample = json.loads(f.readline())\n",
    "    \n",
    "    print(f\"\\nSource: {sample['source']}\")\n",
    "    print(f\"Query type: {sample['query_type']}\")\n",
    "    print(f\"Document type: {sample['doc_type']}\")\n",
    "    print(f\"\\nQuery: {sample['query'][:200]}...\")\n",
    "    print(f\"Document: {sample['document'][:300]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook downloads and processes the pre-training datasets specified in the research paper:\n",
    "\n",
    "**Target datasets (from paper):**\n",
    "- ‚úì S2ORC: 41.7M (Title, Abstract) pairs\n",
    "- ‚úì WikiAnswers: 77.4M duplicate question pairs\n",
    "- ‚úì GOOAQ: 3.0M (Question, Answer) pairs\n",
    "- ‚úì SQuAD: 87K (Question, Context) pairs\n",
    "- ‚úì Natural Questions: 307K (Question, Passage) pairs\n",
    "- ‚úì ELI5: 272K (Question, Explanation) pairs\n",
    "\n",
    "**Total target**: ~122M training pairs\n",
    "\n",
    "**Output structure:**\n",
    "```\n",
    "dataset/pretraining/\n",
    "‚îú‚îÄ‚îÄ s2orc_chunk_*.jsonl\n",
    "‚îú‚îÄ‚îÄ wikianswers_chunk_*.jsonl\n",
    "‚îú‚îÄ‚îÄ gooaq_chunk_*.jsonl\n",
    "‚îú‚îÄ‚îÄ squad_chunk_*.jsonl\n",
    "‚îú‚îÄ‚îÄ natural_questions_chunk_*.jsonl\n",
    "‚îî‚îÄ‚îÄ eli5_chunk_*.jsonl\n",
    "```\n",
    "\n",
    "**Next steps:**\n",
    "1. Hard negatives mining (notebook 04)\n",
    "2. MS MARCO fine-tuning data (notebook 05)\n",
    "3. Model pre-training with these datasets\n",
    "\n",
    "**Note**: Some datasets (especially S2ORC and WikiAnswers) are very large and may require:\n",
    "- Significant disk space (100GB+)\n",
    "- Long download/processing time (hours to days)\n",
    "- Alternative download methods or sampling strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

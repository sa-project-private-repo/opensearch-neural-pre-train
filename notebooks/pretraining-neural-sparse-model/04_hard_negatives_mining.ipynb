{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Negatives Mining\n",
    "\n",
    "This notebook implements hard negatives mining as specified in the research paper:\n",
    "\n",
    "**\"Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers\"**\n",
    "\n",
    "## Hard Negatives Mining Strategy\n",
    "\n",
    "The paper uses a two-step approach:\n",
    "\n",
    "1. **BM25 Retrieval**: For each query, retrieve top-M (typically M=1000) candidate documents\n",
    "2. **Consistency Filtering**: Keep only training samples where the positive document appears in top-10 BM25 results\n",
    "3. **Sampling**: From the remaining candidates, sample N (typically N=7) hard negatives per query\n",
    "\n",
    "## Benefits\n",
    "\n",
    "- Improves model's ability to distinguish between relevant and near-relevant documents\n",
    "- Focuses training on challenging examples\n",
    "- Increases training effectiveness without increasing dataset size\n",
    "\n",
    "## Implementation\n",
    "\n",
    "We'll use:\n",
    "- **BM25** from `rank-bm25` library for initial retrieval\n",
    "- **Consistency filtering** to ensure positive document is retrievable\n",
    "- **Random sampling** for hard negatives selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /home/west/Documents/cursor-workspace/opensearch-neural-pre-train/.venv/lib/python3.12/site-packages (from nltk) (4.66.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Negatives Mining Configuration:\n",
      "  Top-M candidates: 1000\n",
      "  Consistency filter (top-K): 10\n",
      "  Hard negatives per query: 7\n",
      "\n",
      "✓ Output directory: ../../dataset/hard_negatives\n"
     ]
    }
   ],
   "source": [
    "# Input directories\n",
    "paired_data_dir = Path(\"../../dataset/paired_data\")\n",
    "pretraining_data_dir = Path(\"../../dataset/pretraining\")\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path(\"../../dataset/hard_negatives\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Mining parameters (from paper)\n",
    "TOP_M = 1000  # Number of candidates to retrieve\n",
    "TOP_K_FILTER = 10  # Consistency filter: positive must be in top-K\n",
    "NUM_NEGATIVES = 7  # Number of hard negatives to sample\n",
    "CHUNK_SIZE = 100000  # Pairs per output file\n",
    "SKIP_IF_EXISTS = True\n",
    "\n",
    "print(\"Hard Negatives Mining Configuration:\")\n",
    "print(f\"  Top-M candidates: {TOP_M}\")\n",
    "print(f\"  Consistency filter (top-K): {TOP_K_FILTER}\")\n",
    "print(f\"  Hard negatives per query: {NUM_NEGATIVES}\")\n",
    "print(f\"\\n✓ Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install BM25 Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ rank_bm25 already installed\n"
     ]
    }
   ],
   "source": [
    "# Install rank-bm25 if not already installed\n",
    "try:\n",
    "    from rank_bm25 import BM25Okapi\n",
    "    print(\"✓ rank_bm25 already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing rank_bm25...\")\n",
    "    %pip install rank-bm25\n",
    "    from rank_bm25 import BM25Okapi\n",
    "    print(\"✓ rank_bm25 installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Hard Negatives Miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/west/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ HardNegativesMiner class defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK data if needed\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "class HardNegativesMiner:\n",
    "    \"\"\"Mine hard negatives using BM25 and consistency filtering.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        top_m: int = 1000,\n",
    "        top_k_filter: int = 10,\n",
    "        num_negatives: int = 7,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize hard negatives miner.\n",
    "        \n",
    "        Args:\n",
    "            top_m: Number of top candidates to retrieve with BM25\n",
    "            top_k_filter: Consistency filter threshold (positive must be in top-K)\n",
    "            num_negatives: Number of hard negatives to sample per query\n",
    "        \"\"\"\n",
    "        self.top_m = top_m\n",
    "        self.top_k_filter = top_k_filter\n",
    "        self.num_negatives = num_negatives\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize text for BM25.\"\"\"\n",
    "        try:\n",
    "            return word_tokenize(text.lower())\n",
    "        except:\n",
    "            # Fallback to simple split\n",
    "            return text.lower().split()\n",
    "    \n",
    "    def build_bm25_index(self, documents: List[str]) -> BM25Okapi:\n",
    "        \"\"\"Build BM25 index from documents.\"\"\"\n",
    "        print(f\"Tokenizing {len(documents):,} documents...\")\n",
    "        tokenized_docs = [self.tokenize(doc) for doc in tqdm(documents, desc=\"Tokenizing\")]\n",
    "        \n",
    "        print(\"Building BM25 index...\")\n",
    "        bm25 = BM25Okapi(tokenized_docs)\n",
    "        print(\"✓ BM25 index built\")\n",
    "        \n",
    "        return bm25\n",
    "    \n",
    "    def mine_hard_negatives(\n",
    "        self,\n",
    "        query: str,\n",
    "        positive_doc: str,\n",
    "        all_documents: List[str],\n",
    "        bm25_index: BM25Okapi,\n",
    "    ) -> Tuple[bool, List[int]]:\n",
    "        \"\"\"\n",
    "        Mine hard negatives for a single query.\n",
    "        \n",
    "        Args:\n",
    "            query: Query text\n",
    "            positive_doc: Positive (relevant) document\n",
    "            all_documents: List of all candidate documents\n",
    "            bm25_index: Pre-built BM25 index\n",
    "        \n",
    "        Returns:\n",
    "            (passes_filter, hard_negative_indices)\n",
    "            passes_filter: True if positive doc is in top-K\n",
    "            hard_negative_indices: Indices of sampled hard negatives\n",
    "        \"\"\"\n",
    "        # Tokenize query\n",
    "        tokenized_query = self.tokenize(query)\n",
    "        \n",
    "        # Get BM25 scores for all documents\n",
    "        scores = bm25_index.get_scores(tokenized_query)\n",
    "        \n",
    "        # Get top-M document indices\n",
    "        top_m_indices = np.argsort(scores)[::-1][:self.top_m]\n",
    "        \n",
    "        # Find positive document index\n",
    "        try:\n",
    "            positive_idx = all_documents.index(positive_doc)\n",
    "        except ValueError:\n",
    "            # Positive doc not in corpus (shouldn't happen, but handle gracefully)\n",
    "            return False, []\n",
    "        \n",
    "        # Consistency filtering: check if positive is in top-K\n",
    "        top_k_indices = top_m_indices[:self.top_k_filter]\n",
    "        passes_filter = positive_idx in top_k_indices\n",
    "        \n",
    "        if not passes_filter:\n",
    "            return False, []\n",
    "        \n",
    "        # Sample hard negatives from top-M (excluding positive)\n",
    "        candidate_negatives = [idx for idx in top_m_indices if idx != positive_idx]\n",
    "        \n",
    "        # Sample N hard negatives\n",
    "        num_to_sample = min(self.num_negatives, len(candidate_negatives))\n",
    "        hard_negative_indices = random.sample(candidate_negatives, num_to_sample)\n",
    "        \n",
    "        return True, hard_negative_indices\n",
    "\n",
    "print(\"✓ HardNegativesMiner class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Paired Data and Build Document Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Loading Korean Wikipedia paired data (sample)\n",
      "================================================================================\n",
      "Loading from 12 files matching ko_wiki_title_summary_*.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 10,000 paired samples\n",
      "\n",
      "✓ Sample size: 10,000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "def load_paired_data(data_dir: Path, pattern: str, max_samples: int = None) -> List[Dict]:\n",
    "    \"\"\"Load paired data from JSONL files.\"\"\"\n",
    "    files = sorted(glob.glob(str(data_dir / pattern)))\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"⚠ No files found matching pattern: {pattern}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Loading from {len(files)} files matching {pattern}...\")\n",
    "    \n",
    "    data = []\n",
    "    for file_path in tqdm(files, desc=\"Loading files\"):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data.append(json.loads(line))\n",
    "                if max_samples and len(data) >= max_samples:\n",
    "                    break\n",
    "        \n",
    "        if max_samples and len(data) >= max_samples:\n",
    "            break\n",
    "    \n",
    "    print(f\"✓ Loaded {len(data):,} paired samples\")\n",
    "    return data\n",
    "\n",
    "# For demonstration, load a subset of Korean Wikipedia data\n",
    "# In production, you'd process all datasets\n",
    "print(\"=\" * 80)\n",
    "print(\"Loading Korean Wikipedia paired data (sample)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load sample data (first 10K for demo)\n",
    "ko_wiki_data = load_paired_data(\n",
    "    paired_data_dir,\n",
    "    \"ko_wiki_title_summary_*.jsonl\",\n",
    "    max_samples=10000  # Limit for demo; remove in production\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Sample size: {len(ko_wiki_data):,} pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build BM25 Index\n",
    "\n",
    "Build a BM25 index over all documents in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Building BM25 index\n",
      "================================================================================\n",
      "Documents in corpus: 10,000\n",
      "Tokenizing 10,000 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 10000/10000 [00:01<00:00, 8943.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building BM25 index...\n",
      "✓ BM25 index built\n",
      "\n",
      "✓ BM25 index ready\n"
     ]
    }
   ],
   "source": [
    "if len(ko_wiki_data) > 0:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Building BM25 index\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Extract all documents\n",
    "    all_documents = [item[\"document\"] for item in ko_wiki_data]\n",
    "    \n",
    "    print(f\"Documents in corpus: {len(all_documents):,}\")\n",
    "    \n",
    "    # Initialize miner\n",
    "    miner = HardNegativesMiner(\n",
    "        top_m=TOP_M,\n",
    "        top_k_filter=TOP_K_FILTER,\n",
    "        num_negatives=NUM_NEGATIVES,\n",
    "    )\n",
    "    \n",
    "    # Build BM25 index\n",
    "    bm25_index = miner.build_bm25_index(all_documents)\n",
    "    \n",
    "    print(\"\\n✓ BM25 index ready\")\n",
    "else:\n",
    "    print(\"⚠ No data loaded, skipping BM25 index building\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mine Hard Negatives\n",
    "\n",
    "For each query-document pair, mine hard negatives using BM25 and consistency filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Mining hard negatives\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mining hard negatives: 100%|██████████| 10000/10000 [00:17<00:00, 574.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Mining complete\n",
      "  Passed consistency filter: 3,253 (32.5%)\n",
      "  Failed consistency filter: 6,747 (67.5%)\n",
      "  Total results: 3,253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if len(ko_wiki_data) > 0 and 'bm25_index' in locals():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Mining hard negatives\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = []\n",
    "    passed_filter_count = 0\n",
    "    failed_filter_count = 0\n",
    "    \n",
    "    for item in tqdm(ko_wiki_data, desc=\"Mining hard negatives\"):\n",
    "        query = item[\"query\"]\n",
    "        positive_doc = item[\"document\"]\n",
    "        \n",
    "        # Mine hard negatives\n",
    "        passes_filter, hard_neg_indices = miner.mine_hard_negatives(\n",
    "            query=query,\n",
    "            positive_doc=positive_doc,\n",
    "            all_documents=all_documents,\n",
    "            bm25_index=bm25_index,\n",
    "        )\n",
    "        \n",
    "        if passes_filter:\n",
    "            passed_filter_count += 1\n",
    "            \n",
    "            # Get hard negative documents\n",
    "            hard_negatives = [all_documents[idx] for idx in hard_neg_indices]\n",
    "            \n",
    "            result = {\n",
    "                \"query\": query,\n",
    "                \"positive_doc\": positive_doc,\n",
    "                \"hard_negatives\": hard_negatives,\n",
    "                \"query_type\": item.get(\"query_type\", \"\"),\n",
    "                \"doc_type\": item.get(\"doc_type\", \"\"),\n",
    "                \"source\": item.get(\"source\", \"\"),\n",
    "                \"language\": item.get(\"language\", \"\"),\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "        else:\n",
    "            failed_filter_count += 1\n",
    "    \n",
    "    print(f\"\\n✓ Mining complete\")\n",
    "    print(f\"  Passed consistency filter: {passed_filter_count:,} ({passed_filter_count/len(ko_wiki_data)*100:.1f}%)\")\n",
    "    print(f\"  Failed consistency filter: {failed_filter_count:,} ({failed_filter_count/len(ko_wiki_data)*100:.1f}%)\")\n",
    "    print(f\"  Total results: {len(results):,}\")\n",
    "else:\n",
    "    print(\"⚠ Skipping hard negatives mining (no data or index)\")\n",
    "    results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Saving hard negatives\n",
      "================================================================================\n",
      "Saved chunk 1: 3,253 items to ko_wiki_hard_negatives_chunk_001.jsonl\n",
      "\n",
      "✓ Saved 3,253 hard negatives in 1 chunk(s)\n"
     ]
    }
   ],
   "source": [
    "if len(results) > 0:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Saving hard negatives\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Save in chunks\n",
    "    chunk_num = 0\n",
    "    \n",
    "    for i in range(0, len(results), CHUNK_SIZE):\n",
    "        chunk = results[i:i + CHUNK_SIZE]\n",
    "        chunk_num += 1\n",
    "        \n",
    "        output_file = output_dir / f\"ko_wiki_hard_negatives_chunk_{chunk_num:03d}.jsonl\"\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for item in chunk:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        print(f\"Saved chunk {chunk_num}: {len(chunk):,} items to {output_file.name}\")\n",
    "    \n",
    "    print(f\"\\n✓ Saved {len(results):,} hard negatives in {chunk_num} chunk(s)\")\n",
    "else:\n",
    "    print(\"⚠ No results to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE HARD NEGATIVES\n",
      "================================================================================\n",
      "\n",
      "Query: 지미 카터\n",
      "\n",
      "Positive document:\n",
      "  제임스 얼 “지미” 카터 주니어(, 1924년 10월 1일~2024년 12월 29일)는 미국의 제39대 대통령 (1977-81)을 지낸 미국의 정치인이다. 민주당 소속으로 1963년부터 1967년까지 조지아주 상원 의원, 1971년부터 1975년까지 조지아주의 76대 주지사을 지냈다. 카터는 100세까지 산 최초의 대통령으로 미국 역사상 가장 장수한 대통령...\n",
      "\n",
      "Hard negatives (7):\n",
      "\n",
      "  1. 동명여자고등학교(東明女子高等學校)는 대한민국 서울특별시 은평구 대조동에 위치한 사립 고등학교이다. 학교 연혁 1921년 6월 3일 : 현 서대문구 천연동에 향상여자기예학교 개교 1936년 3월 7일 : 향상여자실업학교로 명칭 변경 1945년 11월 26일 : 재단법인 ...\n",
      "\n",
      "  2. 개포고등학교(開浦高等學校)는 대한민국 서울특별시 강남구 개포동에 있는 공립 고등학교이다. 학교 연혁 1987년 1월 12일 : 개포고등학교 설립 인가 1987년 3월 1일 : 박노학 초대 교장 취임 1987년 3월 4일 : 제1회 입학 856명(남 428명, 여 428...\n",
      "\n",
      "  3. 쌍떡잎식물(雙―植物, Magnoliopsida, )은 속씨식물 중 떡잎이 두 장 나는 것을 말하며, 쌍떡잎식물강으로 분류된다. 쌍자엽식물(雙子葉植物)로도 부른다. 쌍떡잎식물은 약 199,350 여 종이 존재한다....\n",
      "\n",
      "  ... and 4 more\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if len(results) > 0:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SAMPLE HARD NEGATIVES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    sample = results[0]\n",
    "    \n",
    "    print(f\"\\nQuery: {sample['query']}\")\n",
    "    print(f\"\\nPositive document:\")\n",
    "    print(f\"  {sample['positive_doc'][:200]}...\")\n",
    "    \n",
    "    print(f\"\\nHard negatives ({len(sample['hard_negatives'])}):\")\n",
    "    for i, neg in enumerate(sample['hard_negatives'][:3], 1):\n",
    "        print(f\"\\n  {i}. {neg[:150]}...\")\n",
    "    \n",
    "    if len(sample['hard_negatives']) > 3:\n",
    "        print(f\"\\n  ... and {len(sample['hard_negatives']) - 3} more\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HARD NEGATIVES STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Total training samples: 3,253\n",
      "Hard negatives per sample:\n",
      "  Mean: 7.00\n",
      "  Min: 7\n",
      "  Max: 7\n",
      "\n",
      "Total training examples (pos + negs): 26,024\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if len(results) > 0:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"HARD NEGATIVES STATISTICS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    num_hard_negs = [len(item['hard_negatives']) for item in results]\n",
    "    \n",
    "    print(f\"\\nTotal training samples: {len(results):,}\")\n",
    "    print(f\"Hard negatives per sample:\")\n",
    "    print(f\"  Mean: {np.mean(num_hard_negs):.2f}\")\n",
    "    print(f\"  Min: {np.min(num_hard_negs)}\")\n",
    "    print(f\"  Max: {np.max(num_hard_negs)}\")\n",
    "    \n",
    "    # Total training examples (1 positive + N negatives per query)\n",
    "    total_examples = sum(1 + len(item['hard_negatives']) for item in results)\n",
    "    print(f\"\\nTotal training examples (pos + negs): {total_examples:,}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements hard negatives mining as specified in the research paper:\n",
    "\n",
    "**Mining Strategy:**\n",
    "1. ✓ BM25 retrieval of top-M candidates (M=1000)\n",
    "2. ✓ Consistency filtering (positive must be in top-10)\n",
    "3. ✓ Sample N=7 hard negatives per query\n",
    "\n",
    "**Output Format:**\n",
    "```json\n",
    "{\n",
    "  \"query\": \"...\",\n",
    "  \"positive_doc\": \"...\",\n",
    "  \"hard_negatives\": [\"...\", \"...\", ...],\n",
    "  \"query_type\": \"title\",\n",
    "  \"doc_type\": \"summary\",\n",
    "  \"source\": \"ko_wiki\",\n",
    "  \"language\": \"ko\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Next Steps:**\n",
    "1. Process all datasets (Wikipedia, NamuWiki, pre-training datasets)\n",
    "2. Prepare MS MARCO fine-tuning data (notebook 05)\n",
    "3. Train model with:\n",
    "   - Positive-negative pairs\n",
    "   - Hard negatives for improved discrimination\n",
    "   - IDF-aware penalty\n",
    "\n",
    "**Note**: This demo processes a small sample (10K pairs). In production:\n",
    "- Process all paired data sources\n",
    "- Use larger BM25 corpus\n",
    "- Consider distributed processing for large datasets\n",
    "- Adjust TOP_M based on corpus size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

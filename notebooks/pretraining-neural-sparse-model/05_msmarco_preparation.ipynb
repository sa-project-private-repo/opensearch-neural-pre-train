{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS MARCO Dataset Preparation\n",
    "\n",
    "This notebook downloads and prepares the **MS MARCO** (Microsoft MAchine Reading COmprehension) dataset for fine-tuning.\n",
    "\n",
    "## MS MARCO Passage Ranking Dataset\n",
    "\n",
    "**Statistics (from paper):**\n",
    "- **8.8M passages**: Document corpus for retrieval\n",
    "- **502K queries**: Training queries with relevance judgments\n",
    "- **Task**: Passage ranking for information retrieval\n",
    "\n",
    "## Dataset Structure\n",
    "\n",
    "MS MARCO provides:\n",
    "1. **Collection**: 8.8M passages (documents)\n",
    "2. **Queries**: Training, dev, and test queries\n",
    "3. **Qrels**: Query-passage relevance judgments\n",
    "4. **Triples**: (query, positive_passage, negative_passage) training triples\n",
    "\n",
    "## Usage in Paper\n",
    "\n",
    "The paper uses MS MARCO for:\n",
    "1. **Fine-tuning**: After pre-training on large datasets\n",
    "2. **Evaluation**: Zero-shot performance on BEIR benchmark\n",
    "3. **Comparison**: Against SPLADE and other baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import gzip\n",
    "from typing import Dict, List\n",
    "from tqdm import tqdm\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Output directory: ../../dataset/msmarco\n",
      "‚úì Cache directory: ../../dataset/msmarco/cache\n"
     ]
    }
   ],
   "source": [
    "# Output directory\n",
    "output_dir = Path(\"../../dataset/msmarco\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cache directory for downloads\n",
    "cache_dir = output_dir / \"cache\"\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Processing settings\n",
    "CHUNK_SIZE = 100000  # 100K items per file\n",
    "SKIP_IF_EXISTS = True\n",
    "\n",
    "print(f\"‚úì Output directory: {output_dir}\")\n",
    "print(f\"‚úì Cache directory: {cache_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download MS MARCO using HuggingFace Datasets\n",
    "\n",
    "We'll use the HuggingFace `datasets` library which provides easy access to MS MARCO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Downloading MS MARCO Dataset\n",
      "================================================================================\n",
      "\n",
      "‚¨á This will download ~40GB of data on first run...\n",
      "‚è≥ Download and processing may take several hours...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Check if dataset is already downloaded\n",
    "import glob\n",
    "\n",
    "passages_file = output_dir / \"passages.jsonl\"\n",
    "queries_file = output_dir / \"queries.jsonl\"\n",
    "triples_files = sorted(glob.glob(str(output_dir / \"triples_chunk_*.jsonl\")))\n",
    "\n",
    "if SKIP_IF_EXISTS and passages_file.exists() and queries_file.exists() and triples_files:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úì MS MARCO dataset already downloaded and processed!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nExisting files:\")\n",
    "    print(f\"  - {passages_file.name}\")\n",
    "    print(f\"  - {queries_file.name}\")\n",
    "    print(f\"  - {len(triples_files)} training triples chunk files\")\n",
    "    print(\"\\nüí° Set SKIP_IF_EXISTS = False to force re-download\")\n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Downloading MS MARCO Dataset\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n‚¨á This will download ~40GB of data on first run...\")\n",
    "    print(\"‚è≥ Download and processing may take several hours...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load MS MARCO Passages (8.8M documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Passages file exists: ../../dataset/msmarco/passages.jsonl\n"
     ]
    }
   ],
   "source": [
    "if not (SKIP_IF_EXISTS and passages_file.exists()):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Processing MS MARCO Passages\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Load passages corpus\n",
    "        print(\"\\nLoading passages corpus from HuggingFace...\")\n",
    "        passages_dataset = load_dataset(\n",
    "            \"ms_marco\",\n",
    "            \"v2.1\",\n",
    "            split=\"corpus\",\n",
    "            cache_dir=str(cache_dir)\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì Loaded {len(passages_dataset):,} passages\")\n",
    "        \n",
    "        # Save passages\n",
    "        print(\"\\nSaving passages to JSONL...\")\n",
    "        with open(passages_file, 'w', encoding='utf-8') as f:\n",
    "            for passage in tqdm(passages_dataset, desc=\"Saving passages\"):\n",
    "                item = {\n",
    "                    \"id\": passage.get(\"pid\", \"\"),\n",
    "                    \"text\": passage.get(\"passage\", \"\"),\n",
    "                }\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        print(f\"‚úì Saved to {passages_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Error loading passages: {e}\")\n",
    "        print(\"   Trying alternative method...\")\n",
    "        \n",
    "        # Alternative: Direct download from MS MARCO website\n",
    "        try:\n",
    "            print(\"\\nDownloading passages from MS MARCO website...\")\n",
    "            passages_url = \"https://msmarco.blob.core.windows.net/msmarcoranking/collection.tsv\"\n",
    "            \n",
    "            response = requests.get(passages_url, stream=True)\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            passages_tsv = cache_dir / \"collection.tsv\"\n",
    "            \n",
    "            with open(passages_tsv, 'wb') as f:\n",
    "                with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Downloading\") as pbar:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                        pbar.update(len(chunk))\n",
    "            \n",
    "            print(\"‚úì Downloaded passages\")\n",
    "            \n",
    "            # Convert TSV to JSONL\n",
    "            print(\"\\nConverting to JSONL...\")\n",
    "            with open(passages_tsv, 'r', encoding='utf-8') as fin:\n",
    "                with open(passages_file, 'w', encoding='utf-8') as fout:\n",
    "                    for line in tqdm(fin, desc=\"Converting\"):\n",
    "                        parts = line.strip().split('\\t')\n",
    "                        if len(parts) == 2:\n",
    "                            pid, passage = parts\n",
    "                            item = {\"id\": pid, \"text\": passage}\n",
    "                            fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "            print(f\"‚úì Saved to {passages_file}\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"\\n‚úó Error with alternative method: {e2}\")\n",
    "            print(\"   You may need to manually download MS MARCO from:\")\n",
    "            print(\"   https://microsoft.github.io/msmarco/\")\n",
    "else:\n",
    "    print(f\"‚úì Passages file exists: {passages_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load MS MARCO Queries (502K training queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Processing MS MARCO Queries\n",
      "================================================================================\n",
      "\n",
      "Loading queries from HuggingFace...\n",
      "‚úì Loaded 808,731 queries\n",
      "\n",
      "Saving queries to JSONL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 808731/808731 [00:28<00:00, 28776.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved to ../../dataset/msmarco/queries.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not (SKIP_IF_EXISTS and queries_file.exists()):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Processing MS MARCO Queries\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Load queries\n",
    "        print(\"\\nLoading queries from HuggingFace...\")\n",
    "        queries_dataset = load_dataset(\n",
    "            \"ms_marco\",\n",
    "            \"v2.1\",\n",
    "            split=\"train\",\n",
    "            cache_dir=str(cache_dir)\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì Loaded {len(queries_dataset):,} queries\")\n",
    "        \n",
    "        # Save queries\n",
    "        print(\"\\nSaving queries to JSONL...\")\n",
    "        with open(queries_file, 'w', encoding='utf-8') as f:\n",
    "            for query in tqdm(queries_dataset, desc=\"Saving queries\"):\n",
    "                item = {\n",
    "                    \"id\": query.get(\"qid\", \"\"),\n",
    "                    \"text\": query.get(\"query\", \"\"),\n",
    "                }\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        print(f\"‚úì Saved to {queries_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Error loading queries: {e}\")\n",
    "        print(\"   Trying alternative method...\")\n",
    "        \n",
    "        # Alternative: Direct download\n",
    "        try:\n",
    "            print(\"\\nDownloading queries from MS MARCO website...\")\n",
    "            queries_url = \"https://msmarco.blob.core.windows.net/msmarcoranking/queries.train.tsv\"\n",
    "            \n",
    "            response = requests.get(queries_url, stream=True)\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            queries_tsv = cache_dir / \"queries.train.tsv\"\n",
    "            \n",
    "            with open(queries_tsv, 'wb') as f:\n",
    "                with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Downloading\") as pbar:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                        pbar.update(len(chunk))\n",
    "            \n",
    "            print(\"‚úì Downloaded queries\")\n",
    "            \n",
    "            # Convert TSV to JSONL\n",
    "            print(\"\\nConverting to JSONL...\")\n",
    "            with open(queries_tsv, 'r', encoding='utf-8') as fin:\n",
    "                with open(queries_file, 'w', encoding='utf-8') as fout:\n",
    "                    for line in tqdm(fin, desc=\"Converting\"):\n",
    "                        parts = line.strip().split('\\t')\n",
    "                        if len(parts) == 2:\n",
    "                            qid, query = parts\n",
    "                            item = {\"id\": qid, \"text\": query}\n",
    "                            fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "            print(f\"‚úì Saved to {queries_file}\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"\\n‚úó Error with alternative method: {e2}\")\n",
    "else:\n",
    "    print(f\"‚úì Queries file exists: {queries_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load MS MARCO Training Triples\n",
    "\n",
    "MS MARCO provides training triples: (query, positive_passage, negative_passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Processing MS MARCO Training Triples\n",
      "================================================================================\n",
      "\n",
      "Downloading training triples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 248/248 [00:00<00:00, 3.29MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Downloaded training triples\n",
      "\n",
      "Loading queries into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading queries: 808731it [00:00, 1239812.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 1 queries\n",
      "\n",
      "Loading passages into memory (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading passages: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 0 passages\n",
      "\n",
      "Processing training triples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing triples: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úó Error processing triples: Not a gzipped file (b'\\xef\\xbb')\n",
      "   You may need to manually download from:\n",
      "   https://microsoft.github.io/msmarco/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "triples_files = sorted(glob.glob(str(output_dir / \"triples_chunk_*.jsonl\")))\n",
    "\n",
    "if not (SKIP_IF_EXISTS and triples_files):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Processing MS MARCO Training Triples\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Download triples file\n",
    "        print(\"\\nDownloading training triples...\")\n",
    "        triples_url = \"https://msmarco.blob.core.windows.net/msmarcoranking/qidpidtriples.train.full.2.tsv.gz\"\n",
    "        \n",
    "        response = requests.get(triples_url, stream=True)\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        \n",
    "        triples_gz = cache_dir / \"qidpidtriples.train.full.2.tsv.gz\"\n",
    "        \n",
    "        with open(triples_gz, 'wb') as f:\n",
    "            with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Downloading\") as pbar:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "        \n",
    "        print(\"‚úì Downloaded training triples\")\n",
    "        \n",
    "        # Load queries and passages into memory for lookup\n",
    "        print(\"\\nLoading queries into memory...\")\n",
    "        queries_map = {}\n",
    "        with open(queries_file, 'r', encoding='utf-8') as f:\n",
    "            for line in tqdm(f, desc=\"Loading queries\"):\n",
    "                item = json.loads(line)\n",
    "                queries_map[item['id']] = item['text']\n",
    "        \n",
    "        print(f\"‚úì Loaded {len(queries_map):,} queries\")\n",
    "        \n",
    "        print(\"\\nLoading passages into memory (this may take a while)...\")\n",
    "        passages_map = {}\n",
    "        with open(passages_file, 'r', encoding='utf-8') as f:\n",
    "            for line in tqdm(f, desc=\"Loading passages\"):\n",
    "                item = json.loads(line)\n",
    "                passages_map[item['id']] = item['text']\n",
    "        \n",
    "        print(f\"‚úì Loaded {len(passages_map):,} passages\")\n",
    "        \n",
    "        # Process triples\n",
    "        print(\"\\nProcessing training triples...\")\n",
    "        chunk_num = 0\n",
    "        current_chunk = []\n",
    "        total_triples = 0\n",
    "        \n",
    "        with gzip.open(triples_gz, 'rt', encoding='utf-8') as f:\n",
    "            for line in tqdm(f, desc=\"Processing triples\"):\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) != 3:\n",
    "                    continue\n",
    "                \n",
    "                qid, pos_pid, neg_pid = parts\n",
    "                \n",
    "                # Look up texts\n",
    "                query_text = queries_map.get(qid, \"\")\n",
    "                pos_text = passages_map.get(pos_pid, \"\")\n",
    "                neg_text = passages_map.get(neg_pid, \"\")\n",
    "                \n",
    "                if not query_text or not pos_text or not neg_text:\n",
    "                    continue\n",
    "                \n",
    "                triple = {\n",
    "                    \"query\": query_text,\n",
    "                    \"positive\": pos_text,\n",
    "                    \"negative\": neg_text,\n",
    "                    \"qid\": qid,\n",
    "                    \"pos_pid\": pos_pid,\n",
    "                    \"neg_pid\": neg_pid,\n",
    "                }\n",
    "                \n",
    "                current_chunk.append(triple)\n",
    "                total_triples += 1\n",
    "                \n",
    "                # Save chunk\n",
    "                if len(current_chunk) >= CHUNK_SIZE:\n",
    "                    chunk_num += 1\n",
    "                    chunk_file = output_dir / f\"triples_chunk_{chunk_num:03d}.jsonl\"\n",
    "                    \n",
    "                    with open(chunk_file, 'w', encoding='utf-8') as fout:\n",
    "                        for item in current_chunk:\n",
    "                            fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "                    \n",
    "                    print(f\"\\nSaved chunk {chunk_num}: {len(current_chunk):,} triples\")\n",
    "                    current_chunk = []\n",
    "        \n",
    "        # Save remaining\n",
    "        if current_chunk:\n",
    "            chunk_num += 1\n",
    "            chunk_file = output_dir / f\"triples_chunk_{chunk_num:03d}.jsonl\"\n",
    "            \n",
    "            with open(chunk_file, 'w', encoding='utf-8') as fout:\n",
    "                for item in current_chunk:\n",
    "                    fout.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "            print(f\"\\nSaved chunk {chunk_num}: {len(current_chunk):,} triples\")\n",
    "        \n",
    "        print(f\"\\n‚úì Processed {total_triples:,} training triples in {chunk_num} chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Error processing triples: {e}\")\n",
    "        print(\"   You may need to manually download from:\")\n",
    "        print(\"   https://microsoft.github.io/msmarco/\")\n",
    "else:\n",
    "    print(f\"‚úì Training triples exist: {len(triples_files)} chunk files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MS MARCO DATASET STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Passages:\n",
      "  Count: 0\n",
      "  File size: 0.00 MB\n",
      "\n",
      "Queries:\n",
      "  Count: 808,731\n",
      "  File size: 45.08 MB\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MS MARCO DATASET STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Count passages\n",
    "if passages_file.exists():\n",
    "    num_passages = sum(1 for _ in open(passages_file))\n",
    "    size_mb = os.path.getsize(passages_file) / 1024 / 1024\n",
    "    print(f\"\\nPassages:\")\n",
    "    print(f\"  Count: {num_passages:,}\")\n",
    "    print(f\"  File size: {size_mb:.2f} MB\")\n",
    "\n",
    "# Count queries\n",
    "if queries_file.exists():\n",
    "    num_queries = sum(1 for _ in open(queries_file))\n",
    "    size_mb = os.path.getsize(queries_file) / 1024 / 1024\n",
    "    print(f\"\\nQueries:\")\n",
    "    print(f\"  Count: {num_queries:,}\")\n",
    "    print(f\"  File size: {size_mb:.2f} MB\")\n",
    "\n",
    "# Count triples\n",
    "triples_files = sorted(glob.glob(str(output_dir / \"triples_chunk_*.jsonl\")))\n",
    "if triples_files:\n",
    "    num_triples = sum(sum(1 for _ in open(f)) for f in triples_files)\n",
    "    total_size_mb = sum(os.path.getsize(f) for f in triples_files) / 1024 / 1024\n",
    "    print(f\"\\nTraining Triples:\")\n",
    "    print(f\"  Count: {num_triples:,}\")\n",
    "    print(f\"  Files: {len(triples_files)}\")\n",
    "    print(f\"  Total size: {total_size_mb:.2f} MB\")\n",
    "\n",
    "# Sample data\n",
    "if triples_files:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAMPLE TRAINING TRIPLE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    with open(triples_files[0], 'r', encoding='utf-8') as f:\n",
    "        sample = json.loads(f.readline())\n",
    "    \n",
    "    print(f\"\\nQuery: {sample['query']}\")\n",
    "    print(f\"\\nPositive passage: {sample['positive'][:200]}...\")\n",
    "    print(f\"\\nNegative passage: {sample['negative'][:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook downloads and prepares the **MS MARCO** dataset for fine-tuning:\n",
    "\n",
    "**Downloaded Files:**\n",
    "1. ‚úì **Passages**: 8.8M document corpus\n",
    "2. ‚úì **Queries**: 502K training queries\n",
    "3. ‚úì **Training Triples**: (query, positive, negative) pairs\n",
    "\n",
    "**Output Structure:**\n",
    "```\n",
    "dataset/msmarco/\n",
    "‚îú‚îÄ‚îÄ passages.jsonl                # 8.8M passages\n",
    "‚îú‚îÄ‚îÄ queries.jsonl                 # 502K queries\n",
    "‚îî‚îÄ‚îÄ triples_chunk_*.jsonl        # Training triples (100K per file)\n",
    "```\n",
    "\n",
    "**Data Format:**\n",
    "\n",
    "Passages:\n",
    "```json\n",
    "{\"id\": \"...\", \"text\": \"...\"}\n",
    "```\n",
    "\n",
    "Queries:\n",
    "```json\n",
    "{\"id\": \"...\", \"text\": \"...\"}\n",
    "```\n",
    "\n",
    "Training Triples:\n",
    "```json\n",
    "{\n",
    "  \"query\": \"...\",\n",
    "  \"positive\": \"...\",\n",
    "  \"negative\": \"...\",\n",
    "  \"qid\": \"...\",\n",
    "  \"pos_pid\": \"...\",\n",
    "  \"neg_pid\": \"...\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Next Steps:**\n",
    "1. Pre-train model on large-scale datasets (S2ORC, WikiAnswers, GOOAQ, etc.)\n",
    "2. Fine-tune on MS MARCO with:\n",
    "   - Training triples\n",
    "   - Hard negatives from BM25\n",
    "   - IDF-aware penalty\n",
    "   - FLOPS regularization\n",
    "3. Evaluate on BEIR benchmark\n",
    "\n",
    "**Download Notes:**\n",
    "- First run will download ~40GB of data\n",
    "- Processing may take several hours\n",
    "- Subsequent runs will use cached files\n",
    "- Alternative download methods are provided if HuggingFace fails\n",
    "\n",
    "**Training Pipeline:**\n",
    "```\n",
    "Pre-training ‚Üí Fine-tuning (MS MARCO) ‚Üí Evaluation (BEIR)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Baseline\n",
    "\n",
    "This notebook demonstrates baseline training for the SPLADE-doc model using **sampled data** from our prepared datasets.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. **Load sample data** from 01 notebook (Korean Wikipedia, NamuWiki)\n",
    "2. **Initialize SPLADE-doc model** with multilingual BERT\n",
    "3. **Train baseline model** with sample data\n",
    "4. **Validate training pipeline** before full-scale training\n",
    "5. **Evaluate on test set**\n",
    "\n",
    "## Key Focus: Korean Data Utilization\n",
    "\n",
    "We prioritize Korean language data from:\n",
    "- Korean Wikipedia paired data\n",
    "- NamuWiki paired data\n",
    "- 모두의 말뭉치 paired data\n",
    "\n",
    "This baseline uses **10K samples** for quick iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Our modules\n",
    "from src.model.splade_model import create_splade_model\n",
    "from src.model.losses import SPLADELoss\n",
    "from src.data.dataset import PairedDataset, create_dataloaders\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'model_name': 'bert-base-multilingual-cased',  # Supports Korean + English\n",
    "    'max_length': 256,\n",
    "    'use_idf': False,  # Will add IDF weights later\n",
    "    \n",
    "    # Data\n",
    "    'sample_size': 10000,  # 10K samples for baseline\n",
    "    'num_negatives': 3,  # Number of negative samples\n",
    "    'batch_size': 8,\n",
    "    'num_workers': 4,\n",
    "    \n",
    "    # Training\n",
    "    'num_epochs': 3,\n",
    "    'learning_rate': 2e-5,\n",
    "    'warmup_steps': 100,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'max_grad_norm': 1.0,\n",
    "    \n",
    "    # Loss weights\n",
    "    'temperature': 0.05,\n",
    "    'lambda_flops': 1e-4,\n",
    "    'lambda_idf': 1e-3,\n",
    "    \n",
    "    # Logging\n",
    "    'log_steps': 50,\n",
    "    'eval_steps': 500,\n",
    "    'save_steps': 1000,\n",
    "    \n",
    "    # Paths\n",
    "    'data_dir': '../../dataset/paired_data',\n",
    "    'output_dir': '../../outputs/baseline',\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "Path(CONFIG['output_dir']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Sample Data\n",
    "\n",
    "Load **Korean-focused** paired data from 01 notebook.\n",
    "\n",
    "Priority:\n",
    "1. Korean Wikipedia (title-summary pairs)\n",
    "2. NamuWiki (title-summary pairs)\n",
    "3. Mix of both for diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_data(data_dir: str, pattern: str, sample_size: int) -> list:\n",
    "    \"\"\"\n",
    "    Load sampled data from JSONL files.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Data directory\n",
    "        pattern: File pattern\n",
    "        sample_size: Number of samples to load\n",
    "    \n",
    "    Returns:\n",
    "        List of samples\n",
    "    \"\"\"\n",
    "    files = sorted(glob.glob(str(Path(data_dir) / pattern)))\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"⚠ No files found matching: {pattern}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(files)} files matching {pattern}\")\n",
    "    \n",
    "    # Load from first file(s) until we have enough samples\n",
    "    samples = []\n",
    "    for file_path in files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                samples.append(json.loads(line))\n",
    "                if len(samples) >= sample_size:\n",
    "                    break\n",
    "        if len(samples) >= sample_size:\n",
    "            break\n",
    "    \n",
    "    # Random shuffle\n",
    "    random.shuffle(samples)\n",
    "    \n",
    "    return samples[:sample_size]\n",
    "\n",
    "# Load Korean Wikipedia samples\n",
    "print(\"=\" * 80)\n",
    "print(\"Loading Korean Wikipedia samples\")\n",
    "print(\"=\" * 80)\n",
    "ko_wiki_samples = load_sample_data(\n",
    "    CONFIG['data_dir'],\n",
    "    'ko_wiki_title_summary*.jsonl',\n",
    "    CONFIG['sample_size'] // 2  # Half from Korean Wiki\n",
    ")\n",
    "print(f\"✓ Loaded {len(ko_wiki_samples):,} Korean Wikipedia samples\")\n",
    "\n",
    "# Load NamuWiki samples\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Loading NamuWiki samples\")\n",
    "print(\"=\" * 80)\n",
    "namu_samples = load_sample_data(\n",
    "    CONFIG['data_dir'],\n",
    "    'namuwiki_title_summary*.jsonl',\n",
    "    CONFIG['sample_size'] // 2  # Half from NamuWiki\n",
    ")\n",
    "print(f\"✓ Loaded {len(namu_samples):,} NamuWiki samples\")\n",
    "\n",
    "# Combine and shuffle\n",
    "all_samples = ko_wiki_samples + namu_samples\n",
    "random.shuffle(all_samples)\n",
    "\n",
    "print(f\"\\n✓ Total samples: {len(all_samples):,}\")\n",
    "\n",
    "# Split into train/val/test (80/10/10)\n",
    "train_size = int(len(all_samples) * 0.8)\n",
    "val_size = int(len(all_samples) * 0.1)\n",
    "\n",
    "train_samples = all_samples[:train_size]\n",
    "val_samples = all_samples[train_size:train_size + val_size]\n",
    "test_samples = all_samples[train_size + val_size:]\n",
    "\n",
    "print(f\"\\nSplit:\")\n",
    "print(f\"  Train: {len(train_samples):,}\")\n",
    "print(f\"  Val:   {len(val_samples):,}\")\n",
    "print(f\"  Test:  {len(test_samples):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save Temporary Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to temporary files for DataLoader\n",
    "temp_dir = Path(CONFIG['output_dir']) / 'temp_data'\n",
    "temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_samples(samples: list, file_path: Path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for sample in samples:\n",
    "            f.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "\n",
    "train_file = temp_dir / 'train.jsonl'\n",
    "val_file = temp_dir / 'val.jsonl'\n",
    "test_file = temp_dir / 'test.jsonl'\n",
    "\n",
    "save_samples(train_samples, train_file)\n",
    "save_samples(val_samples, val_file)\n",
    "save_samples(test_samples, test_file)\n",
    "\n",
    "print(f\"✓ Saved temporary data files to {temp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Initializing model and tokenizer\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "print(f\"✓ Loaded tokenizer: {CONFIG['model_name']}\")\n",
    "\n",
    "# Model\n",
    "model = create_splade_model(\n",
    "    model_name=CONFIG['model_name'],\n",
    "    use_idf=CONFIG['use_idf'],\n",
    "    dropout=0.1,\n",
    ")\n",
    "model = model.to(device)\n",
    "print(f\"✓ Initialized SPLADE-doc model\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Creating dataloaders\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_files=[str(train_file)],\n",
    "    val_files=[str(val_file)],\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    max_length=CONFIG['max_length'],\n",
    "    num_negatives=CONFIG['num_negatives'],\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    use_hard_negatives=False,\n",
    ")\n",
    "\n",
    "print(f\"✓ Train batches: {len(train_loader)}\")\n",
    "print(f\"✓ Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_fn = SPLADELoss(\n",
    "    temperature=CONFIG['temperature'],\n",
    "    lambda_flops=CONFIG['lambda_flops'],\n",
    "    lambda_idf=CONFIG['lambda_idf'],\n",
    "    use_kd=False,  # No teacher for baseline\n",
    "    use_idf_penalty=False,  # Will add later\n",
    ")\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "total_steps = len(train_loader) * CONFIG['num_epochs'] // CONFIG['gradient_accumulation_steps']\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=CONFIG['warmup_steps'],\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "print(f\"✓ Loss function initialized\")\n",
    "print(f\"✓ Optimizer: AdamW (lr={CONFIG['learning_rate']})\")\n",
    "print(f\"✓ Scheduler: Linear with {CONFIG['warmup_steps']} warmup steps\")\n",
    "print(f\"✓ Total training steps: {total_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop (Simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch: int):\n",
    "    \"\"\"Train one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # Move to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        query_repr, _ = model(\n",
    "            batch['query_input_ids'],\n",
    "            batch['query_attention_mask']\n",
    "        )\n",
    "        \n",
    "        pos_doc_repr, _ = model(\n",
    "            batch['pos_doc_input_ids'],\n",
    "            batch['pos_doc_attention_mask']\n",
    "        )\n",
    "        \n",
    "        # Negative documents\n",
    "        batch_size, num_neg, seq_len = batch['neg_doc_input_ids'].shape\n",
    "        neg_input_ids = batch['neg_doc_input_ids'].view(batch_size * num_neg, seq_len)\n",
    "        neg_attention_mask = batch['neg_doc_attention_mask'].view(batch_size * num_neg, seq_len)\n",
    "        \n",
    "        neg_doc_repr_flat, _ = model(neg_input_ids, neg_attention_mask)\n",
    "        neg_doc_repr = neg_doc_repr_flat.view(batch_size, num_neg, -1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, loss_dict = loss_fn(\n",
    "            query_repr,\n",
    "            pos_doc_repr,\n",
    "            neg_doc_repr,\n",
    "        )\n",
    "        \n",
    "        # Backward\n",
    "        loss = loss / CONFIG['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update\n",
    "        if (step + 1) % CONFIG['gradient_accumulation_steps'] == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * CONFIG['gradient_accumulation_steps']\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        if num_batches % CONFIG['log_steps'] == 0:\n",
    "            avg_loss = total_loss / num_batches\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{avg_loss:.4f}\",\n",
    "                'contrastive': f\"{loss_dict['contrastive']:.4f}\",\n",
    "                'flops': f\"{loss_dict['flops']:.4f}\",\n",
    "            })\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate():\n",
    "    \"\"\"Validate on val set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        query_repr, _ = model(batch['query_input_ids'], batch['query_attention_mask'])\n",
    "        pos_doc_repr, _ = model(batch['pos_doc_input_ids'], batch['pos_doc_attention_mask'])\n",
    "        \n",
    "        batch_size, num_neg, seq_len = batch['neg_doc_input_ids'].shape\n",
    "        neg_input_ids = batch['neg_doc_input_ids'].view(batch_size * num_neg, seq_len)\n",
    "        neg_attention_mask = batch['neg_doc_attention_mask'].view(batch_size * num_neg, seq_len)\n",
    "        \n",
    "        neg_doc_repr_flat, _ = model(neg_input_ids, neg_attention_mask)\n",
    "        neg_doc_repr = neg_doc_repr_flat.view(batch_size, num_neg, -1)\n",
    "        \n",
    "        loss, _ = loss_fn(query_repr, pos_doc_repr, neg_doc_repr)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Starting baseline training\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"Train samples: {len(train_samples):,}\")\n",
    "print(f\"Val samples: {len(val_samples):,}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    # Train\n",
    "    train_loss = train_epoch(epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate()\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val loss:   {val_loss:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        checkpoint_path = Path(CONFIG['output_dir']) / 'best_model.pt'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"  ✓ Saved best model (val_loss: {val_loss:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Training complete!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Val Loss', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(Path(CONFIG['output_dir']) / 'training_curve.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved training curve to {CONFIG['output_dir']}/training_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Inference\n",
    "\n",
    "Test the trained model on a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(Path(CONFIG['output_dir']) / 'best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"✓ Loaded best model for inference\")\n",
    "\n",
    "# Test on a sample\n",
    "test_sample = test_samples[0]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Test Inference\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nQuery: {test_sample['query']}\")\n",
    "print(f\"\\nDocument: {test_sample['document'][:200]}...\")\n",
    "\n",
    "# Encode\n",
    "with torch.no_grad():\n",
    "    query_encoded = tokenizer(\n",
    "        test_sample['query'],\n",
    "        max_length=CONFIG['max_length'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    doc_encoded = tokenizer(\n",
    "        test_sample['document'],\n",
    "        max_length=CONFIG['max_length'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    query_encoded = {k: v.to(device) for k, v in query_encoded.items()}\n",
    "    doc_encoded = {k: v.to(device) for k, v in doc_encoded.items()}\n",
    "    \n",
    "    query_repr, _ = model(**query_encoded)\n",
    "    doc_repr, _ = model(**doc_encoded)\n",
    "    \n",
    "    # Compute similarity\n",
    "    similarity = (query_repr * doc_repr).sum().item()\n",
    "    \n",
    "    print(f\"\\nSimilarity score: {similarity:.4f}\")\n",
    "    \n",
    "    # Get top tokens\n",
    "    top_query_tokens = model.get_top_k_tokens(query_repr[0], tokenizer, k=10)\n",
    "    top_doc_tokens = model.get_top_k_tokens(doc_repr[0], tokenizer, k=50)\n",
    "    \n",
    "    print(f\"\\nTop query tokens:\")\n",
    "    for token, weight in list(top_query_tokens.items())[:10]:\n",
    "        print(f\"  {token}: {weight:.3f}\")\n",
    "    \n",
    "    print(f\"\\nTop document tokens:\")\n",
    "    for token, weight in list(top_doc_tokens.items())[:50]:\n",
    "        print(f\"  {token}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This baseline training demonstrates:\n",
    "\n",
    "✅ **Korean data utilization**: 10K samples from Korean Wikipedia and NamuWiki\n",
    "✅ **SPLADE-doc architecture**: Multilingual BERT-based sparse encoder\n",
    "✅ **Training pipeline**: Contrastive learning with hard negatives\n",
    "✅ **Validation**: Proper train/val/test split\n",
    "✅ **Inference**: Sparse token representations with weights\n",
    "\n",
    "**Next Steps for Full Training:**\n",
    "\n",
    "1. **Scale up data**: Use all paired data (millions of samples)\n",
    "   - Korean Wikipedia: ~600K pairs\n",
    "   - NamuWiki: ~1.5M pairs\n",
    "   - English Wikipedia: ~6M pairs\n",
    "   - Pre-training datasets: ~122M pairs\n",
    "\n",
    "2. **Add IDF-aware penalty**: Compute IDF weights from corpus\n",
    "\n",
    "3. **Hard negatives mining**: Use BM25-mined negatives from notebook 04\n",
    "\n",
    "4. **Knowledge distillation**: Add teacher models (dense + sparse)\n",
    "\n",
    "5. **MS MARCO fine-tuning**: Fine-tune on MS MARCO after pre-training\n",
    "\n",
    "6. **BEIR evaluation**: Zero-shot evaluation on BEIR benchmark\n",
    "\n",
    "**Training Pipeline:**\n",
    "```\n",
    "Baseline (✓) → Full Pre-training → Hard Negatives → MS MARCO Fine-tuning → BEIR Eval\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

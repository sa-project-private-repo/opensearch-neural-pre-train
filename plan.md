# Plan: LLM ê¸°ë°˜ í•©ì„± ë°ì´í„° ìƒì„± ë° í•œì˜ í†µí•© ë™ì˜ì–´ ì‚¬ì „ ì¶”ê°€

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

**ëª©í‘œ**: `korean_neural_sparse_training.ipynb`ì— LLM ê¸°ë°˜ í•©ì„± ë°ì´í„° ìƒì„± ê¸°ëŠ¥ê³¼ ì„ë² ë”© ê¸°ë°˜ í•œì˜ ë™ì˜ì–´ ì‚¬ì „ ìƒì„± ê¸°ëŠ¥ ì¶”ê°€

**í•µì‹¬ ìš”êµ¬ì‚¬í•­**:
1. LLMì„ í†µí•œ í•©ì„± ë°ì´í„° ìƒì„± (Query-Document pairs)
2. í•œì˜ í†µí•© ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶• (ì„ë² ë”© ê¸°ë°˜)
3. Localì— gpt-odd-20b ëª¨ë¸ ë¡œë”© ë° í™œìš©
4. ê¸°ì¡´ ì›Œí¬í”Œë¡œìš°ì™€ í†µí•©

---

## ğŸ” í˜„í™© ë¶„ì„

### í˜„ì¬ êµ¬í˜„ëœ ê¸°ëŠ¥
- âœ… í•œì˜ ë™ì˜ì–´ ì‚¬ì „ ê¸°ì´ˆ êµ¬í˜„ (`src/cross_lingual_synonyms.py`)
  - Pattern-based extraction (e.g., "ëª¨ë¸(model)")
  - Embedding similarity ê¸°ë°˜ ë™ì˜ì–´ ë°œê²¬
  - Manual curated pairs
  - ë…¸íŠ¸ë¶ Cell 14ì—ì„œ ì‚¬ìš© ì¤‘

### ì¶”ê°€ í•„ìš” ê¸°ëŠ¥
- âŒ LLM ê¸°ë°˜ í•©ì„± ë°ì´í„° ìƒì„±
- âŒ gpt-odd-20b ëª¨ë¸ ë¡œë”© ë° ì¶”ë¡ 
- âŒ LLMì„ í™œìš©í•œ ê³ í’ˆì§ˆ Query-Document pair ìƒì„±
- âŒ LLM ê¸°ë°˜ ë™ì˜ì–´ ê²€ì¦ ë° í™•ì¥

---

## ğŸ“¦ Phase 1: í™˜ê²½ ì„¤ì • ë° gpt-odd-20b ëª¨ë¸ ë¡œë”©

### 1.1 ì˜ì¡´ì„± ì¶”ê°€
**íŒŒì¼**: `requirements.txt`

ì¶”ê°€í•  íŒ¨í‚¤ì§€:
```txt
# LLM inference (Local model support)
vllm==0.6.4.post1         # Fast LLM inference with GPU
torch==2.5.1              # Already exists
transformers==4.46.3      # Already exists
```

**ëŒ€ì•ˆ**: vLLM ëŒ€ì‹  transformersë§Œ ì‚¬ìš© ê°€ëŠ¥ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì€ ë‚®ì§€ë§Œ ì„¤ì¹˜ ê°„ë‹¨)

### 1.2 ëª¨ë¸ ë¡œë” ëª¨ë“ˆ êµ¬í˜„
**ìƒˆ íŒŒì¼**: `src/llm_loader.py`

ê¸°ëŠ¥:
- gpt-odd-20b ëª¨ë¸ ë¡œë”© (Hugging Face ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ)
- GPU ë©”ëª¨ë¦¬ ìµœì í™” (int8/fp16 quantization)
- Batch inference ì§€ì›
- Prompt template ê´€ë¦¬

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] `load_llm_model()` í•¨ìˆ˜ êµ¬í˜„
- [ ] `generate_text()` í•¨ìˆ˜ êµ¬í˜„
- [ ] Prompt template ì •ì˜
- [ ] GPU ë©”ëª¨ë¦¬ ì²´í¬ ë° ìµœì í™”

---

## ğŸ“ Phase 2: LLM ê¸°ë°˜ í•©ì„± ë°ì´í„° ìƒì„±

### 2.1 í•©ì„± ë°ì´í„° ìƒì„± ëª¨ë“ˆ
**ìƒˆ íŒŒì¼**: `src/synthetic_data_generator.py`

ê¸°ëŠ¥:
- Document â†’ Query ìƒì„± (ì—­ë°©í–¥ ìƒì„±)
- Query â†’ Document ìƒì„± (ì •ë°©í–¥ ìƒì„±)
- Query augmentation (ë™ì˜ì–´, paraphrase)
- Hard negative document ìƒì„±
- í’ˆì§ˆ í•„í„°ë§ (ê¸¸ì´, ì¤‘ë³µ ì œê±°)

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] `generate_queries_from_documents()` í•¨ìˆ˜
- [ ] `generate_documents_from_queries()` í•¨ìˆ˜
- [ ] `augment_query()` í•¨ìˆ˜ (paraphrasing)
- [ ] `generate_hard_negatives()` í•¨ìˆ˜
- [ ] `filter_synthetic_pairs()` í’ˆì§ˆ í•„í„°

### 2.2 Prompt Engineering
**Prompt ì˜ˆì‹œ**:

```python
DOC_TO_QUERY_PROMPT = """
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì½ê³  ì‚¬ìš©ìê°€ ì´ ë¬¸ì„œë¥¼ ì°¾ê¸° ìœ„í•´ ê²€ìƒ‰í•  ë§Œí•œ ì¿¼ë¦¬ 3ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ë¬¸ì„œ: {document}

ê²€ìƒ‰ ì¿¼ë¦¬ (JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ):
"""

SYNONYM_DISCOVERY_PROMPT = """
ë‹¤ìŒ ë‘ ë‹¨ì–´ê°€ ê°™ì€ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.

ë‹¨ì–´ 1: {word1}
ë‹¨ì–´ 2: {word2}

ê°™ì€ ì˜ë¯¸ì´ê±°ë‚˜ ë™ì˜ì–´ë¼ë©´ "ì˜ˆ", ì•„ë‹ˆë©´ "ì•„ë‹ˆì˜¤"ë¡œ ë‹µí•˜ê³  ê°„ë‹¨í•œ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.
"""
```

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] Document â†’ Query prompt ì‘ì„±
- [ ] Query â†’ Document prompt ì‘ì„±
- [ ] Synonym verification prompt ì‘ì„±
- [ ] Hard negative generation prompt ì‘ì„±

---

## ğŸŒ Phase 3: LLM ê¸°ë°˜ í•œì˜ ë™ì˜ì–´ ì‚¬ì „ í™•ì¥

### 3.1 ë™ì˜ì–´ ê²€ì¦ ë° í™•ì¥
**íŒŒì¼**: `src/cross_lingual_synonyms.py` í™•ì¥

ìƒˆ í•¨ìˆ˜:
- `verify_synonyms_with_llm()`: LLMìœ¼ë¡œ ë™ì˜ì–´ ìŒ ê²€ì¦
- `discover_synonyms_with_llm()`: LLMìœ¼ë¡œ ìƒˆ ë™ì˜ì–´ ë°œê²¬
- `enhance_bilingual_dict_with_llm()`: ê¸°ì¡´ ì‚¬ì „ í’ˆì§ˆ í–¥ìƒ

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] `verify_synonyms_with_llm()` êµ¬í˜„
- [ ] `discover_synonyms_with_llm()` êµ¬í˜„
- [ ] `enhance_bilingual_dict_with_llm()` êµ¬í˜„
- [ ] Batch processing ìµœì í™”

### 3.2 ì„ë² ë”© + LLM í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼
**ì „ëµ**:
1. ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ í›„ë³´ ë™ì˜ì–´ ë°œê²¬ (ê¸°ì¡´ ë°©ì‹)
2. LLMìœ¼ë¡œ í›„ë³´ ê²€ì¦ ë° í•„í„°ë§ (ìƒˆë¡œìš´ ë°©ì‹)
3. ê²€ì¦ëœ ë™ì˜ì–´ë§Œ ìµœì¢… ì‚¬ì „ì— ì¶”ê°€

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] ì„ë² ë”© ê¸°ë°˜ í›„ë³´ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸
- [ ] LLM ê²€ì¦ íŒŒì´í”„ë¼ì¸
- [ ] í•˜ì´ë¸Œë¦¬ë“œ í†µí•© í•¨ìˆ˜

---

## ğŸ““ Phase 4: Notebook í†µí•©

### 4.1 ìƒˆ Cell ì¶”ê°€
**íŒŒì¼**: `notebooks/korean_neural_sparse_training.ipynb`

ì¶”ê°€í•  Cell ìœ„ì¹˜: Cell 14 (í•œì˜ ë™ì˜ì–´ ì„¹ì…˜) ì•ì— ì‚½ì…

**ìƒˆ ì„¹ì…˜ 1**: LLM ëª¨ë¸ ë¡œë”©
```python
# Cell: LLM ëª¨ë¸ ë¡œë”©
from src.llm_loader import load_llm_model, check_gpu_memory

print("ğŸ¤– LLM ëª¨ë¸ ë¡œë”© ì¤‘...")
llm_model, llm_tokenizer = load_llm_model(
    model_name="gpt-odd-20b",  # ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ
    device="cuda",
    quantization="int8",  # ë©”ëª¨ë¦¬ ì ˆì•½
)
```

**ìƒˆ ì„¹ì…˜ 2**: í•©ì„± ë°ì´í„° ìƒì„±
```python
# Cell: í•©ì„± ë°ì´í„° ìƒì„±
from src.synthetic_data_generator import generate_synthetic_qd_pairs

synthetic_pairs = generate_synthetic_qd_pairs(
    documents=documents[:1000],  # ìƒ˜í”Œ
    llm_model=llm_model,
    llm_tokenizer=llm_tokenizer,
    num_queries_per_doc=3,
)
```

**ìƒˆ ì„¹ì…˜ 3**: LLM ê¸°ë°˜ ë™ì˜ì–´ ê²€ì¦
```python
# Cell: LLMìœ¼ë¡œ ë™ì˜ì–´ ê²€ì¦ ë° í™•ì¥
from src.cross_lingual_synonyms import enhance_bilingual_dict_with_llm

enhanced_bilingual_dict = enhance_bilingual_dict_with_llm(
    initial_dict=bilingual_dict,
    llm_model=llm_model,
    llm_tokenizer=llm_tokenizer,
    verification_threshold=0.8,
)
```

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] LLM ë¡œë”© Cell ì¶”ê°€
- [ ] í•©ì„± ë°ì´í„° ìƒì„± Cell ì¶”ê°€
- [ ] ë™ì˜ì–´ ê²€ì¦ Cell ì¶”ê°€
- [ ] ê¸°ì¡´ í•™ìŠµ ë°ì´í„°ì— í•©ì„± ë°ì´í„° ë³‘í•©
- [ ] ê²°ê³¼ ì‹œê°í™” ë° í†µê³„

### 4.2 í†µí•© ì›Œí¬í”Œë¡œìš°
```
1. ë°ì´í„° ë¡œë“œ (ê¸°ì¡´)
2. [NEW] LLM ëª¨ë¸ ë¡œë”©
3. [NEW] í•©ì„± ë°ì´í„° ìƒì„±
4. IDF ê³„ì‚° (ê¸°ì¡´)
5. íŠ¸ë Œë“œ ê°ì§€ (ê¸°ì¡´)
6. [ENHANCED] LLM + ì„ë² ë”© ê¸°ë°˜ ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶•
7. ëª¨ë¸ í•™ìŠµ (í•©ì„± ë°ì´í„° í¬í•¨)
8. í‰ê°€ ë° ì €ì¥
```

---

## ğŸ”§ Phase 5: ìµœì í™” ë° í…ŒìŠ¤íŠ¸

### 5.1 ì„±ëŠ¥ ìµœì í™”
ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] LLM inference batching
- [ ] GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
- [ ] í•©ì„± ë°ì´í„° ìºì‹±
- [ ] Parallel processing (ê°€ëŠ¥í•œ ê²½ìš°)

### 5.2 í’ˆì§ˆ ê²€ì¦
ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] í•©ì„± ë°ì´í„° í’ˆì§ˆ í‰ê°€ (ìˆ˜ë™ ìƒ˜í”Œë§)
- [ ] ë™ì˜ì–´ ì •í™•ë„ ì¸¡ì •
- [ ] í•™ìŠµ ì„±ëŠ¥ ë¹„êµ (í•©ì„± ë°ì´í„° ìœ /ë¬´)
- [ ] Ablation study (LLM vs. ì„ë² ë”© only)

### 5.3 ë¬¸ì„œí™”
ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] `src/llm_loader.py` docstring ì‘ì„±
- [ ] `src/synthetic_data_generator.py` docstring ì‘ì„±
- [ ] `src/__init__.py` ì—…ë°ì´íŠ¸ (ìƒˆ í•¨ìˆ˜ export)
- [ ] README ì—…ë°ì´íŠ¸ (ìƒˆ ê¸°ëŠ¥ ì„¤ëª…)
- [ ] Notebookì— ì„¤ëª… markdown cell ì¶”ê°€

---

## âš™ï¸ ê¸°ìˆ ì  ê³ ë ¤ì‚¬í•­

### GPU ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­
- **gpt-odd-20b ëª¨ë¸ í¬ê¸°**: ~40GB (FP16), ~20GB (INT8)
- **BERT í•™ìŠµ ë©”ëª¨ë¦¬**: ~8-12GB
- **ì´ í•„ìš” ë©”ëª¨ë¦¬**: ~30GB ì´ìƒ ê¶Œì¥
- **ëŒ€ì•ˆ**:
  - Smaller LLM ì‚¬ìš© (e.g., GPT-2-XL, Llama-7B)
  - CPU offloading
  - Quantization (INT4/INT8)

### LLM ì„ íƒì§€
1. **gpt-odd-20b** (ìš”êµ¬ì‚¬í•­) - ì„±ëŠ¥ ìš°ìˆ˜, ë©”ëª¨ë¦¬ ë§ì´ í•„ìš”
2. **ëŒ€ì•ˆ 1**: GPT-J-6B (ê²½ëŸ‰, í•œêµ­ì–´ ì„±ëŠ¥ ë‚®ìŒ)
3. **ëŒ€ì•ˆ 2**: Polyglot-Ko-12.8B (í•œêµ­ì–´ íŠ¹í™”, ì¤‘ê°„ í¬ê¸°)
4. **ëŒ€ì•ˆ 3**: OpenAI API (í´ë¼ìš°ë“œ, ë¹„ìš© ë°œìƒ)

### í’ˆì§ˆ vs. ë¹„ìš© íŠ¸ë ˆì´ë“œì˜¤í”„
- **ê³ í’ˆì§ˆ ì „ëµ**: LLMìœ¼ë¡œ ëª¨ë“  ë™ì˜ì–´ ê²€ì¦ (ëŠë¦¼, ë¹„ìš© ë†’ìŒ)
- **ê· í˜• ì „ëµ**: ì„ë² ë”©ìœ¼ë¡œ í›„ë³´ ì¶”ì¶œ + LLMìœ¼ë¡œ ì¼ë¶€ ê²€ì¦ (ê¶Œì¥)
- **ì €ë¹„ìš© ì „ëµ**: ì„ë² ë”©ë§Œ ì‚¬ìš© + ìˆ˜ë™ íë ˆì´ì…˜

---

## ğŸ“… êµ¬í˜„ ìˆœì„œ ë° ìš°ì„ ìˆœìœ„

### High Priority (Core)
1. âœ… Phase 1.2: ëª¨ë¸ ë¡œë” êµ¬í˜„ (`src/llm_loader.py`)
2. âœ… Phase 2.1: í•©ì„± ë°ì´í„° ìƒì„±ê¸° êµ¬í˜„ (`src/synthetic_data_generator.py`)
3. âœ… Phase 4.1: Notebook í†µí•© (ìƒˆ Cell ì¶”ê°€)

### Medium Priority (Enhancement)
4. âœ… Phase 3.1: LLM ê¸°ë°˜ ë™ì˜ì–´ ê²€ì¦
5. âœ… Phase 5.2: í’ˆì§ˆ ê²€ì¦

### Low Priority (Optimization)
6. â¸ï¸ Phase 5.1: ì„±ëŠ¥ ìµœì í™”
7. â¸ï¸ Phase 5.3: ë¬¸ì„œí™” ì™„ì„±

---

## ğŸ¯ ì„±ê³µ ì§€í‘œ

- [ ] gpt-odd-20b ëª¨ë¸ ë¡œë”© ì„±ê³µ
- [ ] ìµœì†Œ 1,000ê°œ ì´ìƒì˜ í•©ì„± Query-Document pairs ìƒì„±
- [ ] í•œì˜ ë™ì˜ì–´ ì‚¬ì „ í¬ê¸° 2ë°° ì´ìƒ ì¦ê°€
- [ ] í•©ì„± ë°ì´í„°ë¡œ í•™ìŠµ ì‹œ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ (MRR/NDCG)
- [ ] Notebook ì „ì²´ ì‹¤í–‰ ì‹œê°„ 3ì‹œê°„ ì´ë‚´ (GPU í™˜ê²½)

---

## ğŸš¨ ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘

### ë¦¬ìŠ¤í¬ 1: GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
**ëŒ€ì‘**:
- INT8 quantization ì‚¬ìš©
- Smaller batch size
- Gradient checkpointing
- CPU offloading (ì†ë„ ì €í•˜ ê°ìˆ˜)

### ë¦¬ìŠ¤í¬ 2: LLM ìƒì„± í’ˆì§ˆ ë‚®ìŒ
**ëŒ€ì‘**:
- Prompt engineering ê°œì„ 
- Few-shot examples ì¶”ê°€
- Temperature/Top-p ì¡°ì •
- ë‹¤ë¥¸ LLM ëª¨ë¸ ì‹œë„

### ë¦¬ìŠ¤í¬ 3: í•©ì„± ë°ì´í„° ê³¼ì í•©
**ëŒ€ì‘**:
- í•©ì„±/ì‹¤ì œ ë°ì´í„° ë¹„ìœ¨ ì¡°ì • (1:1 ë˜ëŠ” 1:2)
- Validation setì€ ì‹¤ì œ ë°ì´í„°ë§Œ ì‚¬ìš©
- Diversity penalty ì¶”ê°€

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [vLLM Documentation](https://docs.vllm.ai/)
- [Hugging Face Transformers - Text Generation](https://huggingface.co/docs/transformers/main_classes/text_generation)
- [InPars: Data Augmentation for Information Retrieval](https://arxiv.org/abs/2202.05144)
- [Promptagator: Few-shot Dense Retrieval](https://arxiv.org/abs/2209.11755)

---

## âœ… Checklist Summary

**Phase 1**: í™˜ê²½ ì„¤ì • ë° ëª¨ë¸ ë¡œë”©
- [ ] requirements.txt ì—…ë°ì´íŠ¸
- [ ] src/llm_loader.py êµ¬í˜„
- [ ] GPU ë©”ëª¨ë¦¬ ì²´í¬ ë° ìµœì í™”

**Phase 2**: í•©ì„± ë°ì´í„° ìƒì„±
- [ ] src/synthetic_data_generator.py êµ¬í˜„
- [ ] Prompt templates ì‘ì„±
- [ ] í’ˆì§ˆ í•„í„°ë§ ë¡œì§

**Phase 3**: ë™ì˜ì–´ ì‚¬ì „ í™•ì¥
- [ ] src/cross_lingual_synonyms.py í™•ì¥
- [ ] LLM ê²€ì¦ í•¨ìˆ˜ ì¶”ê°€
- [ ] í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

**Phase 4**: Notebook í†µí•©
- [ ] ìƒˆ Cell ì¶”ê°€ (LLM ë¡œë”©, í•©ì„± ë°ì´í„°, ë™ì˜ì–´)
- [ ] ê¸°ì¡´ ì›Œí¬í”Œë¡œìš°ì™€ í†µí•©
- [ ] ê²°ê³¼ ì‹œê°í™”

**Phase 5**: ìµœì í™” ë° ê²€ì¦
- [ ] ì„±ëŠ¥ ìµœì í™”
- [ ] í’ˆì§ˆ í‰ê°€
- [ ] ë¬¸ì„œí™”

---

**Updated**: 2025-11-13
**Status**: Ready for implementation

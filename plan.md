# Plan: LLM ê¸°ë°˜ í•©ì„± ë°ì´í„° ìƒì„± ë° í•œì˜ í†µí•© ë™ì˜ì–´ ì‚¬ì „ ì¶”ê°€ (ARM ìµœì í™”)

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

**ëª©í‘œ**: `korean_neural_sparse_training.ipynb`ì— LLM ê¸°ë°˜ í•©ì„± ë°ì´í„° ìƒì„± ê¸°ëŠ¥ê³¼ ì„ë² ë”© ê¸°ë°˜ í•œì˜ ë™ì˜ì–´ ì‚¬ì „ ìƒì„± ê¸°ëŠ¥ ì¶”ê°€

**í•µì‹¬ ìš”êµ¬ì‚¬í•­**:
1. LLMì„ í†µí•œ í•©ì„± ë°ì´í„° ìƒì„± (Query-Document pairs)
2. í•œì˜ í†µí•© ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶• (ì„ë² ë”© ê¸°ë°˜)
3. Localì— ê²½ëŸ‰ LLM ëª¨ë¸ ë¡œë”© ë° í™œìš© (ARM í˜¸í™˜)
4. ê¸°ì¡´ ì›Œí¬í”Œë¡œìš°ì™€ í†µí•©

**ì‹œìŠ¤í…œ í™˜ê²½**:
- **ì•„í‚¤í…ì²˜**: ARM aarch64 (Blackwell GB10)
- **GPU**: NVIDIA GB10 (CUDA 13.0 ì§€ì›)
- **ë©”ëª¨ë¦¬**: ì œí•œì  (í˜„ì¬ 4.5GB GPU ì‚¬ìš© ì¤‘)
- **Python**: 3.12 (venv í™˜ê²½)
- **ì œì•½ì‚¬í•­**: vLLMì€ ARM ì§€ì› ì œí•œì  â†’ ëŒ€ì•ˆ í•„ìš”

---

## ğŸ” í˜„í™© ë¶„ì„

### í˜„ì¬ êµ¬í˜„ëœ ê¸°ëŠ¥
- âœ… í•œì˜ ë™ì˜ì–´ ì‚¬ì „ ê¸°ì´ˆ êµ¬í˜„ (`src/cross_lingual_synonyms.py`)
  - Pattern-based extraction (e.g., "ëª¨ë¸(model)")
  - Embedding similarity ê¸°ë°˜ ë™ì˜ì–´ ë°œê²¬
  - Manual curated pairs
  - ë…¸íŠ¸ë¶ Cell 14ì—ì„œ ì‚¬ìš© ì¤‘

### ì¶”ê°€ í•„ìš” ê¸°ëŠ¥
- âŒ LLM ê¸°ë°˜ í•©ì„± ë°ì´í„° ìƒì„±
- âŒ ARM í˜¸í™˜ LLM ë¡œë”© ë° ì¶”ë¡ 
- âŒ LLMì„ í™œìš©í•œ ê³ í’ˆì§ˆ Query-Document pair ìƒì„±
- âŒ LLM ê¸°ë°˜ ë™ì˜ì–´ ê²€ì¦ ë° í™•ì¥

---

## ğŸ“¦ Phase 1: í™˜ê²½ ì„¤ì • ë° ARM í˜¸í™˜ LLM ë¡œë”©

### 1.1 Python í™˜ê²½ ì„¤ì •
**Python ë²„ì „**: 3.12 (venv)

```bash
# venv ìƒì„± ë° í™œì„±í™”
python3.12 -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ë˜ëŠ” .venv\Scripts\activate  # Windows

# pip ì—…ê·¸ë ˆì´ë“œ
pip install --upgrade pip setuptools wheel
```

**Python 3.12 í˜¸í™˜ì„±**:
- âœ… PyTorch 2.5.1 (Python 3.12 ì§€ì›)
- âœ… Transformers 4.46.3 (Python 3.12 ì§€ì›)
- âœ… AutoAWQ 0.2.7 (Python 3.12 ì§€ì›)
- âš ï¸ llama-cpp-python: ë¹Œë“œ í•„ìš”í•  ìˆ˜ ìˆìŒ (ARM + Python 3.12)

### 1.2 ì˜ì¡´ì„± ì¶”ê°€ (ARM + Python 3.12 ìµœì í™”)
**íŒŒì¼**: `requirements.txt`

ì¶”ê°€í•  íŒ¨í‚¤ì§€:
```txt
# Python 3.12 compatible versions
# LLM inference (ARM-compatible)
# vLLMì€ ARM ì§€ì› ì œí•œì ì´ë¯€ë¡œ ì œì™¸
accelerate==1.1.1         # Already exists - ë©”ëª¨ë¦¬ ìµœì í™”
autoawq==0.2.7            # AWQ quantization (Qwen3 ê¶Œì¥, Python 3.12 OK)
optimum==1.23.3           # ONNX Runtime ìµœì í™”
sentencepiece==0.2.0      # Already exists - tokenizer

# gpt-oss-20b ì‚¬ìš© ì‹œ (GGUF)
# llama-cpp-python==0.3.4  # Optional: gpt-oss-20b GGUF ì§€ì›
#                          # ARM + Python 3.12: ì†ŒìŠ¤ ë¹Œë“œ í•„ìš”í•  ìˆ˜ ìˆìŒ
```

**ì „ëµ**: Hugging Face Transformers + AutoAWQ quantization ì‚¬ìš©
- Qwen3: AutoAWQë¡œ 4-bit ì–‘ìí™” (ARM + Python 3.12 ê²€ì¦)
- gpt-oss-20b: GGUF + llama.cpp (ARM ìµœì í™”, Python 3.12 ë¹Œë“œ í•„ìš”)
- accelerateë¡œ ë©€í‹° GPU/CPU offloading
- ëª¨ë“  íŒ¨í‚¤ì§€ Python 3.12 í˜¸í™˜ ë²„ì „ ì‚¬ìš©

### 1.3 ëª¨ë¸ ë¡œë” ëª¨ë“ˆ êµ¬í˜„ (ARM + Python 3.12 ìµœì í™”)
**ìƒˆ íŒŒì¼**: `src/llm_loader.py`

ê¸°ëŠ¥:
- ARM í˜¸í™˜ ê²½ëŸ‰ LLM ë¡œë”© (Hugging Face)
- GPU ë©”ëª¨ë¦¬ ìµœì í™” (INT8/INT4 quantization via bitsandbytes)
- Batch inference ì§€ì›
- Prompt template ê´€ë¦¬
- CPU offloading ì§€ì› (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ)

**ì‚¬ìš© ëª¨ë¸ (ìš”êµ¬ì‚¬í•­)**:
1. **gpt-oss-20b** (OpenAI, 21B params, 3.6B active)
2. **Qwen3** ì‹œë¦¬ì¦ˆ (Alibaba, ë‹¤ì–‘í•œ í¬ê¸°)

**ì„ íƒ ì „ëµ**: GPU ë©”ëª¨ë¦¬ ê³ ë ¤í•˜ì—¬ Qwen3-14B ë˜ëŠ” gpt-oss-20b (GGUF) ì¶”ì²œ

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] `load_qwen3_awq()` í•¨ìˆ˜ êµ¬í˜„ (AWQ 4-bit)
- [ ] `load_gpt_oss_gguf()` í•¨ìˆ˜ êµ¬í˜„ (GGUF, optional)
- [ ] `generate_text()` í•¨ìˆ˜ êµ¬í˜„
- [ ] `generate_batch()` ë°°ì¹˜ ì¶”ë¡  í•¨ìˆ˜
- [ ] Prompt template ì •ì˜ (Qwen3/gpt-oss ìµœì í™”)
- [ ] GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ìœ í‹¸ë¦¬í‹°
- [ ] CPU offloading ì˜µì…˜

---

## ğŸ“ Phase 2: LLM ê¸°ë°˜ í•©ì„± ë°ì´í„° ìƒì„±

### 2.1 í•©ì„± ë°ì´í„° ìƒì„± ëª¨ë“ˆ
**ìƒˆ íŒŒì¼**: `src/synthetic_data_generator.py`

ê¸°ëŠ¥:
- Document â†’ Query ìƒì„± (ì—­ë°©í–¥ ìƒì„±)
- Query â†’ Document ìƒì„± (ì •ë°©í–¥ ìƒì„±)
- Query augmentation (ë™ì˜ì–´, paraphrase)
- Hard negative document ìƒì„±
- í’ˆì§ˆ í•„í„°ë§ (ê¸¸ì´, ì¤‘ë³µ ì œê±°)

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] `generate_queries_from_documents()` í•¨ìˆ˜
- [ ] `generate_documents_from_queries()` í•¨ìˆ˜
- [ ] `augment_query()` í•¨ìˆ˜ (paraphrasing)
- [ ] `generate_hard_negatives()` í•¨ìˆ˜
- [ ] `filter_synthetic_pairs()` í’ˆì§ˆ í•„í„°

### 2.2 Prompt Engineering
**Prompt ì˜ˆì‹œ**:

```python
DOC_TO_QUERY_PROMPT = """
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì½ê³  ì‚¬ìš©ìê°€ ì´ ë¬¸ì„œë¥¼ ì°¾ê¸° ìœ„í•´ ê²€ìƒ‰í•  ë§Œí•œ ì¿¼ë¦¬ 3ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ë¬¸ì„œ: {document}

ê²€ìƒ‰ ì¿¼ë¦¬ (JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ):
"""

SYNONYM_DISCOVERY_PROMPT = """
ë‹¤ìŒ ë‘ ë‹¨ì–´ê°€ ê°™ì€ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.

ë‹¨ì–´ 1: {word1}
ë‹¨ì–´ 2: {word2}

ê°™ì€ ì˜ë¯¸ì´ê±°ë‚˜ ë™ì˜ì–´ë¼ë©´ "ì˜ˆ", ì•„ë‹ˆë©´ "ì•„ë‹ˆì˜¤"ë¡œ ë‹µí•˜ê³  ê°„ë‹¨í•œ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.
"""
```

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] Document â†’ Query prompt ì‘ì„±
- [ ] Query â†’ Document prompt ì‘ì„±
- [ ] Synonym verification prompt ì‘ì„±
- [ ] Hard negative generation prompt ì‘ì„±

---

## ğŸŒ Phase 3: LLM ê¸°ë°˜ í•œì˜ ë™ì˜ì–´ ì‚¬ì „ í™•ì¥

### 3.1 ë™ì˜ì–´ ê²€ì¦ ë° í™•ì¥
**íŒŒì¼**: `src/cross_lingual_synonyms.py` í™•ì¥

ìƒˆ í•¨ìˆ˜:
- `verify_synonyms_with_llm()`: LLMìœ¼ë¡œ ë™ì˜ì–´ ìŒ ê²€ì¦
- `discover_synonyms_with_llm()`: LLMìœ¼ë¡œ ìƒˆ ë™ì˜ì–´ ë°œê²¬
- `enhance_bilingual_dict_with_llm()`: ê¸°ì¡´ ì‚¬ì „ í’ˆì§ˆ í–¥ìƒ

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] `verify_synonyms_with_llm()` êµ¬í˜„
- [ ] `discover_synonyms_with_llm()` êµ¬í˜„
- [ ] `enhance_bilingual_dict_with_llm()` êµ¬í˜„
- [ ] Batch processing ìµœì í™”

### 3.2 ì„ë² ë”© + LLM í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼
**ì „ëµ**:
1. ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ í›„ë³´ ë™ì˜ì–´ ë°œê²¬ (ê¸°ì¡´ ë°©ì‹)
2. LLMìœ¼ë¡œ í›„ë³´ ê²€ì¦ ë° í•„í„°ë§ (ìƒˆë¡œìš´ ë°©ì‹)
3. ê²€ì¦ëœ ë™ì˜ì–´ë§Œ ìµœì¢… ì‚¬ì „ì— ì¶”ê°€

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] ì„ë² ë”© ê¸°ë°˜ í›„ë³´ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸
- [ ] LLM ê²€ì¦ íŒŒì´í”„ë¼ì¸
- [ ] í•˜ì´ë¸Œë¦¬ë“œ í†µí•© í•¨ìˆ˜

---

## ğŸ““ Phase 4: Notebook í†µí•©

### 4.1 ìƒˆ Cell ì¶”ê°€
**íŒŒì¼**: `notebooks/korean_neural_sparse_training.ipynb`

ì¶”ê°€í•  Cell ìœ„ì¹˜: Cell 14 (í•œì˜ ë™ì˜ì–´ ì„¹ì…˜) ì•ì— ì‚½ì…

**ìƒˆ ì„¹ì…˜ 1**: LLM ëª¨ë¸ ë¡œë”©
```python
# Cell: LLM ëª¨ë¸ ë¡œë”©
from src.llm_loader import load_llm_model, check_gpu_memory

print("ğŸ¤– LLM ëª¨ë¸ ë¡œë”© ì¤‘...")
llm_model, llm_tokenizer = load_llm_model(
    model_name="gpt-odd-20b",  # ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ
    device="cuda",
    quantization="int8",  # ë©”ëª¨ë¦¬ ì ˆì•½
)
```

**ìƒˆ ì„¹ì…˜ 2**: í•©ì„± ë°ì´í„° ìƒì„±
```python
# Cell: í•©ì„± ë°ì´í„° ìƒì„±
from src.synthetic_data_generator import generate_synthetic_qd_pairs

synthetic_pairs = generate_synthetic_qd_pairs(
    documents=documents[:1000],  # ìƒ˜í”Œ
    llm_model=llm_model,
    llm_tokenizer=llm_tokenizer,
    num_queries_per_doc=3,
)
```

**ìƒˆ ì„¹ì…˜ 3**: LLM ê¸°ë°˜ ë™ì˜ì–´ ê²€ì¦
```python
# Cell: LLMìœ¼ë¡œ ë™ì˜ì–´ ê²€ì¦ ë° í™•ì¥
from src.cross_lingual_synonyms import enhance_bilingual_dict_with_llm

enhanced_bilingual_dict = enhance_bilingual_dict_with_llm(
    initial_dict=bilingual_dict,
    llm_model=llm_model,
    llm_tokenizer=llm_tokenizer,
    verification_threshold=0.8,
)
```

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] LLM ë¡œë”© Cell ì¶”ê°€
- [ ] í•©ì„± ë°ì´í„° ìƒì„± Cell ì¶”ê°€
- [ ] ë™ì˜ì–´ ê²€ì¦ Cell ì¶”ê°€
- [ ] ê¸°ì¡´ í•™ìŠµ ë°ì´í„°ì— í•©ì„± ë°ì´í„° ë³‘í•©
- [ ] ê²°ê³¼ ì‹œê°í™” ë° í†µê³„

### 4.2 í†µí•© ì›Œí¬í”Œë¡œìš°
```
1. ë°ì´í„° ë¡œë“œ (ê¸°ì¡´)
2. [NEW] LLM ëª¨ë¸ ë¡œë”©
3. [NEW] í•©ì„± ë°ì´í„° ìƒì„±
4. IDF ê³„ì‚° (ê¸°ì¡´)
5. íŠ¸ë Œë“œ ê°ì§€ (ê¸°ì¡´)
6. [ENHANCED] LLM + ì„ë² ë”© ê¸°ë°˜ ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶•
7. ëª¨ë¸ í•™ìŠµ (í•©ì„± ë°ì´í„° í¬í•¨)
8. í‰ê°€ ë° ì €ì¥
```

---

## ğŸ”§ Phase 5: ìµœì í™” ë° í…ŒìŠ¤íŠ¸

### 5.1 ì„±ëŠ¥ ìµœì í™”
ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] LLM inference batching
- [ ] GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
- [ ] í•©ì„± ë°ì´í„° ìºì‹±
- [ ] Parallel processing (ê°€ëŠ¥í•œ ê²½ìš°)

### 5.2 í’ˆì§ˆ ê²€ì¦
ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] í•©ì„± ë°ì´í„° í’ˆì§ˆ í‰ê°€ (ìˆ˜ë™ ìƒ˜í”Œë§)
- [ ] ë™ì˜ì–´ ì •í™•ë„ ì¸¡ì •
- [ ] í•™ìŠµ ì„±ëŠ¥ ë¹„êµ (í•©ì„± ë°ì´í„° ìœ /ë¬´)
- [ ] Ablation study (LLM vs. ì„ë² ë”© only)

### 5.3 ë¬¸ì„œí™”
ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] `src/llm_loader.py` docstring ì‘ì„±
- [ ] `src/synthetic_data_generator.py` docstring ì‘ì„±
- [ ] `src/__init__.py` ì—…ë°ì´íŠ¸ (ìƒˆ í•¨ìˆ˜ export)
- [ ] README ì—…ë°ì´íŠ¸ (ìƒˆ ê¸°ëŠ¥ ì„¤ëª…)
- [ ] Notebookì— ì„¤ëª… markdown cell ì¶”ê°€

---

## âš™ï¸ ê¸°ìˆ ì  ê³ ë ¤ì‚¬í•­ (ARM GB10 í™˜ê²½)

### GPU ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ (í˜„ì¬: GB10)
- **í˜„ì¬ ì‚¬ìš©ëŸ‰**: 4.5GB (Jupyter í”„ë¡œì„¸ìŠ¤)
- **ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬**: ì˜ˆìƒ ~12-16GB (GB10 ì´ ë©”ëª¨ë¦¬ ë¯¸í™•ì¸)
- **BERT í•™ìŠµ ë©”ëª¨ë¦¬**: ~4-6GB (í˜„ì¬ ì‚¬ìš© ì¤‘)
- **LLM ì¶”ë¡  ë©”ëª¨ë¦¬** (ìš”êµ¬ì‚¬í•­ ëª¨ë¸):
  - Qwen3-14B (AWQ 4-bit): ~4GB â­
  - Qwen3-7B (AWQ 4-bit): ~2GB
  - gpt-oss-20b (GGUF Q4): ~5GB
  - Qwen3-0.6B (INT8): ~0.3GB (í…ŒìŠ¤íŠ¸ìš©)

**ê¶Œì¥ ì „ëµ**:
- **Option A**: Qwen3-14B-AWQ ì‚¬ìš© (4-bit, ~4GB) - ì„±ëŠ¥ ìš°ì„ 
- **Option B**: Qwen3-7B-AWQ ì‚¬ìš© (4-bit, ~2GB) - ì•ˆì •ì„± ìš°ì„ 
- BERT í•™ìŠµ ì™„ë£Œ í›„ LLM ë¡œë”© (ìˆœì°¨ ì‹¤í–‰ ê¶Œì¥)
- í•„ìš” ì‹œ CPU offloading í™œìš© (accelerate)

### LLM ì„ íƒì§€ (ìš”êµ¬ì‚¬í•­: gpt-oss-20b ë˜ëŠ” Qwen3)

#### Option 1: Qwen3-14B-Instruct â­ ìµœìš°ì„  ì¶”ì²œ
- **í¬ê¸°**: 14B params (~28GB FP16, ~7GB INT8, ~4GB Q4)
- **ì¥ì **:
  - ARM aarch64 ì™„ë²½ ì§€ì› (ê²€ì¦ë¨)
  - í•œêµ­ì–´ ìš°ìˆ˜ (ë‹¤êµ­ì–´ ëª¨ë¸)
  - 4-bit/8-bit quantization ì„±ëŠ¥ ìš°ìˆ˜
  - bitsandbytes, AWQ, GPTQ ëª¨ë‘ ì§€ì›
- **ë‹¨ì **: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ
- **Hugging Face**: `Qwen/Qwen3-14B-Instruct`
- **Quantized**: `Qwen/Qwen3-14B-AWQ` (4-bit)

#### Option 2: Qwen3-7B-Instruct
- **í¬ê¸°**: 7B params (~14GB FP16, ~3.5GB INT8)
- **ì¥ì **:
  - ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
  - ARM í˜¸í™˜
  - í•œêµ­ì–´ ì„±ëŠ¥ ìš°ìˆ˜
  - ë¹ ë¥¸ ì¶”ë¡ 
- **ë‹¨ì **: 14B ëŒ€ë¹„ ì„±ëŠ¥ ë‚®ìŒ
- **Hugging Face**: `Qwen/Qwen3-7B-Instruct`
- **Quantized**: `Qwen/Qwen3-7B-AWQ`

#### Option 3: gpt-oss-20b (GGUF)
- **í¬ê¸°**: 21B params (3.6B active MoE), ~16GB MXFP4
- **ì¥ì **:
  - ARM ìë™ ìµœì í™” (GGUF)
  - MoE êµ¬ì¡°ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
  - llama.cpp ì§€ì›
  - Q4_0, IQ4_NL quantization (ARM ìµœì í™”)
- **ë‹¨ì **:
  - Transformers ì§ì ‘ ì§€ì› ì œí•œì  (GGUF ì‚¬ìš© í•„ìš”)
  - llama.cpp ì˜ì¡´ì„±
- **Hugging Face**: `openai/gpt-oss-20b`
- **GGUF**: `ggml-org/gpt-oss-20b-GGUF`

#### Option 4: Qwen3-0.6B (ê²½ëŸ‰ í…ŒìŠ¤íŠ¸ìš©)
- **í¬ê¸°**: 0.6B params (~1.2GB FP16, ~0.3GB INT8)
- **ì¥ì **: ë§¤ìš° ê²½ëŸ‰, ë¹ ë¥¸ ì‹¤í—˜
- **ë‹¨ì **: ì„±ëŠ¥ ì œí•œì 
- **Hugging Face**: `Qwen/Qwen3-0.6B-Instruct`

**ìµœì¢… ì¶”ì²œ**:
- **ë©”ëª¨ë¦¬ ì—¬ìœ  ìˆìŒ**: Qwen3-14B-AWQ (4-bit, ~4GB) â­
- **ë©”ëª¨ë¦¬ ì œí•œì **: Qwen3-7B-AWQ (4-bit, ~2GB)
- **gpt-oss-20b í•„ìˆ˜**: GGUF Q4_0 ë²„ì „ (~5GB)

### í’ˆì§ˆ vs. ë¹„ìš© íŠ¸ë ˆì´ë“œì˜¤í”„
- **ê³ í’ˆì§ˆ ì „ëµ**: LLMìœ¼ë¡œ ëª¨ë“  ë™ì˜ì–´ ê²€ì¦ (ëŠë¦¼, ë¹„ìš© ë†’ìŒ)
- **ê· í˜• ì „ëµ**: ì„ë² ë”©ìœ¼ë¡œ í›„ë³´ ì¶”ì¶œ + LLMìœ¼ë¡œ ì¼ë¶€ ê²€ì¦ (ê¶Œì¥)
- **ì €ë¹„ìš© ì „ëµ**: ì„ë² ë”©ë§Œ ì‚¬ìš© + ìˆ˜ë™ íë ˆì´ì…˜

---

## ğŸ“… êµ¬í˜„ ìˆœì„œ ë° ìš°ì„ ìˆœìœ„

### High Priority (Core)
1. âœ… Phase 1.2: ëª¨ë¸ ë¡œë” êµ¬í˜„ (`src/llm_loader.py`)
2. âœ… Phase 2.1: í•©ì„± ë°ì´í„° ìƒì„±ê¸° êµ¬í˜„ (`src/synthetic_data_generator.py`)
3. âœ… Phase 4.1: Notebook í†µí•© (ìƒˆ Cell ì¶”ê°€)

### Medium Priority (Enhancement)
4. âœ… Phase 3.1: LLM ê¸°ë°˜ ë™ì˜ì–´ ê²€ì¦
5. âœ… Phase 5.2: í’ˆì§ˆ ê²€ì¦

### Low Priority (Optimization)
6. â¸ï¸ Phase 5.1: ì„±ëŠ¥ ìµœì í™”
7. â¸ï¸ Phase 5.3: ë¬¸ì„œí™” ì™„ì„±

---

## ğŸ¯ ì„±ê³µ ì§€í‘œ

- [ ] Qwen3-14B-AWQ ë˜ëŠ” gpt-oss-20b ëª¨ë¸ ë¡œë”© ì„±ê³µ
- [ ] GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 12GB ì´ë‚´ ìœ ì§€
- [ ] ìµœì†Œ 1,000ê°œ ì´ìƒì˜ í•©ì„± Query-Document pairs ìƒì„±
- [ ] í•œì˜ ë™ì˜ì–´ ì‚¬ì „ í¬ê¸° 2ë°° ì´ìƒ ì¦ê°€
- [ ] í•©ì„± ë°ì´í„°ë¡œ í•™ìŠµ ì‹œ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ (MRR/NDCG)
- [ ] Notebook ì „ì²´ ì‹¤í–‰ ì‹œê°„ 4ì‹œê°„ ì´ë‚´ (ARM GPU í™˜ê²½)

---

## ğŸš¨ ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘

### ë¦¬ìŠ¤í¬ 1: GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
**ëŒ€ì‘**:
- AWQ 4-bit quantization ì‚¬ìš©
- Smaller batch size
- Gradient checkpointing
- CPU offloading (ì†ë„ ì €í•˜ ê°ìˆ˜)

### ë¦¬ìŠ¤í¬ 4: Python 3.12 í˜¸í™˜ì„± ë¬¸ì œ
**ëŒ€ì‘**:
- llama-cpp-python: CMAKEë¡œ ì†ŒìŠ¤ ë¹Œë“œ
- autoawq: ìµœì‹  ë²„ì „ ì‚¬ìš© (0.2.7+)
- ì˜ì¡´ì„± ì¶©ëŒ ì‹œ requirements.txt ë²„ì „ ì¡°ì •
- venv í™˜ê²½ ê²©ë¦¬ë¡œ ì‹œìŠ¤í…œ Pythonê³¼ ë¶„ë¦¬

### ë¦¬ìŠ¤í¬ 2: LLM ìƒì„± í’ˆì§ˆ ë‚®ìŒ
**ëŒ€ì‘**:
- Prompt engineering ê°œì„ 
- Few-shot examples ì¶”ê°€
- Temperature/Top-p ì¡°ì •
- ë‹¤ë¥¸ LLM ëª¨ë¸ ì‹œë„

### ë¦¬ìŠ¤í¬ 3: í•©ì„± ë°ì´í„° ê³¼ì í•©
**ëŒ€ì‘**:
- í•©ì„±/ì‹¤ì œ ë°ì´í„° ë¹„ìœ¨ ì¡°ì • (1:1 ë˜ëŠ” 1:2)
- Validation setì€ ì‹¤ì œ ë°ì´í„°ë§Œ ì‚¬ìš©
- Diversity penalty ì¶”ê°€

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [Hugging Face Transformers - Text Generation](https://huggingface.co/docs/transformers/main_classes/text_generation)
- [bitsandbytes - INT8/INT4 Quantization](https://github.com/TimDettmers/bitsandbytes)
- [Accelerate - Memory Optimization](https://huggingface.co/docs/accelerate/index)
- [InPars: Data Augmentation for Information Retrieval](https://arxiv.org/abs/2202.05144)
- [Promptagator: Few-shot Dense Retrieval](https://arxiv.org/abs/2209.11755)
- [Qwen3 Model Card](https://huggingface.co/Qwen/Qwen3-14B-Instruct)
- [Qwen3 AWQ Quantization](https://huggingface.co/Qwen/Qwen3-14B-Instruct-AWQ)
- [gpt-oss-20b Model Card](https://huggingface.co/openai/gpt-oss-20b)
- [gpt-oss-20b GGUF](https://huggingface.co/ggml-org/gpt-oss-20b-GGUF)
- [AutoAWQ Documentation](https://github.com/casper-hansen/AutoAWQ)
- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)

---

## âœ… Checklist Summary

**Phase 1**: í™˜ê²½ ì„¤ì • ë° ëª¨ë¸ ë¡œë”©
- [ ] Python 3.12 venv í™˜ê²½ ì„¤ì •
- [ ] requirements.txt ì—…ë°ì´íŠ¸ (Python 3.12 í˜¸í™˜)
- [ ] src/llm_loader.py êµ¬í˜„
- [ ] GPU ë©”ëª¨ë¦¬ ì²´í¬ ë° ìµœì í™”

**Phase 2**: í•©ì„± ë°ì´í„° ìƒì„±
- [ ] src/synthetic_data_generator.py êµ¬í˜„
- [ ] Prompt templates ì‘ì„±
- [ ] í’ˆì§ˆ í•„í„°ë§ ë¡œì§

**Phase 3**: ë™ì˜ì–´ ì‚¬ì „ í™•ì¥
- [ ] src/cross_lingual_synonyms.py í™•ì¥
- [ ] LLM ê²€ì¦ í•¨ìˆ˜ ì¶”ê°€
- [ ] í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

**Phase 4**: Notebook í†µí•©
- [ ] ìƒˆ Cell ì¶”ê°€ (LLM ë¡œë”©, í•©ì„± ë°ì´í„°, ë™ì˜ì–´)
- [ ] ê¸°ì¡´ ì›Œí¬í”Œë¡œìš°ì™€ í†µí•©
- [ ] ê²°ê³¼ ì‹œê°í™”

**Phase 5**: ìµœì í™” ë° ê²€ì¦
- [ ] ì„±ëŠ¥ ìµœì í™”
- [ ] í’ˆì§ˆ í‰ê°€
- [ ] ë¬¸ì„œí™”

---

---

## ğŸš€ Quick Start (ARM í™˜ê²½)

### Step 1: Python 3.12 venv í™˜ê²½ ì„¤ì • ë° ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# venv ìƒì„± (Python 3.12)
python3.12 -m venv .venv
source .venv/bin/activate

# pip ì—…ê·¸ë ˆì´ë“œ
pip install --upgrade pip setuptools wheel

# PyTorch ì„¤ì¹˜ (CUDA 12.1 for GB10)
pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121

# Qwen3 ì‚¬ìš© ì‹œ (ê¶Œì¥)
pip install autoawq optimum accelerate transformers

# gpt-oss-20b ì‚¬ìš© ì‹œ (ì¶”ê°€) - ARM + Python 3.12 ë¹Œë“œ
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python --no-cache-dir
```

**Python 3.12 ì£¼ì˜ì‚¬í•­**:
- llama-cpp-pythonì€ ì†ŒìŠ¤ ë¹Œë“œê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ (ARM + CUDA)
- CMAKE_ARGSë¡œ CUDA ì§€ì› í™œì„±í™”
- Qwen3-AWQëŠ” Python 3.12ì—ì„œ ë³„ë„ ë¹Œë“œ ë¶ˆí•„ìš”

### Step 2: LLM ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

#### Option A: Qwen3-14B (AWQ 4-bit) - ê¶Œì¥ â­
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen3-14B-Instruct-AWQ"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",  # Auto GPU/CPU placement
    low_cpu_mem_usage=True,
)
```

#### Option B: Qwen3-7B (AWQ 4-bit) - ë©”ëª¨ë¦¬ ì œì•½ ì‹œ
```python
model_name = "Qwen/Qwen3-7B-Instruct-AWQ"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
)
```

#### Option C: gpt-oss-20b (GGUF) - llama.cpp í•„ìš”
```bash
# llama.cpp ì„¤ì¹˜
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make
```

```python
# Python binding ì‚¬ìš©
from llama_cpp import Llama

llm = Llama(
    model_path="gpt-oss-20b-Q4_0.gguf",
    n_ctx=2048,
    n_gpu_layers=-1,  # All layers to GPU
)
```

### Step 3: í•©ì„± ë°ì´í„° ìƒì„±
```python
from src.llm_loader import load_llm_model_quantized
from src.synthetic_data_generator import generate_synthetic_qd_pairs

# Qwen3 ëª¨ë¸ ë¡œë”©
llm_model, llm_tokenizer = load_llm_model_quantized(
    model_name="Qwen/Qwen3-14B-Instruct-AWQ",  # ë˜ëŠ” Qwen3-7B-Instruct-AWQ
    use_awq=True,
)

# í•©ì„± ë°ì´í„° ìƒì„±
synthetic_pairs = generate_synthetic_qd_pairs(
    documents=documents[:100],
    llm_model=llm_model,
    llm_tokenizer=llm_tokenizer,
    batch_size=2,  # Qwen3-14Bì— ìµœì í™”
)
```

---

**Updated**: 2025-11-13
**Status**: ARM + Python 3.12 ìµœì í™” ì™„ë£Œ, Ready for implementation
**Environment**:
- ARM aarch64 (Blackwell GB10)
- NVIDIA GB10 GPU (CUDA 13.0)
- Python 3.12 (venv)
- PyTorch 2.5.1 (CUDA 12.1)

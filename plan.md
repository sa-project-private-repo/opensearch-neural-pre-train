# OpenSearch Neural Sparse í•œêµ­ì–´ íŠ¹í™” ì‚¬ì „ í•™ìŠµ ê³„íš

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

**ëª©í‘œ:** í•œì˜ í˜¼ìš© í™˜ê²½ì— ìµœì í™”ëœ OpenSearch Neural Sparse Retrieval ëª¨ë¸ ì‚¬ì „ í•™ìŠµ

**ê¸°ëŒ€ íš¨ê³¼:**
- í•œêµ­ì–´ ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ
- í•œì˜ ë™ì˜ì–´ ìë™ ë§¤ì¹­ (ëª¨ë¸ â†’ model, ê²€ìƒ‰ â†’ search)
- Cross-lingual retrieval ì§€ì›
- OpenSearch í”ŒëŸ¬ê·¸ì¸ ì§ì ‘ ë°°í¬ ê°€ëŠ¥

---

## ğŸ¯ Phase 1: ì•„í‚¤í…ì²˜ ë° ë°ì´í„° ì¤€ë¹„ (ì™„ë£Œ)

### âœ… 1.1 ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ
- [x] Korean Wikipedia (100K documents)
- [x] KLUE ë°ì´í„°ì…‹
- [x] KorQuAD ë°ì´í„°ì…‹
- [x] Korean News with dates
- [x] LLM í•©ì„± Query-Document pairs
- [x] í•œì˜ ë™ì˜ì–´ ì‚¬ì „ (bilingual synonyms)

**ë°ì´í„° ìœ„ì¹˜:**
- `dataset/base_model/documents.json` (0.7 MB)
- `dataset/base_model/qd_pairs_base.pkl` (54.9 MB)
- `dataset/llm_generated/synthetic_qd_pairs.pkl` (2.6 KB)
- `dataset/llm_generated/enhanced_synonyms.json` (1.5 KB)

### âœ… 1.2 ì¸í”„ë¼ ì¤€ë¹„ ì™„ë£Œ
- [x] Ollama LLM í†µí•© (í•œêµ­ì–´ ë°ì´í„° ìƒì„±)
- [x] Wikipedia loader with caching
- [x] DatasetManager for data management
- [x] í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì¶•

---

## ğŸš€ Phase 2: Neural Sparse ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„¤ê³„ (ì§„í–‰ ì¤‘)

### ğŸ”§ 2.1 Neural Sparse Encoding ì´í•´

**Neural Sparse Retrieval í•µì‹¬ ê°œë…:**
```
Query/Document â†’ BERT Encoder â†’ Sparse Vector (Vocab Size)
                                    â†“
                              Top-K Non-zero Terms
                                    â†“
                         Inverted Index (OpenSearch)
```

**íŠ¹ì§•:**
- BERT ê¸°ë°˜ contextualized term weighting
- Sparse vector (ëŒ€ë¶€ë¶„ì˜ ê°’ì´ 0)
- Inverted indexì™€ í˜¸í™˜ (ê¸°ì¡´ ê²€ìƒ‰ ì¸í”„ë¼ í™œìš©)
- Dense retrievalë³´ë‹¤ í•´ì„ ê°€ëŠ¥ì„± ë†’ìŒ

### ğŸ“ 2.2 ëª¨ë¸ ì•„í‚¤í…ì²˜

```
Input: "í•œêµ­ì–´ ê²€ìƒ‰ ëª¨ë¸"
   â†“
BERT Encoder (klue/bert-base ë˜ëŠ” multilingual)
   â†“
Token Embeddings [CLS] í•œêµ­ì–´ ê²€ìƒ‰ ëª¨ë¸ [SEP]
   â†“
MLM Head (Masked Language Model style)
   â†“
Sparse Weights: {
    í•œêµ­ì–´: 0.85,
    ê²€ìƒ‰: 0.92,
    ëª¨ë¸: 0.88,
    search: 0.45,  â† í•œì˜ ë™ì˜ì–´ í•™ìŠµë¨
    model: 0.42,
    ...
}
```

**í•µì‹¬ ì»´í¬ë„ŒíŠ¸:**
1. **Base Encoder**: `klue/bert-base` ë˜ëŠ” `xlm-roberta-base`
2. **Projection Head**: Token-level MLM-style output
3. **Loss Function**: FLOPS regularization + Ranking loss

### ğŸ¯ 2.3 í•™ìŠµ ëª©í‘œ ì •ì˜

**Multi-task Learning:**

1. **Query-Document Matching (Primary)**
   - Positive pairs: (query, relevant_doc) â†’ high similarity
   - Hard negatives: (query, irrelevant_doc) â†’ low similarity
   - Loss: Contrastive loss or margin ranking loss

2. **Cross-lingual Term Alignment (Secondary)**
   - í•œì˜ ë™ì˜ì–´ ìŒì˜ activation ìœ ì‚¬ë„ ìµœëŒ€í™”
   - Ex: "ëª¨ë¸" vs "model" â†’ similar sparse patterns
   - Loss: Cosine similarity loss

3. **Sparsity Regularization (Constraint)**
   - FLOPS (FLoating point OPerations) ì œì•½
   - ë„ˆë¬´ ë§ì€ termì´ activateë˜ë©´ ì„±ëŠ¥ ì €í•˜
   - Loss: L1 regularization on activations

**ì¢…í•© Loss:**
```python
total_loss = (
    Î± * ranking_loss +           # Query-doc matching
    Î² * cross_lingual_loss +     # í•œì˜ ë™ì˜ì–´
    Î³ * sparsity_loss            # Sparsity constraint
)
```

---

## ğŸ“Š Phase 3: í•™ìŠµ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

### ğŸ”„ 3.1 ë°ì´í„° ì¦ê°• ì „ëµ

**í˜„ì¬ ë°ì´í„°:**
- Base QD pairs: ~100K pairs
- Synthetic QD pairs: ~10 pairs (í…ŒìŠ¤íŠ¸)
- Bilingual synonyms: 32 entries

**ì¦ê°• ê³„íš:**

#### Step 1: LLM í•©ì„± ë°ì´í„° ëŒ€ëŸ‰ ìƒì„±
```bash
# Notebook 2 ì¬ì‹¤í–‰ (1000 documents)
# ì˜ˆìƒ ì¶œë ¥: 3000 synthetic pairs (1000 docs Ã— 3 queries)
```

#### Step 2: Hard Negative Mining
- ê° queryì— ëŒ€í•´ BM25ë¡œ ìƒìœ„ 100ê°œ ë¬¸ì„œ ê²€ìƒ‰
- Positive ì œì™¸í•œ ìƒìœ„ 10ê°œë¥¼ hard negativesë¡œ ì‚¬ìš©
- ì˜ˆìƒ ì¶œë ¥: 100K Ã— 10 = 1M negative pairs

#### Step 3: Cross-lingual Augmentation
- í•œì˜ ë™ì˜ì–´ ì‚¬ì „ í™œìš©
- Queryì˜ termì„ ì˜ì–´ë¡œ ì¹˜í™˜
- Ex: "ê²€ìƒ‰ ëª¨ë¸" â†’ "search ëª¨ë¸", "ê²€ìƒ‰ model", "search model"
- ì˜ˆìƒ ì¶œë ¥: ê¸°ì¡´ ë°ì´í„° Ã— 2-3ë°° ì¦ê°•

### ğŸ“ 3.2 ìµœì¢… í•™ìŠµ ë°ì´í„° êµ¬ì¡°

```
TrainingDataset/
  â”œâ”€ positive_pairs/
  â”‚   â”œâ”€ original_qd_pairs.pkl      # 100K pairs
  â”‚   â”œâ”€ synthetic_qd_pairs.pkl     # 3K pairs
  â”‚   â””â”€ augmented_qd_pairs.pkl     # 300K pairs (cross-lingual)
  â”‚
  â”œâ”€ negative_pairs/
  â”‚   â”œâ”€ hard_negatives.pkl         # 1M pairs (BM25 mining)
  â”‚   â””â”€ random_negatives.pkl       # 100K pairs (sampling)
  â”‚
  â””â”€ bilingual_synonyms/
      â””â”€ synonym_pairs.json          # 32+ entries
```

**ì˜ˆìƒ ì´ í•™ìŠµ ë°ì´í„°:**
- Positive: ~400K pairs
- Negative: ~1.1M pairs
- Synonym pairs: 100+ pairs

---

## ğŸ—ï¸ Phase 4: ëª¨ë¸ í•™ìŠµ êµ¬í˜„

### ğŸ”¨ 4.1 êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### â˜ Step 4.1.1: Base Model ì„ íƒ
```python
# Option 1: í•œêµ­ì–´ íŠ¹í™” (ê¶Œì¥)
base_model = "klue/bert-base"

# Option 2: ë‹¤êµ­ì–´ ì§€ì›
base_model = "xlm-roberta-base"

# Option 3: ê²½ëŸ‰í™”
base_model = "klue/roberta-small"
```

**ì„ íƒ ê¸°ì¤€:**
- í•œêµ­ì–´ ì„±ëŠ¥: klue/bert-base > xlm-roberta-base
- ë‹¤êµ­ì–´ ì§€ì›: xlm-roberta-base
- ì†ë„: klue/roberta-small

#### â˜ Step 4.1.2: Neural Sparse Encoder êµ¬í˜„
```python
# src/models/neural_sparse_encoder.py

class NeuralSparseEncoder(nn.Module):
    def __init__(self, base_model: str, vocab_size: int):
        self.bert = AutoModel.from_pretrained(base_model)
        self.projection = nn.Linear(768, vocab_size)  # BERT hidden â†’ vocab
        self.activation = nn.ReLU()  # Non-negative weights

    def forward(self, input_ids, attention_mask):
        # BERT encoding
        outputs = self.bert(input_ids, attention_mask)
        token_embeddings = outputs.last_hidden_state

        # Sparse projection
        sparse_logits = self.projection(token_embeddings)
        sparse_weights = self.activation(sparse_logits)

        # Max pooling over tokens (query/doc representation)
        sparse_vec, _ = torch.max(sparse_weights, dim=1)

        return sparse_vec  # [batch, vocab_size]
```

#### â˜ Step 4.1.3: Loss Functions êµ¬í˜„
```python
# src/training/losses.py

def ranking_loss(query_vec, pos_doc_vec, neg_doc_vecs):
    """Margin ranking loss for query-document matching."""
    pos_score = torch.sum(query_vec * pos_doc_vec, dim=-1)
    neg_scores = torch.sum(query_vec * neg_doc_vecs, dim=-1)

    margin = 0.1
    loss = torch.relu(margin - pos_score + neg_scores).mean()
    return loss

def cross_lingual_loss(korean_vec, english_vec):
    """Cosine similarity loss for bilingual terms."""
    cos_sim = F.cosine_similarity(korean_vec, english_vec)
    loss = 1 - cos_sim.mean()
    return loss

def flops_loss(sparse_vec, lambda_flops=0.001):
    """FLOPS regularization for sparsity."""
    l1_norm = torch.sum(torch.abs(sparse_vec), dim=-1)
    loss = lambda_flops * l1_norm.mean()
    return loss
```

#### â˜ Step 4.1.4: Training Loop êµ¬í˜„
```python
# src/training/trainer.py

class NeuralSparseTrainer:
    def train_epoch(self):
        for batch in self.train_loader:
            # Forward pass
            query_vec = self.model(batch['query_ids'])
            pos_doc_vec = self.model(batch['pos_doc_ids'])
            neg_doc_vecs = self.model(batch['neg_doc_ids'])

            # Compute losses
            rank_loss = ranking_loss(query_vec, pos_doc_vec, neg_doc_vecs)
            sparse_loss = flops_loss(query_vec) + flops_loss(pos_doc_vec)

            # Optional: cross-lingual loss
            if batch.has('synonym_pairs'):
                kor_vec = self.model(batch['korean_term_ids'])
                eng_vec = self.model(batch['english_term_ids'])
                cl_loss = cross_lingual_loss(kor_vec, eng_vec)
            else:
                cl_loss = 0

            # Total loss
            loss = rank_loss + 0.1 * cl_loss + sparse_loss

            # Backward
            loss.backward()
            self.optimizer.step()
```

### âš™ï¸ 4.2 í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°

```yaml
# config/training_config.yaml

model:
  base: "klue/bert-base"
  hidden_size: 768
  vocab_size: 30000  # BERT vocab size

training:
  epochs: 10
  batch_size: 32
  learning_rate: 2e-5
  warmup_steps: 1000
  max_grad_norm: 1.0

  # Loss weights
  alpha_ranking: 1.0      # Query-doc matching
  beta_cross_lingual: 0.1 # í•œì˜ ë™ì˜ì–´
  gamma_sparsity: 0.001   # FLOPS regularization

  # Negative sampling
  num_hard_negatives: 10
  num_random_negatives: 5

data:
  max_seq_length: 256
  query_max_length: 64
  doc_max_length: 256

evaluation:
  eval_steps: 1000
  save_steps: 2000
  metric: "ndcg@10"
```

### ğŸ–¥ï¸ 4.3 GPU ë©”ëª¨ë¦¬ ìµœì í™”

**ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:**
- Base model (klue/bert-base): ~500MB
- Batch (32 samples): ~2GB
- Optimizer states: ~1GB
- **Total: ~3.5GB**

**ìµœì í™” ì „ëµ:**
```python
# Mixed precision training
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    loss = compute_loss(...)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

**Gradient accumulation:**
```python
# Effective batch size = 32 Ã— 4 = 128
accumulation_steps = 4

for i, batch in enumerate(dataloader):
    loss = compute_loss(batch) / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

---

## ğŸ“ˆ Phase 5: í‰ê°€ ë° ê²€ì¦

### ğŸ¯ 5.1 í‰ê°€ ë°ì´í„°ì…‹

**Evaluation Sets:**
1. **Test set** (held-out 10%)
   - Base QD pairsì—ì„œ ë¶„ë¦¬
   - ~10K pairs

2. **Cross-lingual test**
   - í•œêµ­ì–´ query + ì˜ì–´ term í¬í•¨ ë¬¸ì„œ
   - ~1K pairs

3. **Real-world queries**
   - ì‹¤ì œ ê²€ìƒ‰ ë¡œê·¸ (ìˆë‹¤ë©´)
   - OpenSearch ê³µì‹ ë¬¸ì„œ ê²€ìƒ‰ ë“±

### ğŸ“Š 5.2 í‰ê°€ ë©”íŠ¸ë¦­

**Retrieval Metrics:**
```python
# Mean Reciprocal Rank
MRR = avg(1 / rank_of_first_relevant)

# Normalized Discounted Cumulative Gain
NDCG@10 = DCG@10 / IDCG@10

# Recall@K
Recall@10 = (relevant_in_top10 / total_relevant)

# Precision@K
Precision@10 = (relevant_in_top10 / 10)
```

**Sparsity Metrics:**
```python
# Average number of non-zero terms
avg_active_terms = mean(count(sparse_vec > threshold))

# FLOPS (floating point operations)
flops = sum(sparse_vec)
```

**Cross-lingual Metrics:**
```python
# í•œì˜ ë™ì˜ì–´ activation ìœ ì‚¬ë„
synonym_similarity = cosine_sim(vec["ëª¨ë¸"], vec["model"])
```

### ğŸ” 5.3 ë¶„ì„ ë° ë””ë²„ê¹…

**ë¶„ì„ í•­ëª©:**
1. **Top-K activated terms ë¶„ì„**
   - Query: "í•œêµ­ì–´ ê²€ìƒ‰"
   - Activated: {í•œêµ­ì–´: 0.9, ê²€ìƒ‰: 0.85, search: 0.4, ...}

2. **í•œì˜ ë™ì˜ì–´ ë§¤ì¹­ ê²€ì¦**
   - "ëª¨ë¸" vs "model" activation ë¹„êµ
   - Cross-lingual retrieval ì„±ê³µë¥ 

3. **Failure case ë¶„ì„**
   - Retrieval ì‹¤íŒ¨ ì‚¬ë¡€ ìˆ˜ì§‘
   - Common patterns íŒŒì•…

---

## ğŸš¢ Phase 6: OpenSearch í†µí•© ë° ë°°í¬

### ğŸ”Œ 6.1 OpenSearch í”ŒëŸ¬ê·¸ì¸ ë³€í™˜

**ëª¨ë¸ Export:**
```python
# PyTorch â†’ ONNX
torch.onnx.export(
    model,
    dummy_input,
    "neural_sparse_korean.onnx",
    opset_version=14
)

# ONNX â†’ TorchScript (OpenSearch compatible)
traced_model = torch.jit.trace(model, dummy_input)
traced_model.save("neural_sparse_korean.pt")
```

**OpenSearch ì„¤ì •:**
```json
{
  "model_id": "neural-sparse-korean-v1",
  "model_format": "TORCH_SCRIPT",
  "model_config": {
    "model_type": "bert",
    "embedding_dimension": 768,
    "framework_type": "sentence_transformers"
  }
}
```

### ğŸ§ª 6.2 í†µí•© í…ŒìŠ¤íŠ¸

**End-to-End Test:**
1. ëª¨ë¸ ì—…ë¡œë“œ to OpenSearch
2. Index ìƒì„± with neural sparse pipeline
3. Document ì¸ë±ì‹±
4. ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰
5. ê²°ê³¼ ê²€ì¦

```bash
# OpenSearch neural sparse search
POST /my-index/_search
{
  "query": {
    "neural_sparse": {
      "my_text_field": {
        "query_text": "í•œêµ­ì–´ ê²€ìƒ‰ ëª¨ë¸",
        "model_id": "neural-sparse-korean-v1"
      }
    }
  }
}
```

### ğŸ“Š 6.3 ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

**Baseline ë¹„êµ:**
- BM25 (keyword)
- Dense retrieval (DPR, ANCE)
- Hybrid (BM25 + Dense)
- **Neural Sparse (Ours)**

**ì¸¡ì • í•­ëª©:**
- Retrieval quality (NDCG@10)
- Latency (ms per query)
- Throughput (QPS)
- Index size

---

## ğŸ“ Phase 7: ë¬¸ì„œí™” ë° ë°°í¬

### ğŸ“š 7.1 ë¬¸ì„œ ì‘ì„±

- [ ] `MODEL_CARD.md` - ëª¨ë¸ ì„¤ëª… ë° ì‚¬ìš©ë²•
- [ ] `TRAINING_GUIDE.md` - í•™ìŠµ ê°€ì´ë“œ
- [ ] `DEPLOYMENT_GUIDE.md` - OpenSearch ë°°í¬ ê°€ì´ë“œ
- [ ] `API_REFERENCE.md` - API ë¬¸ì„œ
- [ ] `EVALUATION_REPORT.md` - í‰ê°€ ë¦¬í¬íŠ¸

### ğŸ 7.2 ë°°í¬ ì¤€ë¹„

- [ ] HuggingFace Hub ì—…ë¡œë“œ
- [ ] Docker image ë¹Œë“œ
- [ ] OpenSearch plugin packaging
- [ ] Example notebooks
- [ ] Demo application

---

## ğŸ—“ï¸ íƒ€ì„ë¼ì¸

### Week 1-2: ë°ì´í„° ë° ê¸°ë°˜ êµ¬ì¶• âœ…
- [x] ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬
- [x] LLM í†µí•© ë° í•©ì„± ë°ì´í„° ìƒì„±
- [x] í•œì˜ ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶•

### Week 3: ë°ì´í„° ì¦ê°• ë° íŒŒì´í”„ë¼ì¸
- [ ] Hard negative mining êµ¬í˜„
- [ ] Cross-lingual augmentation
- [ ] DataLoader ë° í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- [ ] Notebook 2 ì¬ì‹¤í–‰ (ëŒ€ëŸ‰ í•©ì„± ë°ì´í„°)

### Week 4: ëª¨ë¸ êµ¬í˜„
- [ ] Neural Sparse Encoder êµ¬í˜„
- [ ] Loss functions êµ¬í˜„
- [ ] Trainer êµ¬í˜„
- [ ] ì´ˆê¸° í•™ìŠµ ì‹¤í—˜

### Week 5: í•™ìŠµ ë° ìµœì í™”
- [ ] Full training run
- [ ] Hyperparameter tuning
- [ ] ëª¨ë¸ ìµœì í™” (pruning, quantization)

### Week 6: í‰ê°€ ë° ë¶„ì„
- [ ] í‰ê°€ ë°ì´í„°ì…‹ êµ¬ì¶•
- [ ] ì¢…í•© í‰ê°€ ì‹¤í–‰
- [ ] Failure case ë¶„ì„
- [ ] ê°œì„  ë°˜ë³µ

### Week 7: OpenSearch í†µí•©
- [ ] ëª¨ë¸ export (ONNX, TorchScript)
- [ ] OpenSearch í”ŒëŸ¬ê·¸ì¸ í†µí•©
- [ ] End-to-end í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### Week 8: ë¬¸ì„œí™” ë° ë°°í¬
- [ ] ë¬¸ì„œ ì‘ì„±
- [ ] HuggingFace Hub ì—…ë¡œë“œ
- [ ] Demo ì• í”Œë¦¬ì¼€ì´ì…˜
- [ ] ìµœì¢… ê²€í†  ë° ë°°í¬

---

## ğŸ“¦ í˜„ì¬ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
opensearch-neural-pre-train/
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ base_model/              # Phase 1 ì™„ë£Œ âœ…
â”‚   â”‚   â”œâ”€â”€ documents.json
â”‚   â”‚   â”œâ”€â”€ qd_pairs_base.pkl
â”‚   â”‚   â””â”€â”€ bilingual_synonyms.json
â”‚   â””â”€â”€ llm_generated/           # Phase 1 ì™„ë£Œ âœ…
â”‚       â”œâ”€â”€ synthetic_qd_pairs.pkl
â”‚       â””â”€â”€ enhanced_synonyms.json
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dataset_manager.py       # ì™„ë£Œ âœ…
â”‚   â”œâ”€â”€ llm_loader.py            # ì™„ë£Œ âœ…
â”‚   â”œâ”€â”€ wikipedia_loader.py      # ì™„ë£Œ âœ…
â”‚   â”œâ”€â”€ synthetic_data_generator.py  # ì™„ë£Œ âœ…
â”‚   â”œâ”€â”€ cross_lingual_synonyms.py    # ì™„ë£Œ âœ…
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                  # TODO ğŸ”¨
â”‚   â”‚   â”œâ”€â”€ neural_sparse_encoder.py
â”‚   â”‚   â””â”€â”€ model_config.py
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                # TODO ğŸ”¨
â”‚   â”‚   â”œâ”€â”€ trainer.py
â”‚   â”‚   â”œâ”€â”€ losses.py
â”‚   â”‚   â”œâ”€â”€ data_collator.py
â”‚   â”‚   â””â”€â”€ hard_negative_miner.py
â”‚   â”‚
â”‚   â””â”€â”€ evaluation/              # TODO ğŸ”¨
â”‚       â”œâ”€â”€ metrics.py
â”‚       â””â”€â”€ evaluator.py
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_neural_sparse_base_training.ipynb  # ì™„ë£Œ âœ…
â”‚   â”œâ”€â”€ 02_llm_synthetic_data_generation.ipynb  # ì™„ë£Œ âœ…
â”‚   â”œâ”€â”€ 03_llm_enhanced_training.ipynb  # ì§„í–‰ ì˜ˆì •
â”‚   â”œâ”€â”€ 04_data_augmentation.ipynb      # TODO ğŸ”¨
â”‚   â”œâ”€â”€ 05_model_training.ipynb         # TODO ğŸ”¨
â”‚   â”œâ”€â”€ 06_evaluation.ipynb             # TODO ğŸ”¨
â”‚   â””â”€â”€ 07_opensearch_integration.ipynb # TODO ğŸ”¨
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ training_config.yaml     # TODO ğŸ”¨
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_korean_generation.py  # ì™„ë£Œ âœ…
â”‚   â””â”€â”€ test_neural_sparse_encoder.py  # TODO ğŸ”¨
â”‚
â”œâ”€â”€ plan.md                      # ì´ ë¬¸ì„œ
â””â”€â”€ README.md
```

---

## ğŸ¯ ê°œë°œ ìˆœì„œ (Implementation Order)

### ğŸ“… **ê¶Œì¥ ê°œë°œ ìˆœì„œ**

ì•„ë˜ ìˆœì„œëŒ€ë¡œ ê°œë°œí•˜ë©´ íš¨ìœ¨ì ìœ¼ë¡œ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

---

#### **Step 1: Hard Negative Miner êµ¬í˜„** â±ï¸ 30ë¶„
**íŒŒì¼:** `src/training/hard_negative_miner.py`

**ì‘ì—… ë‚´ìš©:**
```python
# BM25 ê¸°ë°˜ negative sampling êµ¬í˜„
class HardNegativeMiner:
    def __init__(self, documents, bm25_index):
        pass

    def mine_hard_negatives(self, query, positive_doc_id, top_k=100):
        # 1. BM25ë¡œ top_k ë¬¸ì„œ ê²€ìƒ‰
        # 2. Positive ì œì™¸
        # 3. ìƒìœ„ Nê°œë¥¼ hard negativesë¡œ ë°˜í™˜
        pass
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
- [ ] Queryë³„ top-k retrieval
- [ ] Positive í•„í„°ë§
- [ ] Batch processing ì§€ì›
- [ ] ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì‘ì„±

**ì™„ë£Œ ê¸°ì¤€:** 10ê°œ queryì— ëŒ€í•´ hard negatives ì„±ê³µì ìœ¼ë¡œ ìƒì„±

---

#### **Step 2: Neural Sparse Encoder ê¸°ë³¸ êµ¬í˜„** â±ï¸ 1ì‹œê°„
**íŒŒì¼:** `src/models/neural_sparse_encoder.py`

**ì‘ì—… ë‚´ìš©:**
```python
class NeuralSparseEncoder(nn.Module):
    def __init__(self, base_model="klue/bert-base"):
        # BERT encoder + projection layer
        pass

    def forward(self, input_ids, attention_mask):
        # 1. BERT encoding
        # 2. Sparse projection
        # 3. Max pooling
        # Return: [batch, vocab_size] sparse vector
        pass
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] BERT base ë¡œë“œ (klue/bert-base)
- [ ] Projection layer êµ¬í˜„
- [ ] Forward pass êµ¬í˜„
- [ ] Output shape ê²€ì¦
- [ ] Dummy inputìœ¼ë¡œ í…ŒìŠ¤íŠ¸

**ì™„ë£Œ ê¸°ì¤€:**
- Input: "í•œêµ­ì–´ ê²€ìƒ‰" â†’ Output: [30000] sparse vector
- Non-zero terms < 100ê°œ
- GPU ë©”ëª¨ë¦¬ < 2GB

---

#### **Step 3: Notebook 2 ì¬ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)** â±ï¸ 2-3ì‹œê°„
**íŒŒì¼:** `notebooks/02_llm_synthetic_data_generation.ipynb`

**ì‘ì—… ë‚´ìš©:**
```python
# Cell 8 ìˆ˜ì •
synthetic_pairs = generate_synthetic_qd_pairs(
    documents=documents[:1000],  # 10 â†’ 1000
    llm_model=llm_model,
    llm_tokenizer=llm_tokenizer,
    num_queries_per_doc=3,
    max_documents=1000,  # 10 â†’ 1000
    enable_filtering=True,
    verbose=True,
)
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] max_documents ë³€ê²½: 10 â†’ 1000
- [ ] Notebook ì‹¤í–‰ ì‹œì‘
- [ ] ë°±ê·¸ë¼ìš´ë“œë¡œ ì§„í–‰ (í„°ë¯¸ë„ì—ì„œ nohup ì‚¬ìš©)
- [ ] ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§

**ì™„ë£Œ ê¸°ì¤€:**
- 3000ê°œ synthetic pairs ìƒì„± ì™„ë£Œ
- `dataset/llm_generated/synthetic_qd_pairs.pkl` ì—…ë°ì´íŠ¸
- ë°ì´í„° í’ˆì§ˆ ê²€ì¦ (í•œêµ­ì–´ ì¿¼ë¦¬ ë¹„ìœ¨ > 80%)

**ë³‘ë ¬ ì‘ì—…:** ì´ ë…¸íŠ¸ë¶ì´ ëŒì•„ê°€ëŠ” ë™ì•ˆ Step 4, 5 ì§„í–‰

---

#### **Step 4: Loss Functions êµ¬í˜„** â±ï¸ 1ì‹œê°„
**íŒŒì¼:** `src/training/losses.py`

**ì‘ì—… ë‚´ìš©:**
```python
def ranking_loss(query_vec, pos_doc_vec, neg_doc_vecs, margin=0.1):
    """Margin ranking loss for retrieval."""
    pass

def cross_lingual_loss(korean_vec, english_vec):
    """Cosine similarity loss for bilingual terms."""
    pass

def flops_loss(sparse_vec, lambda_flops=0.001):
    """FLOPS regularization for sparsity."""
    pass

def combined_loss(query_vec, pos_vec, neg_vecs,
                  alpha=1.0, beta=0.1, gamma=0.001):
    """Combined loss function."""
    return (
        alpha * ranking_loss(...) +
        beta * cross_lingual_loss(...) +
        gamma * flops_loss(...)
    )
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] Ranking loss (margin-based)
- [ ] Cross-lingual loss (cosine similarity)
- [ ] FLOPS loss (L1 regularization)
- [ ] Combined loss with weights
- [ ] Unit tests for each loss

**ì™„ë£Œ ê¸°ì¤€:** ëª¨ë“  loss function í…ŒìŠ¤íŠ¸ í†µê³¼

---

#### **Step 5: Data Collator êµ¬í˜„** â±ï¸ 30ë¶„
**íŒŒì¼:** `src/training/data_collator.py`

**ì‘ì—… ë‚´ìš©:**
```python
class NeuralSparseDataCollator:
    def __init__(self, tokenizer, max_length=256):
        pass

    def __call__(self, batch):
        # 1. Query, positive doc, negative docs tokenize
        # 2. Padding and truncation
        # 3. Return batch dict
        pass
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] Query tokenization
- [ ] Positive/negative doc tokenization
- [ ] Dynamic padding
- [ ] Attention mask ìƒì„±
- [ ] Batch êµ¬ì¡° ê²€ì¦

**ì™„ë£Œ ê¸°ì¤€:** DataLoaderì—ì„œ ì •ìƒ ë™ì‘

---

#### **Step 6: Training Dataset êµ¬í˜„** â±ï¸ 1ì‹œê°„
**íŒŒì¼:** `src/training/neural_sparse_dataset.py`

**ì‘ì—… ë‚´ìš©:**
```python
class NeuralSparseDataset(Dataset):
    def __init__(self, qd_pairs, hard_negatives, documents):
        # Load data and create index
        pass

    def __getitem__(self, idx):
        # Return (query, pos_doc, [neg_docs])
        pass
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] QD pairs ë¡œë“œ
- [ ] Hard negatives í†µí•©
- [ ] Random negatives ìƒ˜í”Œë§
- [ ] Document index êµ¬ì¶•
- [ ] Train/val split

**ì™„ë£Œ ê¸°ì¤€:**
- Dataset length > 100K
- ê° sampleì— positive 1ê°œ + negatives 10-15ê°œ

---

#### **Step 7: Trainer êµ¬í˜„** â±ï¸ 2ì‹œê°„
**íŒŒì¼:** `src/training/trainer.py`

**ì‘ì—… ë‚´ìš©:**
```python
class NeuralSparseTrainer:
    def __init__(self, model, train_dataset, eval_dataset, config):
        pass

    def train_epoch(self):
        # Training loop
        pass

    def evaluate(self):
        # Evaluation loop
        pass

    def train(self):
        # Full training with logging
        pass
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] Training loop êµ¬í˜„
- [ ] Gradient accumulation
- [ ] Mixed precision (AMP)
- [ ] Logging (wandb or tensorboard)
- [ ] Checkpointing
- [ ] Early stopping
- [ ] Learning rate scheduling

**ì™„ë£Œ ê¸°ì¤€:**
- 10 step í•™ìŠµ ì„±ê³µ
- Loss ê°ì†Œ í™•ì¸
- Checkpoint ì €ì¥ í™•ì¸

---

#### **Step 8: Training Config ì‘ì„±** â±ï¸ 15ë¶„
**íŒŒì¼:** `config/training_config.yaml`

**ì‘ì—… ë‚´ìš©:**
```yaml
model:
  base: "klue/bert-base"
  vocab_size: 30000

training:
  epochs: 10
  batch_size: 32
  learning_rate: 2e-5

  alpha_ranking: 1.0
  beta_cross_lingual: 0.1
  gamma_sparsity: 0.001
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] ëª¨ë¸ ì„¤ì •
- [ ] í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
- [ ] Loss weights
- [ ] ë°ì´í„° ì„¤ì •
- [ ] í‰ê°€ ì„¤ì •

---

#### **Step 9: ì´ˆê¸° í•™ìŠµ ì‹¤í—˜** â±ï¸ 3-4ì‹œê°„
**íŒŒì¼:** `notebooks/05_model_training.ipynb`

**ì‘ì—… ë‚´ìš©:**
```python
# 1. Config ë¡œë“œ
# 2. Model, dataset, trainer ì´ˆê¸°í™”
# 3. í•™ìŠµ ì‹¤í–‰ (small scale)
# 4. ê²°ê³¼ ë¶„ì„
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] ì†ŒëŸ‰ ë°ì´í„°ë¡œ í•™ìŠµ (10K pairs)
- [ ] 1 epoch ì™„ë£Œ
- [ ] Loss ê°ì†Œ í™•ì¸
- [ ] Sparse vector í’ˆì§ˆ í™•ì¸
- [ ] í•™ìŠµ ì‹œê°„ ì¸¡ì •

**ì™„ë£Œ ê¸°ì¤€:**
- 1 epoch í•™ìŠµ ì„±ê³µ
- Lossê°€ ìˆ˜ë ´í•˜ëŠ” ê²½í–¥ í™•ì¸
- Top-K activated terms ë¶„ì„

---

#### **Step 10: Data Augmentation Pipeline** â±ï¸ 2ì‹œê°„
**íŒŒì¼:** `notebooks/04_data_augmentation.ipynb`

**ì‘ì—… ë‚´ìš©:**
```python
# 1. Cross-lingual augmentation
# 2. Hard negative mining (ì „ì²´ ë°ì´í„°)
# 3. Final training dataset ìƒì„±
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] í•œì˜ ë™ì˜ì–´ ì¹˜í™˜ (2-3ë°° ì¦ê°•)
- [ ] ì „ì²´ ë°ì´í„° hard negative mining
- [ ] Train/val/test split (80/10/10)
- [ ] ìµœì¢… ë°ì´í„°ì…‹ ì €ì¥

**ì™„ë£Œ ê¸°ì¤€:**
- Total pairs: 400K+ positive, 1M+ negative
- ë°ì´í„° í’ˆì§ˆ ê²€ì¦
- í†µê³„ ë¦¬í¬íŠ¸ ìƒì„±

---

#### **Step 11: Full Training Run** â±ï¸ 1-2ì¼
**íŒŒì¼:** `notebooks/05_model_training.ipynb`

**ì‘ì—… ë‚´ìš©:**
```python
# Full training with augmented data
trainer.train(
    epochs=10,
    batch_size=32,
)
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] Full dataset ë¡œë“œ
- [ ] 10 epochs í•™ìŠµ
- [ ] Validation ëª¨ë‹ˆí„°ë§
- [ ] Best checkpoint ì €ì¥
- [ ] Learning curves ë¶„ì„

**ì™„ë£Œ ê¸°ì¤€:**
- í•™ìŠµ ì™„ë£Œ (10 epochs)
- Best validation NDCG@10 > 0.5
- Model checkpoint ì €ì¥

---

#### **Step 12: Evaluation Framework** â±ï¸ 2ì‹œê°„
**íŒŒì¼:** `src/evaluation/evaluator.py`, `notebooks/06_evaluation.ipynb`

**ì‘ì—… ë‚´ìš©:**
```python
# 1. Test set evaluation
# 2. Cross-lingual test
# 3. Baseline ë¹„êµ (BM25)
# 4. Analysis and visualization
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] Metrics êµ¬í˜„ (MRR, NDCG, Recall)
- [ ] Test set evaluation
- [ ] Cross-lingual retrieval test
- [ ] BM25 baseline ë¹„êµ
- [ ] Failure case ë¶„ì„
- [ ] Evaluation report ì‘ì„±

**ì™„ë£Œ ê¸°ì¤€:**
- NDCG@10 > 0.6
- Cross-lingual retrieval ë™ì‘ í™•ì¸
- ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸

---

#### **Step 13: Model Export** â±ï¸ 1ì‹œê°„
**íŒŒì¼:** `src/models/export.py`

**ì‘ì—… ë‚´ìš©:**
```python
# PyTorch â†’ ONNX â†’ TorchScript
torch.onnx.export(model, ...)
traced_model = torch.jit.trace(model, ...)
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] ONNX export
- [ ] TorchScript export
- [ ] Export ê²€ì¦
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

**ì™„ë£Œ ê¸°ì¤€:**
- Exported model ì •ìƒ ë™ì‘
- Latency ì¸¡ì •

---

#### **Step 14: OpenSearch Integration** â±ï¸ 1ì¼
**íŒŒì¼:** `notebooks/07_opensearch_integration.ipynb`

**ì‘ì—… ë‚´ìš©:**
```python
# 1. OpenSearch ì„¤ì •
# 2. Model ì—…ë¡œë“œ
# 3. Index ìƒì„±
# 4. End-to-end test
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] OpenSearch ì„¤ì¹˜/ì„¤ì •
- [ ] Model upload to OpenSearch
- [ ] Neural sparse pipeline ì„¤ì •
- [ ] Document indexing
- [ ] Search query í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

**ì™„ë£Œ ê¸°ì¤€:**
- OpenSearchì—ì„œ ê²€ìƒ‰ ì„±ê³µ
- Latency < 50ms
- ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ìƒíƒœ

---

#### **Step 15: Documentation** â±ï¸ 1ì¼
**ì‘ì—… ë‚´ìš©:**
- [ ] MODEL_CARD.md
- [ ] TRAINING_GUIDE.md
- [ ] DEPLOYMENT_GUIDE.md
- [ ] API_REFERENCE.md
- [ ] EVALUATION_REPORT.md
- [ ] README ì—…ë°ì´íŠ¸

---

### ğŸ“Š **ê°œë°œ ì§„í–‰ ìƒí™© ì²´í¬ë¦¬ìŠ¤íŠ¸**

#### Week 1 (ì™„ë£Œ âœ…)
- [x] ë°ì´í„° ìˆ˜ì§‘
- [x] LLM í†µí•©
- [x] í•œì˜ ë™ì˜ì–´ ì‚¬ì „
- [x] í•œêµ­ì–´ ì¿¼ë¦¬ ìƒì„± ìˆ˜ì •

#### Week 2 (í˜„ì¬)
- [ ] Step 1: Hard Negative Miner
- [ ] Step 2: Neural Sparse Encoder
- [ ] Step 3: Notebook 2 ì¬ì‹¤í–‰
- [ ] Step 4: Loss Functions
- [ ] Step 5: Data Collator

#### Week 3
- [ ] Step 6: Training Dataset
- [ ] Step 7: Trainer
- [ ] Step 8: Training Config
- [ ] Step 9: ì´ˆê¸° í•™ìŠµ ì‹¤í—˜

#### Week 4
- [ ] Step 10: Data Augmentation
- [ ] Step 11: Full Training Run

#### Week 5
- [ ] Step 12: Evaluation Framework
- [ ] Hyperparameter tuning

#### Week 6
- [ ] Step 13: Model Export
- [ ] Step 14: OpenSearch Integration

#### Week 7-8
- [ ] Step 15: Documentation
- [ ] Final testing
- [ ] Deployment

---

### ğŸ¯ **ì§€ê¸ˆ ë°”ë¡œ ì‹œì‘í•  ì‘ì—…**

**ì¦‰ì‹œ ì‹œì‘:** Step 1 (Hard Negative Miner) - 30ë¶„
**ë‹¤ìŒ:** Step 2 (Neural Sparse Encoder) - 1ì‹œê°„
**ë³‘ë ¬:** Step 3 (Notebook 2 ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰) - 2-3ì‹œê°„

ì´ ì˜ˆìƒ ì‹œê°„ (ì˜¤ëŠ˜): **1.5ì‹œê°„ ê°œë°œ + 2-3ì‹œê°„ ë°±ê·¸ë¼ìš´ë“œ**

---

---

## â“ ì˜ì‚¬ê²°ì • í•„ìš” í•­ëª©

### 1. Base Model ì„ íƒ
- [ ] **klue/bert-base** (í•œêµ­ì–´ íŠ¹í™”, ê¶Œì¥)
- [ ] **xlm-roberta-base** (ë‹¤êµ­ì–´)
- [ ] **ê¸°íƒ€**: _____________

### 2. í•™ìŠµ ë°ì´í„° ê·œëª¨
- [ ] Small (100K pairs) - ë¹ ë¥¸ ì‹¤í—˜
- [ ] **Medium (500K pairs)** - ê¶Œì¥
- [ ] Large (1M+ pairs) - ìµœê³  ì„±ëŠ¥

### 3. í‰ê°€ ì „ëµ
- [ ] Offline evaluation only
- [ ] **Online A/B testing** (OpenSearch í†µí•© í›„)

### 4. ë°°í¬ ìš°ì„ ìˆœìœ„
- [ ] HuggingFace Hub
- [ ] **OpenSearch Plugin**
- [ ] Docker Container
- [ ] API Server

---

## ğŸ“ ì°¸ê³  ìë£Œ

### Papers
- [SPLADE](https://arxiv.org/abs/2107.05720) - Sparse Lexical and Expansion Model
- [DeepImpact](https://arxiv.org/abs/2104.12016) - Neural Text Ranking
- [uniCOIL](https://arxiv.org/abs/2106.14807) - Contextualized Term Weighting

### Code References
- [naver/splade](https://github.com/naver/splade)
- [OpenSearch Neural Search](https://opensearch.org/docs/latest/neural-search-plugin/index/)
- [Sentence Transformers](https://www.sbert.net/)

### Datasets
- [KLUE Benchmark](https://klue-benchmark.com/)
- [KorQuAD](https://korquad.github.io/)
- [Korean Wikipedia](https://ko.wikipedia.org/)

---

## âœ… Success Criteria

í”„ë¡œì íŠ¸ ì„±ê³µ ê¸°ì¤€:

1. **ì„±ëŠ¥**
   - NDCG@10 > 0.6 (baseline BM25 ëŒ€ë¹„ +10%)
   - í•œì˜ cross-lingual retrieval ì§€ì›

2. **íš¨ìœ¨ì„±**
   - Query latency < 50ms
   - Average active terms < 100

3. **ë°°í¬**
   - OpenSearch í”ŒëŸ¬ê·¸ì¸ ë™ì‘ í™•ì¸
   - ì‹¤ì œ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì ìš© ê°€ëŠ¥

4. **ì¬í˜„ì„±**
   - ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ìë™í™”
   - ë¬¸ì„œí™” ì™„ë£Œ

---

**Last Updated:** 2025-11-16
**Status:** Phase 2 ì‹œì‘ (Neural Sparse ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„¤ê³„)
**Next Milestone:** Hard negative mining êµ¬í˜„ ë° ëŒ€ëŸ‰ í•©ì„± ë°ì´í„° ìƒì„±

# OpenSearch Neural Sparse Training - ë…¸íŠ¸ë¶ ë¶„ë¦¬ ê³„íš

## ğŸ“‹ ê°œìš”

í˜„ì¬ `korean_neural_sparse_training_v2_llm.ipynb`ëŠ” ëª¨ë“  ì‘ì—…ì„ ì›ìŠ¤í†±ìœ¼ë¡œ ì‹¤í–‰í•˜ë„ë¡ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
ì´ë¥¼ **ëª¨ë“ˆí™”ëœ 3ê°œì˜ ë…¸íŠ¸ë¶**ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.

### ëª©í‘œ
- âœ… ê° ë…¸íŠ¸ë¶ì„ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥
- âœ… LLM ëª¨ë¸ ë¡œë”© ì‹œê°„ ì ˆì•½ (í•œ ë²ˆë§Œ ë¡œë“œ)
- âœ… ì¤‘ê°„ ê²°ê³¼ë¬¼ ì¬ì‚¬ìš© ê°€ëŠ¥
- âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ìš´ì˜ (í•„ìš”í•œ ë…¸íŠ¸ë¶ë§Œ ì‹¤í–‰)
- âœ… ë””ë²„ê¹… ë° ì‹¤í—˜ ìš©ì´

---

## ğŸ—‚ï¸ ë…¸íŠ¸ë¶ ë¶„ë¦¬ êµ¬ì¡°

### ğŸ““ ë…¸íŠ¸ë¶ 1: `01_neural_sparse_base_training.ipynb`
**ëª©ì **: ê¸°ë³¸ Neural Sparse ëª¨ë¸ í•™ìŠµ (LLM ì—†ì´)

**í¬í•¨ ì„¹ì…˜** (ê¸°ì¡´ ì„¹ì…˜ 1-12):
1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
2. í•œêµ­ì–´ ë°ì´í„°ì…‹ ìˆ˜ì§‘
3. IDF ê³„ì‚°
4. í•œêµ­ì–´ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì¶”ê°€
5. ìë™ íŠ¸ë Œë“œ ê°ì§€ (Unsupervised)
6. í•œì˜ í†µí•© ë™ì˜ì–´ ì‚¬ì „ (Cross-lingual)
7. OpenSearch ë¬¸ì„œ ì¸ì½”ë” ëª¨ë¸ ì •ì˜
8. í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„
9. ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
10. í•™ìŠµ ì„¤ì • ë° ì‹¤í–‰
11. ëª¨ë¸ ì €ì¥ (OpenSearch í˜¸í™˜ í˜•ì‹)
12. ëª¨ë¸ í…ŒìŠ¤íŠ¸

**ì €ì¥ ë°ì´í„°** (`/dataset` í´ë”):
```
dataset/
â”œâ”€â”€ base_model/
â”‚   â”œâ”€â”€ korean_documents.json          # í•œêµ­ì–´ ë¬¸ì„œ ë°ì´í„°ì…‹
â”‚   â”œâ”€â”€ idf_statistics.pkl             # IDF í†µê³„
â”‚   â”œâ”€â”€ trend_keywords.json            # íŠ¸ë Œë“œ í‚¤ì›Œë“œ
â”‚   â”œâ”€â”€ bilingual_synonyms.json        # ê¸°ë³¸ í•œì˜ ë™ì˜ì–´ ì‚¬ì „
â”‚   â”œâ”€â”€ qd_pairs_base.pkl              # ê¸°ë³¸ Query-Document pairs
â”‚   â””â”€â”€ neural_sparse_v1_model/        # í•™ìŠµëœ v1 ëª¨ë¸
â”‚       â”œâ”€â”€ pytorch_model.bin
â”‚       â”œâ”€â”€ config.json
â”‚       â””â”€â”€ tokenizer/
â””â”€â”€ metadata.json                       # ë°ì´í„°ì…‹ ë©”íƒ€ì •ë³´
```

**ì‹¤í–‰ ì‹œê°„**: ~30-60ë¶„ (GPU ê¸°ì¤€)

---

### ğŸ““ ë…¸íŠ¸ë¶ 2: `02_llm_synthetic_data_generation.ipynb`
**ëª©ì **: LLM ëª¨ë¸ ë¡œë”© ë° í•©ì„± ë°ì´í„° ìƒì„±

**í¬í•¨ ì„¹ì…˜** (ê¸°ì¡´ ì„¹ì…˜ 13-15):
13. LLM ëª¨ë¸ ë¡œë”© ë° ì´ˆê¸°í™”
14. LLM ê¸°ë°˜ í•©ì„± Query-Document Pairs ìƒì„±
15. LLM ê¸°ë°˜ í•œì˜ ë™ì˜ì–´ ê²€ì¦ ë° í™•ì¥

**ë¡œë“œ ë°ì´í„°** (`/dataset` í´ë”ì—ì„œ):
- `korean_documents.json` - í•©ì„± ì¿¼ë¦¬ ìƒì„±ìš©
- `bilingual_synonyms.json` - ë™ì˜ì–´ ê²€ì¦ ë° í™•ì¥ìš©

**ì €ì¥ ë°ì´í„°** (`/dataset` í´ë”):
```
dataset/
â”œâ”€â”€ llm_generated/
â”‚   â”œâ”€â”€ synthetic_qd_pairs.pkl         # LLM ìƒì„± Query-Document pairs
â”‚   â”œâ”€â”€ enhanced_synonyms.json         # LLM ê²€ì¦/í™•ì¥ëœ ë™ì˜ì–´ ì‚¬ì „
â”‚   â””â”€â”€ generation_metadata.json       # ìƒì„± í†µê³„ ë° ë©”íƒ€ì •ë³´
â””â”€â”€ llm_cache/
    â””â”€â”€ model_cache/                    # LLM ëª¨ë¸ ìºì‹œ (Hugging Face)
```

**ì‹¤í–‰ ì‹œê°„**:
- ì²« ì‹¤í–‰: ~20-30ë¶„ (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ~30GB)
- ì´í›„ ì‹¤í–‰: ~10-15ë¶„ (ìºì‹œ ì‚¬ìš©)

**ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­**: ~35GB GPU VRAM (FP8 ëª¨ë¸)

---

### ğŸ““ ë…¸íŠ¸ë¶ 3: `03_llm_enhanced_training.ipynb`
**ëª©ì **: í•©ì„± ë°ì´í„° í¬í•¨ ì¬í•™ìŠµ ë° ì„±ëŠ¥ ë¹„êµ

**í¬í•¨ ì„¹ì…˜** (ê¸°ì¡´ ì„¹ì…˜ 16-17):
16. í•©ì„± ë°ì´í„° í¬í•¨ ëª¨ë¸ ì¬í•™ìŠµ
17. ì„±ëŠ¥ ë¹„êµ ë¶„ì„ (ê¸°ì¡´ vs LLM í™•ì¥)

**ë¡œë“œ ë°ì´í„°** (`/dataset` í´ë”ì—ì„œ):
- `base_model/` - ê¸°ë³¸ í•™ìŠµ ë°ì´í„° ë° v1 ëª¨ë¸
- `llm_generated/synthetic_qd_pairs.pkl` - í•©ì„± ë°ì´í„°
- `llm_generated/enhanced_synonyms.json` - í™•ì¥ëœ ë™ì˜ì–´

**ì €ì¥ ë°ì´í„°** (`/dataset` í´ë”):
```
dataset/
â”œâ”€â”€ enhanced_model/
â”‚   â”œâ”€â”€ neural_sparse_v2_model/        # LLM í™•ì¥ v2 ëª¨ë¸
â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â””â”€â”€ tokenizer/
â”‚   â”œâ”€â”€ training_history.json          # í•™ìŠµ íˆìŠ¤í† ë¦¬
â”‚   â””â”€â”€ performance_comparison.json    # v1 vs v2 ì„±ëŠ¥ ë¹„êµ
â””â”€â”€ evaluation/
    â”œâ”€â”€ v1_metrics.json                # v1 ëª¨ë¸ í‰ê°€ ì§€í‘œ
    â”œâ”€â”€ v2_metrics.json                # v2 ëª¨ë¸ í‰ê°€ ì§€í‘œ
    â””â”€â”€ comparison_plots/               # ë¹„êµ ì‹œê°í™”
        â”œâ”€â”€ mrr_comparison.png
        â”œâ”€â”€ ndcg_comparison.png
        â””â”€â”€ precision_recall.png
```

**ì‹¤í–‰ ì‹œê°„**: ~40-50ë¶„ (GPU ê¸°ì¤€)

---

## ğŸ“‚ ë°ì´í„° ì €ì¥/ë¡œë“œ ìœ í‹¸ë¦¬í‹°

### ìƒˆ íŒŒì¼: `src/dataset_manager.py`

```python
"""
ë°ì´í„°ì…‹ ì €ì¥ ë° ë¡œë“œ ìœ í‹¸ë¦¬í‹°
"""

import json
import pickle
from pathlib import Path
from typing import Any, Dict, List, Optional
import torch

class DatasetManager:
    """ë…¸íŠ¸ë¶ ê°„ ë°ì´í„° ê³µìœ ë¥¼ ìœ„í•œ ë§¤ë‹ˆì €"""

    def __init__(self, base_path: str = "dataset"):
        self.base_path = Path(base_path)
        self.base_path.mkdir(exist_ok=True)

    def save_json(self, data: Any, filename: str, subdir: str = ""):
        """JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
        path = self.base_path / subdir / filename
        path.parent.mkdir(parents=True, exist_ok=True)

        with open(path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ“ Saved: {path}")

    def load_json(self, filename: str, subdir: str = ""):
        """JSON íŒŒì¼ ë¡œë“œ"""
        path = self.base_path / subdir / filename

        if not path.exists():
            raise FileNotFoundError(f"File not found: {path}")

        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"âœ“ Loaded: {path}")
        return data

    def save_pickle(self, data: Any, filename: str, subdir: str = ""):
        """Pickle í˜•ì‹ìœ¼ë¡œ ì €ì¥ (Python ê°ì²´)"""
        path = self.base_path / subdir / filename
        path.parent.mkdir(parents=True, exist_ok=True)

        with open(path, 'wb') as f:
            pickle.dump(data, f)
        print(f"âœ“ Saved: {path}")

    def load_pickle(self, filename: str, subdir: str = ""):
        """Pickle íŒŒì¼ ë¡œë“œ"""
        path = self.base_path / subdir / filename

        if not path.exists():
            raise FileNotFoundError(f"File not found: {path}")

        with open(path, 'rb') as f:
            data = pickle.load(f)
        print(f"âœ“ Loaded: {path}")
        return data

    def save_model(self, model, tokenizer, model_dir: str, subdir: str = ""):
        """PyTorch ëª¨ë¸ ì €ì¥"""
        path = self.base_path / subdir / model_dir
        path.mkdir(parents=True, exist_ok=True)

        # Save model
        model.save_pretrained(path)
        tokenizer.save_pretrained(path)
        print(f"âœ“ Saved model: {path}")

    def load_model(self, model_class, model_dir: str, subdir: str = ""):
        """PyTorch ëª¨ë¸ ë¡œë“œ"""
        path = self.base_path / subdir / model_dir

        if not path.exists():
            raise FileNotFoundError(f"Model not found: {path}")

        from transformers import AutoTokenizer
        model = model_class.from_pretrained(path)
        tokenizer = AutoTokenizer.from_pretrained(path)
        print(f"âœ“ Loaded model: {path}")
        return model, tokenizer

    def check_data_exists(self, filename: str, subdir: str = "") -> bool:
        """ë°ì´í„° íŒŒì¼ ì¡´ì¬ í™•ì¸"""
        path = self.base_path / subdir / filename
        return path.exists()

    def list_files(self, subdir: str = "") -> List[str]:
        """íŠ¹ì • ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ëª©ë¡"""
        path = self.base_path / subdir
        if not path.exists():
            return []
        return [f.name for f in path.iterdir() if f.is_file()]
```

---

## ğŸ”„ ë…¸íŠ¸ë¶ ê°„ ë°ì´í„° íë¦„

```
[01_base_training] â†’ dataset/base_model/ â†’ [02_llm_generation]
                                          â†“
                     dataset/llm_generated/ â†’ [03_enhanced_training]
                                              â†“
                                       dataset/enhanced_model/
```

### ë°ì´í„° ì˜ì¡´ì„± ë§¤íŠ¸ë¦­ìŠ¤

| ë…¸íŠ¸ë¶ | í•„ìš” ë°ì´í„° | ìƒì„± ë°ì´í„° |
|--------|-------------|-------------|
| **01_base** | ì—†ìŒ (ì™¸ë¶€ ë°ì´í„° ìˆ˜ì§‘) | base_model/* |
| **02_llm** | korean_documents.json<br>bilingual_synonyms.json | llm_generated/* |
| **03_enhanced** | base_model/*<br>llm_generated/* | enhanced_model/*<br>evaluation/* |

---

## ğŸ“ ê° ë…¸íŠ¸ë¶ì˜ ì‹œì‘ ì½”ë“œ

### ë…¸íŠ¸ë¶ 1: Base Training

```python
# 01_neural_sparse_base_training.ipynb
# Cell 1: ì´ˆê¸°í™”
from src.dataset_manager import DatasetManager
from datetime import datetime

# ë°ì´í„°ì…‹ ë§¤ë‹ˆì € ì´ˆê¸°í™”
dm = DatasetManager(base_path="dataset")

# ì €ì¥í•  ë©”íƒ€ì •ë³´
metadata = {
    "notebook": "01_neural_sparse_base_training",
    "created_at": datetime.now().isoformat(),
    "python_version": "3.12",
    "gpu": "NVIDIA GB10",
}

print("âœ“ Dataset Manager initialized")
print(f"  Base path: {dm.base_path.absolute()}")
```

### ë…¸íŠ¸ë¶ 2: LLM Synthetic Data

```python
# 02_llm_synthetic_data_generation.ipynb
# Cell 1: ë°ì´í„° ë¡œë“œ
import os
from src.dataset_manager import DatasetManager

# Disable Triton compilation (ARM compatibility)
os.environ["TRITON_INTERPRET"] = "1"
os.environ["DISABLE_TRITON"] = "1"

dm = DatasetManager(base_path="dataset")

# í•„ìˆ˜ ë°ì´í„° í™•ì¸
required_files = [
    ("base_model", "korean_documents.json"),
    ("base_model", "bilingual_synonyms.json"),
]

print("Checking required data files...")
for subdir, filename in required_files:
    if not dm.check_data_exists(filename, subdir):
        print(f"âŒ Missing: {subdir}/{filename}")
        print("\nğŸ’¡ Please run notebook 1 first:")
        print("   01_neural_sparse_base_training.ipynb")
        raise FileNotFoundError(f"Missing: {subdir}/{filename}")
    print(f"âœ“ Found: {subdir}/{filename}")

print("\nâœ… All required data files found")

# ë°ì´í„° ë¡œë“œ
documents = dm.load_json("korean_documents.json", "base_model")
bilingual_dict = dm.load_json("bilingual_synonyms.json", "base_model")

print(f"\nğŸ“Š Loaded data:")
print(f"  Documents: {len(documents):,}")
print(f"  Bilingual dict: {len(bilingual_dict):,} terms")
```

### ë…¸íŠ¸ë¶ 3: Enhanced Training

```python
# 03_llm_enhanced_training.ipynb
# Cell 1: ë°ì´í„° ë¡œë“œ
from src.dataset_manager import DatasetManager

dm = DatasetManager(base_path="dataset")

# í•„ìˆ˜ ë°ì´í„° í™•ì¸
required_files = [
    ("base_model", "qd_pairs_base.pkl"),
    ("llm_generated", "synthetic_qd_pairs.pkl"),
    ("llm_generated", "enhanced_synonyms.json"),
]

print("Checking required data files...")
for subdir, filename in required_files:
    if not dm.check_data_exists(filename, subdir):
        print(f"âŒ Missing: {subdir}/{filename}")
        print("\nğŸ’¡ Please run previous notebooks first:")
        print("   1. 01_neural_sparse_base_training.ipynb")
        print("   2. 02_llm_synthetic_data_generation.ipynb")
        raise FileNotFoundError(f"Missing: {subdir}/{filename}")
    print(f"âœ“ Found: {subdir}/{filename}")

print("\nâœ… All required data files found")

# ë°ì´í„° ë¡œë“œ
base_qd_pairs = dm.load_pickle("qd_pairs_base.pkl", "base_model")
synthetic_qd_pairs = dm.load_pickle("synthetic_qd_pairs.pkl", "llm_generated")
enhanced_synonyms = dm.load_json("enhanced_synonyms.json", "llm_generated")

print(f"\nğŸ“Š Loaded data:")
print(f"  Base QD pairs: {len(base_qd_pairs):,}")
print(f"  Synthetic QD pairs: {len(synthetic_qd_pairs):,}")
print(f"  Enhanced synonyms: {len(enhanced_synonyms):,} terms")

# v1 ëª¨ë¸ ë¡œë“œ
from src.opensearch_sparse_encoder import OpenSearchSparseEncoder
print("\nLoading v1 model...")
v1_model, v1_tokenizer = dm.load_model(
    OpenSearchSparseEncoder,
    "neural_sparse_v1_model",
    "base_model"
)
print("âœ“ v1 model loaded")
```

---

## ğŸš€ ì‹¤í–‰ ìˆœì„œ

### ì²« ì‹¤í–‰ (ì „ì²´ íŒŒì´í”„ë¼ì¸)

```bash
# 1. ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ (ì„¹ì…˜ 1-12)
jupyter notebook 01_neural_sparse_base_training.ipynb
# â†’ dataset/base_model/ ìƒì„±
# ì‹¤í–‰ ì‹œê°„: ~30-60ë¶„

# 2. LLM í•©ì„± ë°ì´í„° ìƒì„± (ì„¹ì…˜ 13-15)
jupyter notebook 02_llm_synthetic_data_generation.ipynb
# â†’ dataset/llm_generated/ ìƒì„±
# ì‹¤í–‰ ì‹œê°„: ~10-15ë¶„ (ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ +10ë¶„)

# 3. í™•ì¥ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (ì„¹ì…˜ 16-17)
jupyter notebook 03_llm_enhanced_training.ipynb
# â†’ dataset/enhanced_model/ ìƒì„±
# ì‹¤í–‰ ì‹œê°„: ~40-50ë¶„
```

### ì¬ì‹¤í–‰ ì‹œë‚˜ë¦¬ì˜¤

**ì‹œë‚˜ë¦¬ì˜¤ 1**: LLM í•©ì„± ë°ì´í„°ë§Œ ì¬ìƒì„±
```bash
# ê¸°ì¡´ base_model ë°ì´í„° ì‚¬ìš©, LLMë§Œ ì¬ì‹¤í–‰
jupyter notebook 02_llm_synthetic_data_generation.ipynb
# ì‹¤í–‰ ì‹œê°„: ~10-15ë¶„ (ëª¨ë¸ ìºì‹œ ì‚¬ìš©)
```

**ì‹œë‚˜ë¦¬ì˜¤ 2**: ë‹¤ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì¬í•™ìŠµ
```bash
# ê¸°ì¡´ í•©ì„± ë°ì´í„° ì‚¬ìš©, í•™ìŠµë§Œ ì¬ì‹¤í–‰
jupyter notebook 03_llm_enhanced_training.ipynb
# ì‹¤í–‰ ì‹œê°„: ~40-50ë¶„
```

**ì‹œë‚˜ë¦¬ì˜¤ 3**: ì²˜ìŒë¶€í„° ì™„ì „ ì¬êµ¬ì¶•
```bash
# dataset í´ë” ì‚­ì œ í›„ ì „ì²´ ì‹¤í–‰
rm -rf dataset/
jupyter notebook 01_neural_sparse_base_training.ipynb
jupyter notebook 02_llm_synthetic_data_generation.ipynb
jupyter notebook 03_llm_enhanced_training.ipynb
# ì´ ì‹¤í–‰ ì‹œê°„: ~90-120ë¶„
```

---

## ğŸ“Š ì˜ˆìƒ íš¨ê³¼

### â±ï¸ ì‹œê°„ ì ˆì•½
- **ê¸°ì¡´ ë°©ì‹**: ëª¨ë“  ì‘ì—… ì›ìŠ¤í†± ì‹¤í–‰ â†’ ~90-120ë¶„ (ë§¤ë²ˆ)
- **ë¶„ë¦¬ í›„**:
  - ë…¸íŠ¸ë¶ 1: 30-60ë¶„ (1íšŒë§Œ)
  - ë…¸íŠ¸ë¶ 2: 10-15ë¶„ (ì¬ì‚¬ìš© ê°€ëŠ¥, LLM ìºì‹œ)
  - ë…¸íŠ¸ë¶ 3: 40-50ë¶„ (ì¬ì‚¬ìš© ê°€ëŠ¥)
  - **ì¬ì‹¤í—˜ ì‹œ**: ë…¸íŠ¸ë¶ 3ë§Œ ì‹¤í–‰ â†’ ~40ë¶„ âœ… **50% ì‹œê°„ ì ˆì•½**

### ğŸ’¾ ë©”ëª¨ë¦¬ íš¨ìœ¨
- **ê¸°ì¡´**: ëª¨ë“  ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ìœ ì§€ â†’ ~40GB RAM
- **ë¶„ë¦¬ í›„**: í•„ìš”í•œ ë°ì´í„°ë§Œ ë¡œë“œ â†’ ~15-20GB RAM per notebook âœ… **50% ë©”ëª¨ë¦¬ ì ˆê°**

### ğŸ”§ ë””ë²„ê¹… ìš©ì´ì„±
- âœ… ê° ë‹¨ê³„ë³„ ì¤‘ê°„ ê²°ê³¼ë¬¼ í™•ì¸ ê°€ëŠ¥
- âœ… ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ ë…¸íŠ¸ë¶ë§Œ ì¬ì‹¤í–‰
- âœ… LLM ëª¨ë¸ ë¡œë”© ì‹œê°„ ì ˆì•½ (í•œ ë²ˆë§Œ ë¡œë“œ, ìºì‹œ ì‚¬ìš©)
- âœ… ë°ì´í„° ë²„ì „ ê´€ë¦¬ ë° ë¡¤ë°± ê°€ëŠ¥

### ğŸ§ª ì‹¤í—˜ í¸ì˜ì„±
- âœ… ë‹¤ë¥¸ LLM ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ë…¸íŠ¸ë¶ 2ë§Œ ì¬ì‹¤í–‰)
- âœ… ë‹¤ë¥¸ í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸ (ë…¸íŠ¸ë¶ 3ë§Œ ì¬ì‹¤í–‰)
- âœ… í•©ì„± ë°ì´í„° ì–‘ ì¡°ì ˆ ì‹¤í—˜
- âœ… ë™ì˜ì–´ ì‚¬ì „ í•„í„°ë§ ì „ëµ ë¹„êµ

---

## ğŸ”§ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: ìœ í‹¸ë¦¬í‹° êµ¬í˜„ âœ…
- [ ] `src/dataset_manager.py` ìƒì„±
- [ ] JSON ì €ì¥/ë¡œë“œ êµ¬í˜„
- [ ] Pickle ì €ì¥/ë¡œë“œ êµ¬í˜„
- [ ] PyTorch ëª¨ë¸ ì €ì¥/ë¡œë“œ êµ¬í˜„
- [ ] íŒŒì¼ ì¡´ì¬ í™•ì¸ ê¸°ëŠ¥
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±

### Phase 2: ë…¸íŠ¸ë¶ 1 ìƒì„± ğŸ“
- [ ] `01_neural_sparse_base_training.ipynb` ìƒì„±
- [ ] ê¸°ì¡´ ì„¹ì…˜ 1-12 ë³µì‚¬ ë° ìˆ˜ì •
- [ ] DatasetManager í†µí•©
- [ ] ë°ì´í„° ì €ì¥ ë¡œì§ ì¶”ê°€ (ëª¨ë“  ì„¹ì…˜ ë)
- [ ] ì‹¤í–‰ ë° ê²€ì¦

### Phase 3: ë…¸íŠ¸ë¶ 2 ìƒì„± ğŸ¤–
- [ ] `02_llm_synthetic_data_generation.ipynb` ìƒì„±
- [ ] ê¸°ì¡´ ì„¹ì…˜ 13-15 ë³µì‚¬ ë° ìˆ˜ì •
- [ ] Triton ë¹„í™œì„±í™” ì½”ë“œ ì¶”ê°€
- [ ] ë°ì´í„° ë¡œë“œ ë¡œì§ ì¶”ê°€ (ì‹œì‘ ë¶€ë¶„)
- [ ] LLM ìƒì„± ë°ì´í„° ì €ì¥ (ë ë¶€ë¶„)
- [ ] ì‹¤í–‰ ë° ê²€ì¦

### Phase 4: ë…¸íŠ¸ë¶ 3 ìƒì„± ğŸ¯
- [ ] `03_llm_enhanced_training.ipynb` ìƒì„±
- [ ] ê¸°ì¡´ ì„¹ì…˜ 16-17 ë³µì‚¬ ë° ìˆ˜ì •
- [ ] ë°ì´í„° ë¡œë“œ ë¡œì§ ì¶”ê°€
- [ ] ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ì €ì¥
- [ ] ì‹œê°í™” ê²°ê³¼ ì €ì¥
- [ ] ì‹¤í–‰ ë° ê²€ì¦

### Phase 5: í†µí•© í…ŒìŠ¤íŠ¸ ğŸ§ª
- [ ] ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (1â†’2â†’3)
- [ ] ë°ì´í„° ë¬´ê²°ì„± í™•ì¸
- [ ] ì„±ëŠ¥ ë¹„êµ (ê¸°ì¡´ vs ë¶„ë¦¬)
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
- [ ] ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
- [ ] ë¬¸ì„œí™” ì—…ë°ì´íŠ¸

---

## ğŸ“š íŒŒì¼ êµ¬ì¡° ìµœì¢… ëª¨ìŠµ

```
opensearch-neural-pre-train/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_neural_sparse_base_training.ipynb       # ğŸ†• ê¸°ë³¸ í•™ìŠµ
â”‚   â”œâ”€â”€ 02_llm_synthetic_data_generation.ipynb     # ğŸ†• LLM í•©ì„± ë°ì´í„°
â”‚   â”œâ”€â”€ 03_llm_enhanced_training.ipynb             # ğŸ†• í™•ì¥ í•™ìŠµ
â”‚   â””â”€â”€ korean_neural_sparse_training_v2_llm.ipynb # ê¸°ì¡´ (ë³´ê´€ìš©)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dataset_manager.py                         # ğŸ†• ë°ì´í„° ê´€ë¦¬
â”‚   â”œâ”€â”€ llm_loader.py
â”‚   â”œâ”€â”€ synthetic_data_generator.py
â”‚   â”œâ”€â”€ cross_lingual_synonyms.py
â”‚   â””â”€â”€ opensearch_sparse_encoder.py
â”œâ”€â”€ dataset/                                        # ğŸ†• ê³µìœ  ë°ì´í„° ì €ì¥ì†Œ
â”‚   â”œâ”€â”€ base_model/                                # ë…¸íŠ¸ë¶ 1 ê²°ê³¼
â”‚   â”‚   â”œâ”€â”€ korean_documents.json
â”‚   â”‚   â”œâ”€â”€ idf_statistics.pkl
â”‚   â”‚   â”œâ”€â”€ trend_keywords.json
â”‚   â”‚   â”œâ”€â”€ bilingual_synonyms.json
â”‚   â”‚   â”œâ”€â”€ qd_pairs_base.pkl
â”‚   â”‚   â””â”€â”€ neural_sparse_v1_model/
â”‚   â”œâ”€â”€ llm_generated/                             # ë…¸íŠ¸ë¶ 2 ê²°ê³¼
â”‚   â”‚   â”œâ”€â”€ synthetic_qd_pairs.pkl
â”‚   â”‚   â”œâ”€â”€ enhanced_synonyms.json
â”‚   â”‚   â””â”€â”€ generation_metadata.json
â”‚   â”œâ”€â”€ enhanced_model/                            # ë…¸íŠ¸ë¶ 3 ê²°ê³¼
â”‚   â”‚   â”œâ”€â”€ neural_sparse_v2_model/
â”‚   â”‚   â”œâ”€â”€ training_history.json
â”‚   â”‚   â””â”€â”€ performance_comparison.json
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ v1_metrics.json
â”‚   â”‚   â”œâ”€â”€ v2_metrics.json
â”‚   â”‚   â””â”€â”€ comparison_plots/
â”‚   â””â”€â”€ metadata.json
â”œâ”€â”€ plan.md                                         # ì´ ë¬¸ì„œ
â”œâ”€â”€ plan_old.md                                     # ì´ì „ plan ë°±ì—…
â””â”€â”€ requirements.txt
```

### ë°ì´í„° í¬ê¸° ì˜ˆìƒ

```
dataset/
â”œâ”€â”€ base_model/              (~2GB)
â”‚   â”œâ”€â”€ korean_documents.json       (500MB)
â”‚   â”œâ”€â”€ idf_statistics.pkl          (50MB)
â”‚   â”œâ”€â”€ trend_keywords.json         (10MB)
â”‚   â”œâ”€â”€ bilingual_synonyms.json     (5MB)
â”‚   â”œâ”€â”€ qd_pairs_base.pkl           (300MB)
â”‚   â””â”€â”€ neural_sparse_v1_model/     (1GB)
â”œâ”€â”€ llm_generated/           (~1.5GB)
â”‚   â”œâ”€â”€ synthetic_qd_pairs.pkl      (1GB)
â”‚   â”œâ”€â”€ enhanced_synonyms.json      (20MB)
â”‚   â””â”€â”€ generation_metadata.json    (1MB)
â””â”€â”€ enhanced_model/          (~2GB)
    â”œâ”€â”€ neural_sparse_v2_model/     (1GB)
    â”œâ”€â”€ training_history.json       (10MB)
    â””â”€â”€ evaluation/                 (100MB - plots)

Total: ~5.5GB (ëª¨ë¸ ì œì™¸ ì‹œ ~3.5GB)
```

---

## ğŸ’¡ ì¶”ê°€ ê°œì„  ì‚¬í•­

### 1. ë°ì´í„° ë²„ì „ ê´€ë¦¬
```python
# dataset/metadata.json ì˜ˆì‹œ
{
  "version": "1.0.0",
  "created_at": "2025-01-14T10:30:00",
  "python_version": "3.12",
  "gpu": "NVIDIA GB10",
  "datasets": {
    "base_model": {
      "version": "1.0.0",
      "created_by": "01_neural_sparse_base_training.ipynb",
      "created_at": "2025-01-14T10:30:00",
      "num_documents": 10000,
      "num_qd_pairs": 30000
    },
    "llm_generated": {
      "version": "1.0.0",
      "created_by": "02_llm_synthetic_data_generation.ipynb",
      "created_at": "2025-01-14T11:00:00",
      "llm_model": "Qwen/Qwen3-30B-A3B-Thinking-2507-FP8",
      "num_synthetic_pairs": 10000,
      "num_enhanced_synonyms": 5000
    },
    "enhanced_model": {
      "version": "1.0.0",
      "created_by": "03_llm_enhanced_training.ipynb",
      "created_at": "2025-01-14T12:00:00",
      "total_training_pairs": 40000,
      "v1_mrr": 0.85,
      "v2_mrr": 0.92
    }
  }
}
```

### 2. ìë™ ì˜ì¡´ì„± ì²´í¬ í•¨ìˆ˜
```python
# src/dataset_manager.pyì— ì¶”ê°€
def check_dependencies(self, required: List[Tuple[str, str]]) -> bool:
    """ë…¸íŠ¸ë¶ ì‹¤í–‰ ì „ í•„ìš”í•œ ë°ì´í„° í™•ì¸"""
    missing = []
    for subdir, filename in required:
        if not self.check_data_exists(filename, subdir):
            missing.append(f"{subdir}/{filename}")

    if missing:
        print("=" * 70)
        print("âŒ Missing required data files:")
        print("=" * 70)
        for f in missing:
            print(f"   - {f}")
        print("\nğŸ’¡ Please run previous notebooks first:")
        print("   1. 01_neural_sparse_base_training.ipynb")
        print("   2. 02_llm_synthetic_data_generation.ipynb")
        print("=" * 70)
        return False

    print("âœ… All dependencies satisfied")
    return True
```

### 3. ì§„í–‰ ìƒí™© ì¶”ì 
```python
# dataset/progress.json ì˜ˆì‹œ
{
  "01_base_training": {
    "status": "completed",
    "started_at": "2025-01-14T10:00:00",
    "completed_at": "2025-01-14T10:45:00",
    "duration_minutes": 45,
    "success": true
  },
  "02_llm_generation": {
    "status": "completed",
    "started_at": "2025-01-14T11:00:00",
    "completed_at": "2025-01-14T11:15:00",
    "duration_minutes": 15,
    "success": true
  },
  "03_enhanced_training": {
    "status": "in_progress",
    "started_at": "2025-01-14T12:00:00"
  }
}
```

---

## ğŸ¯ ì„±ê³µ ê¸°ì¤€

- [ ] ê° ë…¸íŠ¸ë¶ì´ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥
- [ ] ë°ì´í„° ì €ì¥/ë¡œë“œê°€ ì •ìƒ ì‘ë™
- [ ] ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œê°„ì´ ê¸°ì¡´ ëŒ€ë¹„ íš¨ìœ¨ì 
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ê°œì„ ë¨
- [ ] ëª¨ë¸ ì„±ëŠ¥ì´ ê¸°ì¡´ê³¼ ë™ì¼ ë˜ëŠ” í–¥ìƒ
- [ ] ëª¨ë“  ë…¸íŠ¸ë¶ì´ ë¬¸ì„œí™”ë¨
- [ ] ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì˜ì¡´ì„± ì²´í¬ êµ¬í˜„
- [ ] ë°ì´í„° ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ ì‘ë™

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

### ì¦‰ì‹œ êµ¬í˜„ (ìš°ì„ ìˆœìœ„ ìˆœì„œ)

1. **`src/dataset_manager.py` ìƒì„±** âš¡ (15ë¶„)
   - ê¸°ë³¸ í´ë˜ìŠ¤ êµ¬í˜„
   - ì €ì¥/ë¡œë“œ ë©”ì„œë“œ
   - íŒŒì¼ í™•ì¸ ìœ í‹¸ë¦¬í‹°

2. **ë…¸íŠ¸ë¶ 1 ìƒì„±** ğŸ““ (30ë¶„)
   - ê¸°ì¡´ ì„¹ì…˜ 1-12 ë³µì‚¬
   - DatasetManager í†µí•©
   - ì €ì¥ ë¡œì§ ì¶”ê°€

3. **ë…¸íŠ¸ë¶ 1 í…ŒìŠ¤íŠ¸** ğŸ§ª (60ë¶„)
   - ì „ì²´ ì‹¤í–‰
   - ë°ì´í„° ì €ì¥ ê²€ì¦
   - ë¬¸ì œ ìˆ˜ì •

4. **ë…¸íŠ¸ë¶ 2-3 ìƒì„±** ğŸ““ (30ë¶„)
   - ê¸°ì¡´ ì„¹ì…˜ ë³µì‚¬
   - ë¡œë“œ/ì €ì¥ ë¡œì§ ì¶”ê°€

5. **ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸** ğŸ¯ (90ë¶„)
   - 1â†’2â†’3 ìˆœì°¨ ì‹¤í–‰
   - ì„±ëŠ¥ ì¸¡ì •
   - ë¬¸ì„œí™” ì™„ë£Œ

---

## ğŸš€ ì‹œì‘í•˜ê¸°

êµ¬í˜„ì„ ì‹œì‘í•˜ì‹œë ¤ë©´ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:

1. **`src/dataset_manager.py` êµ¬í˜„ ì‹œì‘**
2. **ë…¸íŠ¸ë¶ 1 ìƒì„± ì‹œì‘**
3. **ì „ì²´ êµ¬í˜„ ê³„íš ìƒì„¸í™”**

ì–´ë–¤ ê²ƒë¶€í„° ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? ğŸ¤”

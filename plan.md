# OpenSearch Neural Sparse í•œêµ­ì–´ íŠ¹í™” ì‚¬ì „ í•™ìŠµ ê³„íš

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

**ëª©í‘œ:** í•œì˜ í˜¼ìš© í™˜ê²½ì— ìµœì í™”ëœ OpenSearch Neural Sparse Retrieval ëª¨ë¸ ì‚¬ì „ í•™ìŠµ

**ê¸°ëŒ€ íš¨ê³¼:**
- í•œêµ­ì–´ ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ
- í•œì˜ ë™ì˜ì–´ ìë™ ë§¤ì¹­ (ëª¨ë¸ â†’ model, ê²€ìƒ‰ â†’ search)
- Cross-lingual retrieval ì§€ì›
- OpenSearch í”ŒëŸ¬ê·¸ì¸ ì§ì ‘ ë°°í¬ ê°€ëŠ¥

---

## ğŸ¯ Phase 1: ì•„í‚¤í…ì²˜ ë° ë°ì´í„° ì¤€ë¹„ (ì™„ë£Œ)

### âœ… 1.1 ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ
- [x] Korean Wikipedia (100K documents)
- [x] KLUE ë°ì´í„°ì…‹
- [x] KorQuAD ë°ì´í„°ì…‹
- [x] Korean News with dates
- [x] LLM í•©ì„± Query-Document pairs
- [x] í•œì˜ ë™ì˜ì–´ ì‚¬ì „ (bilingual synonyms)

**ë°ì´í„° ìœ„ì¹˜:**
- `dataset/base_model/documents.json` (0.7 MB)
- `dataset/base_model/qd_pairs_base.pkl` (54.9 MB)
- `dataset/llm_generated/synthetic_qd_pairs.pkl` (2.6 KB)
- `dataset/llm_generated/enhanced_synonyms.json` (1.5 KB)

### âœ… 1.2 ì¸í”„ë¼ ì¤€ë¹„ ì™„ë£Œ
- [x] Ollama LLM í†µí•© (í•œêµ­ì–´ ë°ì´í„° ìƒì„±)
- [x] Wikipedia loader with caching
- [x] DatasetManager for data management
- [x] í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì¶•

---

## ğŸš€ Phase 2: Neural Sparse ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„¤ê³„ (ì§„í–‰ ì¤‘)

### ğŸ”§ 2.1 Neural Sparse Encoding ì´í•´

**Neural Sparse Retrieval í•µì‹¬ ê°œë…:**
```
Query/Document â†’ BERT Encoder â†’ Sparse Vector (Vocab Size)
                                    â†“
                              Top-K Non-zero Terms
                                    â†“
                         Inverted Index (OpenSearch)
```

**íŠ¹ì§•:**
- BERT ê¸°ë°˜ contextualized term weighting
- Sparse vector (ëŒ€ë¶€ë¶„ì˜ ê°’ì´ 0)
- Inverted indexì™€ í˜¸í™˜ (ê¸°ì¡´ ê²€ìƒ‰ ì¸í”„ë¼ í™œìš©)
- Dense retrievalë³´ë‹¤ í•´ì„ ê°€ëŠ¥ì„± ë†’ìŒ

### ğŸ“ 2.2 ëª¨ë¸ ì•„í‚¤í…ì²˜

```
Input: "í•œêµ­ì–´ ê²€ìƒ‰ ëª¨ë¸"
   â†“
BERT Encoder (klue/bert-base ë˜ëŠ” multilingual)
   â†“
Token Embeddings [CLS] í•œêµ­ì–´ ê²€ìƒ‰ ëª¨ë¸ [SEP]
   â†“
MLM Head (Masked Language Model style)
   â†“
Sparse Weights: {
    í•œêµ­ì–´: 0.85,
    ê²€ìƒ‰: 0.92,
    ëª¨ë¸: 0.88,
    search: 0.45,  â† í•œì˜ ë™ì˜ì–´ í•™ìŠµë¨
    model: 0.42,
    ...
}
```

**í•µì‹¬ ì»´í¬ë„ŒíŠ¸:**
1. **Base Encoder**: `klue/bert-base` ë˜ëŠ” `xlm-roberta-base`
2. **Projection Head**: Token-level MLM-style output
3. **Loss Function**: FLOPS regularization + Ranking loss

### ğŸ¯ 2.3 í•™ìŠµ ëª©í‘œ ì •ì˜

**Multi-task Learning:**

1. **Query-Document Matching (Primary)**
   - Positive pairs: (query, relevant_doc) â†’ high similarity
   - Hard negatives: (query, irrelevant_doc) â†’ low similarity
   - Loss: Contrastive loss or margin ranking loss

2. **Cross-lingual Term Alignment (Secondary)**
   - í•œì˜ ë™ì˜ì–´ ìŒì˜ activation ìœ ì‚¬ë„ ìµœëŒ€í™”
   - Ex: "ëª¨ë¸" vs "model" â†’ similar sparse patterns
   - Loss: Cosine similarity loss

3. **Sparsity Regularization (Constraint)**
   - FLOPS (FLoating point OPerations) ì œì•½
   - ë„ˆë¬´ ë§ì€ termì´ activateë˜ë©´ ì„±ëŠ¥ ì €í•˜
   - Loss: L1 regularization on activations

**ì¢…í•© Loss:**
```python
total_loss = (
    Î± * ranking_loss +           # Query-doc matching
    Î² * cross_lingual_loss +     # í•œì˜ ë™ì˜ì–´
    Î³ * sparsity_loss            # Sparsity constraint
)
```

---

## ğŸ“Š Phase 3: í•™ìŠµ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

### ğŸ”„ 3.1 ë°ì´í„° ì¦ê°• ì „ëµ

**í˜„ì¬ ë°ì´í„°:**
- Base QD pairs: ~100K pairs
- Synthetic QD pairs: ~10 pairs (í…ŒìŠ¤íŠ¸)
- Bilingual synonyms: 32 entries

**ì¦ê°• ê³„íš:**

#### Step 1: LLM í•©ì„± ë°ì´í„° ëŒ€ëŸ‰ ìƒì„±
```bash
# Notebook 2 ì¬ì‹¤í–‰ (1000 documents)
# ì˜ˆìƒ ì¶œë ¥: 3000 synthetic pairs (1000 docs Ã— 3 queries)
```

#### Step 2: Hard Negative Mining
- ê° queryì— ëŒ€í•´ BM25ë¡œ ìƒìœ„ 100ê°œ ë¬¸ì„œ ê²€ìƒ‰
- Positive ì œì™¸í•œ ìƒìœ„ 10ê°œë¥¼ hard negativesë¡œ ì‚¬ìš©
- ì˜ˆìƒ ì¶œë ¥: 100K Ã— 10 = 1M negative pairs

#### Step 3: Cross-lingual Augmentation
- í•œì˜ ë™ì˜ì–´ ì‚¬ì „ í™œìš©
- Queryì˜ termì„ ì˜ì–´ë¡œ ì¹˜í™˜
- Ex: "ê²€ìƒ‰ ëª¨ë¸" â†’ "search ëª¨ë¸", "ê²€ìƒ‰ model", "search model"
- ì˜ˆìƒ ì¶œë ¥: ê¸°ì¡´ ë°ì´í„° Ã— 2-3ë°° ì¦ê°•

### ğŸ“ 3.2 ìµœì¢… í•™ìŠµ ë°ì´í„° êµ¬ì¡°

```
TrainingDataset/
  â”œâ”€ positive_pairs/
  â”‚   â”œâ”€ original_qd_pairs.pkl      # 100K pairs
  â”‚   â”œâ”€ synthetic_qd_pairs.pkl     # 3K pairs
  â”‚   â””â”€ augmented_qd_pairs.pkl     # 300K pairs (cross-lingual)
  â”‚
  â”œâ”€ negative_pairs/
  â”‚   â”œâ”€ hard_negatives.pkl         # 1M pairs (BM25 mining)
  â”‚   â””â”€ random_negatives.pkl       # 100K pairs (sampling)
  â”‚
  â””â”€ bilingual_synonyms/
      â””â”€ synonym_pairs.json          # 32+ entries
```

**ì˜ˆìƒ ì´ í•™ìŠµ ë°ì´í„°:**
- Positive: ~400K pairs
- Negative: ~1.1M pairs
- Synonym pairs: 100+ pairs

---

## ğŸ—ï¸ Phase 4: ëª¨ë¸ í•™ìŠµ êµ¬í˜„

### ğŸ”¨ 4.1 êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### â˜ Step 4.1.1: Base Model ì„ íƒ
```python
# Option 1: í•œêµ­ì–´ íŠ¹í™” (ê¶Œì¥)
base_model = "klue/bert-base"

# Option 2: ë‹¤êµ­ì–´ ì§€ì›
base_model = "xlm-roberta-base"

# Option 3: ê²½ëŸ‰í™”
base_model = "klue/roberta-small"
```

**ì„ íƒ ê¸°ì¤€:**
- í•œêµ­ì–´ ì„±ëŠ¥: klue/bert-base > xlm-roberta-base
- ë‹¤êµ­ì–´ ì§€ì›: xlm-roberta-base
- ì†ë„: klue/roberta-small

#### â˜ Step 4.1.2: Neural Sparse Encoder êµ¬í˜„
```python
# src/models/neural_sparse_encoder.py

class NeuralSparseEncoder(nn.Module):
    def __init__(self, base_model: str, vocab_size: int):
        self.bert = AutoModel.from_pretrained(base_model)
        self.projection = nn.Linear(768, vocab_size)  # BERT hidden â†’ vocab
        self.activation = nn.ReLU()  # Non-negative weights

    def forward(self, input_ids, attention_mask):
        # BERT encoding
        outputs = self.bert(input_ids, attention_mask)
        token_embeddings = outputs.last_hidden_state

        # Sparse projection
        sparse_logits = self.projection(token_embeddings)
        sparse_weights = self.activation(sparse_logits)

        # Max pooling over tokens (query/doc representation)
        sparse_vec, _ = torch.max(sparse_weights, dim=1)

        return sparse_vec  # [batch, vocab_size]
```

#### â˜ Step 4.1.3: Loss Functions êµ¬í˜„
```python
# src/training/losses.py

def ranking_loss(query_vec, pos_doc_vec, neg_doc_vecs):
    """Margin ranking loss for query-document matching."""
    pos_score = torch.sum(query_vec * pos_doc_vec, dim=-1)
    neg_scores = torch.sum(query_vec * neg_doc_vecs, dim=-1)

    margin = 0.1
    loss = torch.relu(margin - pos_score + neg_scores).mean()
    return loss

def cross_lingual_loss(korean_vec, english_vec):
    """Cosine similarity loss for bilingual terms."""
    cos_sim = F.cosine_similarity(korean_vec, english_vec)
    loss = 1 - cos_sim.mean()
    return loss

def flops_loss(sparse_vec, lambda_flops=0.001):
    """FLOPS regularization for sparsity."""
    l1_norm = torch.sum(torch.abs(sparse_vec), dim=-1)
    loss = lambda_flops * l1_norm.mean()
    return loss
```

#### â˜ Step 4.1.4: Training Loop êµ¬í˜„
```python
# src/training/trainer.py

class NeuralSparseTrainer:
    def train_epoch(self):
        for batch in self.train_loader:
            # Forward pass
            query_vec = self.model(batch['query_ids'])
            pos_doc_vec = self.model(batch['pos_doc_ids'])
            neg_doc_vecs = self.model(batch['neg_doc_ids'])

            # Compute losses
            rank_loss = ranking_loss(query_vec, pos_doc_vec, neg_doc_vecs)
            sparse_loss = flops_loss(query_vec) + flops_loss(pos_doc_vec)

            # Optional: cross-lingual loss
            if batch.has('synonym_pairs'):
                kor_vec = self.model(batch['korean_term_ids'])
                eng_vec = self.model(batch['english_term_ids'])
                cl_loss = cross_lingual_loss(kor_vec, eng_vec)
            else:
                cl_loss = 0

            # Total loss
            loss = rank_loss + 0.1 * cl_loss + sparse_loss

            # Backward
            loss.backward()
            self.optimizer.step()
```

### âš™ï¸ 4.2 í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°

```yaml
# config/training_config.yaml

model:
  base: "klue/bert-base"
  hidden_size: 768
  vocab_size: 30000  # BERT vocab size

training:
  epochs: 10
  batch_size: 32
  learning_rate: 2e-5
  warmup_steps: 1000
  max_grad_norm: 1.0

  # Loss weights
  alpha_ranking: 1.0      # Query-doc matching
  beta_cross_lingual: 0.1 # í•œì˜ ë™ì˜ì–´
  gamma_sparsity: 0.001   # FLOPS regularization

  # Negative sampling
  num_hard_negatives: 10
  num_random_negatives: 5

data:
  max_seq_length: 256
  query_max_length: 64
  doc_max_length: 256

evaluation:
  eval_steps: 1000
  save_steps: 2000
  metric: "ndcg@10"
```

### ğŸ–¥ï¸ 4.3 GPU ë©”ëª¨ë¦¬ ìµœì í™”

**ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:**
- Base model (klue/bert-base): ~500MB
- Batch (32 samples): ~2GB
- Optimizer states: ~1GB
- **Total: ~3.5GB**

**ìµœì í™” ì „ëµ:**
```python
# Mixed precision training
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    loss = compute_loss(...)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

**Gradient accumulation:**
```python
# Effective batch size = 32 Ã— 4 = 128
accumulation_steps = 4

for i, batch in enumerate(dataloader):
    loss = compute_loss(batch) / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

---

## ğŸ“ˆ Phase 5: í‰ê°€ ë° ê²€ì¦

### ğŸ¯ 5.1 í‰ê°€ ë°ì´í„°ì…‹

**Evaluation Sets:**
1. **Test set** (held-out 10%)
   - Base QD pairsì—ì„œ ë¶„ë¦¬
   - ~10K pairs

2. **Cross-lingual test**
   - í•œêµ­ì–´ query + ì˜ì–´ term í¬í•¨ ë¬¸ì„œ
   - ~1K pairs

3. **Real-world queries**
   - ì‹¤ì œ ê²€ìƒ‰ ë¡œê·¸ (ìˆë‹¤ë©´)
   - OpenSearch ê³µì‹ ë¬¸ì„œ ê²€ìƒ‰ ë“±

### ğŸ“Š 5.2 í‰ê°€ ë©”íŠ¸ë¦­

**Retrieval Metrics:**
```python
# Mean Reciprocal Rank
MRR = avg(1 / rank_of_first_relevant)

# Normalized Discounted Cumulative Gain
NDCG@10 = DCG@10 / IDCG@10

# Recall@K
Recall@10 = (relevant_in_top10 / total_relevant)

# Precision@K
Precision@10 = (relevant_in_top10 / 10)
```

**Sparsity Metrics:**
```python
# Average number of non-zero terms
avg_active_terms = mean(count(sparse_vec > threshold))

# FLOPS (floating point operations)
flops = sum(sparse_vec)
```

**Cross-lingual Metrics:**
```python
# í•œì˜ ë™ì˜ì–´ activation ìœ ì‚¬ë„
synonym_similarity = cosine_sim(vec["ëª¨ë¸"], vec["model"])
```

### ğŸ” 5.3 ë¶„ì„ ë° ë””ë²„ê¹…

**ë¶„ì„ í•­ëª©:**
1. **Top-K activated terms ë¶„ì„**
   - Query: "í•œêµ­ì–´ ê²€ìƒ‰"
   - Activated: {í•œêµ­ì–´: 0.9, ê²€ìƒ‰: 0.85, search: 0.4, ...}

2. **í•œì˜ ë™ì˜ì–´ ë§¤ì¹­ ê²€ì¦**
   - "ëª¨ë¸" vs "model" activation ë¹„êµ
   - Cross-lingual retrieval ì„±ê³µë¥ 

3. **Failure case ë¶„ì„**
   - Retrieval ì‹¤íŒ¨ ì‚¬ë¡€ ìˆ˜ì§‘
   - Common patterns íŒŒì•…

---

## ğŸš¢ Phase 6: OpenSearch í†µí•© ë° ë°°í¬

### ğŸ”Œ 6.1 OpenSearch í”ŒëŸ¬ê·¸ì¸ ë³€í™˜

**ëª¨ë¸ Export:**
```python
# PyTorch â†’ ONNX
torch.onnx.export(
    model,
    dummy_input,
    "neural_sparse_korean.onnx",
    opset_version=14
)

# ONNX â†’ TorchScript (OpenSearch compatible)
traced_model = torch.jit.trace(model, dummy_input)
traced_model.save("neural_sparse_korean.pt")
```

**OpenSearch ì„¤ì •:**
```json
{
  "model_id": "neural-sparse-korean-v1",
  "model_format": "TORCH_SCRIPT",
  "model_config": {
    "model_type": "bert",
    "embedding_dimension": 768,
    "framework_type": "sentence_transformers"
  }
}
```

### ğŸ§ª 6.2 í†µí•© í…ŒìŠ¤íŠ¸

**End-to-End Test:**
1. ëª¨ë¸ ì—…ë¡œë“œ to OpenSearch
2. Index ìƒì„± with neural sparse pipeline
3. Document ì¸ë±ì‹±
4. ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰
5. ê²°ê³¼ ê²€ì¦

```bash
# OpenSearch neural sparse search
POST /my-index/_search
{
  "query": {
    "neural_sparse": {
      "my_text_field": {
        "query_text": "í•œêµ­ì–´ ê²€ìƒ‰ ëª¨ë¸",
        "model_id": "neural-sparse-korean-v1"
      }
    }
  }
}
```

### ğŸ“Š 6.3 ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

**Baseline ë¹„êµ:**
- BM25 (keyword)
- Dense retrieval (DPR, ANCE)
- Hybrid (BM25 + Dense)
- **Neural Sparse (Ours)**

**ì¸¡ì • í•­ëª©:**
- Retrieval quality (NDCG@10)
- Latency (ms per query)
- Throughput (QPS)
- Index size

---

## ğŸ“ Phase 7: ë¬¸ì„œí™” ë° ë°°í¬

### ğŸ“š 7.1 ë¬¸ì„œ ì‘ì„±

- [ ] `MODEL_CARD.md` - ëª¨ë¸ ì„¤ëª… ë° ì‚¬ìš©ë²•
- [ ] `TRAINING_GUIDE.md` - í•™ìŠµ ê°€ì´ë“œ
- [ ] `DEPLOYMENT_GUIDE.md` - OpenSearch ë°°í¬ ê°€ì´ë“œ
- [ ] `API_REFERENCE.md` - API ë¬¸ì„œ
- [ ] `EVALUATION_REPORT.md` - í‰ê°€ ë¦¬í¬íŠ¸

### ğŸ 7.2 ë°°í¬ ì¤€ë¹„

- [ ] HuggingFace Hub ì—…ë¡œë“œ
- [ ] Docker image ë¹Œë“œ
- [ ] OpenSearch plugin packaging
- [ ] Example notebooks
- [ ] Demo application

---

## ğŸ—“ï¸ íƒ€ì„ë¼ì¸

### Week 1-2: ë°ì´í„° ë° ê¸°ë°˜ êµ¬ì¶• âœ…
- [x] ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬
- [x] LLM í†µí•© ë° í•©ì„± ë°ì´í„° ìƒì„±
- [x] í•œì˜ ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶•

### Week 3: ë°ì´í„° ì¦ê°• ë° íŒŒì´í”„ë¼ì¸
- [ ] Hard negative mining êµ¬í˜„
- [ ] Cross-lingual augmentation
- [ ] DataLoader ë° í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- [ ] Notebook 2 ì¬ì‹¤í–‰ (ëŒ€ëŸ‰ í•©ì„± ë°ì´í„°)

### Week 4: ëª¨ë¸ êµ¬í˜„
- [ ] Neural Sparse Encoder êµ¬í˜„
- [ ] Loss functions êµ¬í˜„
- [ ] Trainer êµ¬í˜„
- [ ] ì´ˆê¸° í•™ìŠµ ì‹¤í—˜

### Week 5: í•™ìŠµ ë° ìµœì í™”
- [ ] Full training run
- [ ] Hyperparameter tuning
- [ ] ëª¨ë¸ ìµœì í™” (pruning, quantization)

### Week 6: í‰ê°€ ë° ë¶„ì„
- [ ] í‰ê°€ ë°ì´í„°ì…‹ êµ¬ì¶•
- [ ] ì¢…í•© í‰ê°€ ì‹¤í–‰
- [ ] Failure case ë¶„ì„
- [ ] ê°œì„  ë°˜ë³µ

### Week 7: OpenSearch í†µí•©
- [ ] ëª¨ë¸ export (ONNX, TorchScript)
- [ ] OpenSearch í”ŒëŸ¬ê·¸ì¸ í†µí•©
- [ ] End-to-end í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### Week 8: ë¬¸ì„œí™” ë° ë°°í¬
- [ ] ë¬¸ì„œ ì‘ì„±
- [ ] HuggingFace Hub ì—…ë¡œë“œ
- [ ] Demo ì• í”Œë¦¬ì¼€ì´ì…˜
- [ ] ìµœì¢… ê²€í†  ë° ë°°í¬

---

## ğŸ“¦ í˜„ì¬ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
opensearch-neural-pre-train/
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ base_model/              # Phase 1 ì™„ë£Œ âœ…
â”‚   â”‚   â”œâ”€â”€ documents.json
â”‚   â”‚   â”œâ”€â”€ qd_pairs_base.pkl
â”‚   â”‚   â””â”€â”€ bilingual_synonyms.json
â”‚   â””â”€â”€ llm_generated/           # Phase 1 ì™„ë£Œ âœ…
â”‚       â”œâ”€â”€ synthetic_qd_pairs.pkl
â”‚       â””â”€â”€ enhanced_synonyms.json
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dataset_manager.py       # ì™„ë£Œ âœ…
â”‚   â”œâ”€â”€ llm_loader.py            # ì™„ë£Œ âœ…
â”‚   â”œâ”€â”€ wikipedia_loader.py      # ì™„ë£Œ âœ…
â”‚   â”œâ”€â”€ synthetic_data_generator.py  # ì™„ë£Œ âœ…
â”‚   â”œâ”€â”€ cross_lingual_synonyms.py    # ì™„ë£Œ âœ…
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                  # TODO ğŸ”¨
â”‚   â”‚   â”œâ”€â”€ neural_sparse_encoder.py
â”‚   â”‚   â””â”€â”€ model_config.py
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                # TODO ğŸ”¨
â”‚   â”‚   â”œâ”€â”€ trainer.py
â”‚   â”‚   â”œâ”€â”€ losses.py
â”‚   â”‚   â”œâ”€â”€ data_collator.py
â”‚   â”‚   â””â”€â”€ hard_negative_miner.py
â”‚   â”‚
â”‚   â””â”€â”€ evaluation/              # TODO ğŸ”¨
â”‚       â”œâ”€â”€ metrics.py
â”‚       â””â”€â”€ evaluator.py
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_neural_sparse_base_training.ipynb  # ì™„ë£Œ âœ…
â”‚   â”œâ”€â”€ 02_llm_synthetic_data_generation.ipynb  # ì™„ë£Œ âœ…
â”‚   â”œâ”€â”€ 03_llm_enhanced_training.ipynb  # ì§„í–‰ ì˜ˆì •
â”‚   â”œâ”€â”€ 04_data_augmentation.ipynb      # TODO ğŸ”¨
â”‚   â”œâ”€â”€ 05_model_training.ipynb         # TODO ğŸ”¨
â”‚   â”œâ”€â”€ 06_evaluation.ipynb             # TODO ğŸ”¨
â”‚   â””â”€â”€ 07_opensearch_integration.ipynb # TODO ğŸ”¨
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ training_config.yaml     # TODO ğŸ”¨
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_korean_generation.py  # ì™„ë£Œ âœ…
â”‚   â””â”€â”€ test_neural_sparse_encoder.py  # TODO ğŸ”¨
â”‚
â”œâ”€â”€ plan.md                      # ì´ ë¬¸ì„œ
â””â”€â”€ README.md
```

---

## ğŸ¯ ë‹¤ìŒ ì¦‰ì‹œ í•  ì¼ (Priority)

### ğŸ”¥ High Priority (ì´ë²ˆ ì£¼)

1. **Notebook 2 ì¬ì‹¤í–‰ - ëŒ€ëŸ‰ í•©ì„± ë°ì´í„° ìƒì„±**
   ```python
   # notebooks/02_llm_synthetic_data_generation.ipynb
   # max_documentsë¥¼ 10 â†’ 1000ìœ¼ë¡œ ë³€ê²½
   synthetic_pairs = generate_synthetic_qd_pairs(
       documents=documents[:1000],  # 1000 documents
       num_queries_per_doc=3,
   )
   # ì˜ˆìƒ ì¶œë ¥: 3000 synthetic pairs
   ```

2. **Hard Negative Mining êµ¬í˜„**
   ```python
   # src/training/hard_negative_miner.py
   - BM25 ê¸°ë°˜ negative sampling
   - Top-K í›„ë³´ ì¤‘ positive ì œì™¸
   - Batch processing for efficiency
   ```

3. **Neural Sparse Encoder ê¸°ë³¸ êµ¬í˜„**
   ```python
   # src/models/neural_sparse_encoder.py
   - BERT base + projection layer
   - Forward pass êµ¬í˜„
   - ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
   ```

### ğŸ“Œ Medium Priority (ë‹¤ìŒ ì£¼)

4. **Loss Functions êµ¬í˜„**
   - Ranking loss
   - Cross-lingual loss
   - FLOPS regularization

5. **Training Loop êµ¬í˜„**
   - Trainer class
   - Logging and checkpointing
   - Early stopping

6. **Evaluation Framework**
   - Test set ë¶„ë¦¬
   - Metrics ê³„ì‚°
   - Baseline ë¹„êµ

### ğŸ’¡ Low Priority (ë‚˜ì¤‘ì—)

7. **OpenSearch í†µí•©**
8. **ë¬¸ì„œí™”**
9. **ë°°í¬ ì¤€ë¹„**

---

## â“ ì˜ì‚¬ê²°ì • í•„ìš” í•­ëª©

### 1. Base Model ì„ íƒ
- [ ] **klue/bert-base** (í•œêµ­ì–´ íŠ¹í™”, ê¶Œì¥)
- [ ] **xlm-roberta-base** (ë‹¤êµ­ì–´)
- [ ] **ê¸°íƒ€**: _____________

### 2. í•™ìŠµ ë°ì´í„° ê·œëª¨
- [ ] Small (100K pairs) - ë¹ ë¥¸ ì‹¤í—˜
- [ ] **Medium (500K pairs)** - ê¶Œì¥
- [ ] Large (1M+ pairs) - ìµœê³  ì„±ëŠ¥

### 3. í‰ê°€ ì „ëµ
- [ ] Offline evaluation only
- [ ] **Online A/B testing** (OpenSearch í†µí•© í›„)

### 4. ë°°í¬ ìš°ì„ ìˆœìœ„
- [ ] HuggingFace Hub
- [ ] **OpenSearch Plugin**
- [ ] Docker Container
- [ ] API Server

---

## ğŸ“ ì°¸ê³  ìë£Œ

### Papers
- [SPLADE](https://arxiv.org/abs/2107.05720) - Sparse Lexical and Expansion Model
- [DeepImpact](https://arxiv.org/abs/2104.12016) - Neural Text Ranking
- [uniCOIL](https://arxiv.org/abs/2106.14807) - Contextualized Term Weighting

### Code References
- [naver/splade](https://github.com/naver/splade)
- [OpenSearch Neural Search](https://opensearch.org/docs/latest/neural-search-plugin/index/)
- [Sentence Transformers](https://www.sbert.net/)

### Datasets
- [KLUE Benchmark](https://klue-benchmark.com/)
- [KorQuAD](https://korquad.github.io/)
- [Korean Wikipedia](https://ko.wikipedia.org/)

---

## âœ… Success Criteria

í”„ë¡œì íŠ¸ ì„±ê³µ ê¸°ì¤€:

1. **ì„±ëŠ¥**
   - NDCG@10 > 0.6 (baseline BM25 ëŒ€ë¹„ +10%)
   - í•œì˜ cross-lingual retrieval ì§€ì›

2. **íš¨ìœ¨ì„±**
   - Query latency < 50ms
   - Average active terms < 100

3. **ë°°í¬**
   - OpenSearch í”ŒëŸ¬ê·¸ì¸ ë™ì‘ í™•ì¸
   - ì‹¤ì œ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì ìš© ê°€ëŠ¥

4. **ì¬í˜„ì„±**
   - ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ìë™í™”
   - ë¬¸ì„œí™” ì™„ë£Œ

---

**Last Updated:** 2025-11-16
**Status:** Phase 2 ì‹œì‘ (Neural Sparse ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„¤ê³„)
**Next Milestone:** Hard negative mining êµ¬í˜„ ë° ëŒ€ëŸ‰ í•©ì„± ë°ì´í„° ìƒì„±

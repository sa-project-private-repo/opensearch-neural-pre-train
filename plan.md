# Plan: LLM ê¸°ë°˜ í•©ì„± ë°ì´í„° ìƒì„± ë° í•œì˜ í†µí•© ë™ì˜ì–´ ì‚¬ì „ ì¶”ê°€ (ARM ìµœì í™”)

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

**ëª©í‘œ**: LLM ê¸°ë°˜ í•©ì„± ë°ì´í„° ìƒì„± ë° í•œì˜ ë™ì˜ì–´ ì‚¬ì „ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ **ìƒˆë¡œìš´ ë…¸íŠ¸ë¶** ìƒì„±

**í•µì‹¬ ìš”êµ¬ì‚¬í•­**:
1. **ìƒˆ ë…¸íŠ¸ë¶ ìƒì„±**: `korean_neural_sparse_training_v2_llm.ipynb`
2. ê¸°ì¡´ `korean_neural_sparse_training.ipynb`ì˜ ëª¨ë“  ë‚´ìš© í¬í•¨ (ëˆ„ë½ ê¸ˆì§€)
3. LLMì„ í†µí•œ í•©ì„± ë°ì´í„° ìƒì„± (Query-Document pairs) ì¶”ê°€
4. í•œì˜ í†µí•© ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶• (ì„ë² ë”© + LLM ê²€ì¦) ì¶”ê°€
5. Localì— ê²½ëŸ‰ LLM ëª¨ë¸ ë¡œë”© ë° í™œìš© (ARM í˜¸í™˜)

**ë…¸íŠ¸ë¶ êµ¬ì¡°**:
- **ê¸°ì¡´ ìœ ì§€**: `korean_neural_sparse_training.ipynb` (ë³€ê²½ ì—†ìŒ)
- **ì‹ ê·œ ìƒì„±**: `korean_neural_sparse_training_v2_llm.ipynb` (LLM ê¸°ëŠ¥ ì¶”ê°€)

**ì‹œìŠ¤í…œ í™˜ê²½**:
- **ì•„í‚¤í…ì²˜**: ARM aarch64 (Blackwell GB10)
- **GPU**: NVIDIA GB10 (CUDA 13.0 ì§€ì›)
- **ë©”ëª¨ë¦¬**: ì œí•œì  (í˜„ì¬ 4.5GB GPU ì‚¬ìš© ì¤‘)
- **Python**: 3.12 (venv í™˜ê²½)
- **ì œì•½ì‚¬í•­**: vLLMì€ ARM ì§€ì› ì œí•œì  â†’ ëŒ€ì•ˆ í•„ìš”

---

## ğŸ” í˜„í™© ë¶„ì„

### í˜„ì¬ êµ¬í˜„ëœ ê¸°ëŠ¥
- âœ… í•œì˜ ë™ì˜ì–´ ì‚¬ì „ ê¸°ì´ˆ êµ¬í˜„ (`src/cross_lingual_synonyms.py`)
  - Pattern-based extraction (e.g., "ëª¨ë¸(model)")
  - Embedding similarity ê¸°ë°˜ ë™ì˜ì–´ ë°œê²¬
  - Manual curated pairs
  - ë…¸íŠ¸ë¶ Cell 14ì—ì„œ ì‚¬ìš© ì¤‘

### ì¶”ê°€ í•„ìš” ê¸°ëŠ¥
- âŒ LLM ê¸°ë°˜ í•©ì„± ë°ì´í„° ìƒì„±
- âŒ ARM í˜¸í™˜ LLM ë¡œë”© ë° ì¶”ë¡ 
- âŒ LLMì„ í™œìš©í•œ ê³ í’ˆì§ˆ Query-Document pair ìƒì„±
- âŒ LLM ê¸°ë°˜ ë™ì˜ì–´ ê²€ì¦ ë° í™•ì¥

---

## ğŸ“¦ Phase 1: í™˜ê²½ ì„¤ì • ë° ARM í˜¸í™˜ LLM ë¡œë”©

### 1.1 Python í™˜ê²½ ì„¤ì •
**Python ë²„ì „**: 3.12 (venv)

```bash
# venv ìƒì„± ë° í™œì„±í™”
python3.12 -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ë˜ëŠ” .venv\Scripts\activate  # Windows

# pip ì—…ê·¸ë ˆì´ë“œ
pip install --upgrade pip setuptools wheel
```

**Python 3.12 í˜¸í™˜ì„±**:
- âœ… PyTorch 2.5.1 (Python 3.12 ì§€ì›)
- âœ… Transformers 4.46.3 (Python 3.12 ì§€ì›)
- âœ… AutoAWQ 0.2.7 (Python 3.12 ì§€ì›)
- âš ï¸ llama-cpp-python: ë¹Œë“œ í•„ìš”í•  ìˆ˜ ìˆìŒ (ARM + Python 3.12)

### 1.2 ì˜ì¡´ì„± ì¶”ê°€ (ARM + Python 3.12 ìµœì í™”)
**íŒŒì¼**: `requirements.txt`

ì¶”ê°€í•  íŒ¨í‚¤ì§€:
```txt
# Python 3.12 compatible versions
# LLM inference (ARM-compatible)
# vLLMì€ ARM ì§€ì› ì œí•œì ì´ë¯€ë¡œ ì œì™¸
accelerate==1.1.1         # Already exists - ë©”ëª¨ë¦¬ ìµœì í™”
autoawq==0.2.7            # AWQ quantization (Qwen2.5 ê¶Œì¥, Python 3.12 OK)
optimum==1.23.3           # ONNX Runtime ìµœì í™”
sentencepiece==0.2.0      # Already exists - tokenizer

# gpt-oss-20b ì‚¬ìš© ì‹œ (GGUF)
# llama-cpp-python==0.3.4  # Optional: gpt-oss-20b GGUF ì§€ì›
#                          # ARM + Python 3.12: ì†ŒìŠ¤ ë¹Œë“œ í•„ìš”í•  ìˆ˜ ìˆìŒ
```

**ì „ëµ**: Hugging Face Transformers + AutoAWQ quantization ì‚¬ìš©
- Qwen2.5: AutoAWQë¡œ 4-bit ì–‘ìí™” (ARM + Python 3.12 ê²€ì¦)
- gpt-oss-20b: GGUF + llama.cpp (ARM ìµœì í™”, Python 3.12 ë¹Œë“œ í•„ìš”)
- accelerateë¡œ ë©€í‹° GPU/CPU offloading
- ëª¨ë“  íŒ¨í‚¤ì§€ Python 3.12 í˜¸í™˜ ë²„ì „ ì‚¬ìš©

### 1.3 ëª¨ë¸ ë¡œë” ëª¨ë“ˆ êµ¬í˜„ (ARM + Python 3.12 ìµœì í™”)
**ìƒˆ íŒŒì¼**: `src/llm_loader.py`

ê¸°ëŠ¥:
- ARM í˜¸í™˜ ê²½ëŸ‰ LLM ë¡œë”© (Hugging Face)
- GPU ë©”ëª¨ë¦¬ ìµœì í™” (INT8/INT4 quantization via bitsandbytes)
- Batch inference ì§€ì›
- Prompt template ê´€ë¦¬
- CPU offloading ì§€ì› (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ)

**ì‚¬ìš© ëª¨ë¸ (ìš”êµ¬ì‚¬í•­)**:
1. **gpt-oss-20b** (OpenAI, 21B params, 3.6B active)
2. **Qwen2.5** ì‹œë¦¬ì¦ˆ (Alibaba, ë‹¤ì–‘í•œ í¬ê¸°)

**ì„ íƒ ì „ëµ**: GPU ë©”ëª¨ë¦¬ ê³ ë ¤í•˜ì—¬ Qwen2.5-14B ë˜ëŠ” gpt-oss-20b (GGUF) ì¶”ì²œ

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] `load_qwen3_awq()` í•¨ìˆ˜ êµ¬í˜„ (AWQ 4-bit)
- [ ] `load_gpt_oss_gguf()` í•¨ìˆ˜ êµ¬í˜„ (GGUF, optional)
- [ ] `generate_text()` í•¨ìˆ˜ êµ¬í˜„
- [ ] `generate_batch()` ë°°ì¹˜ ì¶”ë¡  í•¨ìˆ˜
- [ ] Prompt template ì •ì˜ (Qwen2.5/gpt-oss ìµœì í™”)
- [ ] GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ìœ í‹¸ë¦¬í‹°
- [ ] CPU offloading ì˜µì…˜

---

## ğŸ“ Phase 2: LLM ê¸°ë°˜ í•©ì„± ë°ì´í„° ìƒì„±

### 2.1 í•©ì„± ë°ì´í„° ìƒì„± ëª¨ë“ˆ
**ìƒˆ íŒŒì¼**: `src/synthetic_data_generator.py`

ê¸°ëŠ¥:
- Document â†’ Query ìƒì„± (ì—­ë°©í–¥ ìƒì„±)
- Query â†’ Document ìƒì„± (ì •ë°©í–¥ ìƒì„±)
- Query augmentation (ë™ì˜ì–´, paraphrase)
- Hard negative document ìƒì„±
- í’ˆì§ˆ í•„í„°ë§ (ê¸¸ì´, ì¤‘ë³µ ì œê±°)

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] `generate_queries_from_documents()` í•¨ìˆ˜
- [ ] `generate_documents_from_queries()` í•¨ìˆ˜
- [ ] `augment_query()` í•¨ìˆ˜ (paraphrasing)
- [ ] `generate_hard_negatives()` í•¨ìˆ˜
- [ ] `filter_synthetic_pairs()` í’ˆì§ˆ í•„í„°

### 2.2 Prompt Engineering
**Prompt ì˜ˆì‹œ**:

```python
DOC_TO_QUERY_PROMPT = """
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì½ê³  ì‚¬ìš©ìê°€ ì´ ë¬¸ì„œë¥¼ ì°¾ê¸° ìœ„í•´ ê²€ìƒ‰í•  ë§Œí•œ ì¿¼ë¦¬ 3ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ë¬¸ì„œ: {document}

ê²€ìƒ‰ ì¿¼ë¦¬ (JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ):
"""

SYNONYM_DISCOVERY_PROMPT = """
ë‹¤ìŒ ë‘ ë‹¨ì–´ê°€ ê°™ì€ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.

ë‹¨ì–´ 1: {word1}
ë‹¨ì–´ 2: {word2}

ê°™ì€ ì˜ë¯¸ì´ê±°ë‚˜ ë™ì˜ì–´ë¼ë©´ "ì˜ˆ", ì•„ë‹ˆë©´ "ì•„ë‹ˆì˜¤"ë¡œ ë‹µí•˜ê³  ê°„ë‹¨í•œ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.
"""
```

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] Document â†’ Query prompt ì‘ì„±
- [ ] Query â†’ Document prompt ì‘ì„±
- [ ] Synonym verification prompt ì‘ì„±
- [ ] Hard negative generation prompt ì‘ì„±

---

## ğŸŒ Phase 3: LLM ê¸°ë°˜ í•œì˜ ë™ì˜ì–´ ì‚¬ì „ í™•ì¥

### 3.1 ë™ì˜ì–´ ê²€ì¦ ë° í™•ì¥
**íŒŒì¼**: `src/cross_lingual_synonyms.py` í™•ì¥

ìƒˆ í•¨ìˆ˜:
- `verify_synonyms_with_llm()`: LLMìœ¼ë¡œ ë™ì˜ì–´ ìŒ ê²€ì¦
- `discover_synonyms_with_llm()`: LLMìœ¼ë¡œ ìƒˆ ë™ì˜ì–´ ë°œê²¬
- `enhance_bilingual_dict_with_llm()`: ê¸°ì¡´ ì‚¬ì „ í’ˆì§ˆ í–¥ìƒ

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] `verify_synonyms_with_llm()` êµ¬í˜„
- [ ] `discover_synonyms_with_llm()` êµ¬í˜„
- [ ] `enhance_bilingual_dict_with_llm()` êµ¬í˜„
- [ ] Batch processing ìµœì í™”

### 3.2 ì„ë² ë”© + LLM í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼
**ì „ëµ**:
1. ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ í›„ë³´ ë™ì˜ì–´ ë°œê²¬ (ê¸°ì¡´ ë°©ì‹)
2. LLMìœ¼ë¡œ í›„ë³´ ê²€ì¦ ë° í•„í„°ë§ (ìƒˆë¡œìš´ ë°©ì‹)
3. ê²€ì¦ëœ ë™ì˜ì–´ë§Œ ìµœì¢… ì‚¬ì „ì— ì¶”ê°€

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] ì„ë² ë”© ê¸°ë°˜ í›„ë³´ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸
- [ ] LLM ê²€ì¦ íŒŒì´í”„ë¼ì¸
- [ ] í•˜ì´ë¸Œë¦¬ë“œ í†µí•© í•¨ìˆ˜

---

## ğŸ““ Phase 4: ìƒˆ ë…¸íŠ¸ë¶ ìƒì„± ë° í†µí•©

### 4.1 ìƒˆ ë…¸íŠ¸ë¶ ìƒì„±
**ìƒˆ íŒŒì¼**: `notebooks/korean_neural_sparse_training_v2_llm.ipynb`

**ìƒì„± ì „ëµ**:
1. ê¸°ì¡´ `korean_neural_sparse_training.ipynb` ë³µì‚¬
2. ëª¨ë“  ê¸°ì¡´ Cell ìœ ì§€ (ëˆ„ë½ ê¸ˆì§€)
3. LLM ê´€ë ¨ ìƒˆ ì„¹ì…˜ ì¶”ê°€

### 4.2 ìƒˆ ë…¸íŠ¸ë¶ êµ¬ì¡° (v2_llm)

```
ğŸ“” korean_neural_sparse_training_v2_llm.ipynb

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ê¸°ì¡´ ë…¸íŠ¸ë¶ ë‚´ìš© (ê·¸ëŒ€ë¡œ ìœ ì§€)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (Cell 1-4)         â”‚
â”‚ 2. í•œêµ­ì–´ ë°ì´í„°ì…‹ ìˆ˜ì§‘ (Cell 5-7)                 â”‚
â”‚ 3. IDF ê³„ì‚° (Cell 8-9)                             â”‚
â”‚ 4. í•œêµ­ì–´ íŠ¸ë Œë“œ í‚¤ì›Œë“œ (Cell 10-12)               â”‚
â”‚ 5. í•œì˜ í†µí•© ë™ì˜ì–´ ì‚¬ì „ (Cell 13-14)              â”‚
â”‚ 6. OpenSearch ë¬¸ì„œ ì¸ì½”ë” (Cell 15-16)             â”‚
â”‚ 7. í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„ (Cell 17-20)                 â”‚
â”‚ 8. ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ (Cell 21-22)                     â”‚
â”‚ 9. í•™ìŠµ ì‹¤í–‰ (Cell 23-26)                          â”‚
â”‚ 10. ëª¨ë¸ ì €ì¥ (Cell 27-28)                         â”‚
â”‚ 11. í…ŒìŠ¤íŠ¸ (Cell 29-30)                            â”‚
â”‚ 12. OpenSearch í†µí•© ê°€ì´ë“œ (Cell 31-33)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ†• LLM ê¸°ë°˜ í™•ì¥ (ìƒˆë¡œ ì¶”ê°€)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 13. ğŸ¤– LLM ëª¨ë¸ ë¡œë”© (ìƒˆ ì„¹ì…˜)                     â”‚
â”‚ 14. ğŸ“ í•©ì„± ë°ì´í„° ìƒì„± (ìƒˆ ì„¹ì…˜)                  â”‚
â”‚ 15. ğŸŒ LLM ê¸°ë°˜ ë™ì˜ì–´ ê²€ì¦ (ìƒˆ ì„¹ì…˜)              â”‚
â”‚ 16. ğŸ”„ í•©ì„± ë°ì´í„°ë¡œ ì¬í•™ìŠµ (ìƒˆ ì„¹ì…˜)              â”‚
â”‚ 17. ğŸ“Š ì„±ëŠ¥ ë¹„êµ ë¶„ì„ (ìƒˆ ì„¹ì…˜)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.3 ìƒˆë¡œ ì¶”ê°€í•  ì„¹ì…˜ ìƒì„¸

#### ì„¹ì…˜ 13: ğŸ¤– LLM ëª¨ë¸ ë¡œë”© (ì‹ ê·œ)
```python
print("="*70)
print("ğŸ¤– ì„¹ì…˜ 13: LLM ëª¨ë¸ ë¡œë”© ë° ì´ˆê¸°í™”")
print("="*70)

from src.llm_loader import load_qwen3_awq, check_gpu_memory

# GPU ë©”ëª¨ë¦¬ ì²´í¬
check_gpu_memory()

# Qwen2.5-14B-AWQ ëª¨ë¸ ë¡œë”© (4-bit quantization)
print("\nğŸ“¥ Qwen2.5-14B-AWQ ëª¨ë¸ ë¡œë”© ì¤‘...")
llm_model, llm_tokenizer = load_qwen3_awq(
    model_name="Qwen/Qwen2.5-14B-Instruct-AWQ",
    device_map="auto",
)

print("âœ… LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
```

#### ì„¹ì…˜ 14: ğŸ“ í•©ì„± ë°ì´í„° ìƒì„± (ì‹ ê·œ)
```python
print("\n" + "="*70)
print("ğŸ“ ì„¹ì…˜ 14: LLM ê¸°ë°˜ í•©ì„± Query-Document Pairs ìƒì„±")
print("="*70)

from src.synthetic_data_generator import generate_synthetic_qd_pairs

# ê¸°ì¡´ ë¬¸ì„œì—ì„œ í•©ì„± ì¿¼ë¦¬ ìƒì„±
synthetic_pairs = generate_synthetic_qd_pairs(
    documents=documents[:1000],  # ì²˜ìŒ 1000ê°œ ë¬¸ì„œ
    llm_model=llm_model,
    llm_tokenizer=llm_tokenizer,
    num_queries_per_doc=3,
    batch_size=2,
)

print(f"\nâœ… í•©ì„± ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(synthetic_pairs):,}ê°œ pairs")

# ìƒ˜í”Œ ì¶œë ¥
for i, (query, doc, relevance) in enumerate(synthetic_pairs[:5], 1):
    print(f"\n{i}. Query: {query}")
    print(f"   Document: {doc[:100]}...")
```

#### ì„¹ì…˜ 15: ğŸŒ LLM ê¸°ë°˜ ë™ì˜ì–´ ê²€ì¦ (ì‹ ê·œ)
```python
print("\n" + "="*70)
print("ğŸŒ ì„¹ì…˜ 15: LLM ê¸°ë°˜ í•œì˜ ë™ì˜ì–´ ê²€ì¦ ë° í™•ì¥")
print("="*70)

from src.cross_lingual_synonyms import enhance_bilingual_dict_with_llm

# ê¸°ì¡´ ì„ë² ë”© ê¸°ë°˜ ë™ì˜ì–´ë¥¼ LLMìœ¼ë¡œ ê²€ì¦
enhanced_bilingual_dict = enhance_bilingual_dict_with_llm(
    initial_dict=bilingual_dict,  # Cell 14ì—ì„œ ìƒì„±ëœ ê¸°ì¡´ ì‚¬ì „
    llm_model=llm_model,
    llm_tokenizer=llm_tokenizer,
    verification_threshold=0.8,
    max_verify=100,  # ìƒìœ„ 100ê°œë§Œ ê²€ì¦ (ì‹œê°„ ì ˆì•½)
)

print(f"\nâœ… LLM ê²€ì¦ ì™„ë£Œ!")
print(f"   ê¸°ì¡´ ì‚¬ì „: {len(bilingual_dict):,}ê°œ")
print(f"   ê²€ì¦ í›„: {len(enhanced_bilingual_dict):,}ê°œ")
```

#### ì„¹ì…˜ 16: ğŸ”„ í•©ì„± ë°ì´í„°ë¡œ ì¬í•™ìŠµ (ì‹ ê·œ)
```python
print("\n" + "="*70)
print("ğŸ”„ ì„¹ì…˜ 16: í•©ì„± ë°ì´í„° í¬í•¨ ëª¨ë¸ ì¬í•™ìŠµ")
print("="*70)

# ê¸°ì¡´ ë°ì´í„° + í•©ì„± ë°ì´í„° ë³‘í•©
combined_qd_pairs = korean_data['qd_pairs'] + synthetic_pairs

print(f"ğŸ“Š í•™ìŠµ ë°ì´í„° í†µê³„:")
print(f"   ê¸°ì¡´ ë°ì´í„°: {len(korean_data['qd_pairs']):,}ê°œ")
print(f"   í•©ì„± ë°ì´í„°: {len(synthetic_pairs):,}ê°œ")
print(f"   ì´ ë°ì´í„°: {len(combined_qd_pairs):,}ê°œ")

# Negative sampling ë° ì¬í•™ìŠµ
# (ê¸°ì¡´ Cell 18-26 ì½”ë“œ ì¬ì‚¬ìš©, combined_qd_pairs ì‚¬ìš©)
```

#### ì„¹ì…˜ 17: ğŸ“Š ì„±ëŠ¥ ë¹„êµ ë¶„ì„ (ì‹ ê·œ)
```python
print("\n" + "="*70)
print("ğŸ“Š ì„¹ì…˜ 17: ì„±ëŠ¥ ë¹„êµ ë¶„ì„ (ê¸°ì¡´ vs LLM í™•ì¥)")
print("="*70)

# ê¸°ì¡´ ëª¨ë¸ vs LLM í™•ì¥ ëª¨ë¸ ë¹„êµ
comparison_results = {
    'ëª¨ë¸': ['ê¸°ì¡´ ëª¨ë¸', 'LLM í™•ì¥ ëª¨ë¸'],
    'í•™ìŠµ ë°ì´í„°': [len(korean_data['qd_pairs']), len(combined_qd_pairs)],
    'ë™ì˜ì–´ ì‚¬ì „': [len(bilingual_dict), len(enhanced_bilingual_dict)],
    'Validation Loss': [best_val_loss_v1, best_val_loss_v2],
}

import pandas as pd
df_comparison = pd.DataFrame(comparison_results)
print(df_comparison)
```

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] ê¸°ì¡´ ë…¸íŠ¸ë¶ ì „ì²´ ë³µì‚¬ (ëˆ„ë½ ì—†ì´)
- [ ] ì„¹ì…˜ 13: LLM ë¡œë”© ì¶”ê°€
- [ ] ì„¹ì…˜ 14: í•©ì„± ë°ì´í„° ìƒì„± ì¶”ê°€
- [ ] ì„¹ì…˜ 15: ë™ì˜ì–´ ê²€ì¦ ì¶”ê°€
- [ ] ì„¹ì…˜ 16: ì¬í•™ìŠµ ë¡œì§ ì¶”ê°€
- [ ] ì„¹ì…˜ 17: ì„±ëŠ¥ ë¹„êµ ì¶”ê°€
- [ ] Markdown ì„¤ëª… Cell ì¶”ê°€
- [ ] ì „ì²´ ì‹¤í–‰ ê²€ì¦

### 4.4 í†µí•© ì›Œí¬í”Œë¡œìš° (v2_llm ë…¸íŠ¸ë¶)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° (ê·¸ëŒ€ë¡œ ìœ ì§€)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜                    â”‚
â”‚ 2. í•œêµ­ì–´ ë°ì´í„°ì…‹ ìˆ˜ì§‘                            â”‚
â”‚ 3. IDF ê³„ì‚°                                        â”‚
â”‚ 4. íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìë™ ê°ì§€                         â”‚
â”‚ 5. í•œì˜ ë™ì˜ì–´ ì‚¬ì „ (ì„ë² ë”© ê¸°ë°˜)                 â”‚
â”‚ 6. OpenSearch ë¬¸ì„œ ì¸ì½”ë” ëª¨ë¸ ì •ì˜                â”‚
â”‚ 7. í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„ (ê¸°ì¡´ ë°ì´í„°)               â”‚
â”‚ 8. ì†ì‹¤ í•¨ìˆ˜ ì •ì˜                                  â”‚
â”‚ 9. í•™ìŠµ ì‹¤í–‰ (ê¸°ì¡´ ë°ì´í„°) â†’ ëª¨ë¸ v1              â”‚
â”‚ 10. ëª¨ë¸ ì €ì¥ (v1)                                 â”‚
â”‚ 11. í…ŒìŠ¤íŠ¸ (v1)                                    â”‚
â”‚ 12. OpenSearch í†µí•© ê°€ì´ë“œ                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ†• LLM í™•ì¥ ì›Œí¬í”Œë¡œìš° (ì‹ ê·œ)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 13. [NEW] LLM ëª¨ë¸ ë¡œë”© (Qwen2.5-14B-AWQ)         â”‚
â”‚ 14. [NEW] í•©ì„± ë°ì´í„° ìƒì„±                         â”‚
â”‚     - Document â†’ Query ìƒì„±                        â”‚
â”‚     - í’ˆì§ˆ í•„í„°ë§                                  â”‚
â”‚ 15. [NEW] LLM ê¸°ë°˜ ë™ì˜ì–´ ê²€ì¦                     â”‚
â”‚     - ì„ë² ë”© í›„ë³´ â†’ LLM ê²€ì¦                       â”‚
â”‚ 16. [NEW] ì¬í•™ìŠµ (ê¸°ì¡´ + í•©ì„± ë°ì´í„°)             â”‚
â”‚     - ë°ì´í„° ë³‘í•©                                  â”‚
â”‚     - ëª¨ë¸ í•™ìŠµ â†’ ëª¨ë¸ v2                          â”‚
â”‚     - ëª¨ë¸ ì €ì¥ (v2)                               â”‚
â”‚ 17. [NEW] ì„±ëŠ¥ ë¹„êµ (v1 vs v2)                    â”‚
â”‚     - í•™ìŠµ ë°ì´í„° í¬ê¸°                             â”‚
â”‚     - Validation Loss                              â”‚
â”‚     - ê²€ìƒ‰ ì •í™•ë„                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Phase 5: ìµœì í™” ë° í…ŒìŠ¤íŠ¸

### 5.1 ì„±ëŠ¥ ìµœì í™”
ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] LLM inference batching
- [ ] GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
- [ ] í•©ì„± ë°ì´í„° ìºì‹±
- [ ] Parallel processing (ê°€ëŠ¥í•œ ê²½ìš°)

### 5.2 í’ˆì§ˆ ê²€ì¦
ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] í•©ì„± ë°ì´í„° í’ˆì§ˆ í‰ê°€ (ìˆ˜ë™ ìƒ˜í”Œë§)
- [ ] ë™ì˜ì–´ ì •í™•ë„ ì¸¡ì •
- [ ] í•™ìŠµ ì„±ëŠ¥ ë¹„êµ (í•©ì„± ë°ì´í„° ìœ /ë¬´)
- [ ] Ablation study (LLM vs. ì„ë² ë”© only)

### 5.3 ë¬¸ì„œí™”
ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] `src/llm_loader.py` docstring ì‘ì„±
- [ ] `src/synthetic_data_generator.py` docstring ì‘ì„±
- [ ] `src/__init__.py` ì—…ë°ì´íŠ¸ (ìƒˆ í•¨ìˆ˜ export)
- [ ] README ì—…ë°ì´íŠ¸ (ìƒˆ ê¸°ëŠ¥ ì„¤ëª…)
- [ ] Notebookì— ì„¤ëª… markdown cell ì¶”ê°€

---

## âš™ï¸ ê¸°ìˆ ì  ê³ ë ¤ì‚¬í•­ (ARM GB10 í™˜ê²½)

### GPU ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ (í˜„ì¬: GB10)
- **í˜„ì¬ ì‚¬ìš©ëŸ‰**: 4.5GB (Jupyter í”„ë¡œì„¸ìŠ¤)
- **ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬**: ì˜ˆìƒ ~12-16GB (GB10 ì´ ë©”ëª¨ë¦¬ ë¯¸í™•ì¸)
- **BERT í•™ìŠµ ë©”ëª¨ë¦¬**: ~4-6GB (í˜„ì¬ ì‚¬ìš© ì¤‘)
- **LLM ì¶”ë¡  ë©”ëª¨ë¦¬** (ìš”êµ¬ì‚¬í•­ ëª¨ë¸):
  - Qwen2.5-14B (AWQ 4-bit): ~4GB â­
  - Qwen2.5-7B (AWQ 4-bit): ~2GB
  - gpt-oss-20b (GGUF Q4): ~5GB
  - Qwen2.5-0.6B (INT8): ~0.3GB (í…ŒìŠ¤íŠ¸ìš©)

**ê¶Œì¥ ì „ëµ**:
- **Option A**: Qwen2.5-14B-AWQ ì‚¬ìš© (4-bit, ~4GB) - ì„±ëŠ¥ ìš°ì„ 
- **Option B**: Qwen2.5-7B-AWQ ì‚¬ìš© (4-bit, ~2GB) - ì•ˆì •ì„± ìš°ì„ 
- BERT í•™ìŠµ ì™„ë£Œ í›„ LLM ë¡œë”© (ìˆœì°¨ ì‹¤í–‰ ê¶Œì¥)
- í•„ìš” ì‹œ CPU offloading í™œìš© (accelerate)

### LLM ì„ íƒì§€ (ìš”êµ¬ì‚¬í•­: gpt-oss-20b ë˜ëŠ” Qwen2.5)

#### Option 1: Qwen2.5-14B-Instruct â­ ìµœìš°ì„  ì¶”ì²œ
- **í¬ê¸°**: 14B params (~28GB FP16, ~7GB INT8, ~4GB Q4)
- **ì¥ì **:
  - ARM aarch64 ì™„ë²½ ì§€ì› (ê²€ì¦ë¨)
  - í•œêµ­ì–´ ìš°ìˆ˜ (ë‹¤êµ­ì–´ ëª¨ë¸)
  - 4-bit/8-bit quantization ì„±ëŠ¥ ìš°ìˆ˜
  - bitsandbytes, AWQ, GPTQ ëª¨ë‘ ì§€ì›
- **ë‹¨ì **: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ
- **Hugging Face**: `Qwen/Qwen2.5-14B-Instruct`
- **Quantized**: `Qwen/Qwen2.5-14B-Instruct-AWQ` (4-bit)

#### Option 2: Qwen2.5-7B-Instruct
- **í¬ê¸°**: 7B params (~14GB FP16, ~3.5GB INT8)
- **ì¥ì **:
  - ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
  - ARM í˜¸í™˜
  - í•œêµ­ì–´ ì„±ëŠ¥ ìš°ìˆ˜
  - ë¹ ë¥¸ ì¶”ë¡ 
- **ë‹¨ì **: 14B ëŒ€ë¹„ ì„±ëŠ¥ ë‚®ìŒ
- **Hugging Face**: `Qwen/Qwen2.5-7B-Instruct`
- **Quantized**: `Qwen/Qwen2.5-7B-Instruct-AWQ`

#### Option 3: gpt-oss-20b (GGUF)
- **í¬ê¸°**: 21B params (3.6B active MoE), ~16GB MXFP4
- **ì¥ì **:
  - ARM ìë™ ìµœì í™” (GGUF)
  - MoE êµ¬ì¡°ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
  - llama.cpp ì§€ì›
  - Q4_0, IQ4_NL quantization (ARM ìµœì í™”)
- **ë‹¨ì **:
  - Transformers ì§ì ‘ ì§€ì› ì œí•œì  (GGUF ì‚¬ìš© í•„ìš”)
  - llama.cpp ì˜ì¡´ì„±
- **Hugging Face**: `openai/gpt-oss-20b`
- **GGUF**: `ggml-org/gpt-oss-20b-GGUF`

#### Option 4: Qwen2.5-0.6B (ê²½ëŸ‰ í…ŒìŠ¤íŠ¸ìš©)
- **í¬ê¸°**: 0.6B params (~1.2GB FP16, ~0.3GB INT8)
- **ì¥ì **: ë§¤ìš° ê²½ëŸ‰, ë¹ ë¥¸ ì‹¤í—˜
- **ë‹¨ì **: ì„±ëŠ¥ ì œí•œì 
- **Hugging Face**: `Qwen/Qwen2.5-0.6B-Instruct`

**ìµœì¢… ì¶”ì²œ**:
- **ë©”ëª¨ë¦¬ ì—¬ìœ  ìˆìŒ**: Qwen2.5-14B-AWQ (4-bit, ~4GB) â­
- **ë©”ëª¨ë¦¬ ì œí•œì **: Qwen2.5-7B-AWQ (4-bit, ~2GB)
- **gpt-oss-20b í•„ìˆ˜**: GGUF Q4_0 ë²„ì „ (~5GB)

### í’ˆì§ˆ vs. ë¹„ìš© íŠ¸ë ˆì´ë“œì˜¤í”„
- **ê³ í’ˆì§ˆ ì „ëµ**: LLMìœ¼ë¡œ ëª¨ë“  ë™ì˜ì–´ ê²€ì¦ (ëŠë¦¼, ë¹„ìš© ë†’ìŒ)
- **ê· í˜• ì „ëµ**: ì„ë² ë”©ìœ¼ë¡œ í›„ë³´ ì¶”ì¶œ + LLMìœ¼ë¡œ ì¼ë¶€ ê²€ì¦ (ê¶Œì¥)
- **ì €ë¹„ìš© ì „ëµ**: ì„ë² ë”©ë§Œ ì‚¬ìš© + ìˆ˜ë™ íë ˆì´ì…˜

---

## ğŸ“… êµ¬í˜„ ìˆœì„œ ë° ìš°ì„ ìˆœìœ„

### High Priority (Core)
1. âœ… Phase 1.2: ëª¨ë¸ ë¡œë” êµ¬í˜„ (`src/llm_loader.py`)
2. âœ… Phase 2.1: í•©ì„± ë°ì´í„° ìƒì„±ê¸° êµ¬í˜„ (`src/synthetic_data_generator.py`)
3. âœ… Phase 4.1: Notebook í†µí•© (ìƒˆ Cell ì¶”ê°€)

### Medium Priority (Enhancement)
4. âœ… Phase 3.1: LLM ê¸°ë°˜ ë™ì˜ì–´ ê²€ì¦
5. âœ… Phase 5.2: í’ˆì§ˆ ê²€ì¦

### Low Priority (Optimization)
6. â¸ï¸ Phase 5.1: ì„±ëŠ¥ ìµœì í™”
7. â¸ï¸ Phase 5.3: ë¬¸ì„œí™” ì™„ì„±

---

## ğŸ¯ ì„±ê³µ ì§€í‘œ

- [ ] ìƒˆ ë…¸íŠ¸ë¶ ìƒì„± ì™„ë£Œ (`korean_neural_sparse_training_v2_llm.ipynb`)
- [ ] ê¸°ì¡´ ë…¸íŠ¸ë¶ ë‚´ìš© 100% ìœ ì§€ (ëˆ„ë½ ì—†ìŒ)
- [ ] Qwen2.5-14B-AWQ ë˜ëŠ” gpt-oss-20b ëª¨ë¸ ë¡œë”© ì„±ê³µ
- [ ] GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 12GB ì´ë‚´ ìœ ì§€
- [ ] ìµœì†Œ 1,000ê°œ ì´ìƒì˜ í•©ì„± Query-Document pairs ìƒì„±
- [ ] í•œì˜ ë™ì˜ì–´ ì‚¬ì „ í¬ê¸° 2ë°° ì´ìƒ ì¦ê°€
- [ ] í•©ì„± ë°ì´í„°ë¡œ í•™ìŠµ ì‹œ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ (MRR/NDCG)
- [ ] v1 ëª¨ë¸ vs v2 ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì™„ë£Œ
- [ ] ìƒˆ ë…¸íŠ¸ë¶ ì „ì²´ ì‹¤í–‰ ì‹œê°„ 5ì‹œê°„ ì´ë‚´ (ARM GPU í™˜ê²½)

---

## ğŸš¨ ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘

### ë¦¬ìŠ¤í¬ 1: GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
**ëŒ€ì‘**:
- AWQ 4-bit quantization ì‚¬ìš©
- Smaller batch size
- Gradient checkpointing
- CPU offloading (ì†ë„ ì €í•˜ ê°ìˆ˜)

### ë¦¬ìŠ¤í¬ 4: Python 3.12 í˜¸í™˜ì„± ë¬¸ì œ
**ëŒ€ì‘**:
- llama-cpp-python: CMAKEë¡œ ì†ŒìŠ¤ ë¹Œë“œ
- autoawq: ìµœì‹  ë²„ì „ ì‚¬ìš© (0.2.7+)
- ì˜ì¡´ì„± ì¶©ëŒ ì‹œ requirements.txt ë²„ì „ ì¡°ì •
- venv í™˜ê²½ ê²©ë¦¬ë¡œ ì‹œìŠ¤í…œ Pythonê³¼ ë¶„ë¦¬

### ë¦¬ìŠ¤í¬ 2: LLM ìƒì„± í’ˆì§ˆ ë‚®ìŒ
**ëŒ€ì‘**:
- Prompt engineering ê°œì„ 
- Few-shot examples ì¶”ê°€
- Temperature/Top-p ì¡°ì •
- ë‹¤ë¥¸ LLM ëª¨ë¸ ì‹œë„

### ë¦¬ìŠ¤í¬ 3: í•©ì„± ë°ì´í„° ê³¼ì í•©
**ëŒ€ì‘**:
- í•©ì„±/ì‹¤ì œ ë°ì´í„° ë¹„ìœ¨ ì¡°ì • (1:1 ë˜ëŠ” 1:2)
- Validation setì€ ì‹¤ì œ ë°ì´í„°ë§Œ ì‚¬ìš©
- Diversity penalty ì¶”ê°€

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [Hugging Face Transformers - Text Generation](https://huggingface.co/docs/transformers/main_classes/text_generation)
- [bitsandbytes - INT8/INT4 Quantization](https://github.com/TimDettmers/bitsandbytes)
- [Accelerate - Memory Optimization](https://huggingface.co/docs/accelerate/index)
- [InPars: Data Augmentation for Information Retrieval](https://arxiv.org/abs/2202.05144)
- [Promptagator: Few-shot Dense Retrieval](https://arxiv.org/abs/2209.11755)
- [Qwen2.5 Model Card](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct)
- [Qwen2.5 AWQ Quantization](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct-AWQ)
- [gpt-oss-20b Model Card](https://huggingface.co/openai/gpt-oss-20b)
- [gpt-oss-20b GGUF](https://huggingface.co/ggml-org/gpt-oss-20b-GGUF)
- [AutoAWQ Documentation](https://github.com/casper-hansen/AutoAWQ)
- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)

---

## âœ… Checklist Summary

**Phase 1**: í™˜ê²½ ì„¤ì • ë° ëª¨ë¸ ë¡œë”©
- [ ] Python 3.12 venv í™˜ê²½ ì„¤ì •
- [ ] requirements.txt ì—…ë°ì´íŠ¸ (Python 3.12 í˜¸í™˜)
- [ ] src/llm_loader.py êµ¬í˜„
- [ ] GPU ë©”ëª¨ë¦¬ ì²´í¬ ë° ìµœì í™”

**Phase 2**: í•©ì„± ë°ì´í„° ìƒì„±
- [ ] src/synthetic_data_generator.py êµ¬í˜„
- [ ] Prompt templates ì‘ì„±
- [ ] í’ˆì§ˆ í•„í„°ë§ ë¡œì§

**Phase 3**: ë™ì˜ì–´ ì‚¬ì „ í™•ì¥
- [ ] src/cross_lingual_synonyms.py í™•ì¥
- [ ] LLM ê²€ì¦ í•¨ìˆ˜ ì¶”ê°€
- [ ] í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

**Phase 4**: ìƒˆ ë…¸íŠ¸ë¶ ìƒì„± ë° í†µí•©
- [ ] ê¸°ì¡´ ë…¸íŠ¸ë¶ ì „ì²´ ë³µì‚¬ (korean_neural_sparse_training_v2_llm.ipynb)
- [ ] ëª¨ë“  ê¸°ì¡´ Cell ìœ ì§€ ê²€ì¦ (ëˆ„ë½ ì—†ì´)
- [ ] ì„¹ì…˜ 13-17 ì¶”ê°€ (LLM ë¡œë”©, í•©ì„± ë°ì´í„°, ë™ì˜ì–´, ì¬í•™ìŠµ, ë¹„êµ)
- [ ] Markdown ì„¤ëª… Cell ì¶”ê°€
- [ ] ì „ì²´ ë…¸íŠ¸ë¶ ì‹¤í–‰ ê²€ì¦

**Phase 5**: ìµœì í™” ë° ê²€ì¦
- [ ] ì„±ëŠ¥ ìµœì í™”
- [ ] í’ˆì§ˆ í‰ê°€
- [ ] ë¬¸ì„œí™”

---

---

## ğŸš€ Quick Start (ARM í™˜ê²½)

### Step 1: Python 3.12 venv í™˜ê²½ ì„¤ì • ë° ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# venv ìƒì„± (Python 3.12)
python3.12 -m venv .venv
source .venv/bin/activate

# pip ì—…ê·¸ë ˆì´ë“œ
pip install --upgrade pip setuptools wheel

# PyTorch ì„¤ì¹˜ (CUDA 12.1 for GB10)
pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121

# Qwen2.5 ì‚¬ìš© ì‹œ (ê¶Œì¥)
pip install autoawq optimum accelerate transformers

# gpt-oss-20b ì‚¬ìš© ì‹œ (ì¶”ê°€) - ARM + Python 3.12 ë¹Œë“œ
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python --no-cache-dir
```

**Python 3.12 ì£¼ì˜ì‚¬í•­**:
- llama-cpp-pythonì€ ì†ŒìŠ¤ ë¹Œë“œê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ (ARM + CUDA)
- CMAKE_ARGSë¡œ CUDA ì§€ì› í™œì„±í™”
- Qwen2.5-AWQëŠ” Python 3.12ì—ì„œ ë³„ë„ ë¹Œë“œ ë¶ˆí•„ìš”

### Step 2: LLM ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

#### Option A: Qwen2.5-14B (AWQ 4-bit) - ê¶Œì¥ â­
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen2.5-14B-Instruct-AWQ"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",  # Auto GPU/CPU placement
    low_cpu_mem_usage=True,
)
```

#### Option B: Qwen3-7B (AWQ 4-bit) - ë©”ëª¨ë¦¬ ì œì•½ ì‹œ
```python
model_name = "Qwen/Qwen3-7B-Instruct-AWQ"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
)
```

#### Option C: gpt-oss-20b (GGUF) - llama.cpp í•„ìš”
```bash
# llama.cpp ì„¤ì¹˜
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make
```

```python
# Python binding ì‚¬ìš©
from llama_cpp import Llama

llm = Llama(
    model_path="gpt-oss-20b-Q4_0.gguf",
    n_ctx=2048,
    n_gpu_layers=-1,  # All layers to GPU
)
```

### Step 3: ìƒˆ ë…¸íŠ¸ë¶ ìƒì„± ë° LLM ê¸°ëŠ¥ ì¶”ê°€

```bash
# ê¸°ì¡´ ë…¸íŠ¸ë¶ ë³µì‚¬
cd notebooks
cp korean_neural_sparse_training.ipynb korean_neural_sparse_training_v2_llm.ipynb
```

```python
# ìƒˆ ë…¸íŠ¸ë¶ì—ì„œ ì¶”ê°€ (ì„¹ì…˜ 13-17)

# ì„¹ì…˜ 13: LLM ë¡œë”©
from src.llm_loader import load_qwen3_awq
llm_model, llm_tokenizer = load_qwen3_awq(
    model_name="Qwen/Qwen3-14B-Instruct-AWQ",
)

# ì„¹ì…˜ 14: í•©ì„± ë°ì´í„° ìƒì„±
from src.synthetic_data_generator import generate_synthetic_qd_pairs
synthetic_pairs = generate_synthetic_qd_pairs(
    documents=documents[:1000],
    llm_model=llm_model,
    llm_tokenizer=llm_tokenizer,
    batch_size=2,
)

# ì„¹ì…˜ 15: ë™ì˜ì–´ ê²€ì¦
from src.cross_lingual_synonyms import enhance_bilingual_dict_with_llm
enhanced_dict = enhance_bilingual_dict_with_llm(
    initial_dict=bilingual_dict,
    llm_model=llm_model,
    llm_tokenizer=llm_tokenizer,
)

# ì„¹ì…˜ 16: ì¬í•™ìŠµ (ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©)
combined_qd_pairs = korean_data['qd_pairs'] + synthetic_pairs
# ... í•™ìŠµ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)

# ì„¹ì…˜ 17: ì„±ëŠ¥ ë¹„êµ
print(f"v1 ëª¨ë¸ loss: {best_val_loss_v1:.4f}")
print(f"v2 ëª¨ë¸ loss: {best_val_loss_v2:.4f}")
```

---

**Updated**: 2025-11-13
**Status**: ARM + Python 3.12 ìµœì í™” ì™„ë£Œ, Ready for implementation
**Environment**:
- ARM aarch64 (Blackwell GB10)
- NVIDIA GB10 GPU (CUDA 13.0)
- Python 3.12 (venv)
- PyTorch 2.5.1 (CUDA 12.1)

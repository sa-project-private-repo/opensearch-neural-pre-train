# Plan: LLM ê¸°ë°˜ í•©ì„± ë°ì´í„° ìƒì„± ë° í•œì˜ í†µí•© ë™ì˜ì–´ ì‚¬ì „ ì¶”ê°€ (ARM ìµœì í™”)

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

**ëª©í‘œ**: `korean_neural_sparse_training.ipynb`ì— LLM ê¸°ë°˜ í•©ì„± ë°ì´í„° ìƒì„± ê¸°ëŠ¥ê³¼ ì„ë² ë”© ê¸°ë°˜ í•œì˜ ë™ì˜ì–´ ì‚¬ì „ ìƒì„± ê¸°ëŠ¥ ì¶”ê°€

**í•µì‹¬ ìš”êµ¬ì‚¬í•­**:
1. LLMì„ í†µí•œ í•©ì„± ë°ì´í„° ìƒì„± (Query-Document pairs)
2. í•œì˜ í†µí•© ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶• (ì„ë² ë”© ê¸°ë°˜)
3. Localì— ê²½ëŸ‰ LLM ëª¨ë¸ ë¡œë”© ë° í™œìš© (ARM í˜¸í™˜)
4. ê¸°ì¡´ ì›Œí¬í”Œë¡œìš°ì™€ í†µí•©

**ì‹œìŠ¤í…œ í™˜ê²½**:
- **ì•„í‚¤í…ì²˜**: ARM aarch64 (Blackwell GB10)
- **GPU**: NVIDIA GB10 (CUDA 13.0 ì§€ì›)
- **ë©”ëª¨ë¦¬**: ì œí•œì  (í˜„ì¬ 4.5GB GPU ì‚¬ìš© ì¤‘)
- **ì œì•½ì‚¬í•­**: vLLMì€ ARM ì§€ì› ì œí•œì  â†’ ëŒ€ì•ˆ í•„ìš”

---

## ğŸ” í˜„í™© ë¶„ì„

### í˜„ì¬ êµ¬í˜„ëœ ê¸°ëŠ¥
- âœ… í•œì˜ ë™ì˜ì–´ ì‚¬ì „ ê¸°ì´ˆ êµ¬í˜„ (`src/cross_lingual_synonyms.py`)
  - Pattern-based extraction (e.g., "ëª¨ë¸(model)")
  - Embedding similarity ê¸°ë°˜ ë™ì˜ì–´ ë°œê²¬
  - Manual curated pairs
  - ë…¸íŠ¸ë¶ Cell 14ì—ì„œ ì‚¬ìš© ì¤‘

### ì¶”ê°€ í•„ìš” ê¸°ëŠ¥
- âŒ LLM ê¸°ë°˜ í•©ì„± ë°ì´í„° ìƒì„±
- âŒ ARM í˜¸í™˜ LLM ë¡œë”© ë° ì¶”ë¡ 
- âŒ LLMì„ í™œìš©í•œ ê³ í’ˆì§ˆ Query-Document pair ìƒì„±
- âŒ LLM ê¸°ë°˜ ë™ì˜ì–´ ê²€ì¦ ë° í™•ì¥

---

## ğŸ“¦ Phase 1: í™˜ê²½ ì„¤ì • ë° ARM í˜¸í™˜ LLM ë¡œë”©

### 1.1 ì˜ì¡´ì„± ì¶”ê°€ (ARM ìµœì í™”)
**íŒŒì¼**: `requirements.txt`

ì¶”ê°€í•  íŒ¨í‚¤ì§€:
```txt
# LLM inference (ARM-compatible)
# vLLMì€ ARM ì§€ì› ì œí•œì ì´ë¯€ë¡œ ì œì™¸
accelerate==1.1.1         # Already exists - ë©”ëª¨ë¦¬ ìµœì í™”
bitsandbytes==0.44.1      # INT8/INT4 quantization (ARM ì§€ì›)
optimum==1.23.3           # ONNX Runtime ìµœì í™”
sentencepiece==0.2.0      # Already exists - tokenizer
```

**ì „ëµ**: Hugging Face Transformers + bitsandbytes quantization ì‚¬ìš©
- vLLM ëŒ€ì‹  ê¸°ë³¸ transformers ì‚¬ìš© (ARM í˜¸í™˜)
- bitsandbytesë¡œ INT8/INT4 ì–‘ìí™” (ë©”ëª¨ë¦¬ ì ˆì•½)
- accelerateë¡œ ë©€í‹° GPU/CPU offloading

### 1.2 ëª¨ë¸ ë¡œë” ëª¨ë“ˆ êµ¬í˜„ (ARM ìµœì í™”)
**ìƒˆ íŒŒì¼**: `src/llm_loader.py`

ê¸°ëŠ¥:
- ARM í˜¸í™˜ ê²½ëŸ‰ LLM ë¡œë”© (Hugging Face)
- GPU ë©”ëª¨ë¦¬ ìµœì í™” (INT8/INT4 quantization via bitsandbytes)
- Batch inference ì§€ì›
- Prompt template ê´€ë¦¬
- CPU offloading ì§€ì› (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ)

**ê¶Œì¥ ëª¨ë¸ (ARM í˜¸í™˜ + í•œêµ­ì–´ ì§€ì›)**:
1. **Polyglot-Ko-5.8B** (í•œêµ­ì–´ íŠ¹í™”, 11GB â†’ 3GB with INT8)
2. **Llama-3.2-3B-Instruct** (ë‹¤êµ­ì–´, 6GB â†’ 1.5GB with INT8)
3. **Gemma-2-2B-it** (ê²½ëŸ‰, 4GB â†’ 1GB with INT8)
4. **EEVE-Korean-10.8B** (í•œêµ­ì–´ ìš°ìˆ˜, 20GB â†’ 5GB with INT8)

**ì„ íƒ ì „ëµ**: GPU ë©”ëª¨ë¦¬ ê³ ë ¤í•˜ì—¬ Llama-3.2-3B ë˜ëŠ” Gemma-2-2B ì¶”ì²œ

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] `load_llm_model_quantized()` í•¨ìˆ˜ êµ¬í˜„ (INT8/INT4)
- [ ] `generate_text()` í•¨ìˆ˜ êµ¬í˜„
- [ ] `generate_batch()` ë°°ì¹˜ ì¶”ë¡  í•¨ìˆ˜
- [ ] Prompt template ì •ì˜ (í•œêµ­ì–´ ìµœì í™”)
- [ ] GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ìœ í‹¸ë¦¬í‹°
- [ ] CPU offloading ì˜µì…˜

---

## ğŸ“ Phase 2: LLM ê¸°ë°˜ í•©ì„± ë°ì´í„° ìƒì„±

### 2.1 í•©ì„± ë°ì´í„° ìƒì„± ëª¨ë“ˆ
**ìƒˆ íŒŒì¼**: `src/synthetic_data_generator.py`

ê¸°ëŠ¥:
- Document â†’ Query ìƒì„± (ì—­ë°©í–¥ ìƒì„±)
- Query â†’ Document ìƒì„± (ì •ë°©í–¥ ìƒì„±)
- Query augmentation (ë™ì˜ì–´, paraphrase)
- Hard negative document ìƒì„±
- í’ˆì§ˆ í•„í„°ë§ (ê¸¸ì´, ì¤‘ë³µ ì œê±°)

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] `generate_queries_from_documents()` í•¨ìˆ˜
- [ ] `generate_documents_from_queries()` í•¨ìˆ˜
- [ ] `augment_query()` í•¨ìˆ˜ (paraphrasing)
- [ ] `generate_hard_negatives()` í•¨ìˆ˜
- [ ] `filter_synthetic_pairs()` í’ˆì§ˆ í•„í„°

### 2.2 Prompt Engineering
**Prompt ì˜ˆì‹œ**:

```python
DOC_TO_QUERY_PROMPT = """
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì½ê³  ì‚¬ìš©ìê°€ ì´ ë¬¸ì„œë¥¼ ì°¾ê¸° ìœ„í•´ ê²€ìƒ‰í•  ë§Œí•œ ì¿¼ë¦¬ 3ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ë¬¸ì„œ: {document}

ê²€ìƒ‰ ì¿¼ë¦¬ (JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ):
"""

SYNONYM_DISCOVERY_PROMPT = """
ë‹¤ìŒ ë‘ ë‹¨ì–´ê°€ ê°™ì€ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.

ë‹¨ì–´ 1: {word1}
ë‹¨ì–´ 2: {word2}

ê°™ì€ ì˜ë¯¸ì´ê±°ë‚˜ ë™ì˜ì–´ë¼ë©´ "ì˜ˆ", ì•„ë‹ˆë©´ "ì•„ë‹ˆì˜¤"ë¡œ ë‹µí•˜ê³  ê°„ë‹¨í•œ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.
"""
```

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] Document â†’ Query prompt ì‘ì„±
- [ ] Query â†’ Document prompt ì‘ì„±
- [ ] Synonym verification prompt ì‘ì„±
- [ ] Hard negative generation prompt ì‘ì„±

---

## ğŸŒ Phase 3: LLM ê¸°ë°˜ í•œì˜ ë™ì˜ì–´ ì‚¬ì „ í™•ì¥

### 3.1 ë™ì˜ì–´ ê²€ì¦ ë° í™•ì¥
**íŒŒì¼**: `src/cross_lingual_synonyms.py` í™•ì¥

ìƒˆ í•¨ìˆ˜:
- `verify_synonyms_with_llm()`: LLMìœ¼ë¡œ ë™ì˜ì–´ ìŒ ê²€ì¦
- `discover_synonyms_with_llm()`: LLMìœ¼ë¡œ ìƒˆ ë™ì˜ì–´ ë°œê²¬
- `enhance_bilingual_dict_with_llm()`: ê¸°ì¡´ ì‚¬ì „ í’ˆì§ˆ í–¥ìƒ

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] `verify_synonyms_with_llm()` êµ¬í˜„
- [ ] `discover_synonyms_with_llm()` êµ¬í˜„
- [ ] `enhance_bilingual_dict_with_llm()` êµ¬í˜„
- [ ] Batch processing ìµœì í™”

### 3.2 ì„ë² ë”© + LLM í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼
**ì „ëµ**:
1. ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ í›„ë³´ ë™ì˜ì–´ ë°œê²¬ (ê¸°ì¡´ ë°©ì‹)
2. LLMìœ¼ë¡œ í›„ë³´ ê²€ì¦ ë° í•„í„°ë§ (ìƒˆë¡œìš´ ë°©ì‹)
3. ê²€ì¦ëœ ë™ì˜ì–´ë§Œ ìµœì¢… ì‚¬ì „ì— ì¶”ê°€

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] ì„ë² ë”© ê¸°ë°˜ í›„ë³´ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸
- [ ] LLM ê²€ì¦ íŒŒì´í”„ë¼ì¸
- [ ] í•˜ì´ë¸Œë¦¬ë“œ í†µí•© í•¨ìˆ˜

---

## ğŸ““ Phase 4: Notebook í†µí•©

### 4.1 ìƒˆ Cell ì¶”ê°€
**íŒŒì¼**: `notebooks/korean_neural_sparse_training.ipynb`

ì¶”ê°€í•  Cell ìœ„ì¹˜: Cell 14 (í•œì˜ ë™ì˜ì–´ ì„¹ì…˜) ì•ì— ì‚½ì…

**ìƒˆ ì„¹ì…˜ 1**: LLM ëª¨ë¸ ë¡œë”©
```python
# Cell: LLM ëª¨ë¸ ë¡œë”©
from src.llm_loader import load_llm_model, check_gpu_memory

print("ğŸ¤– LLM ëª¨ë¸ ë¡œë”© ì¤‘...")
llm_model, llm_tokenizer = load_llm_model(
    model_name="gpt-odd-20b",  # ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ
    device="cuda",
    quantization="int8",  # ë©”ëª¨ë¦¬ ì ˆì•½
)
```

**ìƒˆ ì„¹ì…˜ 2**: í•©ì„± ë°ì´í„° ìƒì„±
```python
# Cell: í•©ì„± ë°ì´í„° ìƒì„±
from src.synthetic_data_generator import generate_synthetic_qd_pairs

synthetic_pairs = generate_synthetic_qd_pairs(
    documents=documents[:1000],  # ìƒ˜í”Œ
    llm_model=llm_model,
    llm_tokenizer=llm_tokenizer,
    num_queries_per_doc=3,
)
```

**ìƒˆ ì„¹ì…˜ 3**: LLM ê¸°ë°˜ ë™ì˜ì–´ ê²€ì¦
```python
# Cell: LLMìœ¼ë¡œ ë™ì˜ì–´ ê²€ì¦ ë° í™•ì¥
from src.cross_lingual_synonyms import enhance_bilingual_dict_with_llm

enhanced_bilingual_dict = enhance_bilingual_dict_with_llm(
    initial_dict=bilingual_dict,
    llm_model=llm_model,
    llm_tokenizer=llm_tokenizer,
    verification_threshold=0.8,
)
```

ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] LLM ë¡œë”© Cell ì¶”ê°€
- [ ] í•©ì„± ë°ì´í„° ìƒì„± Cell ì¶”ê°€
- [ ] ë™ì˜ì–´ ê²€ì¦ Cell ì¶”ê°€
- [ ] ê¸°ì¡´ í•™ìŠµ ë°ì´í„°ì— í•©ì„± ë°ì´í„° ë³‘í•©
- [ ] ê²°ê³¼ ì‹œê°í™” ë° í†µê³„

### 4.2 í†µí•© ì›Œí¬í”Œë¡œìš°
```
1. ë°ì´í„° ë¡œë“œ (ê¸°ì¡´)
2. [NEW] LLM ëª¨ë¸ ë¡œë”©
3. [NEW] í•©ì„± ë°ì´í„° ìƒì„±
4. IDF ê³„ì‚° (ê¸°ì¡´)
5. íŠ¸ë Œë“œ ê°ì§€ (ê¸°ì¡´)
6. [ENHANCED] LLM + ì„ë² ë”© ê¸°ë°˜ ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶•
7. ëª¨ë¸ í•™ìŠµ (í•©ì„± ë°ì´í„° í¬í•¨)
8. í‰ê°€ ë° ì €ì¥
```

---

## ğŸ”§ Phase 5: ìµœì í™” ë° í…ŒìŠ¤íŠ¸

### 5.1 ì„±ëŠ¥ ìµœì í™”
ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] LLM inference batching
- [ ] GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
- [ ] í•©ì„± ë°ì´í„° ìºì‹±
- [ ] Parallel processing (ê°€ëŠ¥í•œ ê²½ìš°)

### 5.2 í’ˆì§ˆ ê²€ì¦
ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] í•©ì„± ë°ì´í„° í’ˆì§ˆ í‰ê°€ (ìˆ˜ë™ ìƒ˜í”Œë§)
- [ ] ë™ì˜ì–´ ì •í™•ë„ ì¸¡ì •
- [ ] í•™ìŠµ ì„±ëŠ¥ ë¹„êµ (í•©ì„± ë°ì´í„° ìœ /ë¬´)
- [ ] Ablation study (LLM vs. ì„ë² ë”© only)

### 5.3 ë¬¸ì„œí™”
ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] `src/llm_loader.py` docstring ì‘ì„±
- [ ] `src/synthetic_data_generator.py` docstring ì‘ì„±
- [ ] `src/__init__.py` ì—…ë°ì´íŠ¸ (ìƒˆ í•¨ìˆ˜ export)
- [ ] README ì—…ë°ì´íŠ¸ (ìƒˆ ê¸°ëŠ¥ ì„¤ëª…)
- [ ] Notebookì— ì„¤ëª… markdown cell ì¶”ê°€

---

## âš™ï¸ ê¸°ìˆ ì  ê³ ë ¤ì‚¬í•­ (ARM GB10 í™˜ê²½)

### GPU ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ (í˜„ì¬: GB10)
- **í˜„ì¬ ì‚¬ìš©ëŸ‰**: 4.5GB (Jupyter í”„ë¡œì„¸ìŠ¤)
- **ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬**: ì˜ˆìƒ ~12-16GB (GB10 ì´ ë©”ëª¨ë¦¬ ë¯¸í™•ì¸)
- **BERT í•™ìŠµ ë©”ëª¨ë¦¬**: ~4-6GB (í˜„ì¬ ì‚¬ìš© ì¤‘)
- **LLM ì¶”ë¡  ë©”ëª¨ë¦¬** (ì˜ˆìƒ):
  - Llama-3.2-3B (INT8): ~1.5GB
  - Gemma-2-2B (INT8): ~1GB
  - Polyglot-Ko-5.8B (INT8): ~3GB
  - EEVE-Korean-10.8B (INT8): ~5GB

**ê¶Œì¥ ì „ëµ**:
- BERT í•™ìŠµ ì¤‘ì´ ì•„ë‹ ë•Œ LLM ë¡œë”© (ìˆœì°¨ ì‹¤í–‰)
- ë˜ëŠ” INT8 quantizationìœ¼ë¡œ Llama-3.2-3B ì‚¬ìš© (ê°€ì¥ ì•ˆì „)
- í•„ìš” ì‹œ CPU offloading í™œìš©

### LLM ì„ íƒì§€ (ARM í˜¸í™˜, ìš°ì„ ìˆœìœ„ ìˆœ)

#### Option 1: Llama-3.2-3B-Instruct â­ ì¶”ì²œ
- **í¬ê¸°**: 3B params (~6GB FP16, ~1.5GB INT8)
- **ì¥ì **: ARM ì™„ë²½ ì§€ì›, ë‹¤êµ­ì–´(í•œêµ­ì–´ í¬í•¨), ìµœì‹  ëª¨ë¸
- **ë‹¨ì **: í•œêµ­ì–´ ì „ë¬¸ì„± ë‚®ìŒ
- **Hugging Face**: `meta-llama/Llama-3.2-3B-Instruct`

#### Option 2: Gemma-2-2B-it
- **í¬ê¸°**: 2B params (~4GB FP16, ~1GB INT8)
- **ì¥ì **: ë§¤ìš° ê²½ëŸ‰, ARM ì§€ì›, ë¹ ë¥¸ ì¶”ë¡ 
- **ë‹¨ì **: í•œêµ­ì–´ ì„±ëŠ¥ ì œí•œì 
- **Hugging Face**: `google/gemma-2-2b-it`

#### Option 3: Polyglot-Ko-5.8B
- **í¬ê¸°**: 5.8B params (~11GB FP16, ~3GB INT8)
- **ì¥ì **: í•œêµ­ì–´ íŠ¹í™”, ìš°ìˆ˜í•œ ì„±ëŠ¥
- **ë‹¨ì **: ë©”ëª¨ë¦¬ ë” í•„ìš”
- **Hugging Face**: `EleutherAI/polyglot-ko-5.8b`

#### Option 4: EEVE-Korean-10.8B (ê³ ê¸‰ ì˜µì…˜)
- **í¬ê¸°**: 10.8B params (~20GB FP16, ~5GB INT8)
- **ì¥ì **: í•œêµ­ì–´ ìµœê³  ì„±ëŠ¥
- **ë‹¨ì **: ë©”ëª¨ë¦¬ ë§ì´ í•„ìš”, ëŠë¦¼
- **Hugging Face**: `yanolja/EEVE-Korean-Instruct-10.8B-v1.0`

#### Option 5: OpenAI API (í´ë¼ìš°ë“œ ëŒ€ì•ˆ)
- **ëª¨ë¸**: GPT-4o-mini ë˜ëŠ” GPT-3.5-turbo
- **ì¥ì **: ë¡œì»¬ ë©”ëª¨ë¦¬ ë¶ˆí•„ìš”, í•œêµ­ì–´ ìš°ìˆ˜
- **ë‹¨ì **: ë¹„ìš© ë°œìƒ, ì¸í„°ë„· í•„ìš”
- **ì‚¬ìš©ëŸ‰ ì˜ˆìƒ**: 1,000 ì¿¼ë¦¬ ìƒì„± ì‹œ ~$0.5-1

**ìµœì¢… ì¶”ì²œ**: Llama-3.2-3B-Instruct (INT8) - ARM í˜¸í™˜ + ë©”ëª¨ë¦¬ íš¨ìœ¨

### í’ˆì§ˆ vs. ë¹„ìš© íŠ¸ë ˆì´ë“œì˜¤í”„
- **ê³ í’ˆì§ˆ ì „ëµ**: LLMìœ¼ë¡œ ëª¨ë“  ë™ì˜ì–´ ê²€ì¦ (ëŠë¦¼, ë¹„ìš© ë†’ìŒ)
- **ê· í˜• ì „ëµ**: ì„ë² ë”©ìœ¼ë¡œ í›„ë³´ ì¶”ì¶œ + LLMìœ¼ë¡œ ì¼ë¶€ ê²€ì¦ (ê¶Œì¥)
- **ì €ë¹„ìš© ì „ëµ**: ì„ë² ë”©ë§Œ ì‚¬ìš© + ìˆ˜ë™ íë ˆì´ì…˜

---

## ğŸ“… êµ¬í˜„ ìˆœì„œ ë° ìš°ì„ ìˆœìœ„

### High Priority (Core)
1. âœ… Phase 1.2: ëª¨ë¸ ë¡œë” êµ¬í˜„ (`src/llm_loader.py`)
2. âœ… Phase 2.1: í•©ì„± ë°ì´í„° ìƒì„±ê¸° êµ¬í˜„ (`src/synthetic_data_generator.py`)
3. âœ… Phase 4.1: Notebook í†µí•© (ìƒˆ Cell ì¶”ê°€)

### Medium Priority (Enhancement)
4. âœ… Phase 3.1: LLM ê¸°ë°˜ ë™ì˜ì–´ ê²€ì¦
5. âœ… Phase 5.2: í’ˆì§ˆ ê²€ì¦

### Low Priority (Optimization)
6. â¸ï¸ Phase 5.1: ì„±ëŠ¥ ìµœì í™”
7. â¸ï¸ Phase 5.3: ë¬¸ì„œí™” ì™„ì„±

---

## ğŸ¯ ì„±ê³µ ì§€í‘œ

- [ ] ARM í˜¸í™˜ LLM ëª¨ë¸ ë¡œë”© ì„±ê³µ (Llama-3.2-3B INT8)
- [ ] GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 10GB ì´ë‚´ ìœ ì§€
- [ ] ìµœì†Œ 1,000ê°œ ì´ìƒì˜ í•©ì„± Query-Document pairs ìƒì„±
- [ ] í•œì˜ ë™ì˜ì–´ ì‚¬ì „ í¬ê¸° 2ë°° ì´ìƒ ì¦ê°€
- [ ] í•©ì„± ë°ì´í„°ë¡œ í•™ìŠµ ì‹œ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ (MRR/NDCG)
- [ ] Notebook ì „ì²´ ì‹¤í–‰ ì‹œê°„ 4ì‹œê°„ ì´ë‚´ (ARM GPU í™˜ê²½)

---

## ğŸš¨ ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘

### ë¦¬ìŠ¤í¬ 1: GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
**ëŒ€ì‘**:
- INT8 quantization ì‚¬ìš©
- Smaller batch size
- Gradient checkpointing
- CPU offloading (ì†ë„ ì €í•˜ ê°ìˆ˜)

### ë¦¬ìŠ¤í¬ 2: LLM ìƒì„± í’ˆì§ˆ ë‚®ìŒ
**ëŒ€ì‘**:
- Prompt engineering ê°œì„ 
- Few-shot examples ì¶”ê°€
- Temperature/Top-p ì¡°ì •
- ë‹¤ë¥¸ LLM ëª¨ë¸ ì‹œë„

### ë¦¬ìŠ¤í¬ 3: í•©ì„± ë°ì´í„° ê³¼ì í•©
**ëŒ€ì‘**:
- í•©ì„±/ì‹¤ì œ ë°ì´í„° ë¹„ìœ¨ ì¡°ì • (1:1 ë˜ëŠ” 1:2)
- Validation setì€ ì‹¤ì œ ë°ì´í„°ë§Œ ì‚¬ìš©
- Diversity penalty ì¶”ê°€

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [Hugging Face Transformers - Text Generation](https://huggingface.co/docs/transformers/main_classes/text_generation)
- [bitsandbytes - INT8/INT4 Quantization](https://github.com/TimDettmers/bitsandbytes)
- [Accelerate - Memory Optimization](https://huggingface.co/docs/accelerate/index)
- [InPars: Data Augmentation for Information Retrieval](https://arxiv.org/abs/2202.05144)
- [Promptagator: Few-shot Dense Retrieval](https://arxiv.org/abs/2209.11755)
- [Llama-3.2 Model Card](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)
- [Polyglot-Ko Korean LLM](https://huggingface.co/EleutherAI/polyglot-ko-5.8b)

---

## âœ… Checklist Summary

**Phase 1**: í™˜ê²½ ì„¤ì • ë° ëª¨ë¸ ë¡œë”©
- [ ] requirements.txt ì—…ë°ì´íŠ¸
- [ ] src/llm_loader.py êµ¬í˜„
- [ ] GPU ë©”ëª¨ë¦¬ ì²´í¬ ë° ìµœì í™”

**Phase 2**: í•©ì„± ë°ì´í„° ìƒì„±
- [ ] src/synthetic_data_generator.py êµ¬í˜„
- [ ] Prompt templates ì‘ì„±
- [ ] í’ˆì§ˆ í•„í„°ë§ ë¡œì§

**Phase 3**: ë™ì˜ì–´ ì‚¬ì „ í™•ì¥
- [ ] src/cross_lingual_synonyms.py í™•ì¥
- [ ] LLM ê²€ì¦ í•¨ìˆ˜ ì¶”ê°€
- [ ] í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

**Phase 4**: Notebook í†µí•©
- [ ] ìƒˆ Cell ì¶”ê°€ (LLM ë¡œë”©, í•©ì„± ë°ì´í„°, ë™ì˜ì–´)
- [ ] ê¸°ì¡´ ì›Œí¬í”Œë¡œìš°ì™€ í†µí•©
- [ ] ê²°ê³¼ ì‹œê°í™”

**Phase 5**: ìµœì í™” ë° ê²€ì¦
- [ ] ì„±ëŠ¥ ìµœì í™”
- [ ] í’ˆì§ˆ í‰ê°€
- [ ] ë¬¸ì„œí™”

---

---

## ğŸš€ Quick Start (ARM í™˜ê²½)

### Step 1: ì˜ì¡´ì„± ì„¤ì¹˜
```bash
pip install bitsandbytes optimum
```

### Step 2: LLM ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì„ íƒ)
```python
# Llama-3.2-3B-Instruct (ê¶Œì¥)
from transformers import AutoModelForCausalLM, AutoTokenizer
model_name = "meta-llama/Llama-3.2-3B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_8bit=True,  # INT8 quantization
    device_map="auto",  # Auto GPU/CPU placement
)
```

### Step 3: í•©ì„± ë°ì´í„° ìƒì„±
```python
from src.llm_loader import load_llm_model_quantized
from src.synthetic_data_generator import generate_synthetic_qd_pairs

llm_model, llm_tokenizer = load_llm_model_quantized(
    model_name="meta-llama/Llama-3.2-3B-Instruct",
    quantization_bits=8,
)

synthetic_pairs = generate_synthetic_qd_pairs(
    documents=documents[:100],
    llm_model=llm_model,
    llm_tokenizer=llm_tokenizer,
    batch_size=4,  # ARM í™˜ê²½ ìµœì í™”
)
```

---

**Updated**: 2025-11-13
**Status**: ARM ìµœì í™” ì™„ë£Œ, Ready for implementation
**Environment**: ARM aarch64 + NVIDIA GB10 + CUDA 13.0

"""
ë°ì´í„°ì…‹ ì €ì¥ ë° ë¡œë“œ ìœ í‹¸ë¦¬í‹°

ë…¸íŠ¸ë¶ ê°„ ë°ì´í„° ê³µìœ ë¥¼ ìœ„í•œ DatasetManager í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
JSON, Pickle, PyTorch ëª¨ë¸ ë“± ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Usage:
    from src.dataset_manager import DatasetManager

    # ì´ˆê¸°í™”
    dm = DatasetManager(base_path="dataset")

    # JSON ì €ì¥/ë¡œë“œ
    dm.save_json({"key": "value"}, "data.json", "base_model")
    data = dm.load_json("data.json", "base_model")

    # Pickle ì €ì¥/ë¡œë“œ
    dm.save_pickle(my_object, "data.pkl", "base_model")
    obj = dm.load_pickle("data.pkl", "base_model")

    # ëª¨ë¸ ì €ì¥/ë¡œë“œ
    dm.save_model(model, tokenizer, "my_model", "base_model")
    model, tokenizer = dm.load_model(ModelClass, "my_model", "base_model")
"""

import json
import pickle
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from datetime import datetime


class DatasetManager:
    """ë…¸íŠ¸ë¶ ê°„ ë°ì´í„° ê³µìœ ë¥¼ ìœ„í•œ ë§¤ë‹ˆì €"""

    def __init__(self, base_path: str = "dataset"):
        """
        DatasetManager ì´ˆê¸°í™”

        Args:
            base_path: ë°ì´í„° ì €ì¥ ê¸°ë³¸ ê²½ë¡œ (ê¸°ë³¸ê°’: "dataset")
        """
        self.base_path = Path(base_path)
        self.base_path.mkdir(exist_ok=True)

        # ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        self.metadata_path = self.base_path / "metadata.json"

        # ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™” ë˜ëŠ” ë¡œë“œ
        self._init_metadata()

    def _init_metadata(self):
        """ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™” ë˜ëŠ” ë¡œë“œ"""
        if self.metadata_path.exists():
            with open(self.metadata_path, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
        else:
            self.metadata = {
                "version": "1.0.0",
                "created_at": datetime.now().isoformat(),
                "datasets": {}
            }
            self._save_metadata()

    def _save_metadata(self):
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        with open(self.metadata_path, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, ensure_ascii=False, indent=2)

    def _update_dataset_metadata(self, subdir: str, filename: str, info: Dict[str, Any]):
        """ë°ì´í„°ì…‹ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸"""
        key = f"{subdir}/{filename}" if subdir else filename
        self.metadata["datasets"][key] = {
            **info,
            "updated_at": datetime.now().isoformat()
        }
        self._save_metadata()

    def save_json(self, data: Any, filename: str, subdir: str = "") -> Path:
        """
        JSON í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì €ì¥

        Args:
            data: ì €ì¥í•  ë°ì´í„° (JSON ì§ë ¬í™” ê°€ëŠ¥í•´ì•¼ í•¨)
            filename: íŒŒì¼ëª…
            subdir: í•˜ìœ„ ë””ë ‰í† ë¦¬ (ì„ íƒ)

        Returns:
            ì €ì¥ëœ íŒŒì¼ì˜ ê²½ë¡œ
        """
        path = self.base_path / subdir / filename
        path.parent.mkdir(parents=True, exist_ok=True)

        with open(path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"âœ“ Saved JSON: {path}")

        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        self._update_dataset_metadata(subdir, filename, {
            "type": "json",
            "size_bytes": path.stat().st_size
        })

        return path

    def load_json(self, filename: str, subdir: str = "") -> Any:
        """
        JSON íŒŒì¼ ë¡œë“œ

        Args:
            filename: íŒŒì¼ëª…
            subdir: í•˜ìœ„ ë””ë ‰í† ë¦¬ (ì„ íƒ)

        Returns:
            ë¡œë“œëœ ë°ì´í„°

        Raises:
            FileNotFoundError: íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ë•Œ
        """
        path = self.base_path / subdir / filename

        if not path.exists():
            raise FileNotFoundError(f"File not found: {path}")

        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        print(f"âœ“ Loaded JSON: {path}")
        return data

    def save_pickle(self, data: Any, filename: str, subdir: str = "") -> Path:
        """
        Pickle í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì €ì¥

        Args:
            data: ì €ì¥í•  Python ê°ì²´
            filename: íŒŒì¼ëª…
            subdir: í•˜ìœ„ ë””ë ‰í† ë¦¬ (ì„ íƒ)

        Returns:
            ì €ì¥ëœ íŒŒì¼ì˜ ê²½ë¡œ
        """
        path = self.base_path / subdir / filename
        path.parent.mkdir(parents=True, exist_ok=True)

        with open(path, 'wb') as f:
            pickle.dump(data, f)

        print(f"âœ“ Saved Pickle: {path}")

        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        self._update_dataset_metadata(subdir, filename, {
            "type": "pickle",
            "size_bytes": path.stat().st_size
        })

        return path

    def load_pickle(self, filename: str, subdir: str = "") -> Any:
        """
        Pickle íŒŒì¼ ë¡œë“œ

        Args:
            filename: íŒŒì¼ëª…
            subdir: í•˜ìœ„ ë””ë ‰í† ë¦¬ (ì„ íƒ)

        Returns:
            ë¡œë“œëœ Python ê°ì²´

        Raises:
            FileNotFoundError: íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ë•Œ
        """
        path = self.base_path / subdir / filename

        if not path.exists():
            raise FileNotFoundError(f"File not found: {path}")

        with open(path, 'rb') as f:
            data = pickle.load(f)

        print(f"âœ“ Loaded Pickle: {path}")
        return data

    def save_model(
        self,
        model,
        tokenizer,
        model_dir: str,
        subdir: str = ""
    ) -> Path:
        """
        PyTorch ëª¨ë¸ ì €ì¥ (Hugging Face ë° ì¼ë°˜ PyTorch ëª¨ë¸ ì§€ì›)

        Args:
            model: ì €ì¥í•  ëª¨ë¸ (Hugging Face ë˜ëŠ” ì¼ë°˜ PyTorch ëª¨ë¸)
            tokenizer: ì €ì¥í•  í† í¬ë‚˜ì´ì €
            model_dir: ëª¨ë¸ ë””ë ‰í† ë¦¬ëª…
            subdir: í•˜ìœ„ ë””ë ‰í† ë¦¬ (ì„ íƒ)

        Returns:
            ì €ì¥ëœ ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        import torch
        import json

        path = self.base_path / subdir / model_dir
        path.mkdir(parents=True, exist_ok=True)

        # Save model
        if hasattr(model, 'save_pretrained'):
            # Hugging Face ëª¨ë¸
            model.save_pretrained(path)
        else:
            # ì¼ë°˜ PyTorch ëª¨ë¸
            torch.save(model.state_dict(), path / "pytorch_model.bin")

            # Config ì €ì¥ (ëª¨ë¸ êµ¬ì¡° ì •ë³´)
            config = {
                "model_type": model.__class__.__name__,
                "model_class": f"{model.__class__.__module__}.{model.__class__.__name__}",
            }

            # ëª¨ë¸ì— config ì†ì„±ì´ ìˆìœ¼ë©´ ì¶”ê°€
            if hasattr(model, 'config'):
                if hasattr(model.config, 'to_dict'):
                    config.update(model.config.to_dict())
                else:
                    config['model_config'] = str(model.config)

            # vocab_size ë“± ê¸°ë³¸ ì†ì„± ì €ì¥
            if hasattr(model, 'vocab_size'):
                config['vocab_size'] = model.vocab_size

            # BERT ê¸°ë°˜ ëª¨ë¸ì˜ ê²½ìš° base model ì •ë³´ ì €ì¥
            if hasattr(model, 'bert'):
                if hasattr(model.bert, 'config'):
                    config['base_model_name'] = model.bert.config.name_or_path if hasattr(model.bert.config, 'name_or_path') else 'unknown'

            with open(path / "config.json", 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)

        # Save tokenizer
        if hasattr(tokenizer, 'save_pretrained'):
            tokenizer.save_pretrained(path)
        else:
            # ì¼ë°˜ í† í¬ë‚˜ì´ì €ì¸ ê²½ìš° pickleë¡œ ì €ì¥
            import pickle
            with open(path / "tokenizer.pkl", 'wb') as f:
                pickle.dump(tokenizer, f)

        print(f"âœ“ Saved Model: {path}")

        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        total_size = sum(f.stat().st_size for f in path.rglob('*') if f.is_file())
        self._update_dataset_metadata(subdir, model_dir, {
            "type": "pytorch_model",
            "size_bytes": total_size
        })

        return path

    def load_model(
        self,
        model_class,
        model_dir: str,
        subdir: str = "",
        device: str = "cpu"
    ) -> Tuple[Any, Any]:
        """
        PyTorch ëª¨ë¸ ë¡œë“œ (Hugging Face ë° ì¼ë°˜ PyTorch ëª¨ë¸ ì§€ì›)

        Args:
            model_class: ëª¨ë¸ í´ë˜ìŠ¤ (ì˜ˆ: OpenSearchSparseEncoder)
            model_dir: ëª¨ë¸ ë””ë ‰í† ë¦¬ëª…
            subdir: í•˜ìœ„ ë””ë ‰í† ë¦¬ (ì„ íƒ)
            device: ë¡œë“œí•  ë””ë°”ì´ìŠ¤ ("cpu", "cuda", etc.)

        Returns:
            (model, tokenizer) íŠœí”Œ

        Raises:
            FileNotFoundError: ëª¨ë¸ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ë•Œ
        """
        import torch
        import json

        path = self.base_path / subdir / model_dir

        if not path.exists():
            raise FileNotFoundError(f"Model not found: {path}")

        # Load model
        if hasattr(model_class, 'from_pretrained'):
            # Hugging Face ëª¨ë¸
            model = model_class.from_pretrained(path)
        else:
            # ì¼ë°˜ PyTorch ëª¨ë¸
            # Config ë¡œë“œ
            config_path = path / "config.json"
            if config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                # Configì—ì„œ í•„ìš”í•œ ì¸ìë¥¼ ì¶”ì¶œí•˜ì—¬ ëª¨ë¸ ì´ˆê¸°í™”
                try:
                    # base_model_nameì´ ìˆìœ¼ë©´ ì „ë‹¬ (OpenSearchDocEncoder ë“±)
                    if 'base_model_name' in config:
                        model = model_class(model_name=config['base_model_name'])
                    # vocab_sizeë§Œ ìˆëŠ” ê²½ìš°
                    elif 'vocab_size' in config:
                        model = model_class(vocab_size=config['vocab_size'])
                    else:
                        model = model_class()
                except TypeError as e:
                    # ì¸ìê°€ ë§ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì´ˆê¸°í™” ì‹œë„
                    print(f"âš ï¸  Failed to initialize with config: {e}")
                    print(f"   Trying default initialization...")
                    model = model_class()
            else:
                # Config ì—†ìœ¼ë©´ ê¸°ë³¸ ì´ˆê¸°í™”
                model = model_class()

            # State dict ë¡œë“œ
            model_path = path / "pytorch_model.bin"
            if model_path.exists():
                state_dict = torch.load(model_path, map_location=device)
                model.load_state_dict(state_dict)
                model.to(device)
            else:
                raise FileNotFoundError(f"Model weights not found: {model_path}")

        # Load tokenizer
        try:
            from transformers import AutoTokenizer
            tokenizer = AutoTokenizer.from_pretrained(path)
        except Exception:
            # Pickleë¡œ ì €ì¥ëœ í† í¬ë‚˜ì´ì € ë¡œë“œ
            import pickle
            tokenizer_path = path / "tokenizer.pkl"
            if tokenizer_path.exists():
                with open(tokenizer_path, 'rb') as f:
                    tokenizer = pickle.load(f)
            else:
                raise FileNotFoundError(f"Tokenizer not found in {path}")

        print(f"âœ“ Loaded Model: {path}")
        return model, tokenizer

    def check_data_exists(self, filename: str, subdir: str = "") -> bool:
        """
        ë°ì´í„° íŒŒì¼/ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸

        Args:
            filename: íŒŒì¼ëª… ë˜ëŠ” ë””ë ‰í† ë¦¬ëª…
            subdir: í•˜ìœ„ ë””ë ‰í† ë¦¬ (ì„ íƒ)

        Returns:
            ì¡´ì¬í•˜ë©´ True, ì•„ë‹ˆë©´ False
        """
        path = self.base_path / subdir / filename
        return path.exists()

    def list_files(self, subdir: str = "") -> List[str]:
        """
        íŠ¹ì • ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ëª©ë¡ ì¡°íšŒ

        Args:
            subdir: í•˜ìœ„ ë””ë ‰í† ë¦¬ (ì„ íƒ)

        Returns:
            íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸
        """
        path = self.base_path / subdir
        if not path.exists():
            return []

        return [f.name for f in path.iterdir() if f.is_file()]

    def list_directories(self, subdir: str = "") -> List[str]:
        """
        íŠ¹ì • ë””ë ‰í† ë¦¬ì˜ í•˜ìœ„ ë””ë ‰í† ë¦¬ ëª©ë¡ ì¡°íšŒ

        Args:
            subdir: í•˜ìœ„ ë””ë ‰í† ë¦¬ (ì„ íƒ)

        Returns:
            ë””ë ‰í† ë¦¬ëª… ë¦¬ìŠ¤íŠ¸
        """
        path = self.base_path / subdir
        if not path.exists():
            return []

        return [d.name for d in path.iterdir() if d.is_dir()]

    def check_dependencies(self, required: List[Tuple[str, str]]) -> bool:
        """
        ë…¸íŠ¸ë¶ ì‹¤í–‰ ì „ í•„ìš”í•œ ë°ì´í„° íŒŒì¼ í™•ì¸

        Args:
            required: [(subdir, filename), ...] í˜•ì‹ì˜ í•„ìˆ˜ íŒŒì¼ ë¦¬ìŠ¤íŠ¸

        Returns:
            ëª¨ë“  íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ True, ì•„ë‹ˆë©´ False

        Example:
            >>> required = [
            ...     ("base_model", "korean_documents.json"),
            ...     ("base_model", "qd_pairs_base.pkl"),
            ... ]
            >>> dm.check_dependencies(required)
        """
        missing = []
        for subdir, filename in required:
            if not self.check_data_exists(filename, subdir):
                missing.append(f"{subdir}/{filename}")

        if missing:
            print("=" * 70)
            print("âŒ Missing required data files:")
            print("=" * 70)
            for f in missing:
                print(f"   - {f}")
            print("\nğŸ’¡ Please run previous notebooks first:")
            print("   1. 01_neural_sparse_base_training.ipynb")
            print("   2. 02_llm_synthetic_data_generation.ipynb")
            print("=" * 70)
            return False

        print("âœ… All dependencies satisfied")
        return True

    def get_dataset_info(self) -> Dict[str, Any]:
        """
        ì €ì¥ëœ ëª¨ë“  ë°ì´í„°ì…‹ ì •ë³´ ì¡°íšŒ

        Returns:
            ë°ì´í„°ì…‹ ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        return self.metadata

    def print_summary(self):
        """ë°ì´í„°ì…‹ ìš”ì•½ ì •ë³´ ì¶œë ¥"""
        print("=" * 70)
        print("ğŸ“Š Dataset Summary")
        print("=" * 70)
        print(f"Base path: {self.base_path.absolute()}")
        print(f"Total datasets: {len(self.metadata['datasets'])}")

        # í•˜ìœ„ ë””ë ‰í† ë¦¬ë³„ ê·¸ë£¹í™”
        by_subdir = {}
        for key, info in self.metadata['datasets'].items():
            if '/' in key:
                subdir = key.split('/')[0]
            else:
                subdir = "root"

            if subdir not in by_subdir:
                by_subdir[subdir] = []
            by_subdir[subdir].append((key, info))

        print("\nDatasets by directory:")
        for subdir, datasets in sorted(by_subdir.items()):
            print(f"\n  ğŸ“ {subdir}/")
            total_size = 0
            for key, info in datasets:
                filename = key.split('/')[-1]
                size_mb = info.get('size_bytes', 0) / 1024 / 1024
                dtype = info.get('type', 'unknown')
                print(f"     - {filename:<40} ({dtype:>15}, {size_mb:>6.1f} MB)")
                total_size += info.get('size_bytes', 0)

            print(f"     {'Total:':<40} {'':<15}  {total_size/1024/1024:>6.1f} MB")

        print("=" * 70)

    def clear_subdirectory(self, subdir: str, confirm: bool = False):
        """
        íŠ¹ì • í•˜ìœ„ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ë°ì´í„° ì‚­ì œ

        Args:
            subdir: ì‚­ì œí•  í•˜ìœ„ ë””ë ‰í† ë¦¬
            confirm: í™•ì¸ ì—†ì´ ì‚­ì œ (ê¸°ë³¸ê°’: False)

        Warning:
            ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!
        """
        path = self.base_path / subdir

        if not path.exists():
            print(f"âš ï¸  Directory not found: {path}")
            return

        if not confirm:
            print(f"âš ï¸  WARNING: This will delete all data in {path}")
            print("   To confirm, call with confirm=True")
            return

        import shutil
        shutil.rmtree(path)

        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì œê±°
        keys_to_remove = [k for k in self.metadata['datasets'].keys() if k.startswith(f"{subdir}/")]
        for key in keys_to_remove:
            del self.metadata['datasets'][key]

        self._save_metadata()
        print(f"âœ“ Cleared directory: {path}")

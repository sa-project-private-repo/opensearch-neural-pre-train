#!/usr/bin/env python3
"""
AI ë„ë©”ì¸ ìš©ì–´ì§‘ì„ training notebookì— í†µí•©
"""

import json
import sys

# Read notebook
with open('korean_neural_sparse_training.ipynb', 'r', encoding='utf-8') as f:
    nb = json.load(f)

cells = nb['cells']

# Find section 5 (OpenSearch ë¬¸ì„œ ì¸ì½”ë” ëª¨ë¸ ì •ì˜)
section_5_idx = None
for i, cell in enumerate(cells):
    if cell['cell_type'] == 'markdown':
        source = ''.join(cell['source'])
        if '## 5. OpenSearch ë¬¸ì„œ ì¸ì½”ë” ëª¨ë¸ ì •ì˜' in source:
            section_5_idx = i
            print(f"Found section 5 at cell {i}")
            break

if section_5_idx is None:
    print("Error: Could not find section 5")
    sys.exit(1)

# Create new section 4.5: AI ë„ë©”ì¸ ìš©ì–´ì§‘ í†µí•©
new_cells = []

# Section header
new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## 4.5. AI ë„ë©”ì¸ íŠ¹í™” ìš©ì–´ì§‘ í†µí•©\n",
        "\n",
        "AI/ML/LLM ë„ë©”ì¸ì— íŠ¹í™”ëœ ìš©ì–´ì§‘ì„ ë¡œë“œí•˜ê³ ,\n",
        "ê¸°ìˆ  ìš©ì–´ë¥¼ tokenizer special tokensë¡œ ì¶”ê°€í•˜ì—¬ ë¶„ì ˆì„ ë°©ì§€í•©ë‹ˆë‹¤."
    ]
})

# Load terminology module
new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# AI ë„ë©”ì¸ ìš©ì–´ì§‘ ë¡œë“œ\n",
        "from ai_domain_terminology import (\n",
        "    AI_TERMINOLOGY,\n",
        "    TECHNICAL_SPECIAL_TOKENS,\n",
        "    AI_SYNONYMS\n",
        ")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ¤– AI ë„ë©”ì¸ ìš©ì–´ì§‘ ë¡œë“œ\")\n",
        "print(\"=\"*60)\n",
        "print(f\"âœ“ ì£¼ìš” ìš©ì–´ ì¹´í…Œê³ ë¦¬: {len(AI_TERMINOLOGY)}ê°œ\")\n",
        "print(f\"âœ“ Special tokens: {len(TECHNICAL_SPECIAL_TOKENS)}ê°œ\")\n",
        "print(f\"âœ“ ë™ì˜ì–´ ë§¤í•‘: {len(AI_SYNONYMS)}ê°œ\")\n",
        "print()\n",
        "print(\"ğŸ“ ìƒ˜í”Œ ìš©ì–´:\")\n",
        "for i, (term, synonyms) in enumerate(list(AI_TERMINOLOGY.items())[:5]):\n",
        "    print(f\"  {term}: {', '.join(synonyms[:3])}\")\n"
    ]
})

# Add special tokens to tokenizer
new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 4.5.1. Tokenizerì— ê¸°ìˆ  ìš©ì–´ ì¶”ê°€\n",
        "\n",
        "ChatGPT, OpenSearch ë“± ê¸°ìˆ  ìš©ì–´ê°€ ë¶„ì ˆë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´\n",
        "special tokensë¡œ ì¶”ê°€í•©ë‹ˆë‹¤."
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "print(\"=\"*60)\n",
        "print(\"ğŸ”§ Tokenizerì— ê¸°ìˆ  ìš©ì–´ ì¶”ê°€\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# í˜„ì¬ vocabulary í¬ê¸°\n",
        "original_vocab_size = len(tokenizer)\n",
        "print(f\"Original vocab size: {original_vocab_size:,}\")\n",
        "\n",
        "# Special tokens ì¶”ê°€\n",
        "num_added = tokenizer.add_tokens(TECHNICAL_SPECIAL_TOKENS)\n",
        "new_vocab_size = len(tokenizer)\n",
        "\n",
        "print(f\"Added {num_added} new tokens\")\n",
        "print(f\"New vocab size: {new_vocab_size:,}\")\n",
        "print()\n",
        "\n",
        "# ì¶”ê°€ëœ í† í° ìƒ˜í”Œ í™•ì¸\n",
        "print(\"âœ“ ì¶”ê°€ëœ ê¸°ìˆ  ìš©ì–´ ìƒ˜í”Œ:\")\n",
        "for token in TECHNICAL_SPECIAL_TOKENS[:10]:\n",
        "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "    print(f\"  {token:20s} -> ID: {token_id}\")\n",
        "\n",
        "print()\n",
        "print(\"ğŸ§ª ë¶„ì ˆ ë°©ì§€ í…ŒìŠ¤íŠ¸:\")\n",
        "test_texts = [\n",
        "    \"ChatGPTëŠ” ê°•ë ¥í•œ LLMì…ë‹ˆë‹¤\",\n",
        "    \"OpenSearch ë²¡í„°ê²€ìƒ‰ ê¸°ëŠ¥\",\n",
        "    \"RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\",\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    print(f\"  '{text}'\")\n",
        "    print(f\"    â†’ {tokens}\")\n"
    ]
})

# Merge domain synonyms with auto-discovered synonyms
new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 4.5.2. ë„ë©”ì¸ ë™ì˜ì–´ ë§¤í•‘ ìƒì„±\n",
        "\n",
        "AI ë„ë©”ì¸ ìš©ì–´ì§‘ì˜ ë™ì˜ì–´ë¥¼ í™œìš©í•˜ì—¬ ê²€ìƒ‰ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤."
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "print(\"=\"*60)\n",
        "print(\"ğŸ”— ë„ë©”ì¸ ë™ì˜ì–´ ë§¤í•‘\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# AI_SYNONYMSë¥¼ ì¼ë°˜ dictionary í˜•íƒœë¡œ ë³€í™˜\n",
        "domain_synonym_dict = {}\n",
        "\n",
        "for main_term, synonyms in AI_TERMINOLOGY.items():\n",
        "    # Main termì„ ì†Œë¬¸ìë¡œ\n",
        "    main_key = main_term.lower()\n",
        "    synonym_list = [s.lower() for s in synonyms]\n",
        "    \n",
        "    domain_synonym_dict[main_key] = synonym_list\n",
        "    \n",
        "    # ì–‘ë°©í–¥ ë§¤í•‘: ê° synonymë„ main termì„ ê°€ë¦¬í‚´\n",
        "    for syn in synonym_list:\n",
        "        if syn not in domain_synonym_dict:\n",
        "            domain_synonym_dict[syn] = [main_key]\n",
        "        elif main_key not in domain_synonym_dict[syn]:\n",
        "            domain_synonym_dict[syn].append(main_key)\n",
        "\n",
        "print(f\"âœ“ ë„ë©”ì¸ ë™ì˜ì–´ ë”•ì…”ë„ˆë¦¬ ìƒì„± ì™„ë£Œ\")\n",
        "print(f\"  ì´ {len(domain_synonym_dict):,}ê°œ í•­ëª©\")\n",
        "print()\n",
        "print(\"ğŸ“ ìƒ˜í”Œ ë™ì˜ì–´ ë§¤í•‘:\")\n",
        "samples = [\n",
        "    \"ê²€ìƒ‰\", \"ì¸ê³µì§€ëŠ¥\", \"llm\", \"chatgpt\", \"ì„ë² ë”©\",\n",
        "    \"rag\", \"í”„ë¡¬í”„íŠ¸\", \"ë”¥ëŸ¬ë‹\", \"ë¨¸ì‹ ëŸ¬ë‹\"\n",
        "]\n",
        "for term in samples:\n",
        "    if term in domain_synonym_dict:\n",
        "        syns = domain_synonym_dict[term][:3]  # ìƒìœ„ 3ê°œë§Œ\n",
        "        print(f\"  {term:15s} â†’ {', '.join(syns)}\")\n"
    ]
})

# Summary section
new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 4.5.3. ìš©ì–´ì§‘ í†µí•© ìš”ì•½"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "print(\"=\"*60)\n",
        "print(\"âœ… AI ë„ë©”ì¸ ìš©ì–´ì§‘ í†µí•© ì™„ë£Œ!\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "print(\"ğŸ“Š í†µí•© ê²°ê³¼:\")\n",
        "print(f\"  â€¢ Tokenizer vocabulary: {original_vocab_size:,} â†’ {new_vocab_size:,} (+{num_added})\")\n",
        "print(f\"  â€¢ AI ë„ë©”ì¸ ìš©ì–´: {len(AI_TERMINOLOGY):,}ê°œ ì¹´í…Œê³ ë¦¬\")\n",
        "print(f\"  â€¢ ë™ì˜ì–´ ë§¤í•‘: {len(domain_synonym_dict):,}ê°œ í•­ëª©\")\n",
        "print(f\"  â€¢ Special tokens: {len(TECHNICAL_SPECIAL_TOKENS)}ê°œ\")\n",
        "print()\n",
        "print(\"ğŸ¯ ì£¼ìš” ê°œì„  ì‚¬í•­:\")\n",
        "print(\"  1. ê¸°ìˆ  ìš©ì–´ ë¶„ì ˆ ë°©ì§€ (ChatGPT, OpenSearch, LLM ë“±)\")\n",
        "print(\"  2. AI ë„ë©”ì¸ ë™ì˜ì–´ ìë™ ë§¤í•‘ (ê²€ìƒ‰â†”Searchâ†”íƒìƒ‰)\")\n",
        "print(\"  3. í•œêµ­ì–´-ì˜ì–´ ìš©ì–´ ì–‘ë°©í–¥ ì—°ê²°\")\n",
        "print()\n",
        "print(\"ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:\")\n",
        "print(\"  â†’ ì„¹ì…˜ 7ì—ì„œ ë„ë©”ì¸ ë™ì˜ì–´ì™€ ìë™ ë°œê²¬ ë™ì˜ì–´ë¥¼ ê²°í•©\")\n"
    ]
})

# Insert new cells before section 5
cells = cells[:section_5_idx] + new_cells + cells[section_5_idx:]

# Save notebook
nb['cells'] = cells
with open('korean_neural_sparse_training.ipynb', 'w', encoding='utf-8') as f:
    json.dump(nb, f, ensure_ascii=False, indent=1)

print(f"\nâœ“ AI ë„ë©”ì¸ ìš©ì–´ì§‘ì´ notebookì— í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"  â€¢ ìƒˆë¡œìš´ ì„¹ì…˜: 4.5. AI ë„ë©”ì¸ íŠ¹í™” ìš©ì–´ì§‘ í†µí•©")
print(f"  â€¢ ì¶”ê°€ëœ ì…€: {len(new_cells)}ê°œ")
print(f"  â€¢ ì´ ì…€ ê°œìˆ˜: {len(cells)}ê°œ")

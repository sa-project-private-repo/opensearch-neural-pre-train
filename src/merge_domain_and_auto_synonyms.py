#!/usr/bin/env python3
"""
ë„ë©”ì¸ ë™ì˜ì–´ì™€ ìë™ ë°œê²¬ ë™ì˜ì–´ë¥¼ ê²°í•©
"""

import json
import sys

# Read notebook
with open('korean_neural_sparse_training.ipynb', 'r', encoding='utf-8') as f:
    nb = json.load(f)

cells = nb['cells']

# Find section 7.4 (ìˆ˜ì§‘ ë°ì´í„° ê¸°ë°˜ ë™ì˜ì–´ ìë™ ë°œê²¬)
# ê·¸ ë‹¤ìŒ ì½”ë“œ ì…€ ë’¤ì— ìƒˆë¡œìš´ ì„¹ì…˜ ì‚½ì…
target_idx = None
for i, cell in enumerate(cells):
    if cell['cell_type'] == 'markdown':
        source = ''.join(cell['source'])
        if '### 7.4. ìˆ˜ì§‘ ë°ì´í„° ê¸°ë°˜ ë™ì˜ì–´ ìë™ ë°œê²¬' in source:
            # Find the next code cell
            for j in range(i+1, len(cells)):
                if cells[j]['cell_type'] == 'code':
                    target_idx = j + 1
                    print(f"Will insert after cell {j}")
                    break
            break

if target_idx is None:
    print("Error: Could not find section 7.4")
    sys.exit(1)

# Create new section: ë„ë©”ì¸ ë™ì˜ì–´ + ìë™ ë°œê²¬ ë™ì˜ì–´ ê²°í•©
new_cells = []

# Section header
new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 7.4.1. ë„ë©”ì¸ ë™ì˜ì–´ + ìë™ ë°œê²¬ ë™ì˜ì–´ ê²°í•©\n",
        "\n",
        "AI ë„ë©”ì¸ ì „ë¬¸ ìš©ì–´ì™€ ìë™ ë°œê²¬ëœ ë™ì˜ì–´ë¥¼ ê²°í•©í•˜ì—¬\n",
        "ë” í¬ê´„ì ì¸ ë™ì˜ì–´ ì‚¬ì „ì„ êµ¬ì„±í•©ë‹ˆë‹¤."
    ]
})

# Merge synonyms code
new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "print(\"=\"*60)\n",
        "print(\"ğŸ”— ë„ë©”ì¸ ë™ì˜ì–´ + ìë™ ë°œê²¬ ë™ì˜ì–´ ê²°í•©\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "\n",
        "# 1. ë„ë©”ì¸ ë™ì˜ì–´ í†µê³„\n",
        "print(f\"ğŸ“š ë„ë©”ì¸ ë™ì˜ì–´ (AI ìš©ì–´ì§‘):\")\n",
        "print(f\"  í•­ëª© ìˆ˜: {len(domain_synonym_dict):,}ê°œ\")\n",
        "print()\n",
        "\n",
        "# 2. ìë™ ë°œê²¬ ë™ì˜ì–´ í†µê³„\n",
        "print(f\"ğŸ” ìë™ ë°œê²¬ ë™ì˜ì–´ (ì½”í¼ìŠ¤ ê¸°ë°˜):\")\n",
        "print(f\"  í•­ëª© ìˆ˜: {len(auto_synonym_dict):,}ê°œ\")\n",
        "print()\n",
        "\n",
        "# 3. ê²°í•© ì „ëµ: ë„ë©”ì¸ ìš°ì„ , ìë™ ë°œê²¬ ë³´ì™„\n",
        "merged_synonym_dict = {}\n",
        "\n",
        "# ë¨¼ì € ë„ë©”ì¸ ë™ì˜ì–´ ì¶”ê°€ (ì‹ ë¢°ë„ ë†’ìŒ)\n",
        "for term, synonyms in domain_synonym_dict.items():\n",
        "    merged_synonym_dict[term] = list(set(synonyms))  # ì¤‘ë³µ ì œê±°\n",
        "\n",
        "# ìë™ ë°œê²¬ ë™ì˜ì–´ ì¶”ê°€ (ë„ë©”ì¸ ë™ì˜ì–´ì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²ƒë§Œ)\n",
        "added_from_auto = 0\n",
        "for term, synonyms in auto_synonym_dict.items():\n",
        "    term_lower = term.lower()\n",
        "    \n",
        "    if term_lower in merged_synonym_dict:\n",
        "        # ê¸°ì¡´ í•­ëª©ì— ìƒˆë¡œìš´ ë™ì˜ì–´ ì¶”ê°€\n",
        "        existing = set(merged_synonym_dict[term_lower])\n",
        "        new_synonyms = [s.lower() for s in synonyms if s.lower() not in existing]\n",
        "        if new_synonyms:\n",
        "            merged_synonym_dict[term_lower].extend(new_synonyms)\n",
        "            added_from_auto += len(new_synonyms)\n",
        "    else:\n",
        "        # ìƒˆë¡œìš´ í•­ëª© ì¶”ê°€\n",
        "        merged_synonym_dict[term_lower] = [s.lower() for s in synonyms]\n",
        "        added_from_auto += len(synonyms)\n",
        "\n",
        "print(f\"âœ… ê²°í•© ì™„ë£Œ:\")\n",
        "print(f\"  ì´ í•­ëª© ìˆ˜: {len(merged_synonym_dict):,}ê°œ\")\n",
        "print(f\"  ë„ë©”ì¸ ë™ì˜ì–´ ê¸°ì—¬: {len(domain_synonym_dict):,}ê°œ í•­ëª©\")\n",
        "print(f\"  ìë™ ë°œê²¬ ê¸°ì—¬: {added_from_auto:,}ê°œ ë™ì˜ì–´ ì¶”ê°€\")\n",
        "print()\n",
        "\n",
        "# 4. ìƒ˜í”Œ í™•ì¸\n",
        "print(\"ğŸ“ ê²°í•© ë™ì˜ì–´ ìƒ˜í”Œ:\")\n",
        "sample_terms = ['ê²€ìƒ‰', 'ì¸ê³µì§€ëŠ¥', 'llm', 'chatgpt', 'ì„ë² ë”©', 'rag']\n",
        "for term in sample_terms:\n",
        "    if term in merged_synonym_dict:\n",
        "        syns = merged_synonym_dict[term][:5]  # ìƒìœ„ 5ê°œ\n",
        "        print(f\"  {term:15s} â†’ {', '.join(syns)}\")\n"
    ]
})

# Insert new cells
cells = cells[:target_idx] + new_cells + cells[target_idx:]

# Save notebook
nb['cells'] = cells
with open('korean_neural_sparse_training.ipynb', 'w', encoding='utf-8') as f:
    json.dump(nb, f, ensure_ascii=False, indent=1)

print(f"\nâœ“ ë„ë©”ì¸ ë™ì˜ì–´ ê²°í•© ì„¹ì…˜ ì¶”ê°€ ì™„ë£Œ!")
print(f"  â€¢ ì¶”ê°€ëœ ì…€: {len(new_cells)}ê°œ")
print(f"  â€¢ ì´ ì…€ ê°œìˆ˜: {len(cells)}ê°œ")

"""
Synthetic Data Generator using LLM

This module generates synthetic query-document pairs using LLM models
for training neural sparse retrieval models.

Features:
- Document â†’ Query generation (reverse direction)
- Query augmentation (paraphrasing)
- Quality filtering
- Batch processing for efficiency

Requirements:
- src.llm_loader module
- transformers, torch
"""

from typing import List, Tuple, Optional, Dict, Any
import re
from tqdm import tqdm


# Prompt templates
DOC_TO_QUERY_PROMPT = """ë‹¤ìŒ ë¬¸ì„œë¥¼ ì½ê³  ì‚¬ìš©ìžê°€ ì´ ë¬¸ì„œë¥¼ ì°¾ê¸° ìœ„í•´ ê²€ìƒ‰í•  ë§Œí•œ ì¿¼ë¦¬ë¥¼ {num_queries}ê°œ ìƒì„±í•˜ì„¸ìš”.
ê° ì¿¼ë¦¬ëŠ” ì§§ê³  êµ¬ì²´ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤ (5-15ë‹¨ì–´).

ë¬¸ì„œ: {document}

ê²€ìƒ‰ ì¿¼ë¦¬ ({num_queries}ê°œ, ê° ì¤„ì— í•˜ë‚˜ì”©):"""

QUERY_AUGMENT_PROMPT = """ë‹¤ìŒ ê²€ìƒ‰ ì¿¼ë¦¬ì™€ ê°™ì€ ì˜ë¯¸ë¥¼ ê°€ì§€ì§€ë§Œ í‘œí˜„ì´ ë‹¤ë¥¸ ì¿¼ë¦¬ë¥¼ {num_variants}ê°œ ìƒì„±í•˜ì„¸ìš”.

ì›ë³¸ ì¿¼ë¦¬: {query}

ë³€í˜• ì¿¼ë¦¬ ({num_variants}ê°œ, ê° ì¤„ì— í•˜ë‚˜ì”©):"""


def generate_queries_from_document(
    document: str,
    llm_model: Any,
    llm_tokenizer: Any,
    num_queries: int = 3,
    max_new_tokens: int = 150,
    temperature: float = 0.8,
) -> List[str]:
    """
    Generate queries from a document using LLM.

    Args:
        document: Source document
        llm_model: Loaded LLM model
        llm_tokenizer: Loaded tokenizer
        num_queries: Number of queries to generate
        max_new_tokens: Max tokens in generation
        temperature: Sampling temperature

    Returns:
        List of generated queries

    Example:
        >>> doc = "OpenSearchëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ì—”ì§„ìž…ë‹ˆë‹¤."
        >>> queries = generate_queries_from_document(doc, model, tokenizer)
        >>> print(queries)  # ["OpenSearch ê¸°ëŠ¥", "ê²€ìƒ‰ ì—”ì§„ ë¹„êµ", ...]
    """
    from src.llm_loader import generate_text

    # Truncate long documents
    doc_truncated = document[:500]  # First 500 chars

    prompt = DOC_TO_QUERY_PROMPT.format(
        document=doc_truncated,
        num_queries=num_queries,
    )

    # Generate
    generated = generate_text(
        model=llm_model,
        tokenizer=llm_tokenizer,
        prompt=prompt,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        do_sample=True,
    )

    # Parse queries (each line)
    queries = []
    for line in generated.split('\n'):
        line = line.strip()
        # Remove numbering (1., 2., -, etc.)
        line = re.sub(r'^[\d\-\*\â€¢]+[\.\)]\s*', '', line)
        line = line.strip()

        if line and len(line) > 5:  # Min length
            queries.append(line)

    return queries[:num_queries]  # Limit to requested number


def augment_query(
    query: str,
    llm_model: Any,
    llm_tokenizer: Any,
    num_variants: int = 2,
    max_new_tokens: int = 100,
    temperature: float = 0.9,
) -> List[str]:
    """
    Generate query variations (paraphrasing).

    Args:
        query: Original query
        llm_model: Loaded LLM model
        llm_tokenizer: Loaded tokenizer
        num_variants: Number of variants to generate
        max_new_tokens: Max tokens in generation
        temperature: Sampling temperature

    Returns:
        List of query variants

    Example:
        >>> query = "í•œêµ­ì–´ ê²€ìƒ‰ ìµœì í™”"
        >>> variants = augment_query(query, model, tokenizer)
        >>> print(variants)  # ["í•œê¸€ ê²€ìƒ‰ ê°œì„ ", "ì½”ë¦¬ì•ˆ ê²€ìƒ‰ í–¥ìƒ", ...]
    """
    from src.llm_loader import generate_text

    prompt = QUERY_AUGMENT_PROMPT.format(
        query=query,
        num_variants=num_variants,
    )

    generated = generate_text(
        model=llm_model,
        tokenizer=llm_tokenizer,
        prompt=prompt,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        do_sample=True,
    )

    # Parse variants
    variants = []
    for line in generated.split('\n'):
        line = line.strip()
        line = re.sub(r'^[\d\-\*\â€¢]+[\.\)]\s*', '', line)
        line = line.strip()

        if line and len(line) > 5:
            variants.append(line)

    return variants[:num_variants]


def filter_quality(
    query: str,
    document: str,
    min_query_length: int = 5,
    max_query_length: int = 100,
    min_doc_length: int = 20,
) -> bool:
    """
    Filter low-quality query-document pairs.

    Args:
        query: Query string
        document: Document string
        min_query_length: Minimum query length (chars)
        max_query_length: Maximum query length (chars)
        min_doc_length: Minimum document length (chars)

    Returns:
        True if passes quality check, False otherwise

    Example:
        >>> filter_quality("ê²€ìƒ‰", "ë¬¸ì„œ", min_query_length=5)
        False  # Query too short
    """
    # Length checks
    if len(query) < min_query_length:
        return False

    if len(query) > max_query_length:
        return False

    if len(document) < min_doc_length:
        return False

    # Check if query is not just document prefix
    if document.startswith(query):
        return False

    # Check for too much overlap (potential copying)
    query_words = set(query.split())
    doc_words = set(document.split())
    if len(query_words) > 0:
        overlap = len(query_words & doc_words) / len(query_words)
        if overlap < 0.3:  # Too little overlap (not relevant)
            return False
        if overlap > 0.95:  # Too much overlap (copying)
            return False

    return True


def generate_synthetic_qd_pairs(
    documents: List[str],
    llm_model: Any,
    llm_tokenizer: Any,
    num_queries_per_doc: int = 3,
    batch_size: int = 2,
    max_documents: Optional[int] = None,
    enable_filtering: bool = True,
) -> List[Tuple[str, str, float]]:
    """
    Generate synthetic query-document pairs from documents.

    Args:
        documents: List of documents
        llm_model: Loaded LLM model
        llm_tokenizer: Loaded tokenizer
        num_queries_per_doc: Queries to generate per document
        batch_size: Batch size for processing (not used in current impl)
        max_documents: Maximum documents to process (None = all)
        enable_filtering: Whether to apply quality filtering

    Returns:
        List of (query, document, relevance) tuples

    Example:
        >>> docs = ["OpenSearchëŠ” ê²€ìƒ‰ ì—”ì§„ìž…ë‹ˆë‹¤.", "Elasticsearchì™€ í˜¸í™˜ë©ë‹ˆë‹¤."]
        >>> pairs = generate_synthetic_qd_pairs(docs, model, tokenizer)
        >>> print(len(pairs))  # 6 (3 queries Ã— 2 docs)
    """
    if max_documents is not None:
        documents = documents[:max_documents]

    print("\n" + "="*70)
    print("ðŸ“ Generating Synthetic Query-Document Pairs")
    print("="*70)
    print(f"Documents: {len(documents)}")
    print(f"Queries per doc: {num_queries_per_doc}")
    print(f"Quality filtering: {'ON' if enable_filtering else 'OFF'}")

    synthetic_pairs = []
    failed_count = 0

    for doc in tqdm(documents, desc="Generating queries"):
        try:
            queries = generate_queries_from_document(
                document=doc,
                llm_model=llm_model,
                llm_tokenizer=llm_tokenizer,
                num_queries=num_queries_per_doc,
            )

            for query in queries:
                # Quality filtering
                if enable_filtering:
                    if not filter_quality(query, doc):
                        failed_count += 1
                        continue

                # Add positive pair
                synthetic_pairs.append((query, doc, 1.0))

        except Exception as e:
            print(f"\nâš ï¸  Error generating queries: {e}")
            failed_count += 1
            continue

    print("\n" + "="*70)
    print("âœ… Generation Complete")
    print("="*70)
    print(f"Total pairs generated: {len(synthetic_pairs):,}")
    print(f"Failed/filtered: {failed_count:,}")
    print(f"Average queries per doc: {len(synthetic_pairs) / len(documents):.2f}")

    return synthetic_pairs


def generate_hard_negatives(
    query: str,
    positive_doc: str,
    candidate_docs: List[str],
    llm_model: Any,
    llm_tokenizer: Any,
    num_negatives: int = 2,
) -> List[str]:
    """
    Generate hard negative documents for a query.

    Hard negatives are documents that are semantically similar but not relevant.

    Args:
        query: Query string
        positive_doc: Positive (relevant) document
        candidate_docs: Pool of candidate documents
        llm_model: Loaded LLM model
        llm_tokenizer: Loaded tokenizer
        num_negatives: Number of hard negatives to generate

    Returns:
        List of hard negative documents

    Example:
        >>> hard_negs = generate_hard_negatives(
        ...     "OpenSearch ê¸°ëŠ¥",
        ...     "OpenSearchëŠ” ê²€ìƒ‰ ì—”ì§„ìž…ë‹ˆë‹¤.",
        ...     candidate_docs,
        ...     model,
        ...     tokenizer
        ... )
    """
    # Placeholder: Simple random selection from candidates
    # TODO: Implement LLM-based hard negative generation
    import random
    return random.sample(candidate_docs, min(num_negatives, len(candidate_docs)))


def deduplicate_pairs(
    pairs: List[Tuple[str, str, float]]
) -> List[Tuple[str, str, float]]:
    """
    Remove duplicate query-document pairs.

    Args:
        pairs: List of (query, document, relevance) tuples

    Returns:
        Deduplicated list

    Example:
        >>> pairs = [("q1", "d1", 1.0), ("q1", "d1", 1.0), ("q2", "d2", 1.0)]
        >>> dedup = deduplicate_pairs(pairs)
        >>> len(dedup)  # 2
    """
    seen = set()
    unique_pairs = []

    for query, doc, relevance in pairs:
        key = (query, doc)
        if key not in seen:
            seen.add(key)
            unique_pairs.append((query, doc, relevance))

    return unique_pairs


if __name__ == "__main__":
    print("="*70)
    print("Synthetic Data Generator Module")
    print("="*70)
    print("\nUsage:")
    print("  from src.synthetic_data_generator import generate_synthetic_qd_pairs")
    print("  from src.llm_loader import load_qwen3_awq")
    print()
    print("  model, tokenizer = load_qwen3_awq()")
    print("  pairs = generate_synthetic_qd_pairs(documents, model, tokenizer)")
    print("="*70)

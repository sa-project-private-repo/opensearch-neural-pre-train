#!/usr/bin/env python3
"""
OpenSearch Inference-Free Neural Sparse ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
í•œêµ­ì–´ ìƒ˜í”Œ ë°ì´í„°ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import os
import json
import math
import numpy as np
from collections import Counter
from datetime import datetime

import torch
from torch import nn
from torch.nn import functional as F
from torch.utils.data import DataLoader
from torch.optim import AdamW
from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig

# Import new loss functions
from src.losses import (
    neural_sparse_loss_with_regularization,
    compute_sparsity_metrics,
)

print("=" * 60)
print("OpenSearch Inference-Free Neural Sparse Model Test")
print("=" * 60)

# í•œêµ­ì–´ ìƒ˜í”Œ ë°ì´í„°ì…‹
SAMPLE_DOCUMENTS = [
    "ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„° ì‹œìŠ¤í…œì´ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
    "ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.",
    "ë”¥ëŸ¬ë‹ì€ ì¸ê³µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.",
    "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
    "OpenSearchëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ìœ¼ë¡œ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.",
    "ë²¡í„° ê²€ìƒ‰ì€ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
    "Neural sparse ê²€ìƒ‰ì€ í¬ì†Œ ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ ê²€ìƒ‰ì„ ì œê³µí•©ë‹ˆë‹¤.",
    "í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„ê³¼ í’ˆì‚¬ íƒœê¹…ì„ í¬í•¨í•©ë‹ˆë‹¤.",
    "íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ëŠ” í˜„ëŒ€ ìì—°ì–´ ì²˜ë¦¬ì˜ í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤.",
    "BERT ëª¨ë¸ì€ ì–‘ë°©í–¥ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ë§¥ì„ ì´í•´í•©ë‹ˆë‹¤.",
    "GPTëŠ” ìƒì„±í˜• ì–¸ì–´ ëª¨ë¸ë¡œ ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    "LLMì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ì˜ë¯¸í•˜ë©° ChatGPTê°€ ëŒ€í‘œì ì…ë‹ˆë‹¤.",
    "ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„° ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.",
    "ê²€ìƒ‰ ì—”ì§„ ìµœì í™”ëŠ” ì›¹ì‚¬ì´íŠ¸ì˜ ê°€ì‹œì„±ì„ ë†’ì´ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.",
    "ë°ì´í„°ë² ì´ìŠ¤ëŠ” êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.",
    "í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì€ ì¸í„°ë„·ì„ í†µí•´ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
    "ë¹…ë°ì´í„° ë¶„ì„ì€ ëŒ€ëŸ‰ì˜ ë°ì´í„°ì—ì„œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.",
    "íŒŒì´ì¬ì€ ë°ì´í„° ê³¼í•™ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì— ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì–¸ì–´ì…ë‹ˆë‹¤.",
    "ì•Œê³ ë¦¬ì¦˜ì€ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë‹¨ê³„ì  ì ˆì°¨ì…ë‹ˆë‹¤.",
    "ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì€ í”„ë¡œê·¸ë¨ì„ ì„¤ê³„í•˜ê³  êµ¬í˜„í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.",
]

SAMPLE_QUERIES = [
    ("ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ", "ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„° ì‹œìŠ¤í…œì´ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤."),
    ("ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµ", "ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤."),
    ("OpenSearch ê²€ìƒ‰", "OpenSearchëŠ” ê°•ë ¥í•œ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ìœ¼ë¡œ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤."),
    ("neural sparse", "Neural sparse ê²€ìƒ‰ì€ í¬ì†Œ ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ ê²€ìƒ‰ì„ ì œê³µí•©ë‹ˆë‹¤."),
    ("í•œêµ­ì–´ ì²˜ë¦¬", "í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„ê³¼ í’ˆì‚¬ íƒœê¹…ì„ í¬í•¨í•©ë‹ˆë‹¤."),
    ("BERT ëª¨ë¸", "BERT ëª¨ë¸ì€ ì–‘ë°©í–¥ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ë§¥ì„ ì´í•´í•©ë‹ˆë‹¤."),
    ("GPT ChatGPT", "GPTëŠ” ìƒì„±í˜• ì–¸ì–´ ëª¨ë¸ë¡œ ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."),
    ("ì„ë² ë”© ë²¡í„°", "ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„° ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤."),
]

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\nğŸ–¥ï¸  Device: {device}")
if torch.cuda.is_available():
    print(f"   CUDA: {torch.cuda.get_device_name(0)}")

# Step 1: í† í¬ë‚˜ì´ì € ë¡œë“œ
print("\n" + "=" * 60)
print("Step 1: í† í¬ë‚˜ì´ì € ë¡œë“œ")
print("=" * 60)

MODEL_NAME = "klue/bert-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
print(f"âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ: {MODEL_NAME}")
print(f"  Vocab size: {tokenizer.vocab_size:,}")

# Step 2: IDF ê³„ì‚°
print("\n" + "=" * 60)
print("Step 2: IDF (Inverse Document Frequency) ê³„ì‚°")
print("=" * 60)

def calculate_idf(documents, tokenizer):
    """IDF ê³„ì‚°"""
    N = len(documents)
    df = Counter()

    print(f"ë¬¸ì„œ {N}ê°œì—ì„œ IDF ê³„ì‚° ì¤‘...")

    for doc in documents:
        tokens = tokenizer.encode(doc, add_special_tokens=False, max_length=128, truncation=True)
        unique_tokens = set(tokens)
        for token_id in unique_tokens:
            df[token_id] += 1

    # IDF ê³„ì‚°
    idf_dict = {}
    for token_id, doc_freq in df.items():
        idf_score = math.log((N + 1) / (doc_freq + 1)) + 1.0
        idf_dict[token_id] = idf_score

    # í† í° ë¬¸ìì—´ë¡œ ë³€í™˜
    idf_token_dict = {}
    for token_id, score in idf_dict.items():
        token_str = tokenizer.decode([token_id])
        idf_token_dict[token_str] = float(score)

    print(f"âœ“ {len(idf_token_dict):,}ê°œ í† í°ì˜ IDF ê³„ì‚° ì™„ë£Œ")
    print(f"  í‰ê·  IDF: {np.mean(list(idf_token_dict.values())):.4f}")

    return idf_token_dict, idf_dict

idf_token_dict, idf_id_dict = calculate_idf(SAMPLE_DOCUMENTS, tokenizer)

# íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ…
print("\níŠ¸ë Œë“œ í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì ìš© ì¤‘...")
TREND_BOOST = {
    'LLM': 1.5, 'GPT': 1.5, 'ChatGPT': 1.5,
    'ìƒì„±í˜•': 1.4, 'RAG': 1.4, 'OpenSearch': 1.3,
    'ê²€ìƒ‰': 1.2, 'ì¸ê³µì§€ëŠ¥': 1.2, 'AI': 1.2,
    'BERT': 1.2, 'ì„ë² ë”©': 1.3, 'neural': 1.3, 'sparse': 1.3,
}

boost_count = 0
for keyword, boost_factor in TREND_BOOST.items():
    keyword_tokens = tokenizer.encode(keyword, add_special_tokens=False)
    for token_id in keyword_tokens:
        token_str = tokenizer.decode([token_id])
        if token_str in idf_token_dict:
            idf_token_dict[token_str] *= boost_factor
            boost_count += 1

print(f"âœ“ {boost_count}ê°œ í† í°ì— íŠ¸ë Œë“œ ë¶€ìŠ¤íŒ… ì ìš©")

# Step 3: ëª¨ë¸ ì •ì˜
print("\n" + "=" * 60)
print("Step 3: OpenSearch ë¬¸ì„œ ì¸ì½”ë” ëª¨ë¸ ì •ì˜")
print("=" * 60)

class OpenSearchDocEncoder(nn.Module):
    """OpenSearch Neural Sparse Document Encoder"""
    def __init__(self, model_name="klue/bert-base"):
        super().__init__()
        self.config = AutoConfig.from_pretrained(model_name)
        self.bert = AutoModelForMaskedLM.from_pretrained(model_name)
        self.vocab_size = self.config.vocab_size
        self.activation = lambda x: torch.log1p(torch.relu(x))

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)
        logits = outputs.logits
        activated = self.activation(logits)
        sparse_vector = torch.max(activated * attention_mask.unsqueeze(-1), dim=1).values
        return sparse_vector

doc_encoder = OpenSearchDocEncoder(MODEL_NAME)
doc_encoder = doc_encoder.to(device)

print(f"âœ“ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
print(f"  Parameters: {sum(p.numel() for p in doc_encoder.parameters()):,}")

# Step 4: í•™ìŠµ ë°ì´í„° ì¤€ë¹„
print("\n" + "=" * 60)
print("Step 4: í•™ìŠµ ë°ì´í„° ì¤€ë¹„")
print("=" * 60)

# Query-Document pairs ìƒì„±
qd_pairs = []
for query, pos_doc in SAMPLE_QUERIES:
    qd_pairs.append((query, pos_doc, 1.0))  # Positive

    # Negative sampling
    for neg_doc in SAMPLE_DOCUMENTS:
        if neg_doc != pos_doc:
            qd_pairs.append((query, neg_doc, 0.0))  # Negative
            break  # 1ê°œë§Œ

print(f"âœ“ {len(qd_pairs)}ê°œ query-document pairs ìƒì„±")
print(f"  Positive: {len(SAMPLE_QUERIES)}")
print(f"  Negative: {len(qd_pairs) - len(SAMPLE_QUERIES)}")

class SimpleDataset(torch.utils.data.Dataset):
    def __init__(self, qd_pairs, tokenizer, max_length=64):
        self.qd_pairs = qd_pairs
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.qd_pairs)

    def __getitem__(self, idx):
        query, document, relevance = self.qd_pairs[idx]

        query_encoded = self.tokenizer(
            query, max_length=self.max_length, padding='max_length',
            truncation=True, return_tensors='pt'
        )
        doc_encoded = self.tokenizer(
            document, max_length=self.max_length, padding='max_length',
            truncation=True, return_tensors='pt'
        )

        return {
            'query_input_ids': query_encoded['input_ids'].squeeze(0),
            'query_attention_mask': query_encoded['attention_mask'].squeeze(0),
            'doc_input_ids': doc_encoded['input_ids'].squeeze(0),
            'doc_attention_mask': doc_encoded['attention_mask'].squeeze(0),
            'relevance': torch.tensor(relevance, dtype=torch.float32)
        }

dataset = SimpleDataset(qd_pairs, tokenizer)
# Increase batch size for better in-batch negatives
BATCH_SIZE = 8  # Increased from 4
loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

print(f"âœ“ ë°ì´í„° ë¡œë” ìƒì„± (batch_size={BATCH_SIZE})")

# Step 5: ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
print("\n" + "=" * 60)
print("Step 5: ì†ì‹¤ í•¨ìˆ˜ ì •ì˜")
print("=" * 60)

def compute_query_representation(query_tokens, idf_dict, vocab_size):
    """IDF lookupìœ¼ë¡œ ì¿¼ë¦¬ sparse vector ìƒì„± (Inference-Free!)"""
    batch_size, seq_len = query_tokens.shape
    query_sparse = torch.zeros(batch_size, vocab_size, device=query_tokens.device)

    for b in range(batch_size):
        for token_id in query_tokens[b]:
            token_id = token_id.item()
            if token_id in idf_dict:
                query_sparse[b, token_id] = idf_dict[token_id]

    return query_sparse

print("âœ“ ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")
print("  - NEW: In-Batch Negatives Contrastive Loss")
print("  - L0 Regularization (Sparsity)")
print("  - IDF-aware Penalty (optional)")
print("\nâš ï¸  FIXED: Replaced BCE with proper contrastive loss!")

# Step 6: í•™ìŠµ ì‹¤í–‰
print("\n" + "=" * 60)
print("Step 6: ëª¨ë¸ í•™ìŠµ (ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸)")
print("=" * 60)

optimizer = AdamW(doc_encoder.parameters(), lr=5e-5)
NUM_EPOCHS = 2

# Loss hyperparameters
LAMBDA_L0 = 5e-4  # Reduced from 1e-3 to allow less sparsity
LAMBDA_IDF = 1e-2
TEMPERATURE = 0.05

print(f"í•™ìŠµ ì„¤ì •:")
print(f"  Epochs: {NUM_EPOCHS}")
print(f"  Learning rate: 5e-5")
print(f"  Batch size: {BATCH_SIZE}")
print(f"  Temperature: {TEMPERATURE}")
print(f"  Lambda L0: {LAMBDA_L0}")
print(f"  Lambda IDF: {LAMBDA_IDF}")

for epoch in range(NUM_EPOCHS):
    doc_encoder.train()
    total_loss_sum = 0
    total_ranking_loss = 0
    total_l0_loss = 0
    total_idf_penalty = 0

    print(f"\nEpoch {epoch + 1}/{NUM_EPOCHS}")

    for batch_idx, batch in enumerate(loader):
        query_tokens = batch['query_input_ids'].to(device)
        doc_input_ids = batch['doc_input_ids'].to(device)
        doc_attention_mask = batch['doc_attention_mask'].to(device)
        relevance = batch['relevance'].to(device)

        # Document encoding
        doc_sparse = doc_encoder(doc_input_ids, doc_attention_mask)

        # Query encoding (IDF lookup - Inference-Free!)
        query_sparse = compute_query_representation(
            query_tokens, idf_id_dict, tokenizer.vocab_size
        )

        # NEW: Use improved loss function with in-batch negatives
        total_loss, loss_dict = neural_sparse_loss_with_regularization(
            doc_sparse=doc_sparse,
            query_sparse=query_sparse,
            relevance=relevance,
            idf_dict=idf_id_dict,
            lambda_l0=LAMBDA_L0,
            lambda_idf=LAMBDA_IDF,
            temperature=TEMPERATURE,
            use_in_batch_negatives=True,  # Key improvement!
        )

        # Backward
        optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(doc_encoder.parameters(), 1.0)
        optimizer.step()

        # Accumulate losses
        total_loss_sum += total_loss.item()
        total_ranking_loss += loss_dict['ranking'].item()
        total_l0_loss += loss_dict['l0'].item()
        total_idf_penalty += loss_dict['idf_penalty'].item()

        if (batch_idx + 1) % 2 == 0:
            print(
                f"  Batch {batch_idx + 1}/{len(loader)} - "
                f"Total: {total_loss.item():.4f}, "
                f"Ranking: {loss_dict['ranking'].item():.4f}, "
                f"L0: {loss_dict['l0'].item():.4f}"
            )

    # Epoch summary
    num_batches = len(loader)
    avg_total = total_loss_sum / num_batches
    avg_ranking = total_ranking_loss / num_batches
    avg_l0 = total_l0_loss / num_batches
    avg_idf = total_idf_penalty / num_batches

    print(f"\nâœ“ Epoch {epoch + 1} ì™„ë£Œ:")
    print(f"  Total Loss: {avg_total:.4f}")
    print(f"  Ranking Loss: {avg_ranking:.4f}")
    print(f"  L0 Loss: {avg_l0:.4f}")
    print(f"  IDF Penalty: {avg_idf:.4f}")

    # Compute sparsity metrics
    doc_encoder.eval()
    with torch.no_grad():
        sample_batch = next(iter(loader))
        sample_docs = doc_encoder(
            sample_batch['doc_input_ids'].to(device),
            sample_batch['doc_attention_mask'].to(device)
        )
        sparsity_metrics = compute_sparsity_metrics(sample_docs)
        print(f"  Sparsity: {sparsity_metrics['sparsity']:.2%}")
        print(f"  Avg non-zero tokens: {sparsity_metrics['non_zero_count_mean']:.1f}")

print("\nâœ“ í•™ìŠµ ì™„ë£Œ!")

# Step 7: ëª¨ë¸ ì €ì¥
print("\n" + "=" * 60)
print("Step 7: ëª¨ë¸ ì €ì¥ (OpenSearch í˜¸í™˜ í˜•ì‹)")
print("=" * 60)

OUTPUT_DIR = "./models/test_korean_neural_sparse_model"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# pytorch_model.bin
torch.save(doc_encoder.state_dict(), f"{OUTPUT_DIR}/pytorch_model.bin")
print(f"âœ“ pytorch_model.bin ì €ì¥")

# idf.json
with open(f"{OUTPUT_DIR}/idf.json", 'w', encoding='utf-8') as f:
    json.dump(idf_token_dict, f, ensure_ascii=False, indent=2)
print(f"âœ“ idf.json ì €ì¥ ({len(idf_token_dict):,} tokens)")

# Tokenizer
tokenizer.save_pretrained(OUTPUT_DIR)
print(f"âœ“ Tokenizer íŒŒì¼ ì €ì¥")

# config.json
config = {
    "model_type": "opensearch-neural-sparse-doc-encoder",
    "base_model": MODEL_NAME,
    "vocab_size": tokenizer.vocab_size,
    "mode": "doc-only",
    "output_format": "rank_features",
    "test_info": {
        "documents": len(SAMPLE_DOCUMENTS),
        "queries": len(SAMPLE_QUERIES),
        "epochs": NUM_EPOCHS,
        "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
}

with open(f"{OUTPUT_DIR}/config.json", 'w', encoding='utf-8') as f:
    json.dump(config, f, ensure_ascii=False, indent=2)
print(f"âœ“ config.json ì €ì¥")

print(f"\nëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}/")

# Step 8: ì¶”ë¡  í…ŒìŠ¤íŠ¸
print("\n" + "=" * 60)
print("Step 8: ì¶”ë¡  í…ŒìŠ¤íŠ¸")
print("=" * 60)

doc_encoder.eval()

def encode_document(text, model, tokenizer, device):
    """ë¬¸ì„œ ì¸ì½”ë”© (ëª¨ë¸ ì‚¬ìš©)"""
    encoded = tokenizer(text, max_length=64, padding='max_length',
                       truncation=True, return_tensors='pt')
    input_ids = encoded['input_ids'].to(device)
    attention_mask = encoded['attention_mask'].to(device)

    with torch.no_grad():
        sparse_vec = model(input_ids, attention_mask)

    return sparse_vec.cpu().numpy()[0]

def encode_query_inference_free(text, tokenizer, idf_dict):
    """ì¿¼ë¦¬ ì¸ì½”ë”© (IDF lookup - Inference-Free!)"""
    tokens = tokenizer.encode(text, add_special_tokens=False, max_length=64, truncation=True)
    sparse_vec = np.zeros(tokenizer.vocab_size)

    for token_id in tokens:
        token_str = tokenizer.decode([token_id])
        if token_str in idf_dict:
            sparse_vec[token_id] = idf_dict[token_str]

    return sparse_vec

def get_top_tokens(sparse_vec, tokenizer, top_k=10):
    """ìƒìœ„ í† í° ì¶”ì¶œ"""
    top_indices = np.argsort(sparse_vec)[-top_k:][::-1]
    top_values = sparse_vec[top_indices]

    return [(tokenizer.decode([idx]), val) for idx, val in zip(top_indices, top_values) if val > 0]

# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
test_queries = [
    "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ",
    "OpenSearch ê²€ìƒ‰ ì—”ì§„",
    "í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬",
    "LLM ChatGPT",
]

print("\nğŸ“ ì¿¼ë¦¬ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ (Inference-Free)")
print("-" * 60)

for query in test_queries:
    sparse_vec = encode_query_inference_free(query, tokenizer, idf_token_dict)
    non_zero = np.count_nonzero(sparse_vec)

    print(f"\nQuery: {query}")
    print(f"  Non-zero: {non_zero}/{len(sparse_vec)} ({non_zero/len(sparse_vec)*100:.2f}%)")
    print(f"  ìƒìœ„ í† í°:")

    top_tokens = get_top_tokens(sparse_vec, tokenizer, top_k=5)
    for i, (token, value) in enumerate(top_tokens, 1):
        print(f"    {i}. {token:15s} ({value:.4f})")

# í…ŒìŠ¤íŠ¸ ë¬¸ì„œ
test_docs = [
    "OpenSearchëŠ” neural sparse ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.",
    "ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„° ê³¼í•™ì˜ í•µì‹¬ì…ë‹ˆë‹¤.",
]

print("\n\nğŸ“„ ë¬¸ì„œ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ (Model Inference)")
print("-" * 60)

for doc in test_docs:
    sparse_vec = encode_document(doc, doc_encoder, tokenizer, device)
    non_zero = np.count_nonzero(sparse_vec)
    l1_norm = np.sum(np.abs(sparse_vec))

    print(f"\nDocument: {doc}")
    print(f"  Non-zero: {non_zero}/{len(sparse_vec)} ({non_zero/len(sparse_vec)*100:.2f}%)")
    print(f"  L1 Norm: {l1_norm:.2f}")
    print(f"  ìƒìœ„ í† í°:")

    top_tokens = get_top_tokens(sparse_vec, tokenizer, top_k=5)
    for i, (token, value) in enumerate(top_tokens, 1):
        print(f"    {i}. {token:15s} ({value:.4f})")

# Step 9: ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜
print("\n" + "=" * 60)
print("Step 9: ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜")
print("=" * 60)

# ëª¨ë“  ë¬¸ì„œ ì¸ì½”ë”©
print("\nëª¨ë“  ìƒ˜í”Œ ë¬¸ì„œ ì¸ì½”ë”© ì¤‘...")
doc_vectors = []
for doc in SAMPLE_DOCUMENTS:
    vec = encode_document(doc, doc_encoder, tokenizer, device)
    doc_vectors.append(vec)
doc_vectors = np.array(doc_vectors)

print(f"âœ“ {len(doc_vectors)}ê°œ ë¬¸ì„œ ì¸ì½”ë”© ì™„ë£Œ")

# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
search_queries = [
    "ì¸ê³µì§€ëŠ¥ ë¨¸ì‹ ëŸ¬ë‹",
    "OpenSearch neural sparse ê²€ìƒ‰",
    "í•œêµ­ì–´ ì²˜ë¦¬",
]

print("\nğŸ” ê²€ìƒ‰ ê²°ê³¼:")
print("=" * 60)

for query in search_queries:
    print(f"\nQuery: '{query}'")

    # ì¿¼ë¦¬ ì¸ì½”ë”©
    query_vec = encode_query_inference_free(query, tokenizer, idf_token_dict)

    # ìœ ì‚¬ë„ ê³„ì‚° (dot product)
    similarities = np.dot(doc_vectors, query_vec)

    # ìƒìœ„ 3ê°œ ê²°ê³¼
    top_indices = np.argsort(similarities)[-3:][::-1]

    print("ìƒìœ„ 3ê°œ ê²°ê³¼:")
    for rank, idx in enumerate(top_indices, 1):
        print(f"  {rank}. [Score: {similarities[idx]:.4f}] {SAMPLE_DOCUMENTS[idx][:60]}...")

# ìµœì¢… ìš”ì•½
print("\n" + "=" * 60)
print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
print("=" * 60)

print(f"""
í…ŒìŠ¤íŠ¸ ìš”ì•½:
  âœ“ ëª¨ë¸: {MODEL_NAME}
  âœ“ ìƒ˜í”Œ ë¬¸ì„œ: {len(SAMPLE_DOCUMENTS)}ê°œ
  âœ“ ìƒ˜í”Œ ì¿¼ë¦¬: {len(SAMPLE_QUERIES)}ê°œ
  âœ“ í•™ìŠµ Epochs: {NUM_EPOCHS}
  âœ“ IDF í† í°: {len(idf_token_dict):,}ê°œ
  âœ“ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}/

OpenSearch í†µí•©:
  1. {OUTPUT_DIR}/ë¥¼ ì••ì¶•í•˜ì—¬ OpenSearchì— ì—…ë¡œë“œ
  2. Doc-only modeë¡œ ì„¤ì •
  3. rank_features íƒ€ì… ë§¤í•‘ ì‚¬ìš©
  4. Neural sparse ì¿¼ë¦¬ ì‹¤í–‰

ë‹¤ìŒ ë‹¨ê³„:
  - ë” ë§ì€ ë°ì´í„°ë¡œ ì¬í•™ìŠµ
  - Knowledge distillation ì ìš©
  - BEIR ë²¤ì¹˜ë§ˆí¬ í‰ê°€
  - OpenSearch ì‹¤ì œ ë°°í¬
""")

print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

#!/usr/bin/env python3
"""
BF16 ëª¨ë¸ í…ŒìŠ¤íŠ¸ - Triton ì—†ì´ ì‘ë™í•˜ëŠ” ëª¨ë¸

FP8 ëª¨ë¸ì€ Tritonì´ í•„ìˆ˜ì´ì§€ë§Œ, BF16 ëª¨ë¸ì€ ìˆœìˆ˜ PyTorchë¡œ ì‘ë™í•©ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


def test_bf16_model():
    """BF16 ëª¨ë¸ë¡œ ì¿¼ë¦¬ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print("="*70)
    print("ğŸ§ª Testing BF16 Model (No Triton Required)")
    print("="*70)

    from src.llm_loader import check_gpu_memory

    # Check GPU
    stats = check_gpu_memory()
    if not stats.get('available'):
        print("âŒ No GPU available")
        return False

    print("\n" + "="*70)
    print("ğŸ“¥ Loading Qwen2.5-14B-Instruct (BF16)")
    print("="*70)
    print("Model size: ~28GB VRAM")
    print("Quantization: BF16 (no FP8, no Triton)")
    print("Expected load time: ~3-5 minutes")
    print("="*70)

    import time
    from transformers import AutoModelForCausalLM, AutoTokenizer
    import torch

    model_name = "Qwen/Qwen2.5-14B-Instruct"

    print("\n1ï¸âƒ£ Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    print(f"âœ“ Tokenizer loaded (vocab size: {len(tokenizer):,})")

    print("\n2ï¸âƒ£ Loading model...")
    start_time = time.time()

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype=torch.bfloat16,  # BF16 precision
        trust_remote_code=True,
        low_cpu_mem_usage=True,
    )

    load_time = time.time() - start_time
    print(f"âœ“ Model loaded in {load_time:.2f}s")

    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        print(f"ğŸ“Š GPU Memory: {allocated:.2f} GB")

    # Test generation
    print("\n" + "="*70)
    print("3ï¸âƒ£ Testing Query Generation")
    print("="*70)

    from src.synthetic_data_generator import generate_queries_from_document

    doc = "OpenSearchëŠ” Apache 2.0 ë¼ì´ì„ ìŠ¤ì˜ ì˜¤í”ˆ ì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. " \
          "Elasticsearchì™€ í˜¸í™˜ë˜ë©° ëŒ€ê·œëª¨ ë°ì´í„° ê²€ìƒ‰, ë¡œê·¸ ë¶„ì„, ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì— ì‚¬ìš©ë©ë‹ˆë‹¤."

    print(f"ğŸ“ Document: {doc[:80]}...")
    print("â³ Generating queries...")

    start_time = time.time()

    queries = generate_queries_from_document(
        document=doc,
        llm_model=model,
        llm_tokenizer=tokenizer,
        num_queries=3,
        max_new_tokens=150,
        temperature=0.8,
        verbose=True,
    )

    gen_time = time.time() - start_time

    print(f"\nâœ… Generation completed in {gen_time:.2f}s")
    print(f"ğŸ“Š Generated {len(queries)} queries:")
    for i, q in enumerate(queries, 1):
        print(f"   {i}. {q}")

    print("\n" + "="*70)
    print("âœ… BF16 Model Test Passed!")
    print("="*70)
    print(f"Model: {model_name}")
    print(f"Load time: {load_time:.2f}s")
    print(f"Query gen time: {gen_time:.2f}s")
    print(f"GPU Memory: {allocated:.2f} GB")
    print("="*70)

    return True


if __name__ == "__main__":
    try:
        success = test_bf16_model()
        if success:
            print("\nğŸ’¡ Next steps:")
            print("   1. Update notebook 2 to use BF16 model:")
            print("      model, tokenizer = load_qwen3_awq('Qwen/Qwen2.5-14B-Instruct')")
            print("   2. No Triton environment variables needed")
            print("   3. Pure PyTorch - reliable and stable")
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()

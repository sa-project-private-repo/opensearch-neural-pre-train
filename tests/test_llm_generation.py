#!/usr/bin/env python3
"""
LLM ìƒì„± í…ŒìŠ¤íŠ¸ - í•©ì„± ë°ì´í„° ìƒì„±ì´ ë©ˆì¶”ëŠ” ë¬¸ì œ ë””ë²„ê¹…

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤:
1. LLM ëª¨ë¸ ë¡œë“œ ê°€ëŠ¥ ì—¬ë¶€
2. ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± ê°€ëŠ¥ ì—¬ë¶€
3. í•œê¸€ ë¬¸ì„œì—ì„œ ì¿¼ë¦¬ ìƒì„± ê°€ëŠ¥ ì—¬ë¶€
4. ê° ë‹¨ê³„ë³„ ì†Œìš” ì‹œê°„ ì¸¡ì •
"""

import os
import sys
import time
from pathlib import Path

# Disable Triton to avoid compilation errors (ARM aarch64)
# MUST be set before importing torch or transformers
os.environ["TRITON_INTERPRET"] = "1"  # Use interpreter mode
os.environ["DISABLE_TRITON"] = "1"     # Completely disable
print("ğŸ”§ Triton disabled (TRITON_INTERPRET=1, DISABLE_TRITON=1)")

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


def test_gpu_availability():
    """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    print("="*70)
    print("1ï¸âƒ£ Testing GPU Availability")
    print("="*70)

    import torch

    if torch.cuda.is_available():
        print(f"âœ… CUDA available: {torch.cuda.get_device_name(0)}")
        print(f"   CUDA version: {torch.version.cuda}")

        allocated = torch.cuda.memory_allocated(0) / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"   GPU Memory: {allocated:.2f} / {total:.2f} GB")
        return True
    else:
        print("âŒ CUDA not available")
        return False


def test_model_loading():
    """LLM ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*70)
    print("2ï¸âƒ£ Testing Model Loading")
    print("="*70)

    try:
        from src.llm_loader import load_qwen3_awq

        print("â³ Loading Qwen3 model (this may take a few minutes)...")
        start_time = time.time()

        model, tokenizer = load_qwen3_awq()

        load_time = time.time() - start_time
        print(f"\nâœ… Model loaded successfully in {load_time:.2f}s")

        # Check model device
        if hasattr(model, 'device'):
            print(f"   Model device: {model.device}")

        return model, tokenizer

    except Exception as e:
        print(f"\nâŒ Model loading failed: {e}")
        import traceback
        traceback.print_exc()
        return None, None


def test_simple_generation(model, tokenizer):
    """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*70)
    print("3ï¸âƒ£ Testing Simple Text Generation")
    print("="*70)

    if model is None or tokenizer is None:
        print("âš ï¸  Skipped: Model not loaded")
        return False

    try:
        from src.llm_loader import generate_text

        prompt = "1ë¶€í„° 5ê¹Œì§€ ìˆ«ìë¥¼ ë‚˜ì—´í•˜ì„¸ìš”:"
        print(f"ğŸ“ Prompt: {prompt}")
        print("â³ Generating...")

        start_time = time.time()

        # Set a timeout mechanism
        import signal

        def timeout_handler(signum, frame):
            raise TimeoutError("Generation took too long (>60s)")

        # Set 60 second timeout
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(60)

        try:
            generated = generate_text(
                model=model,
                tokenizer=tokenizer,
                prompt=prompt,
                max_new_tokens=50,
                temperature=0.7,
            )
            signal.alarm(0)  # Cancel timeout

            gen_time = time.time() - start_time

            print(f"\nâœ… Generation completed in {gen_time:.2f}s")
            print(f"ğŸ“„ Output: {generated}")
            return True

        except TimeoutError as e:
            signal.alarm(0)
            print(f"\nâŒ Generation timeout: {e}")
            return False

    except Exception as e:
        print(f"\nâŒ Generation failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_korean_query_generation(model, tokenizer):
    """í•œê¸€ ë¬¸ì„œì—ì„œ ì¿¼ë¦¬ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*70)
    print("4ï¸âƒ£ Testing Korean Query Generation")
    print("="*70)

    if model is None or tokenizer is None:
        print("âš ï¸  Skipped: Model not loaded")
        return False

    try:
        from src.synthetic_data_generator import generate_queries_from_document

        # Sample Korean document
        doc = "OpenSearchëŠ” Apache 2.0 ë¼ì´ì„ ìŠ¤ì˜ ì˜¤í”ˆ ì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. " \
              "Elasticsearchì™€ í˜¸í™˜ë˜ë©° ëŒ€ê·œëª¨ ë°ì´í„° ê²€ìƒ‰, ë¡œê·¸ ë¶„ì„, ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì— ì‚¬ìš©ë©ë‹ˆë‹¤."

        print(f"ğŸ“ Document: {doc[:80]}...")
        print("â³ Generating queries (max 60s timeout)...")

        start_time = time.time()

        # Set timeout
        import signal

        def timeout_handler(signum, frame):
            raise TimeoutError("Query generation took too long (>60s)")

        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(60)

        try:
            queries = generate_queries_from_document(
                document=doc,
                llm_model=model,
                llm_tokenizer=tokenizer,
                num_queries=3,
                max_new_tokens=150,
                temperature=0.8,
                verbose=True,  # Enable verbose logging
            )
            signal.alarm(0)  # Cancel timeout

            gen_time = time.time() - start_time

            print(f"\nâœ… Query generation completed in {gen_time:.2f}s")
            print(f"ğŸ“Š Generated {len(queries)} queries:")
            for i, q in enumerate(queries, 1):
                print(f"   {i}. {q}")

            return True

        except TimeoutError as e:
            signal.alarm(0)
            print(f"\nâŒ Query generation timeout: {e}")
            print("   ğŸ” This suggests the LLM is taking too long to respond")
            print("   ğŸ’¡ Possible causes:")
            print("      - Model too large for available GPU memory")
            print("      - Inference is extremely slow")
            print("      - Model not properly loaded to GPU")
            return False

    except Exception as e:
        print(f"\nâŒ Query generation failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_gpu_memory_after_generation(model):
    """ìƒì„± í›„ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸"""
    print("\n" + "="*70)
    print("5ï¸âƒ£ GPU Memory After Generation")
    print("="*70)

    import torch

    if not torch.cuda.is_available():
        print("âš ï¸  CUDA not available")
        return

    allocated = torch.cuda.memory_allocated(0) / 1024**3
    reserved = torch.cuda.memory_reserved(0) / 1024**3
    total = torch.cuda.get_device_properties(0).total_memory / 1024**3

    print(f"GPU Memory Status:")
    print(f"  Allocated: {allocated:.2f} GB")
    print(f"  Reserved:  {reserved:.2f} GB")
    print(f"  Total:     {total:.2f} GB")
    print(f"  Free:      {total - allocated:.2f} GB")

    if allocated > total * 0.9:
        print("\nâš ï¸  WARNING: GPU memory usage is very high (>90%)")
        print("   This may cause slow inference or OOM errors")


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("\n" + "="*70)
    print("ğŸ§ª LLM Generation Debugging Test")
    print("="*70)
    print("This test will help identify why LLM generation is stuck")
    print("="*70)

    results = {}

    # Test 1: GPU
    results['gpu'] = test_gpu_availability()

    # Test 2: Model loading
    model, tokenizer = test_model_loading()
    results['model_load'] = (model is not None)

    if not results['model_load']:
        print("\n" + "="*70)
        print("âŒ Cannot proceed: Model loading failed")
        print("="*70)
        return

    # Test 3: Simple generation
    results['simple_gen'] = test_simple_generation(model, tokenizer)

    # Test 4: Korean query generation
    results['korean_gen'] = test_korean_query_generation(model, tokenizer)

    # Test 5: GPU memory
    test_gpu_memory_after_generation(model)

    # Summary
    print("\n" + "="*70)
    print("ğŸ“Š Test Summary")
    print("="*70)
    print(f"GPU Available:         {'âœ…' if results.get('gpu') else 'âŒ'}")
    print(f"Model Loading:         {'âœ…' if results.get('model_load') else 'âŒ'}")
    print(f"Simple Generation:     {'âœ…' if results.get('simple_gen') else 'âŒ'}")
    print(f"Korean Query Gen:      {'âœ…' if results.get('korean_gen') else 'âŒ'}")

    print("\n" + "="*70)
    print("ğŸ” Diagnosis")
    print("="*70)

    if not results.get('gpu'):
        print("âŒ No GPU available - LLM inference will be very slow or fail")
        print("   ğŸ’¡ Solution: Ensure CUDA is properly installed")

    elif not results.get('model_load'):
        print("âŒ Model failed to load")
        print("   ğŸ’¡ Solution: Check model name and available memory")

    elif not results.get('simple_gen'):
        print("âŒ Simple generation failed or timed out")
        print("   ğŸ’¡ Possible causes:")
        print("      1. Model is too large for GPU memory")
        print("      2. Inference is extremely slow")
        print("      3. Model not properly configured for generation")
        print("   ğŸ’¡ Solution: Try a smaller model or check GPU memory")

    elif not results.get('korean_gen'):
        print("âŒ Korean query generation failed or timed out")
        print("   ğŸ’¡ Possible causes:")
        print("      1. Prompt is too long")
        print("      2. Model struggles with Korean text")
        print("      3. max_new_tokens too high")
        print("   ğŸ’¡ Solution:")
        print("      - Reduce max_new_tokens from 150 to 50")
        print("      - Simplify the prompt")
        print("      - Try English prompts first")

    else:
        print("âœ… All tests passed!")
        print("   The LLM generation is working correctly in isolation.")
        print("   ğŸ’¡ If notebook is still stuck, possible causes:")
        print("      1. Jupyter kernel issue - restart kernel")
        print("      2. Model was loaded multiple times - check GPU memory")
        print("      3. Interference from other notebook cells")

    print("="*70)


if __name__ == "__main__":
    main()

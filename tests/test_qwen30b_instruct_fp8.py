#!/usr/bin/env python3
"""
Qwen3-30B-A3B-Instruct-2507-FP8 í…ŒìŠ¤íŠ¸

FP8 Instruct ë²„ì „ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
Thinking ë²„ì „ê³¼ ë‹¬ë¦¬ ì¼ë°˜ instruction-following ëª¨ë¸ì…ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


def test_qwen30b_instruct_fp8():
    """Qwen3-30B Instruct FP8 ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("="*70)
    print("ğŸ§ª Testing Qwen3-30B-A3B-Instruct-2507-FP8")
    print("="*70)
    print("Model: Instruction-following variant (not thinking)")
    print("="*70)

    from src.llm_loader import check_gpu_memory

    # Check GPU
    stats = check_gpu_memory()
    if not stats.get('available'):
        print("âŒ No GPU available")
        return False

    total_vram = stats['devices'][0]['total_gb']
    print(f"\nâš ï¸  Model size check:")
    print(f"   Expected VRAM: ~30GB (30B params, FP8)")
    print(f"   Available VRAM: {total_vram:.2f}GB")

    print("\n" + "="*70)
    print("ğŸ“¥ Loading Qwen3-30B-A3B-Instruct-2507-FP8")
    print("="*70)
    print("Model: Qwen/Qwen3-30B-A3B-Instruct-2507-FP8")
    print("Size: 30B parameters")
    print("Quantization: FP8")
    print("Expected VRAM: ~30GB")
    print("Expected load time: ~5-10 minutes")
    print("="*70)

    import time
    from src.llm_loader import load_qwen3_awq

    print("\nâ³ Loading model...")
    print("ğŸ“ Note: This will likely fail with Triton PTX error due to sm_121a")
    start_time = time.time()

    try:
        model, tokenizer = load_qwen3_awq(
            model_name="Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
            device_map="auto",
        )

        load_time = time.time() - start_time
        print(f"\nâœ… Model loaded in {load_time:.2f}s ({load_time/60:.1f} minutes)")

    except Exception as e:
        load_time = time.time() - start_time
        print(f"\nâŒ Model loading failed after {load_time:.2f}s")
        print(f"Error: {e}")

        if "ptxas" in str(e) or "sm_121a" in str(e):
            print("\n" + "="*70)
            print("ğŸ” Diagnosis: Expected Triton PTX Error")
            print("="*70)
            print("This is the same issue as Qwen3-30B-A3B-Thinking-2507-FP8:")
            print("- GPU: NVIDIA GB10 (sm_121a)")
            print("- Triton: Does not support sm_121a")
            print("- PTXAS: Cannot compile for compute capability 12.1")
            print()
            print("âŒ Conclusion: FP8 models cannot run on this GPU")
            print("âœ… Solution: Use BF16 models instead")
            print("="*70)

        import traceback
        traceback.print_exc()
        return False

    # Check GPU memory after loading
    import torch
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        reserved = torch.cuda.memory_reserved(0) / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3

        print("\n" + "="*70)
        print("ğŸ“Š GPU Memory After Loading")
        print("="*70)
        print(f"Allocated: {allocated:.2f} GB")
        print(f"Reserved:  {reserved:.2f} GB")
        print(f"Total:     {total:.2f} GB")
        print(f"Free:      {total - allocated:.2f} GB")

    # Test generation
    print("\n" + "="*70)
    print("ğŸš€ Testing Query Generation")
    print("="*70)

    from src.synthetic_data_generator import generate_queries_from_document

    doc = "OpenSearchëŠ” Apache 2.0 ë¼ì´ì„ ìŠ¤ì˜ ì˜¤í”ˆ ì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. " \
          "Elasticsearchì™€ í˜¸í™˜ë˜ë©° ëŒ€ê·œëª¨ ë°ì´í„° ê²€ìƒ‰, ë¡œê·¸ ë¶„ì„, ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì— ì‚¬ìš©ë©ë‹ˆë‹¤."

    print(f"ğŸ“ Document: {doc[:80]}...")
    print("â³ Generating queries...")

    start_time = time.time()

    try:
        queries = generate_queries_from_document(
            document=doc,
            llm_model=model,
            llm_tokenizer=tokenizer,
            num_queries=3,
            max_new_tokens=150,
            temperature=0.8,
            verbose=True,
        )

        gen_time = time.time() - start_time

        print(f"\nâœ… Generation completed in {gen_time:.2f}s")
        print(f"ğŸ“Š Generated {len(queries)} queries:")
        for i, q in enumerate(queries, 1):
            print(f"   {i}. {q}")

        # Summary
        print("\n" + "="*70)
        print("âœ… Qwen3-30B-Instruct-FP8 Test Summary")
        print("="*70)
        print(f"Model: Qwen3-30B-A3B-Instruct-2507-FP8")
        print(f"Load time: {load_time:.2f}s ({load_time/60:.1f} min)")
        print(f"Generation time: {gen_time:.2f}s")
        print(f"GPU Memory: {allocated:.2f} GB")
        print("="*70)

        print("\nğŸ‰ SUCCESS! FP8 model is working!")
        print("   This is unexpected but great news!")

        return True

    except Exception as e:
        gen_time = time.time() - start_time
        print(f"\nâŒ Generation failed after {gen_time:.2f}s")
        print(f"Error: {e}")

        if "ptxas" in str(e) or "sm_121a" in str(e):
            print("\n" + "="*70)
            print("ğŸ” Diagnosis: Triton PTX Compilation Error")
            print("="*70)
            print("Issue: GPU compute capability 12.1 not supported")
            print("GPU: NVIDIA GB10 (sm_121a)")
            print()
            print("This confirms that FP8 models cannot run on this GPU.")
            print("="*70)

        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    try:
        print("\n" + "="*70)
        print("Qwen3-30B-A3B-Instruct-2507-FP8 Test")
        print("="*70)
        print("Testing Instruct variant (not Thinking)")
        print("Expected to fail with same Triton PTX error")
        print("="*70)

        success = test_qwen30b_instruct_fp8()

        if success:
            print("\nğŸ‰ Unexpected success!")
            print("   FP8 model is working on this GPU")
            print("   Consider using this for production")
        else:
            print("\nâŒ Test failed as expected")
            print("   Recommendation: Use BF16 model")
            print("   - Qwen2.5-14B-Instruct (BF16) is proven to work")
            print("   - Or try Qwen2.5-32B-Instruct (BF16)")

    except KeyboardInterrupt:
        print("\n\nâŒ Test cancelled by user")
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()

#!/usr/bin/env python3
"""
Qwen2.5-32B-Instruct (BF16) í…ŒìŠ¤íŠ¸

ë” í° BF16 ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤. FP8ë³´ë‹¤ ì•ˆì •ì ì´ë©° 1 GPUë¡œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


def test_qwen32b_bf16():
    """Qwen2.5-32B BF16 ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("="*70)
    print("ğŸ§ª Testing Qwen2.5-32B-Instruct (BF16)")
    print("="*70)

    from src.llm_loader import check_gpu_memory

    # Check GPU
    stats = check_gpu_memory()
    if not stats.get('available'):
        print("âŒ No GPU available")
        return False

    total_vram = stats['devices'][0]['total_gb']
    print(f"\nâš ï¸  Model size check:")
    print(f"   Expected VRAM: ~64GB (32B params, BF16)")
    print(f"   Available VRAM: {total_vram:.2f}GB")

    if total_vram < 60:
        print(f"\nâš ï¸  VRAM might be tight!")
        print(f"   This model requires about 64GB VRAM")

    print("\n" + "="*70)
    print("ğŸ“¥ Loading Qwen2.5-32B-Instruct (BF16)")
    print("="*70)
    print("Model: Qwen/Qwen2.5-32B-Instruct")
    print("Size: 32B parameters")
    print("Precision: BF16 (stable, no Triton)")
    print("Expected VRAM: ~64GB")
    print("Expected load time: ~10-15 minutes (first download)")
    print("="*70)

    import time
    from src.llm_loader import load_qwen3_awq

    print("\nâ³ Loading model...")
    start_time = time.time()

    try:
        model, tokenizer = load_qwen3_awq(
            model_name="Qwen/Qwen2.5-32B-Instruct",
            device_map="auto",
        )

        load_time = time.time() - start_time
        print(f"\nâœ… Model loaded in {load_time:.2f}s ({load_time/60:.1f} minutes)")

    except Exception as e:
        load_time = time.time() - start_time
        print(f"\nâŒ Model loading failed after {load_time:.2f}s")
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
        return False

    # Check GPU memory after loading
    import torch
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        reserved = torch.cuda.memory_reserved(0) / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3

        print("\n" + "="*70)
        print("ğŸ“Š GPU Memory After Loading")
        print("="*70)
        print(f"Allocated: {allocated:.2f} GB")
        print(f"Reserved:  {reserved:.2f} GB")
        print(f"Total:     {total:.2f} GB")
        print(f"Free:      {total - allocated:.2f} GB")
        print(f"Usage:     {allocated/total*100:.1f}%")

    # Test generation
    print("\n" + "="*70)
    print("ğŸš€ Testing Query Generation")
    print("="*70)

    from src.synthetic_data_generator import generate_queries_from_document

    doc = "OpenSearchëŠ” Apache 2.0 ë¼ì´ì„ ìŠ¤ì˜ ì˜¤í”ˆ ì†ŒìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. " \
          "Elasticsearchì™€ í˜¸í™˜ë˜ë©° ëŒ€ê·œëª¨ ë°ì´í„° ê²€ìƒ‰, ë¡œê·¸ ë¶„ì„, ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì— ì‚¬ìš©ë©ë‹ˆë‹¤."

    print(f"ğŸ“ Document: {doc[:80]}...")
    print("â³ Generating queries...")

    start_time = time.time()

    try:
        queries = generate_queries_from_document(
            document=doc,
            llm_model=model,
            llm_tokenizer=tokenizer,
            num_queries=3,
            max_new_tokens=150,
            temperature=0.8,
            verbose=True,
        )

        gen_time = time.time() - start_time

        print(f"\nâœ… Generation completed in {gen_time:.2f}s")
        print(f"ğŸ“Š Generated {len(queries)} queries:")
        for i, q in enumerate(queries, 1):
            print(f"   {i}. {q}")

        # Second generation for speed comparison
        print("\n" + "="*70)
        print("âš¡ Second Generation Test")
        print("="*70)

        doc2 = "ElasticsearchëŠ” ì‹¤ì‹œê°„ ë¶„ì‚° ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„ì…ë‹ˆë‹¤. " \
               "JSON ë¬¸ì„œë¥¼ ìƒ‰ì¸í™”í•˜ê³  ë¹ ë¥¸ ê²€ìƒ‰ì„ ì œê³µí•©ë‹ˆë‹¤."

        print(f"ğŸ“ Document: {doc2[:80]}...")

        start_time2 = time.time()
        queries2 = generate_queries_from_document(
            document=doc2,
            llm_model=model,
            llm_tokenizer=tokenizer,
            num_queries=3,
            max_new_tokens=150,
            temperature=0.8,
            verbose=False,
        )
        gen_time2 = time.time() - start_time2

        print(f"âœ… Completed in {gen_time2:.2f}s")
        print(f"ğŸ“Š Generated {len(queries2)} queries:")
        for i, q in enumerate(queries2, 1):
            print(f"   {i}. {q}")

        # Summary
        print("\n" + "="*70)
        print("âœ… Qwen2.5-32B Test Summary")
        print("="*70)
        print(f"Model: Qwen2.5-32B-Instruct (BF16)")
        print(f"Load time: {load_time/60:.1f} minutes")
        print(f"First generation: {gen_time:.2f}s")
        print(f"Second generation: {gen_time2:.2f}s")
        print(f"GPU Memory: {allocated:.2f} GB ({allocated/total*100:.1f}%)")
        print("="*70)

        # Comparison with 14B
        print("\n" + "="*70)
        print("ğŸ“ˆ Comparison with 14B Model")
        print("="*70)
        print(f"Qwen2.5-14B-Instruct (BF16): 27.51 GB, ~20s/query")
        print(f"Qwen2.5-32B-Instruct (BF16): {allocated:.2f} GB, ~{gen_time2:.0f}s/query")
        print()

        if gen_time2 < 30:
            print("âœ… 32B model is reasonably fast!")
            print("   Recommendation: Use 32B for better quality")
        elif gen_time2 < 60:
            print("âš ï¸  32B model is slower than 14B")
            print("   Recommendation: Use 14B for speed, 32B for quality")
        else:
            print("âŒ 32B model is too slow")
            print("   Recommendation: Stick with 14B model")

        print("="*70)

        return True

    except Exception as e:
        gen_time = time.time() - start_time
        print(f"\nâŒ Generation failed after {gen_time:.2f}s")
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    try:
        print("\n" + "="*70)
        print("Qwen2.5-32B-Instruct (BF16) Test")
        print("="*70)
        print("This is a larger BF16 model that should work on 1 GPU")
        print("Expected to use ~64GB VRAM")
        print("="*70)

        success = test_qwen32b_bf16()

        if success:
            print("\nğŸ‰ Test passed!")
            print("   32B model is working and ready to use")
            print("   Consider updating notebook 2 if quality is better")
        else:
            print("\nâŒ Test failed")
            print("   Recommendation: Keep using 14B model")

    except KeyboardInterrupt:
        print("\n\nâŒ Test cancelled by user")
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()

# Vocabulary í¬ê¸° ë¶ˆì¼ì¹˜ ë¬¸ì œ í•´ê²°

## ğŸ› ë¬¸ì œ ì›ì¸

Special tokens ì¶”ê°€ í›„ vocabulary í¬ê¸°ê°€ ë³€ê²½ë˜ì—ˆì§€ë§Œ, ì¼ë¶€ ì½”ë“œì—ì„œ ì—¬ì „íˆ êµ¬ í¬ê¸°ë¥¼ ì‚¬ìš©:

```python
tokenizer.add_tokens(TECHNICAL_SPECIAL_TOKENS)  # 33ê°œ ì¶”ê°€
# Vocab: 32,000 â†’ 32,033

# ë¬¸ì œê°€ ë°œìƒí•œ ë¶€ë¶„
tokenizer.vocab_size  # âŒ ì—¬ì „íˆ 32,000 ë°˜í™˜
len(tokenizer)        # âœ… 32,033 ë°˜í™˜ (ì •í™•í•¨)
```

## âš ï¸ ë°œìƒí•œ ì—ëŸ¬

```python
RuntimeError: The size of tensor a (32033) must match the size of tensor b (32000)
```

- `doc_sparse`: 32,033 ì°¨ì› (ëª¨ë¸ ì¶œë ¥)
- `query_sparse`: 32,000 ì°¨ì› (IDF lookup) â† ë¬¸ì œ!

## âœ… í•´ê²° ë°©ë²•

**5ê°œ ìœ„ì¹˜ì—ì„œ ìˆ˜ì •** (4ê°œ ì…€):

1. **Cell 7**: Tokenizer info ì¶œë ¥
   ```python
   # Before
   print(f"Vocab size: {tokenizer.vocab_size:,}")
   
   # After
   print(f"Vocab size: {len(tokenizer):,}")
   ```

2. **Cell 40**: `compute_query_representation` í•¨ìˆ˜ â­ **ê°€ì¥ ì¤‘ìš”**
   ```python
   # Before
   vocab_size = tokenizer.vocab_size  # 32000
   
   # After
   vocab_size = len(tokenizer)  # 32033
   ```

3. **Cell 44**: ëª¨ë¸ ì €ì¥ config
   ```python
   # Before
   'vocab_size': tokenizer.vocab_size
   
   # After
   'vocab_size': len(tokenizer)
   ```

4. **Cell 46**: config.json ìƒì„± (2ê³³)
   ```python
   # Before
   "vocab_size": tokenizer.vocab_size
   "embedding_dimension": {tokenizer.vocab_size}
   
   # After
   "vocab_size": len(tokenizer)
   "embedding_dimension": {len(tokenizer)}
   ```

5. **Cell 48**: Inference í…ŒìŠ¤íŠ¸
   ```python
   # Before
   sparse_vec = np.zeros(tokenizer.vocab_size)
   
   # After
   sparse_vec = np.zeros(len(tokenizer))
   ```

## ğŸ“Š ìˆ˜ì • ê²°ê³¼

```
âœ“ ìˆ˜ì •ëœ ì½”ë“œ: 5ê°œ ìœ„ì¹˜ (4ê°œ ì…€)
âœ“ ì°¨ì› ì¼ì¹˜: doc_sparse (32033) = query_sparse (32033)
âœ“ í•™ìŠµ ê°€ëŠ¥ ìƒíƒœ
```

## ğŸ’¡ ì¤‘ìš” í¬ì¸íŠ¸

**í•­ìƒ `len(tokenizer)` ì‚¬ìš©:**
```python
# âŒ ì˜ëª»ëœ ë°©ë²•
vocab_size = tokenizer.vocab_size  # ì›ë³¸ BERT vocabë§Œ

# âœ… ì˜¬ë°”ë¥¸ ë°©ë²•
vocab_size = len(tokenizer)  # ì¶”ê°€ëœ special tokens í¬í•¨
```

**ì´ìœ :**
- `tokenizer.vocab_size`: ì½ê¸° ì „ìš© ì†ì„±, ì›ë³¸ ëª¨ë¸ vocabulary í¬ê¸°
- `len(tokenizer)`: í˜„ì¬ tokenizerì˜ ì‹¤ì œ í¬ê¸° (special tokens í¬í•¨)

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

í•™ìŠµ ì¬ì‹œì‘:
```python
# ëª¨ë“  ì°¨ì›ì´ 32,033ìœ¼ë¡œ ì¼ì¹˜ë¨
train_loss, ranking_loss, l0_loss, idf_penalty = train_epoch(...)
```

ë¬¸ì œ í•´ê²° ì™„ë£Œ! âœ…
